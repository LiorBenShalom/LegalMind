{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "# !pip install ipywidgets\n",
    "# !pip install Pillow\n",
    "# !pip install umap\n",
    "# !pip install gensim\n",
    "# !pip install \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load HeBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "model = AutoModel.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling over token embeddings (excluding special tokens)\n",
    "    attention = inputs['attention_mask'].unsqueeze(-1)\n",
    "    embedding = (outputs.last_hidden_state * attention).sum(1) / attention.sum(1)\n",
    "    return embedding.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "verdict_paragraphs = df[\"extracted_gpt_facts\"].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([get_embedding(text) for text in verdict_paragraphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 top words: ['×”× ××©×', '×©×œ', '×¡×', '×’×¨×', '×ž×¡×•×›×Ÿ', '×ž×¡×•×’', '×‘×ž×©×§×œ', '×¡×¢×™×£', '×œ×¤×™', '×§× ×‘×•×¡']\n",
      "Cluster 1 top words: ['×”× ××©×', '×©×œ', '×œ×¤×™', '×¡×', '×ž×¡×•×›×Ÿ', '×¡×¢×™×£', '×¢×œ', '×‘×ž×©×§×œ', '×‘×™×•×', '×’×¨×']\n",
      "Cluster 2 top words: ['×”× ××©×', '×©×œ', '××ª', '×¢×œ', '×”×ž×ª×œ×•× ×Ÿ', '×”×—×‘×™×œ×”', '×¢×', '×¡×', '×›×™', '×¡×¢×™×£']\n",
      "Cluster 3 top words: ['×”× ××©×', '×©×œ', '×¡×', '×ž×¡×•×’', '×¡×¢×™×£', '×œ×¤×™', '×¢×œ', '×ž×¡×•×›×Ÿ', '×’×¨×', '×‘×ž×©×§×œ']\n",
      "Cluster 4 top words: ['×”× ××©×', '×©×œ', '×”×¡×•×›×Ÿ', '× ××©×', '××ª', '×ž×¡×•×’', '×¡×', '×ž×¡×•×›×Ÿ', '×’×¨×', '×¢×œ']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(5):\n",
    "    cluster_texts = [text for text, label in zip(verdict_paragraphs, labels) if label == i]\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(cluster_texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    mean_scores = np.asarray(X.mean(axis=0)).flatten()\n",
    "    top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "    top_words = [terms[ind] for ind in top_indices]\n",
    "    print(f\"Cluster {i} top words:\", top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 top words: ['×”× ××©×', '×©×œ', '×¡×', '×’×¨×', '×ž×¡×•×›×Ÿ', '×ž×¡×•×’', '×‘×ž×©×§×œ', '×¡×¢×™×£', '×œ×¤×™', '×§× ×‘×•×¡']\n",
      "Cluster 1 top words: ['×”× ××©×', '×©×œ', '×œ×¤×™', '×¡×', '×ž×¡×•×›×Ÿ', '×¡×¢×™×£', '×¢×œ', '×‘×ž×©×§×œ', '×‘×™×•×', '×’×¨×']\n",
      "Cluster 2 top words: ['×”× ××©×', '×©×œ', '××ª', '×¢×œ', '×”×ž×ª×œ×•× ×Ÿ', '×”×—×‘×™×œ×”', '×¢×', '×¡×', '×›×™', '×¡×¢×™×£']\n",
      "Cluster 3 top words: ['×”× ××©×', '×©×œ', '×¡×', '×ž×¡×•×’', '×¡×¢×™×£', '×œ×¤×™', '×¢×œ', '×ž×¡×•×›×Ÿ', '×’×¨×', '×‘×ž×©×§×œ']\n",
      "Cluster 4 top words: ['×”× ××©×', '×©×œ', '×”×¡×•×›×Ÿ', '× ××©×', '××ª', '×ž×¡×•×’', '×¡×', '×ž×¡×•×›×Ÿ', '×’×¨×', '×¢×œ']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "k=5\n",
    "for i in range(k):\n",
    "    cluster_texts = [text for text, label in zip(verdict_paragraphs, labels) if label == i]\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(cluster_texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    mean_scores = np.asarray(X.mean(axis=0)).flatten()\n",
    "    top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "    top_words = [terms[ind] for ind in top_indices]\n",
    "    print(f\"Cluster {i} top words:\", top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 2 -words cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 unique words: ['×”× ××©×', '×‘×™×•×', '×”××™×©×•×', '×”×¡×•×›×Ÿ', '×¡×ž×™×', '×œ×•', '×œ××—×¨', '×›×ª×‘', '×”×•×¨×©×¢', '×œ× ××©×']\n",
      "Cluster 1 unique words: ['×”×ž×¤×’×©', '×›×“×™×Ÿ', '×˜×œ×¤×•× ×™', '×•×‘×ª×ž×•×¨×”', '×ª×™××•×', '×œ×¡×š', '×‘×‘×™×ª×•', '×©×•×—×—×•', '×œ×‘×™×ª×•', '×™×ž×›×•×¨']\n",
      "Cluster 2 unique words: ['×”×¡×ž×™×', '×¡×¢×™×£', '×œ×¤×§×•×“×ª', '×”×ž×¡×•×›× ×™×', '×©×œ×', '×¢×‘×™×¨×•×ª', '×¡×—×¨', '×¢×‘×™×¨×”', '×¢×¦×ž×™×ª', '×œ×¦×¨×™×›×”']\n",
      "Cluster 3 unique words: ['×‘×¡×', '×”×•×“××ª×•', '×‘×¢×•×‘×“×•×ª', '×œ×¨×›×•×©', '×§× ××‘×™×¡', '×˜×™×¢×•×Ÿ', '×¢×•×‘×“×•×ª', '×ž×ª×•×§×Ÿ', '×”×©× ×™×™×', '×”×¢×‘×™×¨']\n",
      "Cluster 4 unique words: ['×¡×', '×ž×¡×•×’', '×ž×¡×•×›×Ÿ', '×’×¨×', '×‘×ž×©×§×œ', '× ×˜×•', '×§×•×§××™×Ÿ', '×§× ×‘×•×¡', '×ž×›×¨', '×ª×ž×•×¨×ª']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "# Function to reconstruct full Hebrew words\n",
    "def get_full_words(tokens):\n",
    "    words = []\n",
    "    current = \"\"\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"##\"):\n",
    "            current += tok[2:]\n",
    "        else:\n",
    "            if current:\n",
    "                words.append(current)\n",
    "            current = tok\n",
    "    if current:\n",
    "        words.append(current)\n",
    "    return words\n",
    "\n",
    "# Hebrew stopwords (custom list)\n",
    "hebrew_stopwords = {\n",
    "    '×©×œ', '×¢×œ', '××ª', '×›×™', '×¢×', '×–×”', '×’×', '××', '××•', '×”×™×”', '×”×™×', '×”×•×', '×”×' ,\"×¤×™\", \"×œ×•\"\n",
    "    '××‘×œ', '×× ×™', '×× ×—× ×•', '××ª×', '××ª×Ÿ', '××™×Ÿ', '×›×œ', '×œ×', '×›×Ÿ', '×™×©', '×ž×”', '×ž×™', '×‘×•',\n",
    "    '×›×š', '×œ×¤×™', '×œ×œ×', '×•×›×Ÿ', '×¢×“', '×¨×§', '×›×ž×•', '×ž××•×“', '×–××ª', '×”×–×•', '××•×ª×•', '××•×ª×”'\n",
    "}\n",
    "\n",
    "def is_meaningful(word):\n",
    "    return bool(re.search(r'[×-×ª]', word)) and not word.isdigit() and len(word) >= 2\n",
    "\n",
    "# 1. Load and tokenize\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "verdict_paragraphs = df[\"extracted_gpt_facts\"].dropna().tolist()\n",
    "\n",
    "tokenized_sentences = []\n",
    "for p in verdict_paragraphs:\n",
    "    tokens = tokenizer.tokenize(p)\n",
    "    full_words = get_full_words(tokens)\n",
    "    filtered = [w for w in full_words if is_meaningful(w) and w not in hebrew_stopwords]\n",
    "    tokenized_sentences.append(filtered)\n",
    "\n",
    "# 2. Train Word2Vec\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=5)\n",
    "\n",
    "# 3. Choose words to cluster\n",
    "words = list(w2v_model.wv.index_to_key)[:500]  # top N frequent words\n",
    "word_vectors = [w2v_model.wv[word] for word in words]\n",
    "\n",
    "# 4. Cluster\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')  # n_init='auto' for sklearn >= 1.2\n",
    "labels = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "# 5. Group words by cluster\n",
    "clusters = defaultdict(list)\n",
    "for word, label in zip(words, labels):\n",
    "    clusters[label].append(word)\n",
    "\n",
    "# 6. Print top words per cluster\n",
    "# for cluster_id in sorted(clusters.keys()):\n",
    "#     print(f\"Cluster {cluster_id} top words: {clusters[cluster_id][:10]}\")\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten all cluster words to count frequency\n",
    "all_words = [word for word_list in clusters.values() for word in word_list]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Keep only words that appear in one cluster\n",
    "unique_clusters = {\n",
    "    cid: [w for w in word_list if word_counts[w] == 1][:10]  # top 10 unique words\n",
    "    for cid, word_list in clusters.items()\n",
    "}\n",
    "\n",
    "# Print unique words per cluster\n",
    "for cluster_id in sorted(unique_clusters.keys()):\n",
    "    print(f\"Cluster {cluster_id} unique words: {unique_clusters[cluster_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 3 - embedding api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 30: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 9287 tokens (9287 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 180/180 [01:26<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 5:\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×‘×ª×™×§ ×–×”, ×‘×ž×¡×’×¨×ª ×”×›×¨×¢×ª ×“×™×Ÿ × ×¤×¨×“×ª ×©× ×™×ª× ×” ×‘×ª××¨×™×š 10.9.23 ×‘×‘×™×¦×•×¢ ×”×¢×‘×™×¨×•×ª ×”×‘××•×ª: ×‘×ž×¡×’×¨×ª ×”××™×©×•× ×”×©×œ×™×©×™ ×©×¢× ×™×™× ×• × ×™×¡×™×•×Ÿ ×”×ª× ×§×©×•×ª ×‘×–××‘ ×¨×•×–× ×©×˜×™×™×Ÿ ×‘×ª××¨×™×š 30.6.03, ×›×¤×™ ×©×¢×•×“ ×™×¤×•×¨×˜ â€“ ×‘×‘×™×¦×•×¢ ×¢×‘×™×¨×” ×©×œ ×”×¡×¤×§ ...\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×•×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×”×ž×ª×•×§×Ÿ ×©×¢× ×™×™× ×Ÿ ×¢×‘×™×¨×•×ª ×©×œ × ×™×¡×™×•×Ÿ ×œ×™×™×‘×•× ×¡× ×ž×¡×•×›×Ÿ â€“ ×¢×‘×™×¨×” ×œ×¤×™ ×¡×¢×™×£ 13 ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× [× ×•×¡×— ×—×“×©] ×”×ª×©×œ\"×’-1973 ×•×”×—×–×§×ª ×¡× ×ž×¡×•×›×Ÿ ×©×œ× ×œ×¦×¨×™ ...\n",
      "\n",
      "Cluster 2:\n",
      "- × ××©× 1 (×œ×”×œ×Ÿ: \"×”× ××©×\") ×”×•×¨×©×¢ ×¢×œ ×™×¡×•×“ ×”×•×“××ª×• ×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ, ×œ××—×¨ × ×™×”×•×œ ×—×œ×§×™ ×©×œ ×”×•×›×—×•×ª, ×‘××¨×‘×¢ ×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡× ×ž×¡×•×›×Ÿ ×œ×¤×™ ×¡×¢×™×£ 13 ×‘×¦×™×¨×•×£ ×¡×¢×™×£ 19× ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×ª×©×œ\"×’-1973. ×¢×™×§×¨ ×¢ ...\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×•×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×”×ž×ª×•×§×Ÿ ×•×‘×ª×™×§ ×”×ž×¦×•×¨×£ ×‘×›×ª×‘ ×”××™×©×•× ×”×ž×ª×•×§×Ÿ ××£ ×”×•×. ×‘×ž×•×¢×“×™× ×”×¨×œ×•×•× ×˜×™×™× ×œ××™×¨×•×¢×™× ×©×™×¤×•×¨×˜×• ×œ×”×œ×Ÿ, ×©×™×ž×© × .×¤. 163-21 ×›×¡×•×›×Ÿ ×¡×ž×•×™ ×©×œ ×ž×©×˜×¨×ª ×™×©×¨××œ, ×›×©×ž×˜ ...\n",
      "\n",
      "Cluster 4:\n",
      "- ×‘×™×•× 9.1.24 ×”×•×¨×©×¢ ×”× ××©× ×œ××—×¨ ×©×ž×™×¢×ª ×¨××™×•×ª, ×‘×”×›×¨×¢×ª ×“×™×Ÿ ×ž×¤×•×¨×˜×ª, ×‘×¢×‘×™×¨×•×ª ×©×œ ×™×™×‘×•× ×¡× ×ž×¡×•×›×Ÿ, ×œ×¤×™ ×¡×¢×™×£ 13 ×‘×¦×™×¨×•×£ ×¡×¢×™×£ 19× ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×ª×©×œ\"×’-1973, ×”×—×–×§×ª ×¡× ×ž×¡×•×›×Ÿ ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦×ž×™×ª ×œ×¤×™ ×¡×¢ ...\n",
      "- ×”× ××©× 1 (×œ×”×œ×Ÿ â€“ \"×”× ××©×\"), ×”×•×¨×©×¢ ×¢×œ ×™×¡×•×“ ×”×•×“××ª×• ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× ×ž×ª×•×§×Ÿ, ×‘×¢×‘×™×¨×•×ª ×©×œ×”×œ×Ÿ: ×™×‘×•× ×¡×ž×™× ×ž×¡×•×›× ×™× â€“ ×¢×‘×™×¨×” ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×•-19× ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×ª×©×œ\"×’-1973; ××¡×¤×§×ª ×¡× â€“ ×¢×‘×™×¨×” ×œ×¤×™  ...\n",
      "\n",
      "Cluster 6:\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×œ××—×¨ × ×™×”×•×œ ×”×•×›×—×•×ª, ×‘×¢×‘×™×¨×•×ª ×”×‘××•×ª: ×™×™×¦×•×¨, ×”×›× ×” ×•×”×¤×§×ª ×¡×ž×™× ×ž×¡×•×›× ×™×, ×œ×¤×™ ×¡×¢×™×£ 6 ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×”×ª×©×œ\"×’-1973; ×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦×ž×™×ª, ×œ×¤×™ ×¡×¢×™×£ 7(×) ×•-(×’) ×¨×™×©× ×œ×¤×§×•×“×”; ×”×—×–×§ ...\n",
      "- ×”× ××©× 2 ×ž×—×ž×•×“ ××“×›×™×“×§ ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ, ×‘×›×ª×‘ ××™×©×•× ×ž×ª×•×§×Ÿ, ×‘×¢×‘×™×¨×•×ª ×©×œ ×¡×—×™×˜×” ×‘××™×•×ž×™×, ×œ×¤×™ ×¡×¢×™×£ 428 ×¨×™×©× ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×–-1977, ×•×¡×—×™×˜×” ×‘×›×•×— ×œ×¤×™ ×¡×¢×™×£ 427 ×¨×™×©× ×œ×—×•×§. ×‘× ×•×¡×£, ×¦×™×¨×£ ×”× ××© ...\n",
      "\n",
      "Cluster 3:\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×“×™×•× ×™ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× ×ž×ª×•×§×Ÿ ×‘×¢×‘×™×¨×•×ª ×™×™×¦×•×¨, ×”×›× ×” ×•×”×¤×§×ª ×¡× ×ž×¡×•×›×Ÿ ×œ×¤×™ ×¡×¢×™×£ 6 ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™×, ×”×ª×©×œ\"×’-1973, ×•× ×˜×™×œ×ª ×—×©×ž×œ, ×œ×¤×™ ×¡×¢×™×£ 400 ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×–-1977. ×›×ž×¤×•×¨×˜ ×‘×¢×•×‘×“ ...\n",
      "- × ××©× 3 ×”×•×“×” ×‘×›×ª×‘ ××™×©×•× ×ž×ª×•×§×Ÿ ×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×—×œ×§×™ ×©×œ× ×›×œ×œ ×”×¡×“×¨ ××•×“×•×ª ×”×¢×•× ×©, ×‘×¢×‘×™×¨×•×ª ×©×œ ×¡×™×•×¢ ×œ×™×¦×•×¨, ×”×›× ×” ×•×”×¤×§×” ×œ×¤×™ ×¡×¢×™×£ 6 ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×ª×©×œ\"×’-1973 ×•×¡×™×•×¢ ×œ× ×˜×™×œ×ª ×—×©×ž×œ ×œ×¤×™ ×¡×¢×™×£ 400 + 31 ×œ×— ...\n",
      "\n",
      "Cluster 0:\n",
      "- ×”× ××©× ×”×•×“×” ×‘×›×ª×‘ ×”××™×©×•× ×”×ž×ª×•×§×Ÿ ×‘×‘×™×¦×•×¢ ×ª×©×¢ ×¢×‘×™×¨×•×ª ×¡×—×¨ ×‘×¡×ž×™×, ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×•-19× ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™×; ×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦×ž×™×ª ×œ×¤×™ ×¡×¢×™×¤×™× 7(×)(×’) ×¨×™×©× ×œ×¤×§×•×“×”, ×•× ×”×™×’×” ×‘×©×›×¨×•×ª, ×œ×¤×™ ×¡×¢×™×£ 39× ×œ×¤×§×•×“×ª ×”×ª×¢×‘×• ...\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×‘×™×¨×•×ª ×”×‘××•×ª: 2 ×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡×ž×™× ×‘×¦×•×•×ª×, ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×•-19× ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™× ×‘×¦×™×¨×•×£ ×¡×¢×™×£ 29(×) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×”×ª×©×œ\"×–-1977; ×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦×ž×™×ª, ×œ×¤×™ ×¡×¢×™×¤×™× 7(×)( ...\n",
      "\n",
      "Cluster 1:\n",
      "- ×‘×ž×¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ ×”×•×¨×©×¢ ×”× ××©× ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×©× ×™ ×›×ª×‘×™ ××™×©×•× ×ž×ª×•×§× ×™× ×•×”×•×¤× ×” ×œ×©×™×¨×•×ª ×”×ž×‘×—×Ÿ ×œ×§×‘×œ×ª ×ª×¡×§×™×¨. ×œ× ×’×•×‘×©×” ×”×¡×›×ž×” ×œ×¢× ×™×™×Ÿ ×”×¢×•× ×©. ×ª\"×¤ 59578-12-21: ×‘×ª××¨×™×š 23.10.19 ×¨×›×© ×”× ××©× ×§×˜× ×•×¢ ×©× ×’× ×‘ ×¢×œ ×™×“×™ ××—×¨, ×‘×ª×ž×• ...\n",
      "- ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘×¢×‘×™×¨×•×ª ×”×ž×™×•×—×¡×•×ª ×œ×• ×‘×›×ª×‘ ××™×©×•× ×ž×ª×•×§×Ÿ ×‘×¢×‘×™×¨×•×ª ×©×œ ×”×¡×¤×§×ª ×¡× ×ž×¡×•×›×Ÿ (×¨×™×‘×•×™ ×¢×‘×™×¨×•×ª) â€“ ×œ×¤×™ ×¡×¢×™×¤×™× 13 + 19× ×œ×¤×§×•×“×ª ×”×¡×ž×™× ×”×ž×¡×•×›× ×™×, (× ×•×¡×— ×—×“×©) ×ª×©×œ\"×’ â€“ 1973, × ×”×™×’×” ×‘×–×ž×Ÿ ×¤×¡×™×œ×” â€“ ×œ×¤×™ ×¡×¢×™ ...\n",
      "\n",
      "ðŸ”¹ Cluster 5 top TF-IDF words: ['××ª', '×’×¨×', '×”× ××©×', '×›×™', '×œ×¤×™', '×¡×', '×¡×¢×™×£', '×¢×œ', '×¢×', '×©×œ']\n",
      "\n",
      "ðŸ”¹ Cluster 2 top TF-IDF words: ['××ª', '×‘×ž×©×§×œ', '×’×¨×', '×”× ××©×', '×”×¡×•×›×Ÿ', '×›×™', '×ž×¡×•×’', '×ž×¡×•×›×Ÿ', '×¡×', '×©×œ']\n",
      "\n",
      "ðŸ”¹ Cluster 4 top TF-IDF words: ['××ª', '×‘×™×•×', '×”× ××©×', '×”×¡×ž×™×', '×ž×¡×•×’', '×ž×¡×•×›×Ÿ', '×¡×', '×¢×œ', '×¢×', '×©×œ']\n",
      "\n",
      "ðŸ”¹ Cluster 6 top TF-IDF words: ['×‘×ž×©×§×œ', '×’×¨×', '×”× ××©×', '×”×¡×ž×™×', '×œ×¤×™', '×ž×¡×•×’', '×ž×¡×•×›×Ÿ', '×¡×', '×¢×œ', '×©×œ']\n",
      "\n",
      "ðŸ”¹ Cluster 3 top TF-IDF words: ['××ª', '×”× ××©×', '×œ×¤×™', '×ž×¡×•×’', '×ž×¡×•×›×Ÿ', '×¡×', '×¡×¢×™×£', '×¢×œ', '×§× ×‘×•×¡', '×©×œ']\n",
      "\n",
      "ðŸ”¹ Cluster 0 top TF-IDF words: ['×‘×™×•×', '×’×¨×', '×”× ××©×', '×›×™', '×ž×”××™×©×•×', '×ž×›×¨', '×¢×•×œ×”', '×§× ×‘×™×¡', '×©×œ', '×ª×ž×•×¨×ª']\n",
      "\n",
      "ðŸ”¹ Cluster 1 top TF-IDF words: ['2019', '×‘×™×•×', '×‘×ž×©×§×œ', '×’×¨×', '×”× ××©×', '×ž×¡×•×’', '×ž×¡×•×›×Ÿ', '×¡×', '×©×œ', '×ª×ž×•×¨×ª']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "key = \"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\"  # replace with your key\n",
    "client = OpenAI(api_key=key)\n",
    "# Load data\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "verdicts = df[\"extracted_gpt_facts\"].dropna().tolist()\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Get embeddings (can be slow â€” use tqdm for progress bar)\n",
    "from tqdm import tqdm\n",
    "embeddings = []\n",
    "\n",
    "for i, v in enumerate(tqdm(verdicts)):\n",
    "    try:\n",
    "        emb = get_embedding(v)\n",
    "        if emb is None:\n",
    "            print(f\"Warning: embedding failed at index {i}\")\n",
    "            emb = [0] * 1536\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        emb = [0] * 1536\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Remove None entries\n",
    "valid_pairs = [(v, e) for v, e in zip(verdicts, embeddings) if e is not None]\n",
    "verdicts_clean, embeddings_clean = zip(*valid_pairs)\n",
    "\n",
    "# Cluster\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings_clean)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster_to_verdicts = defaultdict(list)\n",
    "for label, verdict in zip(labels, verdicts_clean):\n",
    "    cluster_to_verdicts[label].append(verdict)\n",
    "\n",
    "# Print top 2 examples per cluster\n",
    "for cluster_id, v_list in cluster_to_verdicts.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for example in v_list[:2]:\n",
    "        print(\"-\", example[:200], \"...\")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ×ª×•×¦××”: ×ž×™×œ×™× ×‘×•×œ×˜×•×ª ×œ×›×œ ××©×›×•×œ\n",
    "for cluster_id, texts in cluster_to_verdicts.items():\n",
    "    vectorizer = TfidfVectorizer(max_features=10, stop_words='english')  # ××™×Ÿ ×¡×˜×•×¤ ×•×•×¨×“×¡ ×œ×¢×‘×¨×™×ª, ××– ×ª×•×›×œ ×œ×”×¡×™×¨ ×™×“× ×™×ª\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    top_words = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nðŸ”¹ Cluster {cluster_id} top TF-IDF words: {list(top_words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdicts over 8192 tokens: 3/354\n",
      "Verdict names with too many tokens:\n",
      "- SH-20-02-48152-11\n",
      "- SH-22-11-64232-785\n",
      "- SH-22-11-64232-785\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# Count how many verdicts exceed 8192 tokens\n",
    "too_long_count = 0\n",
    "token_counts = []\n",
    "\n",
    "for text in verdicts:\n",
    "    tokens = encoding.encode(text)\n",
    "    token_counts.append(len(tokens))\n",
    "    if len(tokens) > 8192:\n",
    "        too_long_count += 1\n",
    "\n",
    "print(f\"Verdicts over 8192 tokens: {too_long_count}/{len(verdicts)}\")\n",
    "long_indices = []\n",
    "long_verdicts = []\n",
    "\n",
    "for i, text in enumerate(verdicts):\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > 8192:\n",
    "        long_indices.append(i)\n",
    "        long_verdicts.append(text)\n",
    "\n",
    "# Assuming df and verdicts are aligned (i.e., df[\"extracted_gpt_facts\"].dropna() == verdicts)\n",
    "long_verdict_names = df[\"verdict\"].dropna().iloc[long_indices].tolist()\n",
    "\n",
    "print(\"Verdict names with too many tokens:\")\n",
    "for name in long_verdict_names:\n",
    "    print(\"-\", name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
