{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration, \n",
    "    MT5Tokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "def split_to_windows(text: str, tokenizer, max_length: int = 256, stride: int = 128) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping windows based on token count\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # If text is shorter than max_length, return as is\n",
    "    if len(tokens) <= max_length:\n",
    "        return [text]\n",
    "    \n",
    "    windows = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_length, len(tokens))\n",
    "        \n",
    "        # Extract window tokens\n",
    "        window_tokens = tokens[start:end]\n",
    "        \n",
    "        # Convert back to text\n",
    "        window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "        windows.append(window_text)\n",
    "        \n",
    "        # Move start position by stride\n",
    "        start += stride\n",
    "        \n",
    "        # If we've covered all tokens, break\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return windows\n",
    "\n",
    "class MLMDataCollator:\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mlm_probability = mlm_probability\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __call__(self, examples):\n",
    "        # Prepare inputs for span corruption (T5/mT5 style)\n",
    "        input_texts = []\n",
    "        target_texts = []\n",
    "        \n",
    "        for example in examples:\n",
    "            text = example['text']  \n",
    "            \n",
    "            # Create span corruption\n",
    "            input_text, target_text = self.create_span_corruption(text)\n",
    "            \n",
    "            input_texts.append(input_text)\n",
    "            target_texts.append(target_text)\n",
    "        \n",
    "        # Tokenize inputs and targets\n",
    "        model_inputs = self.tokenizer(\n",
    "            input_texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = self.tokenizer(\n",
    "            target_texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Replace padding token id with -100 to ignore in loss calculation\n",
    "        labels[\"input_ids\"][labels[\"input_ids\"] == self.tokenizer.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    def create_span_corruption(self, text: str) -> Tuple[str, str]:\n",
    "        \"\"\"Create span corruption similar to T5 pre-training\"\"\"\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        if len(tokens) == 0:\n",
    "            return text, \"\"\n",
    "        \n",
    "        # Calculate number of spans to mask (roughly 15% of tokens in spans)\n",
    "        total_mask_length = max(1, int(len(tokens) * self.mlm_probability))\n",
    "        \n",
    "        # Create spans to mask\n",
    "        masked_tokens = tokens.copy()\n",
    "        target_spans = []\n",
    "        \n",
    "        current_span_id = 0\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(tokens) and current_span_id * 3 < total_mask_length:  # Average span length ~3\n",
    "            if random.random() < 0.15:  # Probability of starting a span\n",
    "                # Determine span length (1-5 tokens)\n",
    "                span_length = min(random.randint(1, 5), len(tokens) - i)\n",
    "                \n",
    "                # Extract original tokens for target\n",
    "                original_span = tokens[i:i + span_length]\n",
    "                original_text = self.tokenizer.convert_tokens_to_string(original_span)\n",
    "                \n",
    "                # Create target span\n",
    "                target_span = f\"<extra_id_{current_span_id}> {original_text}\"\n",
    "                target_spans.append(target_span)\n",
    "                \n",
    "                # Replace with sentinel token in input\n",
    "                masked_tokens[i:i + span_length] = [f\"<extra_id_{current_span_id}>\"]\n",
    "                \n",
    "                current_span_id += 1\n",
    "                i += span_length\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Add final sentinel to target\n",
    "        if target_spans:\n",
    "            target_spans.append(f\"<extra_id_{current_span_id}>\")\n",
    "        \n",
    "        # Convert back to text\n",
    "        input_text = self.tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "        target_text = \" \".join(target_spans) if target_spans else \"\"\n",
    "        \n",
    "        return input_text, target_text\n",
    "\n",
    "def prepare_dataset(df: pd.DataFrame, tokenizer, text_column: str = 'extracted_gpt_facts') -> Dataset:\n",
    "    \"\"\"\n",
    "    Prepare dataset from DataFrame with windowing\n",
    "    \"\"\"\n",
    "    all_windows = []\n",
    "    \n",
    "    print(f\"Processing {len(df)} texts...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        text = str(row[text_column])\n",
    "        \n",
    "        # Skip empty texts\n",
    "        if not text or text.strip() == '':\n",
    "            continue\n",
    "            \n",
    "        # Split text into windows\n",
    "        windows = split_to_windows(text, tokenizer, max_length=256, stride=128)\n",
    "        all_windows.extend(windows)\n",
    "        \n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"Processed {idx + 1} texts, generated {len(all_windows)} windows\")\n",
    "    \n",
    "    print(f\"Total windows generated: {len(all_windows)}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict({\"text\": all_windows})\n",
    "    return dataset\n",
    "\n",
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Load your data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\")\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"google/mt5-base\"  \n",
    "    print(f\"Loading model and tokenizer: {model_name}\")\n",
    "    \n",
    "    # tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "    # model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"imvladikon/het5-base\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"imvladikon/het5-base\")  \n",
    "    # Add extra sentinel tokens for span corruption\n",
    "    sentinel_tokens = [f\"<extra_id_{i}>\" for i in range(100)]\n",
    "    num_added = tokenizer.add_tokens(sentinel_tokens)\n",
    "    print(f\"Added {num_added} sentinel tokens\")\n",
    "    \n",
    "    # Resize model embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Prepare dataset with windowing\n",
    "    dataset = prepare_dataset(df, tokenizer, text_column='extracted_gpt_facts')\n",
    "    \n",
    "    # Split dataset (80% train, 20% validation)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    eval_size = len(dataset) - train_size\n",
    "    \n",
    "    train_dataset = dataset.select(range(train_size))\n",
    "    eval_dataset = dataset.select(range(train_size, train_size + eval_size))\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = MLMDataCollator(tokenizer, mlm_probability=0.15, max_length=256)\n",
    "    \n",
    " # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir = \"./mt5-mlm-trained\",\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    fp16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    disable_tqdm=False,\n",
    "    log_level='info',\n",
    "    save_strategy=\"no\")\n",
    "    \n",
    "    \n",
    "    # Initialize trainer\n",
    "    print(\"Initializing trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the final model\n",
    "    final_output_dir = \"./m5-mlm-final\"\n",
    "    print(f\"Saving final model to {final_output_dir}\")\n",
    "    trainer.save_model(final_output_dir)\n",
    "    tokenizer.save_pretrained(final_output_dir)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    # Optional: Test the trained model\n",
    "    print(\"Testing trained model...\")\n",
    "    test_text = \"This is a sample text to test the trained model.\"\n",
    "    \n",
    "    # # Load the saved model for testing\n",
    "    # # trained_model = MT5ForConditionalGeneration.from_pretrained(final_output_dir)\n",
    "    # # trained_tokenizer = MT5Tokenizer.from_pretrained(final_output_dir)\n",
    "    # trained_tokenizer = AutoTokenizer.from_pretrained(\"imvladikon/het5-base\")\n",
    "    # trained_model = AutoModelForSeq2SeqLM.from_pretrained(\"imvladikon/het5-base\")  \n",
    "    # # Create a corrupted version for testing\n",
    "    # collator = MLMDataCollator(trained_tokenizer)\n",
    "    # input_text, target_text = collator.create_span_corruption(test_text)\n",
    "    \n",
    "    # print(f\"Original: {test_text}\")\n",
    "    # print(f\"Corrupted: {input_text}\")\n",
    "    # print(f\"Target: {target_text}\")\n",
    "    \n",
    "    # # Generate prediction\n",
    "    # inputs = trained_tokenizer(input_text, return_tensors=\"pt\", max_length=256, truncation=True)\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     outputs = trained_model.generate(\n",
    "    #         inputs.input_ids,\n",
    "    #         max_length=256,\n",
    "    #         num_beams=4,\n",
    "    #         early_stopping=True,\n",
    "    #         pad_token_id=trained_tokenizer.pad_token_id\n",
    "    #     )\n",
    "    \n",
    "    # prediction = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # print(f\"Prediction: {prediction}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HET5 Perplexity Comparison (same masking, same data) ===\n",
      "Base mT5    | loss: 4.4979 | ppl: 89.83 | tokens: 597303\n",
      "Fine-tuned   | loss: 9.0837   | ppl: 8810.11   | tokens: 597303\n",
      "Δloss: -4.5858 | Δppl: -8720.28\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "\n",
    "# ---------- config ----------\n",
    "CSV_PATH         = \"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\"\n",
    "BASE_MODEL_DIR   = \"imvladikon/het5-base\"               # base HET5\n",
    "FT_MODEL_DIR     = \"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\"  # your fine-tuned HET5\n",
    "TEXT_COL         = \"extracted_gpt_facts\"\n",
    "MAX_LEN          = 256\n",
    "STRIDE           = 128\n",
    "MLM_PROB         = 0.15\n",
    "BATCH_SIZE       = 4\n",
    "SEED             = 42\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def set_seeds(seed: int = 42):\n",
    "    random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "def split_to_windows(text: str, tokenizer, max_length=256, stride=128) -> List[str]:\n",
    "    toks = tokenizer.tokenize(text)\n",
    "    if len(toks) <= max_length:\n",
    "        return [text]\n",
    "    out, start = [], 0\n",
    "    while start < len(toks):\n",
    "        end = min(start + max_length, len(toks))\n",
    "        out.append(tokenizer.convert_tokens_to_string(toks[start:end]))\n",
    "        start += stride\n",
    "        if end >= len(toks): break\n",
    "    return out\n",
    "\n",
    "def load_windows(csv_path: str, tokenizer, text_col: str) -> Dataset:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    wins = []\n",
    "    for _, r in df.iterrows():\n",
    "        txt = str(r.get(text_col, \"\") or \"\").strip()\n",
    "        if not txt: continue\n",
    "        wins.extend(split_to_windows(txt, tokenizer, MAX_LEN, STRIDE))\n",
    "    return Dataset.from_dict({\"text\": wins})\n",
    "\n",
    "class MLMDataCollator:\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15, max_length=256):\n",
    "        self.tok = tokenizer\n",
    "        self.p = mlm_probability\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        inputs, targets = [], []\n",
    "        for ex in examples:\n",
    "            inp, tgt = self._span_corrupt(ex[\"text\"])\n",
    "            inputs.append(inp); targets.append(tgt)\n",
    "        model_inputs = self.tok(inputs, truncation=True, padding=True,\n",
    "                                max_length=self.max_len, return_tensors=\"pt\")\n",
    "        labels = self.tok(targets, truncation=True, padding=True,\n",
    "                          max_length=self.max_len, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    def _span_corrupt(self, text: str):\n",
    "        toks = self.tok.tokenize(text)\n",
    "        if not toks: return text, \"\"\n",
    "        total_mask = max(1, int(len(toks) * self.p))\n",
    "        masked = toks.copy(); targets = []; sid, i = 0, 0\n",
    "        while i < len(toks) and sid * 3 < total_mask:\n",
    "            if random.random() < 0.15:\n",
    "                span_len = min(random.randint(1, 5), len(toks) - i)\n",
    "                orig = self.tok.convert_tokens_to_string(toks[i:i+span_len])\n",
    "                targets.append(f\"<extra_id_{sid}> {orig}\")\n",
    "                masked[i:i+span_len] = [f\"<extra_id_{sid}>\"]\n",
    "                sid += 1; i += span_len\n",
    "            else:\n",
    "                i += 1\n",
    "        if targets:\n",
    "            targets.append(f\"<extra_id_{sid}>\")\n",
    "        return (self.tok.convert_tokens_to_string(masked),\n",
    "                \" \".join(targets) if targets else \"\")\n",
    "\n",
    "def build_frozen_batches(tokenizer, csv_path: str, batch_size=4):\n",
    "    \"\"\"\n",
    "    Create one fixed set of masked batches ONCE using the base tokenizer.\n",
    "    Reuse these exact tensors for all models for a fair comparison.\n",
    "    \"\"\"\n",
    "    set_seeds(SEED)\n",
    "    ds = load_windows(csv_path, tokenizer, TEXT_COL)\n",
    "    collator = MLMDataCollator(tokenizer, mlm_probability=MLM_PROB, max_length=MAX_LEN)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "    frozen = []\n",
    "    for batch in loader:\n",
    "        # keep on CPU; we’ll move to device during eval\n",
    "        frozen.append({k: v.clone() for k, v in batch.items()})\n",
    "    return frozen\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_perplexity(model_dir: str, tokenizer, frozen_batches) -> float:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n",
    "\n",
    "    # ensure sentinel tokens exist (het5 usually has them already)\n",
    "    if \"<extra_id_0>\" not in tokenizer.get_vocab():\n",
    "        tokenizer.add_tokens([f\"<extra_id_{i}>\" for i in range(100)])\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for batch in frozen_batches:\n",
    "        batch_dev = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch_dev)\n",
    "        valid = (batch_dev[\"labels\"] != -100).sum().item()\n",
    "        total_loss += out.loss.item() * valid\n",
    "        total_tokens += valid\n",
    "\n",
    "    mean_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(mean_loss)\n",
    "    return ppl, mean_loss, total_tokens\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the BASE tokenizer to freeze masks for both models\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_DIR, use_fast=False)\n",
    "\n",
    "    # Build frozen masked batches once\n",
    "    frozen_batches = build_frozen_batches(base_tokenizer, CSV_PATH, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Evaluate base and fine-tuned on the SAME masked inputs\n",
    "    base_ppl, base_loss, base_tokens = eval_perplexity(BASE_MODEL_DIR, base_tokenizer, frozen_batches)\n",
    "    ft_ppl,   ft_loss,   ft_tokens   = eval_perplexity(FT_MODEL_DIR,   base_tokenizer, frozen_batches)\n",
    "\n",
    "    print(\"=== HET5 Perplexity Comparison (same masking, same data) ===\")\n",
    "    print(f\"Base mT5    | loss: {base_loss:.4f} | ppl: {base_ppl:.2f} | tokens: {base_tokens}\")\n",
    "    print(f\"Fine-tuned   | loss: {ft_loss:.4f}   | ppl: {ft_ppl:.2f}   | tokens: {ft_tokens}\")\n",
    "    print(f\"Δloss: {base_loss - ft_loss:.4f} | Δppl: {base_ppl - ft_ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10975/10975 [00:00<00:00, 22620.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked eval dataset saved to masked_eval with 10975 samples.\n",
      "imvladikon/het5-base | loss: 4.4979 | ppl: 89.83 | tokens: 597303\n",
      "/home/liorkob/M.Sc/thesis/t5/het5-mlm-final | loss: 9.0837 | ppl: 8810.11 | tokens: 597303\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "def split_to_windows(text, tokenizer, max_length=256, stride=128):\n",
    "    toks = tokenizer.tokenize(text)\n",
    "    if len(toks) <= max_length: return [text]\n",
    "    out, start = [], 0\n",
    "    while start < len(toks):\n",
    "        end = min(start + max_length, len(toks))\n",
    "        out.append(tokenizer.convert_tokens_to_string(toks[start:end]))\n",
    "        start += stride\n",
    "        if end >= len(toks): break\n",
    "    return out\n",
    "\n",
    "class MLMDataCollator:\n",
    "    def __init__(self, tokenizer, mlm_probability=0.15, max_length=256):\n",
    "        self.tok = tokenizer\n",
    "        self.p = mlm_probability\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        inputs, targets = [], []\n",
    "        for ex in examples:\n",
    "            inp, tgt = self._span_corrupt(ex[\"text\"])\n",
    "            inputs.append(inp); targets.append(tgt)\n",
    "        model_inputs = self.tok(\n",
    "            inputs, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = self.tok(\n",
    "            targets, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        labels[labels == self.tok.pad_token_id] = -100\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "    def _span_corrupt(self, text):\n",
    "        toks = self.tok.tokenize(text)\n",
    "        if not toks: return text, \"\"\n",
    "        total_mask = max(1, int(len(toks) * self.p))\n",
    "        masked = toks.copy(); targets = []; sid, i = 0, 0\n",
    "        while i < len(toks) and sid * 3 < total_mask:\n",
    "            if random.random() < 0.15:\n",
    "                span_len = min(random.randint(1,5), len(toks)-i)\n",
    "                orig = self.tok.convert_tokens_to_string(toks[i:i+span_len])\n",
    "                targets.append(f\"<extra_id_{sid}> {orig}\")\n",
    "                masked[i:i+span_len] = [f\"<extra_id_{sid}>\"]\n",
    "                sid += 1; i += span_len\n",
    "            else:\n",
    "                i += 1\n",
    "        if targets: targets.append(f\"<extra_id_{sid}>\")\n",
    "        return self.tok.convert_tokens_to_string(masked), (\" \".join(targets) if targets else \"\")\n",
    "\n",
    "def prepare_dataset(csv_path, tokenizer, text_col=\"extracted_gpt_facts\", max_length=256, stride=128):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    windows = []\n",
    "    for _, r in df.iterrows():\n",
    "        txt = str(r.get(text_col, \"\") or \"\").strip()\n",
    "        if not txt: continue\n",
    "        windows.extend(split_to_windows(txt, tokenizer, max_length, stride))\n",
    "    return Dataset.from_dict({\"text\": windows})\n",
    "\n",
    "# ------------- create masked eval set (one-time) -------------\n",
    "def create_masked_eval(csv_path, tokenizer, out_dir=\"masked_eval\", seed=42, batch_size=32):\n",
    "    random.seed(seed)\n",
    "    dataset = prepare_dataset(csv_path, tokenizer)\n",
    "    collator = MLMDataCollator(tokenizer, mlm_probability=0.15, max_length=256)\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collator)\n",
    "    all_input_ids, all_attn, all_labels = [], [], []\n",
    "    for batch in loader:\n",
    "        all_input_ids.append(batch[\"input_ids\"])\n",
    "        all_attn.append(batch[\"attention_mask\"])\n",
    "        all_labels.append(batch[\"labels\"])\n",
    "\n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    all_attn     = torch.cat(all_attn,     dim=0)\n",
    "    all_labels   = torch.cat(all_labels,   dim=0)\n",
    "\n",
    "    ds_masked = Dataset.from_dict({\n",
    "        \"input_ids\": all_input_ids.tolist(),\n",
    "        \"attention_mask\": all_attn.tolist(),\n",
    "        \"labels\": all_labels.tolist()\n",
    "    })\n",
    "    ds_masked.save_to_disk(out_dir)\n",
    "    print(f\"Masked eval dataset saved to {out_dir} with {len(ds_masked)} samples.\")\n",
    "\n",
    "# ------------- perplexity evaluation -------------\n",
    "@torch.no_grad()\n",
    "def compute_perplexity_from_masked(model_dir, masked_dir, batch_size=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n",
    "\n",
    "    if \"<extra_id_0>\" not in tok.get_vocab():\n",
    "        tok.add_tokens([f\"<extra_id_{i}>\" for i in range(100)])\n",
    "        model.resize_token_embeddings(len(tok))\n",
    "\n",
    "    ds = load_from_disk(masked_dir)\n",
    "\n",
    "    def collate(rows):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor([r[\"input_ids\"] for r in rows], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor([r[\"attention_mask\"] for r in rows], dtype=torch.long),\n",
    "            \"labels\": torch.tensor([r[\"labels\"] for r in rows], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model(**batch)\n",
    "        valid = (batch[\"labels\"] != -100).sum().item()\n",
    "        total_loss += out.loss.item() * valid\n",
    "        total_tokens += valid\n",
    "\n",
    "    mean_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(mean_loss)\n",
    "    print(f\"{model_dir} | loss: {mean_loss:.4f} | ppl: {ppl:.2f} | tokens: {total_tokens}\")\n",
    "    return ppl\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CSV_PATH   = \"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\"\n",
    "    MASKED_DIR = \"masked_eval\"\n",
    "\n",
    "    # 1) build masked eval once (padding fixed to max_length)\n",
    "    tok_mask = AutoTokenizer.from_pretrained(\"imvladikon/het5-base\", use_fast=False)\n",
    "    create_masked_eval(CSV_PATH, tok_mask, out_dir=MASKED_DIR, seed=42, batch_size=32)\n",
    "\n",
    "    # 2) evaluate models\n",
    "    compute_perplexity_from_masked(\"imvladikon/het5-base\", MASKED_DIR, batch_size=8)\n",
    "    compute_perplexity_from_masked(\"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\", MASKED_DIR, batch_size=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4gemma_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
