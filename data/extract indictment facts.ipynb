{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"×¢×•×‘×“×•×ª×\", \"×›×œ×œ×™\", \"×›×ª×‘ ×”××™×©×•×\", \"×”××™×©×•×\", \"××™×©×•×\", \"×¨×§×¢\", \"×’×–×¨\", \"×“×™×Ÿ\", \"×¤×¡×§\",\"××‘×•×\",\"×”×¨×©×¢×ª\" ,\"×‘×¢× ×™×™× ×•\",\"×¢×‘×™×¨×•×ª\",\"×”×•×¨×©×¢\",\"×¢×•×‘×“×•×ª\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\n",
    "    \"×˜×¢× ×•×ª\", \"×¢××“×ª\", \"×ª×¡×§×™×¨\", \"×©×™×¨×•×ª\", \"××‘×—×Ÿ\", \"×“×™×•×Ÿ\", \"×”×ª×¡×§×™×¨\",\n",
    "    \"×˜×™×¢×•× ×™\", \"×”×¦×“×“×™×\", \"×¦×“×“×™×\", \"×•×”×›×¨×¢×”\", \"×”××™×©×•× ×”×©× ×™\", \"×¨××™×•×ª\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"âŒ No indictment facts found\", None, None\n",
    "\n",
    "    # Find the first row where 'part' contains a start pattern (case-insensitive, partial match)\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    if start_row.empty:\n",
    "        return \"âŒ No indictment facts found\", None, None\n",
    "    start_idx = start_row.index.min()\n",
    "    start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Find the first row where 'part' contains an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Ensure end is after start and not identical in name\n",
    "    if not end_row.empty:\n",
    "        potential_end_idx = end_row.index.min()\n",
    "\n",
    "        # If the end part is the same as the start part, look further down\n",
    "        if df.loc[potential_end_idx, \"part\"] == df.loc[start_idx, \"part\"]:\n",
    "            print(f\"âš ï¸ Warning: Start and End have the same name for verdict '{df['verdict'].iloc[0]}'. Searching for next distinct part.\")\n",
    "\n",
    "            # Find the next part that is different from the start part\n",
    "            extended_end_row = df[df.index > potential_end_idx][df[\"part\"] != df.loc[start_idx, \"part\"]]\n",
    "\n",
    "            if not extended_end_row.empty:\n",
    "                end_idx = extended_end_row.index.min()\n",
    "            else:\n",
    "                end_idx = len(df)  # Default to full text if no better match is found\n",
    "        else:\n",
    "            end_idx = potential_end_idx  # Use valid end index if found\n",
    "    else:\n",
    "        end_idx = len(df)  # Default to full text if no end marker is found\n",
    "\n",
    "    # Assign extracted part\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if end_idx < len(df) else \"âŒ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"âŒ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_csv_{year}\"  # Change this to your actual directory\n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"âŒ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str).unique())  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\nâŒ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"ğŸ“Œ Available Parts: {failed_verdicts[-1]['all_parts']}\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"start_part\": start_part if start_part else \"âŒ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"âŒ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with start and end parts\n",
    "print(\"\\n=== Sample of Successful Extractions (Start & End Parts) ===\")\n",
    "print(final_df[[\"verdict\", \"start_part\", \"end_part\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\nâœ… Process complete. Results saved as 'processed_verdicts.csv' and 'failed_verdicts.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts with API gpt-VERDICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"×¢×•×‘×“×•×ª×\", \"×›×œ×œ×™\", \"×›×ª×‘ ×”××™×©×•×\", \"×”××™×©×•×\", \"××™×©×•×\", \"×¨×§×¢\", \"×’×–×¨\", \"×“×™×Ÿ\", \"×¤×¡×§\",\"××‘×•×\",\"×”×¨×©×¢×ª\" ,\"×‘×¢× ×™×™× ×•\",\"×¢×‘×™×¨×•×ª\",\"×”×•×¨×©×¢\",\"×¢×•×‘×“×•×ª\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\n",
    "    \"×˜×¢× ×•×ª\", \"×¢××“×ª\", \"×ª×¡×§×™×¨\", \"×©×™×¨×•×ª\", \"××‘×—×Ÿ\", \"×“×™×•×Ÿ\", \"×”×ª×¡×§×™×¨\",\n",
    "    \"×˜×™×¢×•× ×™\", \"×”×¦×“×“×™×\", \"×¦×“×“×™×\", \"×•×”×›×¨×¢×”\",  \"×¨××™×•×ª\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"âŒ No indictment facts found\", None, None\n",
    "\n",
    "    # Find the first row where 'part' contains a start pattern (case-insensitive, partial match)\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    if start_row.empty:\n",
    "        return \"âŒ No indictment facts found\", None, None\n",
    "    start_idx = start_row.index.min()\n",
    "    start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Find the first row where 'part' contains an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Handle case where start and end are identical (wrong extraction range)\n",
    "    if not end_row.empty and end_row.index.min() == start_idx:\n",
    "        print(f\"âš ï¸ Warning: Start and End are the same for verdict '{df['verdict'].iloc[0]}'. Extending search.\")\n",
    "        end_row = df[df.index > start_idx + 1][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    end_idx = end_row.index.min() if not end_row.empty else len(df)\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if not end_row.empty else \"âŒ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"âŒ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "\n",
    "def extract_facts_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Sends extracted text to GPT API and extracts specific facts.\n",
    "    \"\"\"\n",
    "    if text == \"âŒ No indictment facts found\":\n",
    "        return \"âŒ No facts extracted\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Extract only the factual allegations from the provided legal text, preserving the original wording without summarizing, rephrasing, or omitting details. Present the extracted facts as a single paragraph rather than a structured list.\n",
    "\n",
    "    **Guidelines:**\n",
    "    - Do not include conclusions, arguments, or legal interpretations.\n",
    "    - Keep the extracted text **exactly as it appears** in the original source.\n",
    "    - Maintain coherence when merging multiple allegations into a single paragraph.\n",
    "\n",
    "    **Example:**\n",
    "    \n",
    "    **Input:**\n",
    "    ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª ×—×œ×§ ×©×œ × ×©×§ ××• ×ª×—××•×©×ª, ×œ×¤×™ ×¡×¢×™×£ 144 (×) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×– 1977 (×œ×”×œ×Ÿ: \"×—×•×§ ×”×¢×•× ×©×™×Ÿ\") ×•× ×©×™××”/×”×•×‘×œ×ª ×—×œ×§ ×©×œ × ×©×§ ××• ×ª×—××•×©×ª, ×œ×¤×™ ×¡×¢×™×£ 144(×‘) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ. ×¢×œ ×¤×™ ×”× ×˜×¢×Ÿ ×‘×›×ª×‘ ×”××™×©×•× ×‘×™×•× 28.8.2022, ×‘×©×¢×” 00:20 ×œ×¢×¨×š, × ×”×’ ×”× ××©× ×‘×¨×›×‘ ××¡×•×’ ×§×™×” ×¡×¤×•×¨×˜×’' × ×•×©× ×œ×•×—×™×ª ×¨×™×©×•×™ ××¡×¤×¨ 13-608-201 ××œ ×¢×‘×¨ ××¢×‘×¨ ×”×œ\"×” ×‘×“×¨×›×• ×œ×©×˜×—×™ ×”××–×•×¨, ×›×œ ×–××ª ×›××©×¨ ×”×•× × ×•×©× ××ª×—×ª ×œ××•×©×‘ ×”× ×”×’ ×‘×¨×›×‘ ×©×§×™×ª ×•×‘×” 6 ××›×œ×•×œ×™× ×©×œ × ×©×§ ××¡×•×’ M16. ×‘× ×•×¡×£ ×‘×ª× ×”××˜×¢×Ÿ ×©×œ ×”×¨×›×‘ × ×©× ×”× ××©× ×©×‘×¢×” ××¨×’×–×™ ×ª×—××•×©×ª ×•××¨×’×– ×§×¨×˜×•×Ÿ ××©×¨ ×”×›×™×œ×• ×™×—×“×™×• ×›-9000 ×›×“×•×¨×™× ×‘×§×•×˜×¨ 5.56 ×\"× ××©×¨ ×”×™×• ××›×•×¡×™× ×•××•×¡×ª×¨×™×.\n",
    "\n",
    "    **Expected Output:**\n",
    "    ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª ×—×œ×§ ×©×œ × ×©×§ ××• ×ª×—××•×©×ª, ×œ×¤×™ ×¡×¢×™×£ 144 (×) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×– 1977 ×•× ×©×™××”/×”×•×‘×œ×ª ×—×œ×§ ×©×œ × ×©×§ ××• ×ª×—××•×©×ª, ×œ×¤×™ ×¡×¢×™×£ 144(×‘) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ. ×¢×œ ×¤×™ ×”× ×˜×¢×Ÿ ×‘×›×ª×‘ ×”××™×©×•×, ×‘×™×•× 28.8.2022 ×‘×©×¢×” 00:20 ×œ×¢×¨×š, × ×”×’ ×”× ××©× ×‘×¨×›×‘ ××¡×•×’ ×§×™×” ×¡×¤×•×¨×˜×’' ×¢× ×œ×•×—×™×ª ×¨×™×©×•×™ ××¡×¤×¨ 13-608-201 ×œ×›×™×•×•×Ÿ ××¢×‘×¨ ×”×œ\"×” ×‘×“×¨×›×• ×œ×©×˜×—×™ ×”××–×•×¨, ×›××©×¨ ××ª×—×ª ×œ××•×©×‘ ×”× ×”×’ ×‘×¨×›×‘ ×”×™×™×ª×” ×©×§×™×ª ×•×‘×” 6 ××›×œ×•×œ×™× ×©×œ × ×©×§ ××¡×•×’ M16. ×‘× ×•×¡×£, ×‘×ª× ×”××˜×¢×Ÿ ×©×œ ×”×¨×›×‘ × ×©× ×©×‘×¢×” ××¨×’×–×™ ×ª×—××•×©×ª ×•××¨×’×– ×§×¨×˜×•×Ÿ ×©×”×›×™×œ×• ×™×—×“×™×• ×›-9000 ×›×“×•×¨×™× ×‘×§×•×˜×¨ 5.56 ×\"×, ×©×”×™×• ××›×•×¡×™× ×•××•×¡×ª×¨×™×.\n",
    "\n",
    "    **Input Text:**\n",
    "    {text}\n",
    "\n",
    "    **Extracted Facts:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract factual allegations from legal texts, ensuring no interpretation or rewording.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extracted_facts = response.choices[0].message.content.strip()\n",
    "    print(\"Input Text:\", text)\n",
    "    print(\"Extracted Facts:\", extracted_facts)\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "\n",
    "\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_csv_{year}\"  # Change this to your actual directory\n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Extract facts using GPT\n",
    "            extracted_gpt_facts = extract_facts_with_gpt(extracted_facts)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"âŒ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str))  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\nâŒ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"ğŸ“Œ Available Parts: {failed_verdicts[-1]['all_parts']}\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"extracted_gpt_facts\": extracted_gpt_facts,\n",
    "                \"start_part\": start_part if start_part else \"âŒ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"âŒ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_verdicts_with_gpt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with GPT output\n",
    "print(\"\\n=== Sample of Successful Extractions (GPT Facts) ===\")\n",
    "print(final_df[[\"verdict\", \"extracted_gpt_facts\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\nâœ… Process complete. Results saved as 'processed_verdicts_with_gpt.csv' and 'failed_verdicts.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts with API gpt-APPEALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"×¢×•×‘×“×•×ª×\", \"×›×œ×œ×™\", \"×›×ª×‘ ×”××™×©×•×\", \"×”××™×©×•×\", \"××™×©×•×\", \"×¨×§×¢\", \"×’×–×¨\", \"×“×™×Ÿ\", \"×¤×¡×§\",\"××‘×•×\",\"×”×¨×©×¢×ª\" ,\"×‘×¢× ×™×™× ×•\",\"×¢×‘×™×¨×•×ª\",\"×”×•×¨×©×¢\",\"×¢×•×‘×“×•×ª\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\"×× ×™ ××¡×›×™×\"\n",
    "    \"×˜×¢× ×•×ª\", \"×¢××“×ª\", \"×ª×¡×§×™×¨\", \"×©×™×¨×•×ª\", \"××‘×—×Ÿ\", \"×“×™×•×Ÿ\", \"×”×ª×¡×§×™×¨\",\n",
    "    \"×˜×™×¢×•× ×™\", \"×”×¦×“×“×™×\", \"×¦×“×“×™×\", \"×•×”×›×¨×¢×”\", \"×¨××™×•×ª\",\"×”×›×¨×¢×”\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - If no start is found, it attempts a secondary search in the text.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"âŒ No indictment facts found\", None, None\n",
    "\n",
    "    # Search for start part in 'part' column\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    if start_row.empty:\n",
    "        # Secondary search: check if the 'text' column contains possible indicators\n",
    "        text_match = df[df[\"text\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "        if text_match.empty:\n",
    "            return \"âŒ No indictment facts found (start section missing)\", None, None\n",
    "        else:\n",
    "            start_idx = text_match.index.min()\n",
    "            start_part_name = \"ğŸ” Found in text column\"\n",
    "    else:\n",
    "        start_idx = start_row.index.min()\n",
    "        start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Search for the first row containing an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Handle case where start and end are identical (wrong extraction range)\n",
    "    if not end_row.empty and end_row.index.min() == start_idx:\n",
    "        print(f\"âš ï¸ Warning: Start and End are the same for verdict '{df['verdict'].iloc[0]}'. Extending search.\")\n",
    "        end_row = df[df.index > start_idx + 1][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    end_idx = end_row.index.min() if not end_row.empty else len(df)\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if not end_row.empty else \"âŒ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"âŒ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "\n",
    "def extract_facts_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Sends extracted text to GPT API and extracts the original indictment details from the case being appealed.\n",
    "    \"\"\"\n",
    "    if text == \"âŒ No indictment facts found\":\n",
    "        return \"âŒ No facts extracted\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Extract only the original indictment details of the case being appealed. Ignore all references to the appeal decision, legal arguments, and judicial reasoning. The extracted text should contain only the original facts that led to the indictment, exactly as they appear in the text.\n",
    "\n",
    "    **Guidelines:**\n",
    "    - Extract **only the factual allegations** from the original indictment.\n",
    "    - **Do not include** details about the appeal, court rulings, or sentencing decisions.\n",
    "    - Maintain the **exact wording** of the indictment without summarizing or omitting details.\n",
    "    - If the indictment contains multiple allegations, present them in a coherent paragraph.\n",
    "\n",
    "    **Example:**\n",
    "    \n",
    "    **Input:**\n",
    "    ×¢\"\"×¤ 761âˆ•07 - ×¢×¨×¢×•×¨ ×¢×œ ×’×–×¨ ×“×™× ×• ×©×œ ×‘×™×ª ×”××©×¤×˜ ×”××—×•×–×™. \n",
    "    ×‘××—×“ ××™××™×• ×©×œ ×—×•×“×© ×™×•× ×™ 2006, ×‘×©×¢×•×ª ×”×¢×¨×‘, × ×”×’ ×”× ××©× ×‘×¨×›×‘, ×•×›××©×¨ × ×¢×¦×¨ ×¢×œ ×™×“×™ ×©×•×˜×¨×™× ×œ×‘×“×™×§×”, ×”×•× × ××¦× ××—×–×™×§ ×‘××§×“×—, ××—×¡× ×™×ª ×•×ª×—××•×©×ª ×›×©××œ×” ×¢×˜×•×¤×™× ×‘×’×¨×‘ ×•××•×¡×ª×¨×™× ×‘×ª×—×ª×•× ×™×•.\n",
    "    ×›×Ÿ × ×˜×¢×Ÿ, ×›×™ ×”× ××©× ×”×¦×™×’ ×‘×¤× ×™ ×”×©×•×˜×¨×™× ×ª×¢×•×“×ª ×–×”×•×ª ×©×œ ××—×¨ ××ª×•×š ×›×•×•× ×” ×œ×”×•× ×•×ª×.\n",
    "    ×”× ××©× ×”×•×“×” ×‘×¢×•×‘×“×•×ª ×”×××•×¨×•×ª, ×•×‘×¢×§×‘×•×ª ×›×š ×”×•×¨×©×¢ ×‘×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª × ×©×§ ×©×œ× ×›×“×™×Ÿ ×•×”×¤×¨×¢×” ×œ×©×•×˜×¨ ×‘××™×œ×•×™ ×ª×¤×§×™×“×•, ×¢×‘×™×¨×•×ª ×œ×¤×™ ×¡×¢×™×¤×™× 144 ×¨×™×©× ×•-275 ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ.\n",
    "\n",
    "    **Expected Output:**\n",
    "    ×‘××—×“ ××™××™×• ×©×œ ×—×•×“×© ×™×•× ×™ 2006, ×‘×©×¢×•×ª ×”×¢×¨×‘, × ×”×’ ×”× ××©× ×‘×¨×›×‘, ×•×›××©×¨ × ×¢×¦×¨ ×¢×œ ×™×“×™ ×©×•×˜×¨×™× ×œ×‘×“×™×§×”, × ××¦× ××—×–×™×§ ×‘××§×“×—, ××—×¡× ×™×ª ×•×ª×—××•×©×ª ×¢×˜×•×¤×™× ×‘×’×¨×‘ ×•××•×¡×ª×¨×™× ×‘×ª×—×ª×•× ×™×•. ×‘× ×•×¡×£, ×”×¦×™×’ ×œ×©×•×˜×¨×™× ×ª×¢×•×“×ª ×–×”×•×ª ×©×œ ××—×¨ ×‘×›×•×•× ×” ×œ×”×•× ×•×ª×. ×¢×œ ×¡××š ×¢×•×‘×“×•×ª ××œ×”, ×”×•××©× ×‘×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª × ×©×§ ×©×œ× ×›×“×™×Ÿ ×•×”×¤×¨×¢×” ×œ×©×•×˜×¨ ×‘××™×œ×•×™ ×ª×¤×§×™×“×• ×œ×¤×™ ×¡×¢×™×¤×™× 144 ×¨×™×©× ×•-275 ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ.\n",
    "\n",
    "    **Input Text:**\n",
    "    {text}\n",
    "\n",
    "    **Extracted Indictment Details:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract factual allegations from legal texts, ensuring no interpretation or rewording.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extracted_facts = response.choices[0].message.content.strip()\n",
    "    # print(\"Input Text:\", text)\n",
    "    print(\"Extracted Facts:\", extracted_facts)\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "\n",
    "\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_citations_csv_{year}\"  \n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Extract facts using GPT\n",
    "            extracted_gpt_facts = extract_facts_with_gpt(extracted_facts)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"âŒ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str))  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\nâŒ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"ğŸ“Œ Available Parts: {failed_verdicts[-1]['all_parts']}.unique()\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"extracted_gpt_facts\": extracted_gpt_facts,\n",
    "                \"start_part\": start_part if start_part else \"âŒ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"âŒ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_appeals_with_gpt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with GPT output\n",
    "print(\"\\n=== Sample of Successful Extractions (GPT Facts) ===\")\n",
    "print(final_df[[\"verdict\", \"extracted_gpt_facts\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\nâœ… Process complete. Results saved as 'processed_appeals_with_gpt.csv' and 'failed_verdicts.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judgeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
