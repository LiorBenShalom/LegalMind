{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\n",
    "    \"טענות\", \"עמדת\", \"תסקיר\", \"שירות\", \"מבחן\", \"דיון\", \"התסקיר\",\n",
    "    \"טיעוני\", \"הצדדים\", \"צדדים\", \"והכרעה\", \"האישום השני\", \"ראיות\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "\n",
    "    # Find the first row where 'part' contains a start pattern (case-insensitive, partial match)\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    if start_row.empty:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "    start_idx = start_row.index.min()\n",
    "    start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Find the first row where 'part' contains an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Ensure end is after start and not identical in name\n",
    "    if not end_row.empty:\n",
    "        potential_end_idx = end_row.index.min()\n",
    "\n",
    "        # If the end part is the same as the start part, look further down\n",
    "        if df.loc[potential_end_idx, \"part\"] == df.loc[start_idx, \"part\"]:\n",
    "            print(f\"⚠️ Warning: Start and End have the same name for verdict '{df['verdict'].iloc[0]}'. Searching for next distinct part.\")\n",
    "\n",
    "            # Find the next part that is different from the start part\n",
    "            extended_end_row = df[df.index > potential_end_idx][df[\"part\"] != df.loc[start_idx, \"part\"]]\n",
    "\n",
    "            if not extended_end_row.empty:\n",
    "                end_idx = extended_end_row.index.min()\n",
    "            else:\n",
    "                end_idx = len(df)  # Default to full text if no better match is found\n",
    "        else:\n",
    "            end_idx = potential_end_idx  # Use valid end index if found\n",
    "    else:\n",
    "        end_idx = len(df)  # Default to full text if no end marker is found\n",
    "\n",
    "    # Assign extracted part\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if end_idx < len(df) else \"❌ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"❌ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_csv_{year}\"  # Change this to your actual directory\n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"❌ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str).unique())  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\n❌ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"📌 Available Parts: {failed_verdicts[-1]['all_parts']}\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"start_part\": start_part if start_part else \"❌ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"❌ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with start and end parts\n",
    "print(\"\\n=== Sample of Successful Extractions (Start & End Parts) ===\")\n",
    "print(final_df[[\"verdict\", \"start_part\", \"end_part\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\n✅ Process complete. Results saved as 'processed_verdicts.csv' and 'failed_verdicts.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts with API gpt-VERDICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\n",
    "    \"טענות\", \"עמדת\", \"תסקיר\", \"שירות\", \"מבחן\", \"דיון\", \"התסקיר\",\n",
    "    \"טיעוני\", \"הצדדים\", \"צדדים\", \"והכרעה\",  \"ראיות\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "\n",
    "    # Find the first row where 'part' contains a start pattern (case-insensitive, partial match)\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    if start_row.empty:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "    start_idx = start_row.index.min()\n",
    "    start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Find the first row where 'part' contains an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Handle case where start and end are identical (wrong extraction range)\n",
    "    if not end_row.empty and end_row.index.min() == start_idx:\n",
    "        print(f\"⚠️ Warning: Start and End are the same for verdict '{df['verdict'].iloc[0]}'. Extending search.\")\n",
    "        end_row = df[df.index > start_idx + 1][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    end_idx = end_row.index.min() if not end_row.empty else len(df)\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if not end_row.empty else \"❌ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"❌ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "\n",
    "def extract_facts_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Sends extracted text to GPT API and extracts specific facts.\n",
    "    \"\"\"\n",
    "    if text == \"❌ No indictment facts found\":\n",
    "        return \"❌ No facts extracted\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Extract only the factual allegations from the provided legal text, preserving the original wording without summarizing, rephrasing, or omitting details. Present the extracted facts as a single paragraph rather than a structured list.\n",
    "\n",
    "    **Guidelines:**\n",
    "    - Do not include conclusions, arguments, or legal interpretations.\n",
    "    - Keep the extracted text **exactly as it appears** in the original source.\n",
    "    - Maintain coherence when merging multiple allegations into a single paragraph.\n",
    "\n",
    "    **Example:**\n",
    "    \n",
    "    **Input:**\n",
    "    הנאשם הורשע על פי הודאתו בעבירות של החזקת חלק של נשק או תחמושת, לפי סעיף 144 (א) לחוק העונשין, תשל\"ז 1977 (להלן: \"חוק העונשין\") ונשיאה/הובלת חלק של נשק או תחמושת, לפי סעיף 144(ב) לחוק העונשין. על פי הנטען בכתב האישום ביום 28.8.2022, בשעה 00:20 לערך, נהג הנאשם ברכב מסוג קיה ספורטג' נושא לוחית רישוי מספר 13-608-201 אל עבר מעבר הל\"ה בדרכו לשטחי האזור, כל זאת כאשר הוא נושא מתחת למושב הנהג ברכב שקית ובה 6 מכלולים של נשק מסוג M16. בנוסף בתא המטען של הרכב נשא הנאשם שבעה ארגזי תחמושת וארגז קרטון אשר הכילו יחדיו כ-9000 כדורים בקוטר 5.56 מ\"מ אשר היו מכוסים ומוסתרים.\n",
    "\n",
    "    **Expected Output:**\n",
    "    הנאשם הורשע על פי הודאתו בעבירות של החזקת חלק של נשק או תחמושת, לפי סעיף 144 (א) לחוק העונשין, תשל\"ז 1977 ונשיאה/הובלת חלק של נשק או תחמושת, לפי סעיף 144(ב) לחוק העונשין. על פי הנטען בכתב האישום, ביום 28.8.2022 בשעה 00:20 לערך, נהג הנאשם ברכב מסוג קיה ספורטג' עם לוחית רישוי מספר 13-608-201 לכיוון מעבר הל\"ה בדרכו לשטחי האזור, כאשר מתחת למושב הנהג ברכב הייתה שקית ובה 6 מכלולים של נשק מסוג M16. בנוסף, בתא המטען של הרכב נשא שבעה ארגזי תחמושת וארגז קרטון שהכילו יחדיו כ-9000 כדורים בקוטר 5.56 מ\"מ, שהיו מכוסים ומוסתרים.\n",
    "\n",
    "    **Input Text:**\n",
    "    {text}\n",
    "\n",
    "    **Extracted Facts:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract factual allegations from legal texts, ensuring no interpretation or rewording.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extracted_facts = response.choices[0].message.content.strip()\n",
    "    print(\"Input Text:\", text)\n",
    "    print(\"Extracted Facts:\", extracted_facts)\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "\n",
    "\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_csv_{year}\"  # Change this to your actual directory\n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Extract facts using GPT\n",
    "            extracted_gpt_facts = extract_facts_with_gpt(extracted_facts)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"❌ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str))  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\n❌ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"📌 Available Parts: {failed_verdicts[-1]['all_parts']}\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"extracted_gpt_facts\": extracted_gpt_facts,\n",
    "                \"start_part\": start_part if start_part else \"❌ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"❌ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_verdicts_with_gpt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with GPT output\n",
    "print(\"\\n=== Sample of Successful Extractions (GPT Facts) ===\")\n",
    "print(final_df[[\"verdict\", \"extracted_gpt_facts\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\n✅ Process complete. Results saved as 'processed_verdicts_with_gpt.csv' and 'failed_verdicts.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts with API gpt-APPEALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\"אני מסכים\"\n",
    "    \"טענות\", \"עמדת\", \"תסקיר\", \"שירות\", \"מבחן\", \"דיון\", \"התסקיר\",\n",
    "    \"טיעוני\", \"הצדדים\", \"צדדים\", \"והכרעה\", \"ראיות\",\"הכרעה\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - If no start is found, it attempts a secondary search in the text.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "\n",
    "    # Search for start part in 'part' column\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    if start_row.empty:\n",
    "        # Secondary search: check if the 'text' column contains possible indicators\n",
    "        text_match = df[df[\"text\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "        if text_match.empty:\n",
    "            return \"❌ No indictment facts found (start section missing)\", None, None\n",
    "        else:\n",
    "            start_idx = text_match.index.min()\n",
    "            start_part_name = \"🔍 Found in text column\"\n",
    "    else:\n",
    "        start_idx = start_row.index.min()\n",
    "        start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Search for the first row containing an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Handle case where start and end are identical (wrong extraction range)\n",
    "    if not end_row.empty and end_row.index.min() == start_idx:\n",
    "        print(f\"⚠️ Warning: Start and End are the same for verdict '{df['verdict'].iloc[0]}'. Extending search.\")\n",
    "        end_row = df[df.index > start_idx + 1][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    end_idx = end_row.index.min() if not end_row.empty else len(df)\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if not end_row.empty else \"❌ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"❌ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "\n",
    "def extract_facts_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Sends extracted text to GPT API and extracts the original indictment details from the case being appealed.\n",
    "    \"\"\"\n",
    "    if text == \"❌ No indictment facts found\":\n",
    "        return \"❌ No facts extracted\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Extract only the original indictment details of the case being appealed. Ignore all references to the appeal decision, legal arguments, and judicial reasoning. The extracted text should contain only the original facts that led to the indictment, exactly as they appear in the text.\n",
    "\n",
    "    **Guidelines:**\n",
    "    - Extract **only the factual allegations** from the original indictment.\n",
    "    - **Do not include** details about the appeal, court rulings, or sentencing decisions.\n",
    "    - Maintain the **exact wording** of the indictment without summarizing or omitting details.\n",
    "    - If the indictment contains multiple allegations, present them in a coherent paragraph.\n",
    "\n",
    "    **Example:**\n",
    "    \n",
    "    **Input:**\n",
    "    ע\"\"פ 761∕07 - ערעור על גזר דינו של בית המשפט המחוזי. \n",
    "    באחד מימיו של חודש יוני 2006, בשעות הערב, נהג הנאשם ברכב, וכאשר נעצר על ידי שוטרים לבדיקה, הוא נמצא מחזיק באקדח, מחסנית ותחמושת כשאלה עטופים בגרב ומוסתרים בתחתוניו.\n",
    "    כן נטען, כי הנאשם הציג בפני השוטרים תעודת זהות של אחר מתוך כוונה להונותם.\n",
    "    הנאשם הודה בעובדות האמורות, ובעקבות כך הורשע בעבירות של החזקת נשק שלא כדין והפרעה לשוטר במילוי תפקידו, עבירות לפי סעיפים 144 רישא ו-275 לחוק העונשין.\n",
    "\n",
    "    **Expected Output:**\n",
    "    באחד מימיו של חודש יוני 2006, בשעות הערב, נהג הנאשם ברכב, וכאשר נעצר על ידי שוטרים לבדיקה, נמצא מחזיק באקדח, מחסנית ותחמושת עטופים בגרב ומוסתרים בתחתוניו. בנוסף, הציג לשוטרים תעודת זהות של אחר בכוונה להונותם. על סמך עובדות אלה, הואשם בעבירות של החזקת נשק שלא כדין והפרעה לשוטר במילוי תפקידו לפי סעיפים 144 רישא ו-275 לחוק העונשין.\n",
    "\n",
    "    **Input Text:**\n",
    "    {text}\n",
    "\n",
    "    **Extracted Indictment Details:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract factual allegations from legal texts, ensuring no interpretation or rewording.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extracted_facts = response.choices[0].message.content.strip()\n",
    "    # print(\"Input Text:\", text)\n",
    "    print(\"Extracted Facts:\", extracted_facts)\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "\n",
    "\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_citations_csv_{year}\"  \n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Extract facts using GPT\n",
    "            extracted_gpt_facts = extract_facts_with_gpt(extracted_facts)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"❌ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str))  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\n❌ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"📌 Available Parts: {failed_verdicts[-1]['all_parts']}.unique()\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"extracted_gpt_facts\": extracted_gpt_facts,\n",
    "                \"start_part\": start_part if start_part else \"❌ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"❌ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_appeals_with_gpt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with GPT output\n",
    "print(\"\\n=== Sample of Successful Extractions (GPT Facts) ===\")\n",
    "print(final_df[[\"verdict\", \"extracted_gpt_facts\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\n✅ Process complete. Results saved as 'processed_appeals_with_gpt.csv' and 'failed_verdicts.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judgeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
