{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation no API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "\n",
    "required_parts = [\n",
    "    \"××ª×—××™ ×¢× ×™×©×”\", \"××—×™×“×•×ª ×‘×¢× ×™×©×”\", \"××ª×—× ×”×¢× ×™×©×”\", \"××ª×—× ×¢× ×™×©×”\", \"×“×™×•×Ÿ\",\n",
    "    \"×¢× ×™×©×” × ×”×•×’×”\", \"×”×¢× ×™×©×” ×”× ×•×”×’×ª\", \"×¢× ×™×©×” × ×•×”×’×ª\", \"××ª×—× ×”×¢×•× ×©\", \"××ª×—× ×¢×•× ×©\",\n",
    "    \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\", \"×•×”×›×¨×¢×”\", \"×”×”×¨×©×¢×”\", \"××“×™× ×™×•×ª ×”×¢× ×™×©×” ×”× ×”×•×’×”\"\n",
    "]\n",
    "citation_patterns = {\n",
    "    '×¢\"×¤': r'×¢\"×¤ (\\d+/\\d+)',\n",
    "    '×¢×¤\"×’': r'×¢×¤\"×’ (\\d+/\\d+)',\n",
    "    '×ª×´×¤': r'×ª×´×¤ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    '×¢×¤×´×’': r'×¢×¤×´×’ (\\d+/\\d+)',\n",
    "    '×¨×¢×´×¤': r'×¨×¢×´×¤ (\\d+/\\d+)',\n",
    "    '×ª×¤\"×—': r'×ª×¤\"×—\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "}\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/home/liorkob/best_model.pt\"  # Path to your saved model\n",
    "tokenizer = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def extract_citations(para_text):\n",
    "    \"\"\"\n",
    "    Extracts all citations and their full references from the text.\n",
    "    Returns a list of tuples: (citation_type, full_citation).\n",
    "    \"\"\"\n",
    "    citations = []\n",
    "\n",
    "    for citation_type, pattern in citation_patterns.items():\n",
    "        matches = re.findall(pattern, para_text)  # Find all matches for the pattern\n",
    "        for match in matches:\n",
    "            full_citation = f\"{citation_type} {match}\"  # Construct full citation\n",
    "            citations.append((citation_type, full_citation))\n",
    "\n",
    "    return citations  # List of (citation_type, full_citation)\n",
    "\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"Extracts the first occurrence of a required part in the CSV and all subsequent rows.\"\"\"\n",
    "    start_index = None\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "    return csv_data.iloc[start_index:] if start_index is not None else pd.DataFrame(columns=csv_data.columns)\n",
    "import re\n",
    "\n",
    "def process_and_tag(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"Process a .docx document and its corresponding CSV to check citations and tag with predictions.\"\"\"\n",
    "    try:\n",
    "        # Load the document and CSV\n",
    "        doc = docx.Document(docx_path)\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Iterate through paragraphs\n",
    "        for i, paragraph in enumerate(doc.paragraphs):\n",
    "            para_text = paragraph.text.strip()\n",
    "            if not para_text:\n",
    "                continue  # Skip empty paragraphs\n",
    "\n",
    "            found_citations = extract_citations(para_text)\n",
    "\n",
    "            if not found_citations:\n",
    "                continue  # No citations found, move to the next paragraph\n",
    "\n",
    "            for found_citation, full_citation in found_citations:\n",
    "\n",
    "                is_relevant = False\n",
    "                matching_part = None\n",
    "\n",
    "                # Check if the citation is in relevant parts\n",
    "                for _, row in csv_data.iterrows():\n",
    "                    part_text = row.get(\"text\", \"\")\n",
    "                    if any(req_part in row.get(\"part\", \"\") for req_part in required_parts) and part_text in para_text:\n",
    "                        is_relevant = True\n",
    "                        matching_part = row[\"part\"]\n",
    "                        break  # Stop searching once a match is found\n",
    "\n",
    "                if is_relevant:\n",
    "                    # Tag the paragraph using the model\n",
    "                    encoding = tokenizer(para_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "                    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "                    with torch.no_grad():\n",
    "                        output = model(**encoding)\n",
    "                        prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "                    # Append only when is_relevant = True\n",
    "                    results.append({\n",
    "                        'paragraph_number': i,\n",
    "                        'context_text': para_text,\n",
    "                        'citation': full_citation,\n",
    "                        'part': matching_part,\n",
    "                        'predicted_label': prediction, \n",
    "                    })\n",
    "\n",
    "                    print(f\"Tagged citation: Paragraph {i}, Part: {matching_part}, Prediction: {prediction}\")\n",
    "                    print(f\"Text: {para_text}\\n\")\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "        # Save results\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Tagged citations saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {docx_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        docx_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_{year}')\n",
    "        csv_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_csv_{year}')\n",
    "        output_directory = Path(f'/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}')\n",
    "\n",
    "        output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file_path in docx_directory.glob(\"*.docx\"):\n",
    "            try:\n",
    "                new_file_path = file_path.stem\n",
    "                print(f\"Processing {new_file_path}\")\n",
    "\n",
    "                csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "                if file_path.exists() and csv_file.exists():\n",
    "                    output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "                    process_and_tag(str(file_path), str(csv_file), str(output_file))\n",
    "\n",
    "                else:\n",
    "                    if not file_path.exists():\n",
    "                        print(f\"Document file not found: {file_path}\")\n",
    "                    if not csv_file.exists():\n",
    "                        print(f\"CSV file not found for: {csv_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all results to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import docx\n",
    "\n",
    "def merge_results(csv_directory: str, output_csv: str):\n",
    "    csv_directory = Path(csv_directory)\n",
    "    all_data = []\n",
    "    \n",
    "    # Iterate over CSV files\n",
    "    for file_path in csv_directory.glob(\"*.csv\"):\n",
    "        try:\n",
    "            if file_path.stat().st_size == 0:  # Check if file is empty\n",
    "                print(f\"Skipping empty file: {file_path.name}\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty:  # Check if the file is empty even after reading\n",
    "                print(f\"Skipping empty DataFrame: {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            df[\"source_file\"] = file_path.name  # Add filename column\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path.name}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        merged_df = pd.concat(all_data, ignore_index=True)\n",
    "        merged_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Merged CSV saved to: {output_csv}\")\n",
    "    else:\n",
    "        print(\"No valid CSV files found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        csv_directory = f\"/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}\"\n",
    "        output_csv = f\"{csv_directory}/merged_results_{year}.csv\"\n",
    "        \n",
    "        merge_results(csv_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation WITH API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_leading_prefix(citation):\n",
    "    match = re.match(r'^([×œ×‘×•×”])\\s*([×-×ª\"]+)', citation)\n",
    "    if not match:\n",
    "        return citation\n",
    "    prefix = match.group(1)\n",
    "    maybe_acronym = match.group(2)\n",
    "    \n",
    "    # ×‘×•× ×” ××ª ×”×¦×™×¨×•×£ ×”××œ× ×›×•×œ×œ ×”×§×™×“×•××ª\n",
    "    full = prefix + maybe_acronym\n",
    "\n",
    "    # ×× ×¨××œ (××¡×™×¨ ×’×¨×©×™×™×) ×‘×©×‘×™×œ ×œ×”×©×•×•×ª ×œ×¨×©×™××ª ×¨××©×™ ×”×ª×™×‘×•×ª\n",
    "    def normalize(text):\n",
    "        return text.replace('\"', '').replace(\"×´\", \"\").replace(\"'\", \"\").replace(\"×³\", \"\")\n",
    "\n",
    "    norm_maybe = normalize(maybe_acronym)\n",
    "    norm_full = normalize(full)\n",
    "\n",
    "    # ×ª× ××™ ×”×¡×¨×”: ×”×—×œ×§ ×©××—×¨×™ ×”×§×™×“×•××ª ××•×›×¨, ××‘×œ ×”×¦×™×¨×•×£ ×›×•×œ×• ×œ× ××•×›×¨\n",
    "    if norm_maybe in acronyms and norm_full not in acronyms:\n",
    "        return citation[len(prefix):].lstrip()\n",
    "\n",
    "    return citation\n",
    "\n",
    "# List of legal acronyms (same as yours)\n",
    "acronyms = [\n",
    "    \"××‘\", \"××‘×¢\", \"××™××•×¦\", \"×××¦\", \"××¤\", \"××¤×—\", \"××ª\", \"××ª×¤\", \"×‘××¤\", \"×‘××©\", \"×‘×‘× \", \"×‘×’×¦\", \"×‘×“×\", \"×‘×“×\",\n",
    "    \"×‘×“××©\", \"×‘×”× \", \"×‘×”×¢\", \"×‘×”×©\", \"×‘×™×“×\", \"×‘×™×“×¢\", \"×‘×œ\", \"×‘×œ×\", \"×‘×\", \"×‘×¢×\", \"×‘×¢×—\", \"×‘×¢×\", \"×‘×¢×§\", \"×‘×¤\",\n",
    "    \"×‘×¤×\", \"×‘×¤×ª\", \"×‘×¦×\", \"×‘×¦×”×\", \"×‘×§\", \"×‘×§×\", \"×‘×§×©×”\", \"×‘×¨×\", \"×‘×¨×¢\", \"×‘×¨×©\", \"×‘×©\", \"×‘×©×\",\n",
    "    \"×‘×©×’×¦\", \"×‘×©×”×ª\", \"×‘×©×–\", \"×‘×©×\", \"×‘×©×¢\", \"×‘×©×¤\", \"×‘×ª×ª\", \"×’×–×–\", \"×’××¨\", \"×’×¤\", \"×“×‘×¢\", \"×“×—\", \"×“×˜\", \"×“×™×•× \",\n",
    "    \"×“×\", \"×“××¨\", \"×“××©\", \"×“× \", \"×“× ×\", \"×“× ×’×¦\", \"×“× ×\", \"×“× ×¤\", \"×”×“\", \"×”×“×¤\", \"×”×•×¦×œ×¤\", \"×”×˜\", \"×”×›\", \"×”×\",\n",
    "    \"×”××“\", \"×”××\", \"×”××¢\", \"×”××©\", \"×”× \", \"×”×¡×ª\", \"×”×¢\", \"×”×¢×–\", \"×”×¤\", \"×”×¤×‘\", \"×”×¤×\", \"×”×¦×\", \"×”×©\", \"×”×©×\",\n",
    "    \"×”×©×’×¦\", \"×”×©×¤\", \"×”×©×¨\", \"×”×ª\", \"×•×—×§\", \"×•×¢\", \"×•×©×\", \"×•×©×§\", \"×•×©×¨\", \"×–×™\", \"×—×\", \"×—×‘×¨\", \"×—×“\", \"×—×“×\",\n",
    "    \"×—×“×œ×¤\", \"×—×“×œ×ª\", \"×—×“×\", \"×—×“×¤\", \"×—×”×¢\", \"×—×™\", \"×—× \", \"×—×¡×\", \"×—×¢×\", \"×—×¢×§\", \"×—×©\", \"×™×•×©\", \"×™×™×ª×\", \"×™××\",\n",
    "    \"×™×¡\", \"×›×¦\", \"×\", \"××\", \"××‘×›\", \"××‘×¡\", \"××•× ×•×¤×•×œ×™× \", \"××–×’\", \"××—\", \"××—×•×–\", \"××—×¢\", \"××˜\", \"××˜×›×œ\", \"××™\",\n",
    "    \"××™×‘\", \"××›\", \"××\", \"××¡\", \"××¡×˜\", \"××¢×™\", \"××¢×ª\", \"××§×\", \"××¨×›×–\", \"××ª\", \"× \", \"× ×‘\", \"× ×‘×\", \"× ×\", \"× ××‘\",\n",
    "    \"× ×¢×“\", \"× ×¢×¨\", \"×¡×‘×\", \"×¡×¢\", \"×¡×¢×©\", \"×¡×§\", \"×¡×§×›\", \"×¢\", \"×¢×\", \"×¢××—\", \"×¢××¤\", \"×¢×‘\", \"×¢×‘××¤\", \"×¢×‘×–\", \"×¢×‘×—\",\n",
    "    \"×¢×‘×™\", \"×¢×‘×œ\", \"×¢×‘××¦\", \"×¢×‘×¢×—\", \"×¢×‘×¤\", \"×¢×‘×¨\", \"×¢×‘×©×”×ª\", \"×¢×’×¨\", \"×¢×“×™\", \"×¢×“×\", \"×¢×”×’\", \"×¢×”×¡\", \"×¢×”×¤\",\n",
    "    \"×¢×•\", \"×¢×•×¨×¤\", \"×¢×–\", \"×¢×—\", \"×¢×—×\", \"×¢×—×“×œ×¤\", \"×¢×—×“×¤\", \"×¢×—×“×ª\", \"×¢×—×”×¡\", \"×¢×—×¢\", \"×¢×—×§\", \"×¢×—×¨\", \"×¢×›×‘\",\n",
    "    \"×¢×œ\", \"×¢×œ×\", \"×¢×œ×‘×©\", \"×¢×œ×—\", \"×¢×œ×¢\", \"×¢×\", \"×¢××\", \"×¢××”\", \"×¢××–\", \"×¢××—\", \"×¢××™\", \"×¢××œ×¢\", \"×¢××\", \"×¢×× \",\n",
    "    \"×¢××¤\", \"×¢××¦\", \"×¢××§\", \"×¢××¨×\", \"×¢××©\", \"×¢××©×\", \"×¢××ª\", \"×¢× \", \"×¢× ×\", \"×¢× ×\", \"×¢× ××\", \"×¢× ××©\", \"×¢× ×¤\",\n",
    "    \"×¢×¡×\", \"×¢×¡×§\", \"×¢×¢\", \"×¢×¢×\", \"×¢×¢×\", \"×¢×¢×¨\", \"×¢×¢×ª×\", \"×¢×¤\", \"×¢×¤×\", \"×¢×¤×’\", \"×¢×¤×”×’\", \"×¢×¤×\", \"×¢×¤××§\",\n",
    "    \"×¢×¤× \", \"×¢×¤×¡\", \"×¢×¤×¡×¤\", \"×¢×¤×¢\", \"×¢×¤×¨\", \"×¢×¤×ª\", \"×¢×¦×\", \"×¢×§\", \"×¢×§×’\", \"×¢×§×\", \"×¢×§× \", \"×¢×§×¤\", \"×¢×¨\", \"×¢×¨×\",\n",
    "    \"×¢×¨×’×¦\", \"×¢×¨×\", \"×¢×¨×¢×•×¨\", \"×¢×¨×¤\", \"×¢×¨×¨\", \"×¢×©\", \"×¢×©×\", \"×¢×©×\", \"×¢×©×¨\", \"×¢×©×ª\", \"×¢×©×ª×©\", \"×¢×ª\", \"×¢×ª×\",\n",
    "    \"×¢×ª×\", \"×¢×ª×¤×‘\", \"×¢×ª×¦\", \"×¤×\", \"×¤×”\", \"×¤×œ\", \"×¤×œ×\", \"×¤×\", \"×¤××¨\", \"×¤×¢×\", \"×¤×§×—\", \"×¤×¨\", \"×¤×¨×§\", \"×¤×©×–\",\n",
    "    \"×¤×©×¨\", \"×¤×ª\", \"×¦×\", \"×¦×‘× \", \"×¦×”\", \"×¦×•\", \"×¦×—\", \"×¦×\", \"×§×’\", \"×§×¤\", \"×¨×—×“×¤\", \"×¨××©\", \"×¨×¢\", \"×¨×¢×\", \"×¨×¢×‘\",\n",
    "    \"×¨×¢×‘×¡\", \"×¨×¢×•\", \"×¨×¢×\", \"×¨×¢×¡\", \"×¨×¢×¤\", \"×¨×¢×¤×\", \"×¨×¢×¦\", \"×¨×¢×¨\", \"×¨×¢×¨×¦\", \"×¨×¢×©\", \"×¨×¢×ª×\", \"×¨×¦×¤\", \"×¨×ª×§\",\n",
    "    \"×©\", \"×©×‘×“\", \"×©×\", \"×©××™\", \"×©× ×\", \"×©×¢\", \"×©×¢×\", \"×©×§\", \"×©×©\", \"×ª×\", \"×ª××“×\", \"×ª××—\", \"×ª××\", \"×ª××§\", \"×ª×‘\",\n",
    "    \"×ª×‘×›\", \"×ª×‘×¢\", \"×ª×’\", \"×ª×’×\", \"×ª×“\", \"×ª×“×\", \"×ª×”×’\", \"×ª×”× \", \"×ª×”×¡\", \"×ª×•×‘\", \"×ª×•×—\", \"×ª×—\", \"×ª×—×¤\", \"×ª×—×ª\",\n",
    "    \"×ª×˜\", \"×ª×™\", \"×ª×›\", \"×ª×œ×\", \"×ª×œ×‘\", \"×ª×œ×”×\", \"×ª×œ×¤\", \"×ª×œ×ª×\", \"×ª×\", \"×ª××”×—\", \"×ª××\", \"×ª××§\", \"×ª××¨\",\n",
    "    \"×ª××©\", \"×ª× ×’\", \"×ª× ×–\", \"×ª×¢\", \"×ª×¢×\", \"×ª×¢×–\", \"×ª×¤\", \"×ª×¤×‘\", \"×ª×¤×—\", \"×ª×¤×—×¢\", \"×ª×¤×›\", \"×ª×¤×\", \"×ª×¤×¢\",\n",
    "    \"×ª×¤×§\", \"×ª×¦\", \"×ª×§\", \"×ª×§×—\", \"×ª×§×\", \"×ª×¨×\", \"×ª×ª\", \"×ª×ª×—\", \"×ª×ª×¢\", \"×ª×ª×¢×\", \"×ª×ª×§\"\n",
    "]\n",
    "\n",
    "def create_acronym_variants(acronyms):\n",
    "    acronym_variants = []\n",
    "    for a in acronyms:\n",
    "        if len(a) > 1:\n",
    "            # Case 1: Original acronym with quotes/dots before last letter\n",
    "            base_acronym = a\n",
    "            if a.startswith('×‘') or a.startswith('×•') or a.startswith('×”'):\n",
    "                # Also add variant without the prefix letter\n",
    "                base_acronym = a[1:]\n",
    "            \n",
    "            # For each acronym (both with and without prefix)\n",
    "            for acr in [a, base_acronym]:\n",
    "                if len(acr) > 1:\n",
    "                    # Standard quote/dot before last letter\n",
    "                    quoted = rf\"{acr[:-1]}[\\\"'×´]{acr[-1]}\"\n",
    "                    with_dot = rf\"{acr[:-1]}\\.{acr[-1]}\"\n",
    "                    acronym_variants.append(f\"(?:{quoted}|{with_dot})\")\n",
    "                    \n",
    "                    # Add dot-separated variant\n",
    "                    dots_between = '\\.'.join(list(acr))\n",
    "                    acronym_variants.append(dots_between)\n",
    "    \n",
    "    return '|'.join(acronym_variants)\n",
    "        \n",
    "acronym_pattern = create_acronym_variants(acronyms)\n",
    "\n",
    "# Ensure the numbers follow the correct format\n",
    "number_pattern = r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "'''\n",
    "citation_pattern = fr'''\n",
    "    (?<!\\w)                      # Ensure no letter before\n",
    "    ([×-×ª]?)                     # Optional single Hebrew prefix letter (but no isolated matches)\n",
    "    ({acronym_pattern})           # Captures acronym (short & long)\n",
    "    \\.?                          # Optional dot after acronym\n",
    "    \\s*                          # Optional spaces\n",
    "    (\\((.*?)\\))?                  # Optional court location in parentheses\n",
    "    \\s*[-/]?\\s*                  # Required space or separator before case number\n",
    "    ({number_pattern})            # Captures case number formats\n",
    "    (?!\\w)                       # Ensure no letter after\n",
    "'''.strip()\n",
    "\n",
    "# Compile regex with verbose flag for readability\n",
    "citation_regex = re.compile(citation_pattern, re.VERBOSE)\n",
    "\n",
    "\n",
    "def extract_citations_from_csv(csv_data):\n",
    "    citations = []\n",
    "    text_column = csv_data[\"text\"].astype(str)  # Convert to string to avoid NaN issues\n",
    "    pd.set_option(\"display.max_colwidth\", None)  # Ensure full text is displayed\n",
    "    # print(\"\\n\".join(text_column))  # Print each row as a full text\n",
    "    # for i, text in enumerate(text_column, 1):\n",
    "    #     print(f\"{i}. {text}\")\n",
    "\n",
    "    matches = text_column.str.extractall(citation_regex)  # Extract structured matches\n",
    "    # print(\"Extracted Matches:\")\n",
    "    # print(matches)\n",
    "\n",
    "    # print(\"Extracted DataFrame:\", matches)  # Debugging step\n",
    "    \n",
    "    for _, row in matches.iterrows():\n",
    "        # Build the citation string, joining all valid elements\n",
    "        citation = \" \".join(map(str, filter(pd.notna, row))).strip()\n",
    "\n",
    "        # Clean up extra spaces\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "\n",
    "\n",
    "        # # Remove invalid extra words (e.g., \"×¢×œ 12\")\n",
    "        if re.match(r\"^×¢×œ \\d+$\", citation):  \n",
    "            print('Skip invalid cases like \"×¢×œ 12')\n",
    "            continue  # Skip invalid cases like \"×¢×œ 12\"\n",
    "\n",
    "        # Fix duplicated court locations, e.g., \"(××—×•×–×™ ××¨×›×–) ××—×•×–×™ ××¨×›×–\" â†’ \"(××—×•×–×™ ××¨×›×–)\"\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        citation=clean_leading_prefix(citation)\n",
    "        # Add the cleaned citation to the list\n",
    "        citations.append(citation)\n",
    "    \n",
    "    # Return citations as a list, even if some are empty or missing optional groups\n",
    "    return citations if citations else []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import docx\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" \n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Define required sections and citation patterns\n",
    "required_parts = [\n",
    "    \"××ª×—××™ ×¢× ×™×©×”\", \"××—×™×“×•×ª ×‘×¢× ×™×©×”\", \"××ª×—× ×”×¢× ×™×©×”\", \"××ª×—× ×¢× ×™×©×”\", \"×“×™×•×Ÿ\",\n",
    "    \"×¢× ×™×©×” × ×”×•×’×”\", \"×”×¢× ×™×©×” ×”× ×•×”×’×ª\", \"×¢× ×™×©×” × ×•×”×’×ª\", \"××ª×—× ×”×¢×•× ×©\", \"××ª×—× ×¢×•× ×©\",\n",
    "    \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\", \"×•×”×›×¨×¢×”\", \"×”×”×¨×©×¢×”\", \"××“×™× ×™×•×ª ×”×¢× ×™×©×” ×”× ×”×•×’×”\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model_path = \"/home/liorkob/classifier_relvant_citation_model.pt\" \n",
    "tokenizer_bert = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model_bert.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "def split_preserving_structure(text):\n",
    "    paragraphs = re.split(r'(?<=\\d\\.)\\s', text)  # Split after numbers followed by a period\n",
    "    return [para.strip() for para in paragraphs if para.strip()]\n",
    "\n",
    "def query_gpt(text,citation):\n",
    "    \"\"\"\n",
    "    Queries gpt-4.1-mini to extract and segment legal citations.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following legal text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Your task is to extract **only** the part of the text that directly relates to the citation \"{citation}\".\n",
    "    \n",
    "    **Extraction Rules:**\n",
    "    - **Do not modify any wording.** Keep the original phrasing exactly as it appears in the provided document.\n",
    "    - **Do not summarize or rephrase.**\n",
    "    - **Return only the relevant portion**, not the full text.\n",
    "    - **Handle grouped citations carefully:**\n",
    "        - If the citation appears in a list following \"×¨××• ×œ××©×œ ...\" or similar, include the preceding explanation that applies to all citations.\n",
    "        - Do not include other citations from the listâ€”return only the text relevant to \"{citation}\".\n",
    "    - **Handle case explanations properly:**\n",
    "        - If the citation is explained in a specific section (e.g., \"×‘×¢\"×¤ 9373/10 ×•×ª×“ × ' ××“×™× ×ª ×™×©×¨××œ...\"), extract the **entire explanation** of the case.\n",
    "        - Do not remove any important context about the court ruling.\n",
    "    - Do **not** extract only \"(×¨×¢\"×¤ 2718/04)\" without the legal principle it supports.\n",
    "\n",
    "\n",
    "    Only return the extracted text. Do not include unrelated content or formatting.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ§  Sending to GPT for extraction...\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to extract and structure legal citations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        processed_text = response.choices[0].message.content\n",
    "        return processed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ GPT API error: {e}\")\n",
    "        return [text]  # Return original text in case of failure\n",
    "    \n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a required part in the CSV and all subsequent rows.\n",
    "    \"\"\"\n",
    "    start_index = None\n",
    "\n",
    "    # Find the first row containing a required part\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    # If a match is found, return only relevant rows\n",
    "    if start_index is not None:\n",
    "        return csv_data.iloc[start_index:]\n",
    "    else:\n",
    "        # print(\"NO required parts in data\")\n",
    "        # print(\"parts in data:\")\n",
    "        # print(csv_data[\"part\"].unique())\n",
    "        return pd.DataFrame(columns=csv_data.columns)  # Return an empty DataFrame if no matches found\n",
    "\n",
    "\n",
    "\n",
    "# Function to find all occurrences of a citation in the document\n",
    "def find_all_occurrences(doc, citation):\n",
    "    indices = []\n",
    "    for i, paragraph in enumerate(doc.paragraphs):\n",
    "        if citation in paragraph.text:\n",
    "            indices.append(i)  # Store all occurrences of the citation\n",
    "    return indices\n",
    "\n",
    "# Function to get relevant context for each occurrence of the citation\n",
    "def get_context_paragraphs(doc, index, citation):\n",
    "    context_text = []\n",
    "\n",
    "    # Search for the closest non-empty previous paragraph\n",
    "    prev_index = index - 1\n",
    "    while prev_index >= 0 and not doc.paragraphs[prev_index].text.strip():\n",
    "        prev_index -= 1  # Move backwards until finding text\n",
    "\n",
    "    if prev_index >= 0:\n",
    "        context_text.append(doc.paragraphs[prev_index].text.strip())\n",
    "\n",
    "    # Get the current paragraph (must exist, but check if empty)\n",
    "    curr_text = doc.paragraphs[index].text.strip()\n",
    "    if curr_text:\n",
    "        context_text.append(curr_text)\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: Empty paragraph for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None  # Skip this occurrence if the current paragraph is empty\n",
    "\n",
    "    # Search for the closest non-empty next paragraph\n",
    "    next_index = index + 1\n",
    "    while next_index < len(doc.paragraphs) and not doc.paragraphs[next_index].text.strip():\n",
    "        next_index += 1  # Move forward until finding text\n",
    "\n",
    "    if next_index < len(doc.paragraphs):\n",
    "        context_text.append(doc.paragraphs[next_index].text.strip())\n",
    "\n",
    "    # Ensure we have at least one non-empty paragraph\n",
    "    if not context_text:\n",
    "        print(f\"âš ï¸ Warning: No valid text found for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None\n",
    "\n",
    "    return \"\\n\".join(context_text).strip()\n",
    "def normalize_case_name_2(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name)\n",
    "    name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "    name = re.sub(r\"[âˆ•/\\\\]\", \"-\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = name.strip().lower().replace(\" \", \"_\")\n",
    "    return name\n",
    "\n",
    "\n",
    "# Function to process and tag document paragraphs\n",
    "def process_and_tag_with_split(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a .docx document and its corresponding CSV, find relevant paragraphs with context, \n",
    "    extract relevant text using GPT, tag with BERT, and store results.\n",
    "    \"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    filtered_csv_data = filter_csv_relevant_parts(csv_data)\n",
    "    if filtered_csv_data.empty:\n",
    "        # print(\"âš ï¸ Skipping file â€” no relevant parts found.\")\n",
    "        return\n",
    "\n",
    "    citations = extract_citations_from_csv(filtered_csv_data)\n",
    "    results = []\n",
    "    if len(citations) > 30:\n",
    "        print(f\"TOO MANY CITATIONS IN CSV Found {len(citations)}\")\n",
    "        print(docx_path)\n",
    "\n",
    "\n",
    "        return\n",
    "    print(f\"ğŸ” Found {len(citations)} citations in CSV\")\n",
    "\n",
    "    for citation in citations:\n",
    "        citation_indices = find_all_occurrences(doc, citation)  # Find all occurrences\n",
    "\n",
    "        # Collect all contexts where the citation appears\n",
    "        merged_contexts = []\n",
    "        for index in citation_indices:\n",
    "            full_context = get_context_paragraphs(doc, index, citation)\n",
    "            if full_context:\n",
    "                merged_contexts.append(full_context)\n",
    "\n",
    "        # If no valid contexts found, skip this citation\n",
    "        if not merged_contexts:\n",
    "            continue  \n",
    "\n",
    "        # Merge all valid contexts into one, ensuring uniqueness\n",
    "        final_context = \"\\n\".join(set(merged_contexts)).strip()  # Remove duplicates\n",
    "        # print(citation)\n",
    "        # print(final_context)\n",
    "\n",
    "        # Ask GPT to extract the relevant part\n",
    "        extracted_text = query_gpt(final_context, citation)\n",
    "\n",
    "        # Tag the extracted text with BERT\n",
    "        encoding = tokenizer_bert(extracted_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "        encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model_bert(**encoding)\n",
    "            prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "\n",
    "        citation=normalize_case_name_2(citation)    \n",
    "        # Store only one result per citation\n",
    "        result = {\n",
    "            'citation': citation,\n",
    "            'context_text': final_context,\n",
    "            'extracted_text': extracted_text,\n",
    "            'predicted_label': prediction\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed document saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    docx_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs_3k/docx/verdict')\n",
    "    csv_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs_3k/verdict_csv')\n",
    "    output_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdicts_tagged_citations')\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Stats counters\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    skipped_empty_or_missing = 0\n",
    "    missing_csv = 0\n",
    "    files_with_citations = 0\n",
    "    total_citations = 0\n",
    "    total_tagged_as_1 = 0\n",
    "\n",
    "    all_files = list(docx_directory.glob(\"*.docx\"))\n",
    "    print(f\"ğŸ—‚ Total DOCX files found: {len(all_files)}\")\n",
    "\n",
    "    for file_path in tqdm(all_files, desc=\"Processing DOCX files\"):\n",
    "        total_files += 1\n",
    "        new_file_path = file_path.stem\n",
    "        csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "        output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "\n",
    "        if not csv_file.exists():\n",
    "            print(f\"CSV file not found for: {csv_file}\")\n",
    "            missing_csv += 1\n",
    "            continue\n",
    "\n",
    "        if output_file.exists() and output_file.stat().st_size > 0:\n",
    "            try:\n",
    "                df_existing = pd.read_csv(output_file)\n",
    "                num_citations = len(df_existing)\n",
    "                num_tagged_1 = (df_existing[\"predicted_label\"] == 1).sum()\n",
    "\n",
    "                total_citations += num_citations\n",
    "                total_tagged_as_1 += num_tagged_1\n",
    "                files_with_citations += 1\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error reading {output_file.name}: {e}\")\n",
    "                skipped_empty_or_missing += 1\n",
    "                continue\n",
    "\n",
    "        if output_file.exists() and output_file.stat().st_size == 0:\n",
    "            skipped_empty_or_missing += 1\n",
    "            continue\n",
    "\n",
    "        if not output_file.exists():\n",
    "            process_and_tag_with_split(str(file_path), str(csv_file), str(output_file))\n",
    "            if output_file.exists() and output_file.stat().st_size > 0:\n",
    "                try:\n",
    "                    df_new = pd.read_csv(output_file)\n",
    "                    num_citations = len(df_new)\n",
    "                    num_tagged_1 = (df_new[\"predicted_label\"] == 1).sum()\n",
    "                    total_citations += num_citations\n",
    "                    total_tagged_as_1 += num_tagged_1\n",
    "                    files_with_citations += 1\n",
    "                    processed_files += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Failed to read newly written output: {output_file.name}\")\n",
    "                    skipped_empty_or_missing += 1\n",
    "\n",
    "    # Averages\n",
    "    avg_citations_per_file = total_citations / files_with_citations if files_with_citations else 0\n",
    "    avg_tagged_1_per_file = total_tagged_as_1 / files_with_citations if files_with_citations else 0\n",
    "\n",
    "    print(\"\\n===== ğŸ“Š Processing Summary =====\")\n",
    "    print(f\"Total DOCX files:               {total_files}\")\n",
    "    print(f\"Processed files:                {files_with_citations}  # output file exists and is not empty\")\n",
    "    print(f\"Skipped (already processed):    {total_files - files_with_citations}  # output missing or empty\")\n",
    "    print(f\"Missing CSV files:              {missing_csv}\")\n",
    "    print(f\"Files with citation data:       {files_with_citations}\")\n",
    "    print(f\"Total citations:                {total_citations}\")\n",
    "    print(f\"Total tagged as 1:              {total_tagged_as_1}\")\n",
    "    print(f\"Avg citations per file:         {avg_citations_per_file:.2f}\")\n",
    "    print(f\"Avg tagged=1 per file:          {avg_tagged_1_per_file:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_part_only_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×” ×›×œ×œ×™×ª =====\n",
      "×¡×”\"×› ×§×‘×¦×™× ×©× ×‘×“×§×•:               3046\n",
      "×§×‘×¦×™× ×¢× ×—×œ×§ ×¨×œ×•×•× ×˜×™:            2491\n",
      "×¡×”\"×› ×¦×™×˜×•×˜×™× ×©× ××¦××• ×‘-part:      163\n",
      "ğŸ” ××¡×¤×¨ ×“×•×’×××•×ª ××•×¦×’×•×ª:           28\n",
      "\n",
      "===== ğŸ§¾ ×“×•×’×××•×ª =====\n",
      "\n",
      "ğŸ“ ×§×•×‘×¥: ×ª×¤_41970-12-21.csv\n",
      "ğŸ“Œ ×¦×™×˜×•×˜: ×ª\"×¤ 19611-03\n",
      "ğŸ“‚ part: ×ª\"×¤ 19611-03-21 ×¢×•×¡×§ ×‘×¢×‘×™×¨×ª ×”×—×–×§×ª ×¡××™× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª.\n",
      "ğŸ“ text: × ×’×¢ ×”×¡××™× ×¤×•×’×¢ ×‘×¦×™×‘×•×¨ ×§×©×•×ª. ×§×˜×™× ×™×, ×¦×¢×™×¨×™× ×•×‘×•×’×¨×™× × ×—×©×¤×™× ××œ×™×•, ×œ×¢×ª×™× ×‘××§×¨××™, ××ª× ×¡×™× ×‘×©×™××•×© ×‘×¡× ×•×‘××§×¨×™× ×¨×‘×™× ××ª××›×¨×™× ×œ×•. ×”×©×™××•×© ×‘×¡× ×¤×•×’×¢ ×œ× ×¨×§ ×‘××©×ª××©×™× ×‘×• ×•×‘×‘×¨×™××•×ª×, ××œ× ×’× ×‘××¢×’×œ×™× ×”×§×¨×•×‘×™× ×•×”×¨×—×•×§×™× ××”×, ×‘×©×œ ×”×”×©×œ×›×•×ª ×”×¤×•×’×¢× ×™×•×ª ×©×œ ×”×©×™××•×© ×‘×•- ×”×”×ª××›×¨×•×ª, ×”×¢×‘×¨×™×™× ×•×ª ×”× ×œ×•×•×ª ×œ×©×™××•×© ×‘××§×¨×™× ×¨×‘×™×, ×•×”×©×¤×¢×•×ª ×©×œ×™×œ×™×•×ª × ×•×¡×¤×•×ª.\n",
      "\n",
      "ğŸ“ ×§×•×‘×¥: ×ª×¤_41970-12-21.csv\n",
      "ğŸ“Œ ×¦×™×˜×•×˜: ×ª\"×¤ 19611-03\n",
      "ğŸ“‚ part: ×ª\"×¤ 19611-03-21 ×¢×•×¡×§ ×‘×¢×‘×™×¨×ª ×”×—×–×§×ª ×¡××™× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª.\n",
      "ğŸ“ text: ×¢×‘×™×¨×•×ª ×”×”×¤×¦×” ×•×”×¡×—×¨ ×‘×¡××™×, ×”×Ÿ ××œ×” ×”××‘×™××•×ª ×œ×”×ª×¤×©×˜×•×ª ×”× ×’×¢ ×•×œ×”×™×•×ª×• ×–××™×Ÿ ×œ×›×œ, ×•××›××Ÿ ×”×—×•××¨×” ×©×™×© ×‘×”×Ÿ. ×›×›×œ ×©×”×¢×‘×¨×™×™×Ÿ ×××§×•× ×’×‘×•×” ×™×•×ª×¨ ×‘×©×¨×©×¨×ª ×”×¤×¦×ª ×”×¡×, ×—×œ×§×• ×‘×”×¤×¦×ª×• ×’×“×•×œ ×™×•×ª×¨, ×•×‘×”×ª×××”- ×¢×•× ×©×• ×™×”× ×—××•×¨ ×™×•×ª×¨.\n",
      "\n",
      "ğŸ“ ×§×•×‘×¥: ×ª×¤_43665-08-21.csv\n",
      "ğŸ“Œ ×¦×™×˜×•×˜: ×ª\"×¤ 1324-01\n",
      "ğŸ“‚ part: ×ª\"×¤ 1324-01-22\n",
      "ğŸ“ text: ×”××œ×—××” ×‘× ×’×¢ ×”×¡××™× ×”×™× ×” ×§×©×” ×•×‘×œ×ª×™ ×¤×•×¡×§×ª, ×¢×‘×™×¨×•×ª ×”×¡××™× ×¤×•×’×¢×ª ×‘×¢×¨×›×™× ×—×‘×¨×ª×™×™× ×©×œ ×”×’× ×” ×¢×œ ×”×¦×™×‘×•×¨ ×‘×›×œ×œ×•×ª×• ××¤× ×™ ×”× ×–×§×™× ×”×™×©×™×¨×™× ×•×”×¢×§×™×¤×™× ××©×¨ × ×’×¨××™× ×¢×§×‘ ×”×©×™××•×© ×‘×¡××™× ×•×–××ª ×œ×¦×“ ×”× ×–×§×™× ×”×›×œ×›×œ×™×™× ×•×”×—×‘×¨×ª×™×™× ×”× ×’×¨××™× ×¢×§×‘ ×©×™××•×© ×‘×¡××™×.  ××™×“×ª ×”×¤×’×™×¢×” ×‘×¢×¨×š ×”××•×’×Ÿ  ×‘×¢×‘×™×¨×” ×©×œ ×”×—×–×§×ª ×¡× ×œ×¦×¨×™×›×” ×¢×¦××™×ª ×”×™× ×” ×‘×¨×£ ×”× ××•×š. ×œ×¢× ×™×™×Ÿ ×”×”×©×¤×¢×•×ª ×”×”×¨×¡× ×™×•×ª ×©×œ ×¡× ×”×§×•×§××™×Ÿ, ×¨××” ×”× ×××¨ ×‘×¢\"×¤ 972/11 ××“×™× ×ª ×™×©×¨××œ × ' ×™×•× ×” (04.07.12).\n",
      "\n",
      "ğŸ“ ×§×•×‘×¥: ×ª×¤_43665-08-21.csv\n",
      "ğŸ“Œ ×¦×™×˜×•×˜: ×ª\"×¤ 1324-01\n",
      "ğŸ“‚ part: ×ª\"×¤ 1324-01-22\n",
      "ğŸ“ text: ×‘××¡×’×¨×ª ×”× ×¡×™×‘×•×ª ×”×§×©×•×¨×•×ª ×‘×‘×™×¦×•×¢ ×”×¢×‘×™×¨×” (×¡×¢×™×£ 40 ×˜' ×œ×—×•×§), ×™×© ×œ×ª×ª ××ª ×”×“×¢×ª ×œ×©×™×§×•×œ×™× ×”×‘××™×:  ×”× ××©× × ×ª×¤×¡ ××—×–×™×§ ×¡× ××¡×•×’ ×§×•×§××™×Ÿ ×‘××©×§×œ 0.0946 ×’×¨×, ×¢×œ ×’×•×¤×• ××•×¡×œ×§ ×‘××’×‘×•×Ÿ.\n",
      "\n",
      "ğŸ“ ×§×•×‘×¥: ×ª×_40004-08-18.csv\n",
      "ğŸ“Œ ×¦×™×˜×•×˜: ×ª\"×¤ 47326-10\n",
      "ğŸ“‚ part: ×ª\"×¤ 47326-10-18\n",
      "ğŸ“ text: ×›×ª×‘ ×”××™×©×•× ×‘×ª×™×§ ×–×” ×›×•×œ×œ ×©× ×™ ××™×©×•××™× ×‘×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡××™× ××¡×•×›× ×™× ×œ××•×ª×• ×¡×•×›×Ÿ ××©×˜×¨×ª×™, ××š ×‘×™× ×™×”× ××¤×¨×™×“×™× ×—×•×“×©×™×™× ×•××™×Ÿ ×‘×™× ×™×”× ×§×©×¨ ×”×“×•×§.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === ×ª×‘× ×™×ª ×‘×¡×™×¡×™×ª ×œ×¦×™×˜×•×˜ ××©×¤×˜×™ (×¨×§ ×ª\"×¤ ×œ×¤×™ ×”×‘×§×©×” ×©×œ×š) ===\n",
    "citation_regex = re.compile(r'(?<!\\w)(×ª\"×¤\\s*\\d{1,6}[-/]\\d{2})(?!\\w)')\n",
    "\n",
    "# === ×¨×©×™××ª ×—×œ×§×™× ×¨×œ×•×•× ×˜×™×™× ×œ×¡×™× ×•×Ÿ ×¨××©×•× ×™ ===\n",
    "required_parts = [\n",
    "    \"××ª×—××™ ×¢× ×™×©×”\", \"××—×™×“×•×ª ×‘×¢× ×™×©×”\", \"××ª×—× ×”×¢× ×™×©×”\", \"××ª×—× ×¢× ×™×©×”\", \"×“×™×•×Ÿ\",\n",
    "    \"×¢× ×™×©×” × ×”×•×’×”\", \"×”×¢× ×™×©×” ×”× ×•×”×’×ª\", \"×¢× ×™×©×” × ×•×”×’×ª\", \"××ª×—× ×”×¢×•× ×©\", \"××ª×—× ×¢×•× ×©\",\n",
    "    \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\", \"×•×”×›×¨×¢×”\", \"×”×”×¨×©×¢×”\", \"××“×™× ×™×•×ª ×”×¢× ×™×©×” ×”× ×”×•×’×”\"\n",
    "]\n",
    "\n",
    "# === × ×ª×™×‘ ×œ×ª×™×§×™×™×ª ×”×§×‘×¦×™× ===\n",
    "csv_dir = Path(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/verdict_csv\")\n",
    "\n",
    "# === ×¡×˜×˜×™×¡×˜×™×§×•×ª ===\n",
    "total_files = 0\n",
    "files_with_required_part = 0\n",
    "total_citations_found = 0\n",
    "example_rows = []\n",
    "\n",
    "# === ×¤×•× ×§×¦×™×” ×œ×¡×™× ×•×Ÿ ×”×—×œ×§ ×”×¨×œ×•×•× ×˜×™ ×•×œ×§×˜×™×¤×ª ×¦×™×˜×•×˜×™× ××ª×•×š part ===\n",
    "def filter_and_find_citations(csv_data):\n",
    "    start_index = None\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        part_val = str(row.get(\"part\", \"\"))\n",
    "        if any(req in part_val for req in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    if start_index is not None:\n",
    "        filtered = csv_data.iloc[start_index:]\n",
    "        citations = []\n",
    "        for row in filtered.itertuples():\n",
    "            part_text = str(getattr(row, \"part\", \"\"))\n",
    "            matches = citation_regex.findall(part_text)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    citations.append((match, part_text.strip(), getattr(row, \"text\", \"\").strip()))\n",
    "        return filtered, citations\n",
    "    return pd.DataFrame(columns=csv_data.columns), []\n",
    "\n",
    "# === ××¢×‘×¨ ×¢×œ ×›×œ ×”×§×‘×¦×™× ===\n",
    "for file_path in csv_dir.glob(\"*.csv\"):\n",
    "    total_files += 1\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'part' not in df.columns or 'text' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        filtered_df, citations = filter_and_find_citations(df)\n",
    "\n",
    "        if not filtered_df.empty:\n",
    "            files_with_required_part += 1\n",
    "            total_citations_found += len(citations)\n",
    "            if citations:\n",
    "                for citation, part, text in citations[:2]:  # ×©××•×¨ ×¢×“ 2 ×“×•×’×××•×ª ×œ×§×•×‘×¥\n",
    "                    example_rows.append({\n",
    "                        \"file\": file_path.name,\n",
    "                        \"citation\": citation,\n",
    "                        \"part\": part,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ×©×’×™××” ×‘×§×•×‘×¥ {file_path.name}: {e}\")\n",
    "\n",
    "# === ×”×“×¤×¡×” ××¡×›××ª ===\n",
    "print(f\"\\n===== ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×” ×›×œ×œ×™×ª =====\")\n",
    "print(f\"×¡×”\\\"×› ×§×‘×¦×™× ×©× ×‘×“×§×•:               {total_files}\")\n",
    "print(f\"×§×‘×¦×™× ×¢× ×—×œ×§ ×¨×œ×•×•× ×˜×™:            {files_with_required_part}\")\n",
    "print(f\"×¡×”\\\"×› ×¦×™×˜×•×˜×™× ×©× ××¦××• ×‘-part:      {total_citations_found}\")\n",
    "print(f\"ğŸ” ××¡×¤×¨ ×“×•×’×××•×ª ××•×¦×’×•×ª:           {len(example_rows)}\")\n",
    "\n",
    "print(\"\\n===== ğŸ§¾ ×“×•×’×××•×ª =====\")\n",
    "for row in example_rows[:5]:\n",
    "    print(f\"\\nğŸ“ ×§×•×‘×¥: {row['file']}\")\n",
    "    print(f\"ğŸ“Œ ×¦×™×˜×•×˜: {row['citation']}\")\n",
    "    print(f\"ğŸ“‚ part: {row['part']}\")\n",
    "    print(f\"ğŸ“ text: {row['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get URLS from docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx.oxml.ns import qn\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    case_name = case_name.replace('×´', '\"')\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('âˆ•', '/')).strip()\n",
    "\n",
    "\n",
    "# def normalize_citation(citation):\n",
    "#     \"\"\"Normalize citation by removing prefixes and standardizing format.\"\"\"\n",
    "#     if not citation:\n",
    "#         return None\n",
    "#     # Standardize quotes\n",
    "#     citation = citation.replace('×´', '\"').replace('×´', '\"').replace('×´', '\"')\n",
    "#     # Remove extra spaces\n",
    "#     citation = re.sub(r'\\s+', ' ', citation).strip()\n",
    "#     # Remove common prefixes, including ×¨×¢\"×¤\n",
    "#     citation = re.sub(r'^(×¢\"?×¤|×ª\"?×¤|×¢×¤\"?×’|×¨×¢\"?×¤)\\s+', '', citation)\n",
    "#     return citation\n",
    "\n",
    "\n",
    "def extract_citations(text):\n",
    "    \"\"\"Extracts legal citations from a single text string.\"\"\"\n",
    "    matches = citation_regex.findall(text)\n",
    "    citations = []\n",
    "    for match in matches:\n",
    "        citation = \" \".join(filter(None, match)).strip()\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        if not re.match(r\"^×¢×œ \\d+$\", citation):\n",
    "            citations.append(citation)\n",
    "    return citations[0] if citations else None\n",
    "\n",
    "\n",
    "def getLinkedText(soup):\n",
    "    links = []\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            parts = tag.text.split('\"')\n",
    "            if len(parts) > 1:  # Ensure the URL exists before accessing index 1\n",
    "                url = parts[1]\n",
    "            else:\n",
    "                print(f\"âš ï¸ Warning: No valid URL found in HYPERLINK tag: {tag.text}\")\n",
    "                url = None  # Assign None if URL is missing\n",
    "\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None and maybe_text.text.strip() != \"\":\n",
    "                    text += maybe_text.text.strip()\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None and maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                    break\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "    return links\n",
    "def getURLs(soup, links):\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            for rel in soup.find_all(\"Relationship\"):\n",
    "                if rel[\"Id\"] == link[\"id\"]:\n",
    "                    link[\"href\"] = rel[\"Target\"]\n",
    "    return links\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_hyperlinks(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from a .docx file and returns a dictionary \n",
    "    where the linked text is mapped to its corresponding URL.\n",
    "    \"\"\"\n",
    "    # Open the .docx file as a zip archive\n",
    "    try:\n",
    "        archive = zipfile.ZipFile(docx_path, \"r\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"âŒ Error: Cannot open {docx_path} (Bad ZIP format)\")\n",
    "        return {}\n",
    "\n",
    "    # Extract main document XML\n",
    "    try:\n",
    "        file_data = archive.read(\"word/document.xml\")\n",
    "        doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "        linked_text = getLinkedText(doc_soup)\n",
    "    except KeyError:\n",
    "        print(f\"âš ï¸ Warning: No document.xml found in {docx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Extract hyperlink relationships from _rels/document.xml.rels\n",
    "    try:\n",
    "        url_data = archive.read(\"word/_rels/document.xml.rels\")\n",
    "        url_soup = BeautifulSoup(url_data, \"xml\")\n",
    "        links_with_urls = getURLs(url_soup, linked_text)\n",
    "    except KeyError:\n",
    "        print(f\"âš ï¸ Warning: No _rels/document.xml.rels found in {docx_path}\")\n",
    "        links_with_urls = linked_text\n",
    "\n",
    "    # Extract footnotes (if available)\n",
    "    try:\n",
    "        footnote_data = archive.read(\"word/footnotes.xml\")\n",
    "        footnote_soup = BeautifulSoup(footnote_data, \"xml\")\n",
    "        footnote_links = getLinkedText(footnote_soup)\n",
    "\n",
    "        footnote_url_data = archive.read(\"word/_rels/footnotes.xml.rels\")\n",
    "        footnote_url_soup = BeautifulSoup(footnote_url_data, \"xml\")\n",
    "        footnote_links_with_urls = getURLs(footnote_url_soup, footnote_links)\n",
    "\n",
    "        # Merge footnote links\n",
    "        links_with_urls += footnote_links_with_urls\n",
    "    except KeyError:\n",
    "        pass  # No footnotes found, continue\n",
    "\n",
    "    # Convert extracted links to a dictionary: {linked_text: URL}\n",
    "    return {link[\"text\"]: link.get(\"href\", None) for link in links_with_urls}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def update_csv_with_links(csv_path, doc_path):\n",
    "    csv_path = Path(csv_path)  # Convert to Path object if not already\n",
    "    \n",
    "    # **Check if CSV is empty before reading**\n",
    "    if not csv_path.exists() or csv_path.stat().st_size == 0:  \n",
    "        print(f\"Skipping empty or missing file: {csv_path.name}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        # **Check if the DataFrame is empty after loading**\n",
    "        if df.empty:\n",
    "            print(f\"Skipping empty DataFrame: {csv_path.name}\")\n",
    "            return\n",
    "        # Skip if file already updated with links\n",
    "        if \"link\" in df.columns and df[\"link\"].notna().sum() > 0:\n",
    "            print(f\"â­ï¸ Skipping {csv_path.name} â€” already has links.\")\n",
    "            return\n",
    "\n",
    "        # Normalize extracted citations\n",
    "        df[\"extracted_citation\"] = df[\"context_text\"].apply(\n",
    "            lambda text: normalize_case_name(cit) if (pd.notna(text) and (cit := extract_citations(text))) else None\n",
    "        )\n",
    "        \n",
    "        # Normalize citation_links keys\n",
    "        citation_links = extract_hyperlinks(doc_path)\n",
    "        normalized_citation_links = {normalize_case_name(k): v for k, v in citation_links.items()}\n",
    "        \n",
    "        # Assign URLs to citations\n",
    "        df[\"link\"] = df[\"extracted_citation\"].apply(\n",
    "            lambda text: normalized_citation_links.get(text, None) if pd.notna(text) else None\n",
    "        )\n",
    "        # Print only the rows that got a link\n",
    "        # linked_rows = df[df[\"link\"].notna()]\n",
    "        # if not linked_rows.empty:\n",
    "        #     print(f\"ğŸ”— {len(linked_rows)} rows updated with links in: {csv_path.name}\")\n",
    "        #     print(linked_rows[[\"extracted_citation\", \"link\"]].head(5).to_string(index=False))  # Show first 5\n",
    "\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated CSV saved to: {csv_path}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Skipping {csv_path.name}: CSV file is empty or unreadable.\")\n",
    "        return\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"âš ï¸ Skipping (encoding error): {csv_path}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def find_matching_docx(csv_name, docx_directory):\n",
    "    normalized_csv_name = normalize_case_name(csv_name.replace('.csv', '.docx'))\n",
    "    for root, _, files in os.walk(docx_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".docx\") and normalize_case_name(file) == normalized_csv_name:\n",
    "                return os.path.join(root, file)\n",
    "    print(f\"âŒ No match found for {csv_name}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_all_csvs(citations_dir, docx_directory):\n",
    "    # Collect all CSV files first\n",
    "    all_csv_files = []\n",
    "    for root, _, files in os.walk(citations_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                all_csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Iterate with tqdm progress bar\n",
    "    for csv_path in tqdm(all_csv_files, desc=\"ğŸ” Processing CSVs\"):\n",
    "        file = os.path.basename(csv_path)\n",
    "        docx_path = find_matching_docx(file, docx_directory)\n",
    "\n",
    "        if docx_path:\n",
    "            update_csv_with_links(csv_path, docx_path)\n",
    "        else:\n",
    "            print(f\"âŒ No matching DOCX found for: {file}\")\n",
    "\n",
    "docx_dir = f\"/home/liorkob/M.Sc/thesis/data/5k/docx/verdict\"\n",
    "citations_dir = f\"/home/liorkob/M.Sc/thesis/data/5k/gpt/verdict_tagged_citations\"\n",
    "process_all_csvs(citations_dir, docx_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrate smilar pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import difflib\n",
    "def normalize_case_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name)\n",
    "    name = name.replace('\"', '').replace(\"×´\", '').replace(\"'\", \"\")\n",
    "    name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "    name = re.sub(r\"[âˆ•/\\\\]\", \"-\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = name.strip().lower().replace(\" \", \"_\")\n",
    "    return name\n",
    "\n",
    "# def normalize_case_name(name):\n",
    "#     if pd.isna(name):\n",
    "#         return \"\"\n",
    "#     name = str(name)\n",
    "#     name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "#     name = re.sub(r\"[âˆ•/\\\\]\", \"-\", name)\n",
    "#     name = re.sub(r\"\\s+\", \" \", name)\n",
    "#     name = name.strip().lower().replace(\" \", \"_\")\n",
    "#     return name\n",
    "\n",
    "def get_only_numbers(name):\n",
    "    return ''.join(filter(str.isdigit, name))\n",
    "\n",
    "# Load and normalize all verdicts\n",
    "df1 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\")\n",
    "# df2 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_appeals_with_gpt_2.csv\")\n",
    "# combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Map normalized verdicts to their extracted_gpt_facts\n",
    "verdict_to_facts = {\n",
    "    normalize_case_name(row[\"verdict\"]): row[\"extracted_gpt_facts\"]\n",
    "    for _, row in df1.iterrows()\n",
    "    if pd.notna(row[\"verdict\"]) and pd.notna(row[\"extracted_gpt_facts\"])\n",
    "}\n",
    "\n",
    "# Define directories\n",
    "citations_dirs = [\n",
    "    \"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdicts_tagged_citations\"\n",
    "#     ,\"/home/liorkob/M.Sc/thesis/data/5k/gpt/appeals_tagged_citations\"\n",
    "]\n",
    "\n",
    "records = []\n",
    "missing_verdicts = set()\n",
    "all_verdicts = list(verdict_to_facts.keys())\n",
    "\n",
    "# Loop\n",
    "for citations_dir in citations_dirs:\n",
    "    for file_path in glob(os.path.join(citations_dir, \"*.csv\")):\n",
    "        try:\n",
    "            df_cite = pd.read_csv(file_path)\n",
    "            source_verdict = normalize_case_name(os.path.basename(file_path).replace(\".csv\", \"\"))\n",
    "\n",
    "            if source_verdict not in verdict_to_facts:\n",
    "                continue\n",
    "\n",
    "            df_cite = df_cite[df_cite[\"predicted_label\"] == 1]\n",
    "\n",
    "            for cited in df_cite[\"citation\"].dropna():\n",
    "                cited_norm = normalize_case_name(cited)\n",
    "\n",
    "                if cited_norm in verdict_to_facts:\n",
    "                    # Exact match\n",
    "                    print(f\"âœ… Exact match:\")\n",
    "                    print(f\"   Source (raw): {os.path.basename(file_path).replace('.csv', '')} â†’ Cited (raw): {cited}\")\n",
    "                    print(f\"   Source (normalized): {source_verdict} â†’ Cited (normalized): {cited_norm}\")\n",
    "\n",
    "                    log = []\n",
    "                    if not verdict_to_facts.get(source_verdict):\n",
    "                        log.append(\"missing facts_a\")\n",
    "                    if not verdict_to_facts.get(cited_norm):\n",
    "                        log.append(\"missing facts_b\")\n",
    "\n",
    "                    records.append({\n",
    "                        \"verdict_a\": source_verdict,\n",
    "                        \"verdict_b\": cited_norm,\n",
    "                        \"gpt_facts_a\": verdict_to_facts.get(source_verdict),\n",
    "                        \"gpt_facts_b\": verdict_to_facts.get(cited_norm),\n",
    "                        \"log\": \"; \".join(log) if log else \"ok\"\n",
    "                    })\n",
    "\n",
    "                else:\n",
    "                    # Try smart match\n",
    "                    matches = difflib.get_close_matches(cited_norm, all_verdicts, n=1, cutoff=0.8)\n",
    "                    if matches:\n",
    "                        match = matches[0]\n",
    "                        if cited_norm.split(\"_\")[0] == match.split(\"_\")[0]:  # same type\n",
    "                            num_cited = get_only_numbers(cited_norm)\n",
    "                            num_match = get_only_numbers(match)\n",
    "\n",
    "                            min_len = min(len(num_cited), len(num_match))\n",
    "                            if min_len >= 4 and num_cited[:min_len] == num_match[:min_len]:\n",
    "                                # Smart match found, ask user\n",
    "                                print(f\"ğŸ§ Smart match suggestion:\", flush=True)\n",
    "                                print(f\"   Citation: {cited_norm}\", flush=True)\n",
    "                                print(f\"   Closest:  {match}\", flush=True)\n",
    "                                answer = input(\"ğŸ‘‰ Accept this match? (y/n): \").strip().lower()\n",
    "\n",
    "                                if answer == \"y\":\n",
    "                                    print(f\"âœ… Smart accepted match:\")\n",
    "                                    print(f\"   Source (raw): {os.path.basename(file_path).replace('.csv', '')} â†’ Cited (raw): {cited}\")\n",
    "                                    print(f\"   Source (normalized): {source_verdict} â†’ Closest (normalized): {match}\")\n",
    "\n",
    "                                    log = []\n",
    "                                    if not verdict_to_facts.get(source_verdict):\n",
    "                                        log.append(\"missing facts_a\")\n",
    "                                    if not verdict_to_facts.get(match):\n",
    "                                        log.append(\"missing facts_b\")\n",
    "\n",
    "                                    records.append({\n",
    "                                        \"verdict_a\": source_verdict,\n",
    "                                        \"verdict_b\": match,\n",
    "                                        \"gpt_facts_a\": verdict_to_facts.get(source_verdict),\n",
    "                                        \"gpt_facts_b\": verdict_to_facts.get(match),\n",
    "                                        \"log\": \"; \".join(log) if log else \"smart match\"\n",
    "                                    })\n",
    "                                    continue\n",
    "\n",
    "                    # No match accepted â†’ mark as missing\n",
    "                    # print(f\"âŒ Could not find match for: {cited_norm}\")\n",
    "                    missing_verdicts.add(cited_norm)\n",
    "                    records.append({\n",
    "                        \"verdict_a\": source_verdict,\n",
    "                        \"verdict_b\": cited_norm,\n",
    "                        \"gpt_facts_a\": verdict_to_facts.get(source_verdict),\n",
    "                        \"gpt_facts_b\": None,\n",
    "                        \"log\": \"missing cited verdict\"\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Remove duplicates based on verdict pairs\n",
    "valid_pairs_df = pd.DataFrame(records)\n",
    "valid_pairs_df = valid_pairs_df.drop_duplicates(subset=[\"verdict_a\", \"verdict_b\"])\n",
    "\n",
    "# Save\n",
    "valid_pairs_df.to_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/valid_pairs_with_log.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Save unique missing verdicts not present in verdict_to_facts\n",
    "filtered_missing = [v for v in sorted(missing_verdicts) if v not in verdict_to_facts]\n",
    "missing_df = pd.DataFrame(filtered_missing, columns=[\"missing_verdict\"])\n",
    "missing_df = missing_df.drop_duplicates()\n",
    "missing_df.to_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/missing_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nâœ… Total valid pairs collected (including missing): {len(valid_pairs_df)}\")\n",
    "print(f\"âš ï¸ Total unmatched citations: {len(missing_verdicts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for citation in df_cite[\"citation\"].dropna():\n",
    "    cited_norm = normalize_case_name(citation)\n",
    "    if cited_norm not in verdict_to_facts:\n",
    "        print(\"âŒ No match for:\", cited_norm)\n",
    "        close = difflib.get_close_matches(cited_norm, verdict_to_facts.keys(), n=1, cutoff=0.8)\n",
    "        print(\"ğŸ” Closest match:\", close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalize_case_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name)\n",
    "    name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "    name = re.sub(r\"[âˆ•/\\\\]\", \"-\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = name.strip().lower().replace(\" \", \"_\")\n",
    "    return name\n",
    "# Load data\n",
    "valid_pairs_df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/valid_pairs_with_log_2.csv\")\n",
    "df1 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_verdicts_with_gpt.csv\")\n",
    "df2 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_appeals_with_gpt.csv\")\n",
    "\n",
    "# Normalize verdict names\n",
    "verdicts = set(df1[\"verdict\"].dropna().apply(normalize_case_name))\n",
    "appeals = set(df2[\"verdict\"].dropna().apply(normalize_case_name))\n",
    "\n",
    "def get_type(name):\n",
    "    if name in appeals:\n",
    "        return \"appeal\"\n",
    "    elif name in verdicts:\n",
    "        return \"verdict\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Normalize all verdicts before checking type\n",
    "valid_pairs_df[\"norm_a\"] = valid_pairs_df[\"verdict_a\"].apply(normalize_case_name)\n",
    "valid_pairs_df[\"norm_b\"] = valid_pairs_df[\"verdict_b\"].apply(normalize_case_name)\n",
    "valid_pairs_df[\"type_a\"] = valid_pairs_df[\"norm_a\"].apply(get_type)\n",
    "valid_pairs_df[\"type_b\"] = valid_pairs_df[\"norm_b\"].apply(get_type)\n",
    "\n",
    "# Count each category\n",
    "appeal_to_verdict = ((valid_pairs_df[\"type_a\"] == \"appeal\") & (valid_pairs_df[\"type_b\"] == \"verdict\")).sum()\n",
    "verdict_to_verdict = ((valid_pairs_df[\"type_a\"] == \"verdict\") & (valid_pairs_df[\"type_b\"] == \"verdict\")).sum()\n",
    "appeal_to_appeal = ((valid_pairs_df[\"type_a\"] == \"appeal\") & (valid_pairs_df[\"type_b\"] == \"appeal\")).sum()\n",
    "verdict_to_appeal = ((valid_pairs_df[\"type_a\"] == \"verdict\") & (valid_pairs_df[\"type_b\"] == \"appeal\")).sum()\n",
    "\n",
    "# Debug unknowns\n",
    "unknown_a = (valid_pairs_df[\"type_a\"] == \"unknown\").sum()\n",
    "unknown_b = (valid_pairs_df[\"type_b\"] == \"unknown\").sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"ğŸ“š Appeal â†’ Verdict pairs: {appeal_to_verdict}\")\n",
    "print(f\"ğŸ“š Verdict â†’ Verdict pairs: {verdict_to_verdict}\")\n",
    "print(f\"ğŸ“š Appeal â†’ Appeal pairs: {appeal_to_appeal}\")\n",
    "print(f\"ğŸ“š Verdict â†’ Appeal pairs: {verdict_to_appeal}\")\n",
    "print(f\"â“ Unknown type in verdict_a: {unknown_a}\")\n",
    "print(f\"â“ Unknown type in verdict_b: {unknown_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# def reextract_full_citation_if_prefix_stripped_single_file(csv_path):\n",
    "#     print(f\"ğŸ” Fixing citations in: {csv_path.name}\")\n",
    "#     try:\n",
    "#         df = pd.read_csv(csv_path)\n",
    "#         if not {\"citation\", \"extracted_text\"}.issubset(df.columns):\n",
    "#             print(f\"âš ï¸ Skipping {csv_path.name} â€” missing required columns\")\n",
    "#             return\n",
    "\n",
    "#         new_citations = []\n",
    "\n",
    "#         for citation, text in zip(df[\"citation\"], df[\"extracted_text\"]):\n",
    "#             old_matches = []\n",
    "\n",
    "#             # OLD extraction simulation\n",
    "#             for m in citation_regex.finditer(str(text)):\n",
    "#                 row = m.groups()\n",
    "#                 old_cit = \" \".join(filter(None, row)).strip()\n",
    "#                 old_cit = re.sub(r\"\\s{2,}\", \" \", old_cit)\n",
    "#                 old_cit = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", old_cit)\n",
    "#                 old_cit_stripped = re.sub(r\"^[×‘×•×•×¨]\\\"\", \"\\\"\", old_cit)\n",
    "#                 old_cit_stripped = re.sub(r\"^[×‘×•×•×¨] \", \"\", old_cit_stripped)\n",
    "\n",
    "#                 if old_cit_stripped == citation:\n",
    "#                     old_matches.append((old_cit, citation))\n",
    "\n",
    "#             # NEW extraction\n",
    "#             fixed_citation = citation\n",
    "#             for m in citation_regex.finditer(str(text)):\n",
    "#                 row = m.groups()\n",
    "#                 full_cit = \" \".join(filter(None, row)).strip()\n",
    "#                 full_cit = re.sub(r\"\\s{2,}\", \" \", full_cit)\n",
    "#                 full_cit = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", full_cit)\n",
    "#                 full_cit = clean_leading_prefix(full_cit)\n",
    "\n",
    "#                 if any(stripped == citation for stripped_full, stripped in old_matches if stripped_full == full_cit):\n",
    "#                     fixed_citation = full_cit\n",
    "#                     if citation != full_cit:\n",
    "#                         print(f\"âœ… Updating citation: OLD='{citation}' â†’ NEW='{full_cit}'\")\n",
    "#                     break\n",
    "\n",
    "#             new_citations.append(fixed_citation)\n",
    "\n",
    "#         df[\"citation\"] = new_citations\n",
    "#         df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "#         print(\"âœ… File saved\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ Error processing {csv_path.name}: {e}\")\n",
    "\n",
    "# # ==== Run ====\n",
    "# # target_file = Path(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/verdict_tagged_citations/×ª\\\"×¤ 29454-01-13.csv\")\n",
    "# # reextract_full_citation_if_prefix_stripped_single_file(target_file)\n",
    "# csv_dir = Path(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/appeals_tagged_citations\")\n",
    "# csv_files = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "# print(f\"ğŸ” Fixing citations using updated logic in {len(csv_files)} files...\")\n",
    "\n",
    "# for csv_file in tqdm(csv_files, desc=\"Fixing citation prefixes\"):\n",
    "#     reextract_full_citation_if_prefix_stripped_single_file(csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
