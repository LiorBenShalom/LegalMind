{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation no API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "\n",
    "required_parts = [\n",
    "    \"מתחמי ענישה\", \"אחידות בענישה\", \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\",\n",
    "    \"ענישה נהוגה\", \"הענישה הנוהגת\", \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\",\n",
    "    \"מדיניות הענישה\", \"והכרעה\", \"ההרשעה\", \"מדיניות הענישה הנהוגה\"\n",
    "]\n",
    "citation_patterns = {\n",
    "    'ע\"פ': r'ע\"פ (\\d+/\\d+)',\n",
    "    'עפ\"ג': r'עפ\"ג (\\d+/\\d+)',\n",
    "    'ת״פ': r'ת״פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ״ג': r'עפ״ג (\\d+/\\d+)',\n",
    "    'רע״פ': r'רע״פ (\\d+/\\d+)',\n",
    "    'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "}\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/home/liorkob/best_model.pt\"  # Path to your saved model\n",
    "tokenizer = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def extract_citations(para_text):\n",
    "    \"\"\"\n",
    "    Extracts all citations and their full references from the text.\n",
    "    Returns a list of tuples: (citation_type, full_citation).\n",
    "    \"\"\"\n",
    "    citations = []\n",
    "\n",
    "    for citation_type, pattern in citation_patterns.items():\n",
    "        matches = re.findall(pattern, para_text)  # Find all matches for the pattern\n",
    "        for match in matches:\n",
    "            full_citation = f\"{citation_type} {match}\"  # Construct full citation\n",
    "            citations.append((citation_type, full_citation))\n",
    "\n",
    "    return citations  # List of (citation_type, full_citation)\n",
    "\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"Extracts the first occurrence of a required part in the CSV and all subsequent rows.\"\"\"\n",
    "    start_index = None\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "    return csv_data.iloc[start_index:] if start_index is not None else pd.DataFrame(columns=csv_data.columns)\n",
    "import re\n",
    "\n",
    "def process_and_tag(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"Process a .docx document and its corresponding CSV to check citations and tag with predictions.\"\"\"\n",
    "    try:\n",
    "        # Load the document and CSV\n",
    "        doc = docx.Document(docx_path)\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Iterate through paragraphs\n",
    "        for i, paragraph in enumerate(doc.paragraphs):\n",
    "            para_text = paragraph.text.strip()\n",
    "            if not para_text:\n",
    "                continue  # Skip empty paragraphs\n",
    "\n",
    "            found_citations = extract_citations(para_text)\n",
    "\n",
    "            if not found_citations:\n",
    "                continue  # No citations found, move to the next paragraph\n",
    "\n",
    "            for found_citation, full_citation in found_citations:\n",
    "\n",
    "                is_relevant = False\n",
    "                matching_part = None\n",
    "\n",
    "                # Check if the citation is in relevant parts\n",
    "                for _, row in csv_data.iterrows():\n",
    "                    part_text = row.get(\"text\", \"\")\n",
    "                    if any(req_part in row.get(\"part\", \"\") for req_part in required_parts) and part_text in para_text:\n",
    "                        is_relevant = True\n",
    "                        matching_part = row[\"part\"]\n",
    "                        break  # Stop searching once a match is found\n",
    "\n",
    "                if is_relevant:\n",
    "                    # Tag the paragraph using the model\n",
    "                    encoding = tokenizer(para_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "                    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "                    with torch.no_grad():\n",
    "                        output = model(**encoding)\n",
    "                        prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "                    # Append only when is_relevant = True\n",
    "                    results.append({\n",
    "                        'paragraph_number': i,\n",
    "                        'context_text': para_text,\n",
    "                        'citation': full_citation,\n",
    "                        'part': matching_part,\n",
    "                        'predicted_label': prediction, \n",
    "                    })\n",
    "\n",
    "                    print(f\"Tagged citation: Paragraph {i}, Part: {matching_part}, Prediction: {prediction}\")\n",
    "                    print(f\"Text: {para_text}\\n\")\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "        # Save results\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Tagged citations saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {docx_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        docx_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_{year}')\n",
    "        csv_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_csv_{year}')\n",
    "        output_directory = Path(f'/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}')\n",
    "\n",
    "        output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file_path in docx_directory.glob(\"*.docx\"):\n",
    "            try:\n",
    "                new_file_path = file_path.stem\n",
    "                print(f\"Processing {new_file_path}\")\n",
    "\n",
    "                csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "                if file_path.exists() and csv_file.exists():\n",
    "                    output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "                    process_and_tag(str(file_path), str(csv_file), str(output_file))\n",
    "\n",
    "                else:\n",
    "                    if not file_path.exists():\n",
    "                        print(f\"Document file not found: {file_path}\")\n",
    "                    if not csv_file.exists():\n",
    "                        print(f\"CSV file not found for: {csv_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all results to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import docx\n",
    "\n",
    "def merge_results(csv_directory: str, output_csv: str):\n",
    "    csv_directory = Path(csv_directory)\n",
    "    all_data = []\n",
    "    \n",
    "    # Iterate over CSV files\n",
    "    for file_path in csv_directory.glob(\"*.csv\"):\n",
    "        try:\n",
    "            if file_path.stat().st_size == 0:  # Check if file is empty\n",
    "                print(f\"Skipping empty file: {file_path.name}\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty:  # Check if the file is empty even after reading\n",
    "                print(f\"Skipping empty DataFrame: {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            df[\"source_file\"] = file_path.name  # Add filename column\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path.name}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        merged_df = pd.concat(all_data, ignore_index=True)\n",
    "        merged_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Merged CSV saved to: {output_csv}\")\n",
    "    else:\n",
    "        print(\"No valid CSV files found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        csv_directory = f\"/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}\"\n",
    "        output_csv = f\"{csv_directory}/merged_results_{year}.csv\"\n",
    "        \n",
    "        merge_results(csv_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation WITH API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_leading_prefix(citation):\n",
    "    match = re.match(r'^([לבוה])\\s*([א-ת\"]+)', citation)\n",
    "    if not match:\n",
    "        return citation\n",
    "    prefix = match.group(1)\n",
    "    maybe_acronym = match.group(2)\n",
    "    \n",
    "    # בונה את הצירוף המלא כולל הקידומת\n",
    "    full = prefix + maybe_acronym\n",
    "\n",
    "    # מנרמל (מסיר גרשיים) בשביל להשוות לרשימת ראשי התיבות\n",
    "    def normalize(text):\n",
    "        return text.replace('\"', '').replace(\"״\", \"\").replace(\"'\", \"\").replace(\"׳\", \"\")\n",
    "\n",
    "    norm_maybe = normalize(maybe_acronym)\n",
    "    norm_full = normalize(full)\n",
    "\n",
    "    # תנאי הסרה: החלק שאחרי הקידומת מוכר, אבל הצירוף כולו לא מוכר\n",
    "    if norm_maybe in acronyms and norm_full not in acronyms:\n",
    "        return citation[len(prefix):].lstrip()\n",
    "\n",
    "    return citation\n",
    "\n",
    "# List of legal acronyms (same as yours)\n",
    "acronyms = [\n",
    "    \"אב\", \"אבע\", \"אימוצ\", \"אמצ\", \"אפ\", \"אפח\", \"את\", \"אתפ\", \"באפ\", \"באש\", \"בבנ\", \"בגצ\", \"בדא\", \"בדמ\",\n",
    "    \"בדמש\", \"בהנ\", \"בהע\", \"בהש\", \"בידמ\", \"בידע\", \"בל\", \"בלמ\", \"במ\", \"בעא\", \"בעח\", \"בעמ\", \"בעק\", \"בפ\",\n",
    "    \"בפמ\", \"בפת\", \"בצא\", \"בצהמ\", \"בק\", \"בקמ\", \"בקשה\", \"ברמ\", \"ברע\", \"ברש\", \"בש\", \"בשא\",\n",
    "    \"בשגצ\", \"בשהת\", \"בשז\", \"בשמ\", \"בשע\", \"בשפ\", \"בתת\", \"גזז\", \"גמר\", \"גפ\", \"דבע\", \"דח\", \"דט\", \"דיונ\",\n",
    "    \"דמ\", \"דמר\", \"דמש\", \"דנ\", \"דנא\", \"דנגצ\", \"דנמ\", \"דנפ\", \"הד\", \"הדפ\", \"הוצלפ\", \"הט\", \"הכ\", \"המ\",\n",
    "    \"המד\", \"הממ\", \"המע\", \"המש\", \"הנ\", \"הסת\", \"הע\", \"העז\", \"הפ\", \"הפב\", \"הפמ\", \"הצמ\", \"הש\", \"השא\",\n",
    "    \"השגצ\", \"השפ\", \"השר\", \"הת\", \"וחק\", \"וע\", \"ושמ\", \"ושק\", \"ושר\", \"זי\", \"חא\", \"חבר\", \"חד\", \"חדא\",\n",
    "    \"חדלפ\", \"חדלת\", \"חדמ\", \"חדפ\", \"חהע\", \"חי\", \"חנ\", \"חסמ\", \"חעמ\", \"חעק\", \"חש\", \"יוש\", \"ייתא\", \"ימא\",\n",
    "    \"יס\", \"כצ\", \"מ\", \"מא\", \"מבכ\", \"מבס\", \"מונופולינ\", \"מזג\", \"מח\", \"מחוז\", \"מחע\", \"מט\", \"מטכל\", \"מי\",\n",
    "    \"מיב\", \"מכ\", \"ממ\", \"מס\", \"מסט\", \"מעי\", \"מעת\", \"מקמ\", \"מרכז\", \"מת\", \"נ\", \"נב\", \"נבא\", \"נמ\", \"נמב\",\n",
    "    \"נעד\", \"נער\", \"סבא\", \"סע\", \"סעש\", \"סק\", \"סקכ\", \"ע\", \"עא\", \"עאח\", \"עאפ\", \"עב\", \"עבאפ\", \"עבז\", \"עבח\",\n",
    "    \"עבי\", \"עבל\", \"עבמצ\", \"עבעח\", \"עבפ\", \"עבר\", \"עבשהת\", \"עגר\", \"עדי\", \"עדמ\", \"עהג\", \"עהס\", \"עהפ\",\n",
    "    \"עו\", \"עורפ\", \"עז\", \"עח\", \"עחא\", \"עחדלפ\", \"עחדפ\", \"עחדת\", \"עחהס\", \"עחע\", \"עחק\", \"עחר\", \"עכב\",\n",
    "    \"על\", \"עלא\", \"עלבש\", \"עלח\", \"עלע\", \"עמ\", \"עמא\", \"עמה\", \"עמז\", \"עמח\", \"עמי\", \"עמלע\", \"עממ\", \"עמנ\",\n",
    "    \"עמפ\", \"עמצ\", \"עמק\", \"עמרמ\", \"עמש\", \"עמשמ\", \"עמת\", \"ענ\", \"ענא\", \"ענמ\", \"ענמא\", \"ענמש\", \"ענפ\",\n",
    "    \"עסא\", \"עסק\", \"עע\", \"עעא\", \"עעמ\", \"עער\", \"עעתא\", \"עפ\", \"עפא\", \"עפג\", \"עפהג\", \"עפמ\", \"עפמק\",\n",
    "    \"עפנ\", \"עפס\", \"עפספ\", \"עפע\", \"עפר\", \"עפת\", \"עצמ\", \"עק\", \"עקג\", \"עקמ\", \"עקנ\", \"עקפ\", \"ער\", \"ערא\",\n",
    "    \"ערגצ\", \"ערמ\", \"ערעור\", \"ערפ\", \"ערר\", \"עש\", \"עשא\", \"עשמ\", \"עשר\", \"עשת\", \"עשתש\", \"עת\", \"עתא\",\n",
    "    \"עתמ\", \"עתפב\", \"עתצ\", \"פא\", \"פה\", \"פל\", \"פלא\", \"פמ\", \"פמר\", \"פעמ\", \"פקח\", \"פר\", \"פרק\", \"פשז\",\n",
    "    \"פשר\", \"פת\", \"צא\", \"צבנ\", \"צה\", \"צו\", \"צח\", \"צמ\", \"קג\", \"קפ\", \"רחדפ\", \"רמש\", \"רע\", \"רעא\", \"רעב\",\n",
    "    \"רעבס\", \"רעו\", \"רעמ\", \"רעס\", \"רעפ\", \"רעפא\", \"רעצ\", \"רער\", \"רערצ\", \"רעש\", \"רעתא\", \"רצפ\", \"רתק\",\n",
    "    \"ש\", \"שבד\", \"שמ\", \"שמי\", \"שנא\", \"שע\", \"שעמ\", \"שק\", \"שש\", \"תא\", \"תאדמ\", \"תאח\", \"תאמ\", \"תאק\", \"תב\",\n",
    "    \"תבכ\", \"תבע\", \"תג\", \"תגא\", \"תד\", \"תדא\", \"תהג\", \"תהנ\", \"תהס\", \"תוב\", \"תוח\", \"תח\", \"תחפ\", \"תחת\",\n",
    "    \"תט\", \"תי\", \"תכ\", \"תלא\", \"תלב\", \"תלהמ\", \"תלפ\", \"תלתמ\", \"תמ\", \"תמהח\", \"תממ\", \"תמק\", \"תמר\",\n",
    "    \"תמש\", \"תנג\", \"תנז\", \"תע\", \"תעא\", \"תעז\", \"תפ\", \"תפב\", \"תפח\", \"תפחע\", \"תפכ\", \"תפמ\", \"תפע\",\n",
    "    \"תפק\", \"תצ\", \"תק\", \"תקח\", \"תקמ\", \"תרמ\", \"תת\", \"תתח\", \"תתע\", \"תתעא\", \"תתק\"\n",
    "]\n",
    "\n",
    "def create_acronym_variants(acronyms):\n",
    "    acronym_variants = []\n",
    "    for a in acronyms:\n",
    "        if len(a) > 1:\n",
    "            # Case 1: Original acronym with quotes/dots before last letter\n",
    "            base_acronym = a\n",
    "            if a.startswith('ב') or a.startswith('ו') or a.startswith('ה'):\n",
    "                # Also add variant without the prefix letter\n",
    "                base_acronym = a[1:]\n",
    "            \n",
    "            # For each acronym (both with and without prefix)\n",
    "            for acr in [a, base_acronym]:\n",
    "                if len(acr) > 1:\n",
    "                    # Standard quote/dot before last letter\n",
    "                    quoted = rf\"{acr[:-1]}[\\\"'״]{acr[-1]}\"\n",
    "                    with_dot = rf\"{acr[:-1]}\\.{acr[-1]}\"\n",
    "                    acronym_variants.append(f\"(?:{quoted}|{with_dot})\")\n",
    "                    \n",
    "                    # Add dot-separated variant\n",
    "                    dots_between = '\\.'.join(list(acr))\n",
    "                    acronym_variants.append(dots_between)\n",
    "    \n",
    "    return '|'.join(acronym_variants)\n",
    "        \n",
    "acronym_pattern = create_acronym_variants(acronyms)\n",
    "\n",
    "# Ensure the numbers follow the correct format\n",
    "number_pattern = r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "'''\n",
    "citation_pattern = fr'''\n",
    "    (?<!\\w)                      # Ensure no letter before\n",
    "    ([א-ת]?)                     # Optional single Hebrew prefix letter (but no isolated matches)\n",
    "    ({acronym_pattern})           # Captures acronym (short & long)\n",
    "    \\.?                          # Optional dot after acronym\n",
    "    \\s*                          # Optional spaces\n",
    "    (\\((.*?)\\))?                  # Optional court location in parentheses\n",
    "    \\s*[-/]?\\s*                  # Required space or separator before case number\n",
    "    ({number_pattern})            # Captures case number formats\n",
    "    (?!\\w)                       # Ensure no letter after\n",
    "'''.strip()\n",
    "\n",
    "# Compile regex with verbose flag for readability\n",
    "citation_regex = re.compile(citation_pattern, re.VERBOSE)\n",
    "\n",
    "\n",
    "def extract_citations_from_csv(csv_data):\n",
    "    citations = []\n",
    "    text_column = csv_data[\"text\"].astype(str)  # Convert to string to avoid NaN issues\n",
    "    pd.set_option(\"display.max_colwidth\", None)  # Ensure full text is displayed\n",
    "    # print(\"\\n\".join(text_column))  # Print each row as a full text\n",
    "    # for i, text in enumerate(text_column, 1):\n",
    "    #     print(f\"{i}. {text}\")\n",
    "\n",
    "    matches = text_column.str.extractall(citation_regex)  # Extract structured matches\n",
    "    # print(\"Extracted Matches:\")\n",
    "    # print(matches)\n",
    "\n",
    "    # print(\"Extracted DataFrame:\", matches)  # Debugging step\n",
    "    \n",
    "    for _, row in matches.iterrows():\n",
    "        # Build the citation string, joining all valid elements\n",
    "        citation = \" \".join(map(str, filter(pd.notna, row))).strip()\n",
    "\n",
    "        # Clean up extra spaces\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "\n",
    "\n",
    "        # # Remove invalid extra words (e.g., \"על 12\")\n",
    "        if re.match(r\"^על \\d+$\", citation):  \n",
    "            print('Skip invalid cases like \"על 12')\n",
    "            continue  # Skip invalid cases like \"על 12\"\n",
    "\n",
    "        # Fix duplicated court locations, e.g., \"(מחוזי מרכז) מחוזי מרכז\" → \"(מחוזי מרכז)\"\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        citation=clean_leading_prefix(citation)\n",
    "        # Add the cleaned citation to the list\n",
    "        citations.append(citation)\n",
    "    \n",
    "    # Return citations as a list, even if some are empty or missing optional groups\n",
    "    return citations if citations else []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import docx\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\" \n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Define required sections and citation patterns\n",
    "required_parts = [\n",
    "    \"מתחמי ענישה\", \"אחידות בענישה\", \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\",\n",
    "    \"ענישה נהוגה\", \"הענישה הנוהגת\", \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\",\n",
    "    \"מדיניות הענישה\", \"והכרעה\", \"ההרשעה\", \"מדיניות הענישה הנהוגה\"\n",
    "]\n",
    "\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model_path = \"/home/liorkob/classifier_relvant_citation_model.pt\" \n",
    "tokenizer_bert = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model_bert.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "def split_preserving_structure(text):\n",
    "    paragraphs = re.split(r'(?<=\\d\\.)\\s', text)  # Split after numbers followed by a period\n",
    "    return [para.strip() for para in paragraphs if para.strip()]\n",
    "\n",
    "def query_gpt(text,citation):\n",
    "    \"\"\"\n",
    "    Queries gpt-4.1-mini to extract and segment legal citations.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following legal text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Your task is to extract **only** the part of the text that directly relates to the citation \"{citation}\".\n",
    "    \n",
    "    **Extraction Rules:**\n",
    "    - **Do not modify any wording.** Keep the original phrasing exactly as it appears in the provided document.\n",
    "    - **Do not summarize or rephrase.**\n",
    "    - **Return only the relevant portion**, not the full text.\n",
    "    - **Handle grouped citations carefully:**\n",
    "        - If the citation appears in a list following \"ראו למשל ...\" or similar, include the preceding explanation that applies to all citations.\n",
    "        - Do not include other citations from the list—return only the text relevant to \"{citation}\".\n",
    "    - **Handle case explanations properly:**\n",
    "        - If the citation is explained in a specific section (e.g., \"בע\"פ 9373/10 ותד נ' מדינת ישראל...\"), extract the **entire explanation** of the case.\n",
    "        - Do not remove any important context about the court ruling.\n",
    "    - Do **not** extract only \"(רע\"פ 2718/04)\" without the legal principle it supports.\n",
    "\n",
    "\n",
    "    Only return the extracted text. Do not include unrelated content or formatting.\n",
    "    \"\"\"\n",
    "    print(f\"🧠 Sending to GPT for extraction...\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to extract and structure legal citations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        processed_text = response.choices[0].message.content\n",
    "        return processed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 GPT API error: {e}\")\n",
    "        return [text]  # Return original text in case of failure\n",
    "    \n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a required part in the CSV and all subsequent rows.\n",
    "    \"\"\"\n",
    "    start_index = None\n",
    "\n",
    "    # Find the first row containing a required part\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    # If a match is found, return only relevant rows\n",
    "    if start_index is not None:\n",
    "        return csv_data.iloc[start_index:]\n",
    "    else:\n",
    "        # print(\"NO required parts in data\")\n",
    "        # print(\"parts in data:\")\n",
    "        # print(csv_data[\"part\"].unique())\n",
    "        return pd.DataFrame(columns=csv_data.columns)  # Return an empty DataFrame if no matches found\n",
    "\n",
    "\n",
    "\n",
    "# Function to find all occurrences of a citation in the document\n",
    "def find_all_occurrences(doc, citation):\n",
    "    indices = []\n",
    "    for i, paragraph in enumerate(doc.paragraphs):\n",
    "        if citation in paragraph.text:\n",
    "            indices.append(i)  # Store all occurrences of the citation\n",
    "    return indices\n",
    "\n",
    "# Function to get relevant context for each occurrence of the citation\n",
    "def get_context_paragraphs(doc, index, citation):\n",
    "    context_text = []\n",
    "\n",
    "    # Search for the closest non-empty previous paragraph\n",
    "    prev_index = index - 1\n",
    "    while prev_index >= 0 and not doc.paragraphs[prev_index].text.strip():\n",
    "        prev_index -= 1  # Move backwards until finding text\n",
    "\n",
    "    if prev_index >= 0:\n",
    "        context_text.append(doc.paragraphs[prev_index].text.strip())\n",
    "\n",
    "    # Get the current paragraph (must exist, but check if empty)\n",
    "    curr_text = doc.paragraphs[index].text.strip()\n",
    "    if curr_text:\n",
    "        context_text.append(curr_text)\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Empty paragraph for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None  # Skip this occurrence if the current paragraph is empty\n",
    "\n",
    "    # Search for the closest non-empty next paragraph\n",
    "    next_index = index + 1\n",
    "    while next_index < len(doc.paragraphs) and not doc.paragraphs[next_index].text.strip():\n",
    "        next_index += 1  # Move forward until finding text\n",
    "\n",
    "    if next_index < len(doc.paragraphs):\n",
    "        context_text.append(doc.paragraphs[next_index].text.strip())\n",
    "\n",
    "    # Ensure we have at least one non-empty paragraph\n",
    "    if not context_text:\n",
    "        print(f\"⚠️ Warning: No valid text found for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None\n",
    "\n",
    "    return \"\\n\".join(context_text).strip()\n",
    "def normalize_case_name_2(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name)\n",
    "    name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "    name = re.sub(r\"[∕/\\\\]\", \"-\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = name.strip().lower().replace(\" \", \"_\")\n",
    "    return name\n",
    "\n",
    "\n",
    "# Function to process and tag document paragraphs\n",
    "def process_and_tag_with_split(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a .docx document and its corresponding CSV, find relevant paragraphs with context, \n",
    "    extract relevant text using GPT, tag with BERT, and store results.\n",
    "    \"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    filtered_csv_data = filter_csv_relevant_parts(csv_data)\n",
    "    if filtered_csv_data.empty:\n",
    "        # print(\"⚠️ Skipping file — no relevant parts found.\")\n",
    "        return\n",
    "\n",
    "    citations = extract_citations_from_csv(filtered_csv_data)\n",
    "    results = []\n",
    "    if len(citations) > 30:\n",
    "        print(f\"TOO MANY CITATIONS IN CSV Found {len(citations)}\")\n",
    "        print(docx_path)\n",
    "\n",
    "\n",
    "        return\n",
    "    print(f\"🔍 Found {len(citations)} citations in CSV\")\n",
    "\n",
    "    for citation in citations:\n",
    "        citation_indices = find_all_occurrences(doc, citation)  # Find all occurrences\n",
    "\n",
    "        # Collect all contexts where the citation appears\n",
    "        merged_contexts = []\n",
    "        for index in citation_indices:\n",
    "            full_context = get_context_paragraphs(doc, index, citation)\n",
    "            if full_context:\n",
    "                merged_contexts.append(full_context)\n",
    "\n",
    "        # If no valid contexts found, skip this citation\n",
    "        if not merged_contexts:\n",
    "            continue  \n",
    "\n",
    "        # Merge all valid contexts into one, ensuring uniqueness\n",
    "        final_context = \"\\n\".join(set(merged_contexts)).strip()  # Remove duplicates\n",
    "        # print(citation)\n",
    "        # print(final_context)\n",
    "\n",
    "        # Ask GPT to extract the relevant part\n",
    "        extracted_text = query_gpt(final_context, citation)\n",
    "\n",
    "        # Tag the extracted text with BERT\n",
    "        encoding = tokenizer_bert(extracted_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "        encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model_bert(**encoding)\n",
    "            prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "\n",
    "        citation=normalize_case_name_2(citation)    \n",
    "        # Store only one result per citation\n",
    "        result = {\n",
    "            'citation': citation,\n",
    "            'context_text': final_context,\n",
    "            'extracted_text': extracted_text,\n",
    "            'predicted_label': prediction\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed document saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    docx_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs_3k/docx/verdict')\n",
    "    csv_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs_3k/verdict_csv')\n",
    "    output_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdicts_tagged_citations')\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Stats counters\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    skipped_empty_or_missing = 0\n",
    "    missing_csv = 0\n",
    "    files_with_citations = 0\n",
    "    total_citations = 0\n",
    "    total_tagged_as_1 = 0\n",
    "\n",
    "    all_files = list(docx_directory.glob(\"*.docx\"))\n",
    "    print(f\"🗂 Total DOCX files found: {len(all_files)}\")\n",
    "\n",
    "    for file_path in tqdm(all_files, desc=\"Processing DOCX files\"):\n",
    "        total_files += 1\n",
    "        new_file_path = file_path.stem\n",
    "        csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "        output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "\n",
    "        if not csv_file.exists():\n",
    "            print(f\"CSV file not found for: {csv_file}\")\n",
    "            missing_csv += 1\n",
    "            continue\n",
    "\n",
    "        if output_file.exists() and output_file.stat().st_size > 0:\n",
    "            try:\n",
    "                df_existing = pd.read_csv(output_file)\n",
    "                num_citations = len(df_existing)\n",
    "                num_tagged_1 = (df_existing[\"predicted_label\"] == 1).sum()\n",
    "\n",
    "                total_citations += num_citations\n",
    "                total_tagged_as_1 += num_tagged_1\n",
    "                files_with_citations += 1\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error reading {output_file.name}: {e}\")\n",
    "                skipped_empty_or_missing += 1\n",
    "                continue\n",
    "\n",
    "        if output_file.exists() and output_file.stat().st_size == 0:\n",
    "            skipped_empty_or_missing += 1\n",
    "            continue\n",
    "\n",
    "        if not output_file.exists():\n",
    "            process_and_tag_with_split(str(file_path), str(csv_file), str(output_file))\n",
    "            if output_file.exists() and output_file.stat().st_size > 0:\n",
    "                try:\n",
    "                    df_new = pd.read_csv(output_file)\n",
    "                    num_citations = len(df_new)\n",
    "                    num_tagged_1 = (df_new[\"predicted_label\"] == 1).sum()\n",
    "                    total_citations += num_citations\n",
    "                    total_tagged_as_1 += num_tagged_1\n",
    "                    files_with_citations += 1\n",
    "                    processed_files += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Failed to read newly written output: {output_file.name}\")\n",
    "                    skipped_empty_or_missing += 1\n",
    "\n",
    "    # Averages\n",
    "    avg_citations_per_file = total_citations / files_with_citations if files_with_citations else 0\n",
    "    avg_tagged_1_per_file = total_tagged_as_1 / files_with_citations if files_with_citations else 0\n",
    "\n",
    "    print(\"\\n===== 📊 Processing Summary =====\")\n",
    "    print(f\"Total DOCX files:               {total_files}\")\n",
    "    print(f\"Processed files:                {files_with_citations}  # output file exists and is not empty\")\n",
    "    print(f\"Skipped (already processed):    {total_files - files_with_citations}  # output missing or empty\")\n",
    "    print(f\"Missing CSV files:              {missing_csv}\")\n",
    "    print(f\"Files with citation data:       {files_with_citations}\")\n",
    "    print(f\"Total citations:                {total_citations}\")\n",
    "    print(f\"Total tagged as 1:              {total_tagged_as_1}\")\n",
    "    print(f\"Avg citations per file:         {avg_citations_per_file:.2f}\")\n",
    "    print(f\"Avg tagged=1 per file:          {avg_tagged_1_per_file:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_part_only_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 📊 סטטיסטיקה כללית =====\n",
      "סה\"כ קבצים שנבדקו:               3046\n",
      "קבצים עם חלק רלוונטי:            2491\n",
      "סה\"כ ציטוטים שנמצאו ב-part:      163\n",
      "🔍 מספר דוגמאות מוצגות:           28\n",
      "\n",
      "===== 🧾 דוגמאות =====\n",
      "\n",
      "📁 קובץ: תפ_41970-12-21.csv\n",
      "📌 ציטוט: ת\"פ 19611-03\n",
      "📂 part: ת\"פ 19611-03-21 עוסק בעבירת החזקת סמים שלא לצריכה עצמית.\n",
      "📝 text: נגע הסמים פוגע בציבור קשות. קטינים, צעירים ובוגרים נחשפים אליו, לעתים באקראי, מתנסים בשימוש בסם ובמקרים רבים מתמכרים לו. השימוש בסם פוגע לא רק במשתמשים בו ובבריאותם, אלא גם במעגלים הקרובים והרחוקים מהם, בשל ההשלכות הפוגעניות של השימוש בו- ההתמכרות, העבריינות הנלוות לשימוש במקרים רבים, והשפעות שליליות נוספות.\n",
      "\n",
      "📁 קובץ: תפ_41970-12-21.csv\n",
      "📌 ציטוט: ת\"פ 19611-03\n",
      "📂 part: ת\"פ 19611-03-21 עוסק בעבירת החזקת סמים שלא לצריכה עצמית.\n",
      "📝 text: עבירות ההפצה והסחר בסמים, הן אלה המביאות להתפשטות הנגע ולהיותו זמין לכל, ומכאן החומרה שיש בהן. ככל שהעבריין ממקום גבוה יותר בשרשרת הפצת הסם, חלקו בהפצתו גדול יותר, ובהתאמה- עונשו יהא חמור יותר.\n",
      "\n",
      "📁 קובץ: תפ_43665-08-21.csv\n",
      "📌 ציטוט: ת\"פ 1324-01\n",
      "📂 part: ת\"פ 1324-01-22\n",
      "📝 text: המלחמה בנגע הסמים הינה קשה ובלתי פוסקת, עבירות הסמים פוגעת בערכים חברתיים של הגנה על הציבור בכללותו מפני הנזקים הישירים והעקיפים אשר נגרמים עקב השימוש בסמים וזאת לצד הנזקים הכלכליים והחברתיים הנגרמים עקב שימוש בסמים.  מידת הפגיעה בערך המוגן  בעבירה של החזקת סם לצריכה עצמית הינה ברף הנמוך. לעניין ההשפעות ההרסניות של סם הקוקאין, ראה הנאמר בע\"פ 972/11 מדינת ישראל נ' יונה (04.07.12).\n",
      "\n",
      "📁 קובץ: תפ_43665-08-21.csv\n",
      "📌 ציטוט: ת\"פ 1324-01\n",
      "📂 part: ת\"פ 1324-01-22\n",
      "📝 text: במסגרת הנסיבות הקשורות בביצוע העבירה (סעיף 40 ט' לחוק), יש לתת את הדעת לשיקולים הבאים:  הנאשם נתפס מחזיק סם מסוג קוקאין במשקל 0.0946 גרם, על גופו מוסלק במגבון.\n",
      "\n",
      "📁 קובץ: תא_40004-08-18.csv\n",
      "📌 ציטוט: ת\"פ 47326-10\n",
      "📂 part: ת\"פ 47326-10-18\n",
      "📝 text: כתב האישום בתיק זה כולל שני אישומים בעבירות של סחר בסמים מסוכנים לאותו סוכן משטרתי, אך ביניהם מפרידים חודשיים ואין ביניהם קשר הדוק.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === תבנית בסיסית לציטוט משפטי (רק ת\"פ לפי הבקשה שלך) ===\n",
    "citation_regex = re.compile(r'(?<!\\w)(ת\"פ\\s*\\d{1,6}[-/]\\d{2})(?!\\w)')\n",
    "\n",
    "# === רשימת חלקים רלוונטיים לסינון ראשוני ===\n",
    "required_parts = [\n",
    "    \"מתחמי ענישה\", \"אחידות בענישה\", \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\",\n",
    "    \"ענישה נהוגה\", \"הענישה הנוהגת\", \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\",\n",
    "    \"מדיניות הענישה\", \"והכרעה\", \"ההרשעה\", \"מדיניות הענישה הנהוגה\"\n",
    "]\n",
    "\n",
    "# === נתיב לתיקיית הקבצים ===\n",
    "csv_dir = Path(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/verdict_csv\")\n",
    "\n",
    "# === סטטיסטיקות ===\n",
    "total_files = 0\n",
    "files_with_required_part = 0\n",
    "total_citations_found = 0\n",
    "example_rows = []\n",
    "\n",
    "# === פונקציה לסינון החלק הרלוונטי ולקטיפת ציטוטים מתוך part ===\n",
    "def filter_and_find_citations(csv_data):\n",
    "    start_index = None\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        part_val = str(row.get(\"part\", \"\"))\n",
    "        if any(req in part_val for req in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    if start_index is not None:\n",
    "        filtered = csv_data.iloc[start_index:]\n",
    "        citations = []\n",
    "        for row in filtered.itertuples():\n",
    "            part_text = str(getattr(row, \"part\", \"\"))\n",
    "            matches = citation_regex.findall(part_text)\n",
    "            if matches:\n",
    "                for match in matches:\n",
    "                    citations.append((match, part_text.strip(), getattr(row, \"text\", \"\").strip()))\n",
    "        return filtered, citations\n",
    "    return pd.DataFrame(columns=csv_data.columns), []\n",
    "\n",
    "# === מעבר על כל הקבצים ===\n",
    "for file_path in csv_dir.glob(\"*.csv\"):\n",
    "    total_files += 1\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'part' not in df.columns or 'text' not in df.columns:\n",
    "            continue\n",
    "\n",
    "        filtered_df, citations = filter_and_find_citations(df)\n",
    "\n",
    "        if not filtered_df.empty:\n",
    "            files_with_required_part += 1\n",
    "            total_citations_found += len(citations)\n",
    "            if citations:\n",
    "                for citation, part, text in citations[:2]:  # שמור עד 2 דוגמאות לקובץ\n",
    "                    example_rows.append({\n",
    "                        \"file\": file_path.name,\n",
    "                        \"citation\": citation,\n",
    "                        \"part\": part,\n",
    "                        \"text\": text\n",
    "                    })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ שגיאה בקובץ {file_path.name}: {e}\")\n",
    "\n",
    "# === הדפסה מסכמת ===\n",
    "print(f\"\\n===== 📊 סטטיסטיקה כללית =====\")\n",
    "print(f\"סה\\\"כ קבצים שנבדקו:               {total_files}\")\n",
    "print(f\"קבצים עם חלק רלוונטי:            {files_with_required_part}\")\n",
    "print(f\"סה\\\"כ ציטוטים שנמצאו ב-part:      {total_citations_found}\")\n",
    "print(f\"🔍 מספר דוגמאות מוצגות:           {len(example_rows)}\")\n",
    "\n",
    "print(\"\\n===== 🧾 דוגמאות =====\")\n",
    "for row in example_rows[:5]:\n",
    "    print(f\"\\n📁 קובץ: {row['file']}\")\n",
    "    print(f\"📌 ציטוט: {row['citation']}\")\n",
    "    print(f\"📂 part: {row['part']}\")\n",
    "    print(f\"📝 text: {row['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get URLS from docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx.oxml.ns import qn\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    case_name = case_name.replace('״', '\"')\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "\n",
    "# def normalize_citation(citation):\n",
    "#     \"\"\"Normalize citation by removing prefixes and standardizing format.\"\"\"\n",
    "#     if not citation:\n",
    "#         return None\n",
    "#     # Standardize quotes\n",
    "#     citation = citation.replace('״', '\"').replace('״', '\"').replace('״', '\"')\n",
    "#     # Remove extra spaces\n",
    "#     citation = re.sub(r'\\s+', ' ', citation).strip()\n",
    "#     # Remove common prefixes, including רע\"פ\n",
    "#     citation = re.sub(r'^(ע\"?פ|ת\"?פ|עפ\"?ג|רע\"?פ)\\s+', '', citation)\n",
    "#     return citation\n",
    "\n",
    "\n",
    "def extract_citations(text):\n",
    "    \"\"\"Extracts legal citations from a single text string.\"\"\"\n",
    "    matches = citation_regex.findall(text)\n",
    "    citations = []\n",
    "    for match in matches:\n",
    "        citation = \" \".join(filter(None, match)).strip()\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        if not re.match(r\"^על \\d+$\", citation):\n",
    "            citations.append(citation)\n",
    "    return citations[0] if citations else None\n",
    "\n",
    "\n",
    "def getLinkedText(soup):\n",
    "    links = []\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            parts = tag.text.split('\"')\n",
    "            if len(parts) > 1:  # Ensure the URL exists before accessing index 1\n",
    "                url = parts[1]\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: No valid URL found in HYPERLINK tag: {tag.text}\")\n",
    "                url = None  # Assign None if URL is missing\n",
    "\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None and maybe_text.text.strip() != \"\":\n",
    "                    text += maybe_text.text.strip()\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None and maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                    break\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "    return links\n",
    "def getURLs(soup, links):\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            for rel in soup.find_all(\"Relationship\"):\n",
    "                if rel[\"Id\"] == link[\"id\"]:\n",
    "                    link[\"href\"] = rel[\"Target\"]\n",
    "    return links\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_hyperlinks(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from a .docx file and returns a dictionary \n",
    "    where the linked text is mapped to its corresponding URL.\n",
    "    \"\"\"\n",
    "    # Open the .docx file as a zip archive\n",
    "    try:\n",
    "        archive = zipfile.ZipFile(docx_path, \"r\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ Error: Cannot open {docx_path} (Bad ZIP format)\")\n",
    "        return {}\n",
    "\n",
    "    # Extract main document XML\n",
    "    try:\n",
    "        file_data = archive.read(\"word/document.xml\")\n",
    "        doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "        linked_text = getLinkedText(doc_soup)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No document.xml found in {docx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Extract hyperlink relationships from _rels/document.xml.rels\n",
    "    try:\n",
    "        url_data = archive.read(\"word/_rels/document.xml.rels\")\n",
    "        url_soup = BeautifulSoup(url_data, \"xml\")\n",
    "        links_with_urls = getURLs(url_soup, linked_text)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No _rels/document.xml.rels found in {docx_path}\")\n",
    "        links_with_urls = linked_text\n",
    "\n",
    "    # Extract footnotes (if available)\n",
    "    try:\n",
    "        footnote_data = archive.read(\"word/footnotes.xml\")\n",
    "        footnote_soup = BeautifulSoup(footnote_data, \"xml\")\n",
    "        footnote_links = getLinkedText(footnote_soup)\n",
    "\n",
    "        footnote_url_data = archive.read(\"word/_rels/footnotes.xml.rels\")\n",
    "        footnote_url_soup = BeautifulSoup(footnote_url_data, \"xml\")\n",
    "        footnote_links_with_urls = getURLs(footnote_url_soup, footnote_links)\n",
    "\n",
    "        # Merge footnote links\n",
    "        links_with_urls += footnote_links_with_urls\n",
    "    except KeyError:\n",
    "        pass  # No footnotes found, continue\n",
    "\n",
    "    # Convert extracted links to a dictionary: {linked_text: URL}\n",
    "    return {link[\"text\"]: link.get(\"href\", None) for link in links_with_urls}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def update_csv_with_links(csv_path, doc_path):\n",
    "    csv_path = Path(csv_path)  # Convert to Path object if not already\n",
    "    \n",
    "    # **Check if CSV is empty before reading**\n",
    "    if not csv_path.exists() or csv_path.stat().st_size == 0:  \n",
    "        print(f\"Skipping empty or missing file: {csv_path.name}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        # **Check if the DataFrame is empty after loading**\n",
    "        if df.empty:\n",
    "            print(f\"Skipping empty DataFrame: {csv_path.name}\")\n",
    "            return\n",
    "        # Skip if file already updated with links\n",
    "        if \"link\" in df.columns and df[\"link\"].notna().sum() > 0:\n",
    "            print(f\"⏭️ Skipping {csv_path.name} — already has links.\")\n",
    "            return\n",
    "\n",
    "        # Normalize extracted citations\n",
    "        df[\"extracted_citation\"] = df[\"context_text\"].apply(\n",
    "            lambda text: normalize_case_name(cit) if (pd.notna(text) and (cit := extract_citations(text))) else None\n",
    "        )\n",
    "        \n",
    "        # Normalize citation_links keys\n",
    "        citation_links = extract_hyperlinks(doc_path)\n",
    "        normalized_citation_links = {normalize_case_name(k): v for k, v in citation_links.items()}\n",
    "        \n",
    "        # Assign URLs to citations\n",
    "        df[\"link\"] = df[\"extracted_citation\"].apply(\n",
    "            lambda text: normalized_citation_links.get(text, None) if pd.notna(text) else None\n",
    "        )\n",
    "        # Print only the rows that got a link\n",
    "        # linked_rows = df[df[\"link\"].notna()]\n",
    "        # if not linked_rows.empty:\n",
    "        #     print(f\"🔗 {len(linked_rows)} rows updated with links in: {csv_path.name}\")\n",
    "        #     print(linked_rows[[\"extracted_citation\", \"link\"]].head(5).to_string(index=False))  # Show first 5\n",
    "\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated CSV saved to: {csv_path}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Skipping {csv_path.name}: CSV file is empty or unreadable.\")\n",
    "        return\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"⚠️ Skipping (encoding error): {csv_path}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def find_matching_docx(csv_name, docx_directory):\n",
    "    normalized_csv_name = normalize_case_name(csv_name.replace('.csv', '.docx'))\n",
    "    for root, _, files in os.walk(docx_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".docx\") and normalize_case_name(file) == normalized_csv_name:\n",
    "                return os.path.join(root, file)\n",
    "    print(f\"❌ No match found for {csv_name}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_all_csvs(citations_dir, docx_directory):\n",
    "    # Collect all CSV files first\n",
    "    all_csv_files = []\n",
    "    for root, _, files in os.walk(citations_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                all_csv_files.append(os.path.join(root, file))\n",
    "\n",
    "    # Iterate with tqdm progress bar\n",
    "    for csv_path in tqdm(all_csv_files, desc=\"🔍 Processing CSVs\"):\n",
    "        file = os.path.basename(csv_path)\n",
    "        docx_path = find_matching_docx(file, docx_directory)\n",
    "\n",
    "        if docx_path:\n",
    "            update_csv_with_links(csv_path, docx_path)\n",
    "        else:\n",
    "            print(f\"❌ No matching DOCX found for: {file}\")\n",
    "\n",
    "docx_dir = f\"/home/liorkob/M.Sc/thesis/data/5k/docx/verdict\"\n",
    "citations_dir = f\"/home/liorkob/M.Sc/thesis/data/5k/gpt/verdict_tagged_citations\"\n",
    "process_all_csvs(citations_dir, docx_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrate smilar pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import difflib\n",
    "def normalize_case_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name)\n",
    "    name = name.replace('\"', '').replace(\"״\", '').replace(\"'\", \"\")\n",
    "    name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "    name = re.sub(r\"[∕/\\\\]\", \"-\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = name.strip().lower().replace(\" \", \"_\")\n",
    "    return name\n",
    "\n",
    "# def normalize_case_name(name):\n",
    "#     if pd.isna(name):\n",
    "#         return \"\"\n",
    "#     name = str(name)\n",
    "#     name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "#     name = re.sub(r\"[∕/\\\\]\", \"-\", name)\n",
    "#     name = re.sub(r\"\\s+\", \" \", name)\n",
    "#     name = name.strip().lower().replace(\" \", \"_\")\n",
    "#     return name\n",
    "\n",
    "def get_only_numbers(name):\n",
    "    return ''.join(filter(str.isdigit, name))\n",
    "\n",
    "# Load and normalize all verdicts\n",
    "df1 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\")\n",
    "# df2 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_appeals_with_gpt_2.csv\")\n",
    "# combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Map normalized verdicts to their extracted_gpt_facts\n",
    "verdict_to_facts = {\n",
    "    normalize_case_name(row[\"verdict\"]): row[\"extracted_gpt_facts\"]\n",
    "    for _, row in df1.iterrows()\n",
    "    if pd.notna(row[\"verdict\"]) and pd.notna(row[\"extracted_gpt_facts\"])\n",
    "}\n",
    "\n",
    "# Define directories\n",
    "citations_dirs = [\n",
    "    \"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdicts_tagged_citations\"\n",
    "#     ,\"/home/liorkob/M.Sc/thesis/data/5k/gpt/appeals_tagged_citations\"\n",
    "]\n",
    "\n",
    "records = []\n",
    "missing_verdicts = set()\n",
    "all_verdicts = list(verdict_to_facts.keys())\n",
    "\n",
    "# Loop\n",
    "for citations_dir in citations_dirs:\n",
    "    for file_path in glob(os.path.join(citations_dir, \"*.csv\")):\n",
    "        try:\n",
    "            df_cite = pd.read_csv(file_path)\n",
    "            source_verdict = normalize_case_name(os.path.basename(file_path).replace(\".csv\", \"\"))\n",
    "\n",
    "            if source_verdict not in verdict_to_facts:\n",
    "                continue\n",
    "\n",
    "            df_cite = df_cite[df_cite[\"predicted_label\"] == 1]\n",
    "\n",
    "            for cited in df_cite[\"citation\"].dropna():\n",
    "                cited_norm = normalize_case_name(cited)\n",
    "\n",
    "                if cited_norm in verdict_to_facts:\n",
    "                    # Exact match\n",
    "                    print(f\"✅ Exact match:\")\n",
    "                    print(f\"   Source (raw): {os.path.basename(file_path).replace('.csv', '')} → Cited (raw): {cited}\")\n",
    "                    print(f\"   Source (normalized): {source_verdict} → Cited (normalized): {cited_norm}\")\n",
    "\n",
    "                    log = []\n",
    "                    if not verdict_to_facts.get(source_verdict):\n",
    "                        log.append(\"missing facts_a\")\n",
    "                    if not verdict_to_facts.get(cited_norm):\n",
    "                        log.append(\"missing facts_b\")\n",
    "\n",
    "                    records.append({\n",
    "                        \"verdict_a\": source_verdict,\n",
    "                        \"verdict_b\": cited_norm,\n",
    "                        \"gpt_facts_a\": verdict_to_facts.get(source_verdict),\n",
    "                        \"gpt_facts_b\": verdict_to_facts.get(cited_norm),\n",
    "                        \"log\": \"; \".join(log) if log else \"ok\"\n",
    "                    })\n",
    "\n",
    "                else:\n",
    "                    # Try smart match\n",
    "                    matches = difflib.get_close_matches(cited_norm, all_verdicts, n=1, cutoff=0.8)\n",
    "                    if matches:\n",
    "                        match = matches[0]\n",
    "                        if cited_norm.split(\"_\")[0] == match.split(\"_\")[0]:  # same type\n",
    "                            num_cited = get_only_numbers(cited_norm)\n",
    "                            num_match = get_only_numbers(match)\n",
    "\n",
    "                            min_len = min(len(num_cited), len(num_match))\n",
    "                            if min_len >= 4 and num_cited[:min_len] == num_match[:min_len]:\n",
    "                                # Smart match found, ask user\n",
    "                                print(f\"🧐 Smart match suggestion:\", flush=True)\n",
    "                                print(f\"   Citation: {cited_norm}\", flush=True)\n",
    "                                print(f\"   Closest:  {match}\", flush=True)\n",
    "                                answer = input(\"👉 Accept this match? (y/n): \").strip().lower()\n",
    "\n",
    "                                if answer == \"y\":\n",
    "                                    print(f\"✅ Smart accepted match:\")\n",
    "                                    print(f\"   Source (raw): {os.path.basename(file_path).replace('.csv', '')} → Cited (raw): {cited}\")\n",
    "                                    print(f\"   Source (normalized): {source_verdict} → Closest (normalized): {match}\")\n",
    "\n",
    "                                    log = []\n",
    "                                    if not verdict_to_facts.get(source_verdict):\n",
    "                                        log.append(\"missing facts_a\")\n",
    "                                    if not verdict_to_facts.get(match):\n",
    "                                        log.append(\"missing facts_b\")\n",
    "\n",
    "                                    records.append({\n",
    "                                        \"verdict_a\": source_verdict,\n",
    "                                        \"verdict_b\": match,\n",
    "                                        \"gpt_facts_a\": verdict_to_facts.get(source_verdict),\n",
    "                                        \"gpt_facts_b\": verdict_to_facts.get(match),\n",
    "                                        \"log\": \"; \".join(log) if log else \"smart match\"\n",
    "                                    })\n",
    "                                    continue\n",
    "\n",
    "                    # No match accepted → mark as missing\n",
    "                    # print(f\"❌ Could not find match for: {cited_norm}\")\n",
    "                    missing_verdicts.add(cited_norm)\n",
    "                    records.append({\n",
    "                        \"verdict_a\": source_verdict,\n",
    "                        \"verdict_b\": cited_norm,\n",
    "                        \"gpt_facts_a\": verdict_to_facts.get(source_verdict),\n",
    "                        \"gpt_facts_b\": None,\n",
    "                        \"log\": \"missing cited verdict\"\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Remove duplicates based on verdict pairs\n",
    "valid_pairs_df = pd.DataFrame(records)\n",
    "valid_pairs_df = valid_pairs_df.drop_duplicates(subset=[\"verdict_a\", \"verdict_b\"])\n",
    "\n",
    "# Save\n",
    "valid_pairs_df.to_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/valid_pairs_with_log.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Save unique missing verdicts not present in verdict_to_facts\n",
    "filtered_missing = [v for v in sorted(missing_verdicts) if v not in verdict_to_facts]\n",
    "missing_df = pd.DataFrame(filtered_missing, columns=[\"missing_verdict\"])\n",
    "missing_df = missing_df.drop_duplicates()\n",
    "missing_df.to_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/missing_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\n✅ Total valid pairs collected (including missing): {len(valid_pairs_df)}\")\n",
    "print(f\"⚠️ Total unmatched citations: {len(missing_verdicts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for citation in df_cite[\"citation\"].dropna():\n",
    "    cited_norm = normalize_case_name(citation)\n",
    "    if cited_norm not in verdict_to_facts:\n",
    "        print(\"❌ No match for:\", cited_norm)\n",
    "        close = difflib.get_close_matches(cited_norm, verdict_to_facts.keys(), n=1, cutoff=0.8)\n",
    "        print(\"🔍 Closest match:\", close)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def normalize_case_name(name):\n",
    "    if pd.isna(name):\n",
    "        return \"\"\n",
    "    name = str(name)\n",
    "    name = re.sub(r\"\\(.*?\\)\", \"\", name)\n",
    "    name = re.sub(r\"[∕/\\\\]\", \"-\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = name.strip().lower().replace(\" \", \"_\")\n",
    "    return name\n",
    "# Load data\n",
    "valid_pairs_df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/valid_pairs_with_log_2.csv\")\n",
    "df1 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_verdicts_with_gpt.csv\")\n",
    "df2 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_appeals_with_gpt.csv\")\n",
    "\n",
    "# Normalize verdict names\n",
    "verdicts = set(df1[\"verdict\"].dropna().apply(normalize_case_name))\n",
    "appeals = set(df2[\"verdict\"].dropna().apply(normalize_case_name))\n",
    "\n",
    "def get_type(name):\n",
    "    if name in appeals:\n",
    "        return \"appeal\"\n",
    "    elif name in verdicts:\n",
    "        return \"verdict\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Normalize all verdicts before checking type\n",
    "valid_pairs_df[\"norm_a\"] = valid_pairs_df[\"verdict_a\"].apply(normalize_case_name)\n",
    "valid_pairs_df[\"norm_b\"] = valid_pairs_df[\"verdict_b\"].apply(normalize_case_name)\n",
    "valid_pairs_df[\"type_a\"] = valid_pairs_df[\"norm_a\"].apply(get_type)\n",
    "valid_pairs_df[\"type_b\"] = valid_pairs_df[\"norm_b\"].apply(get_type)\n",
    "\n",
    "# Count each category\n",
    "appeal_to_verdict = ((valid_pairs_df[\"type_a\"] == \"appeal\") & (valid_pairs_df[\"type_b\"] == \"verdict\")).sum()\n",
    "verdict_to_verdict = ((valid_pairs_df[\"type_a\"] == \"verdict\") & (valid_pairs_df[\"type_b\"] == \"verdict\")).sum()\n",
    "appeal_to_appeal = ((valid_pairs_df[\"type_a\"] == \"appeal\") & (valid_pairs_df[\"type_b\"] == \"appeal\")).sum()\n",
    "verdict_to_appeal = ((valid_pairs_df[\"type_a\"] == \"verdict\") & (valid_pairs_df[\"type_b\"] == \"appeal\")).sum()\n",
    "\n",
    "# Debug unknowns\n",
    "unknown_a = (valid_pairs_df[\"type_a\"] == \"unknown\").sum()\n",
    "unknown_b = (valid_pairs_df[\"type_b\"] == \"unknown\").sum()\n",
    "\n",
    "# Print results\n",
    "print(f\"📚 Appeal → Verdict pairs: {appeal_to_verdict}\")\n",
    "print(f\"📚 Verdict → Verdict pairs: {verdict_to_verdict}\")\n",
    "print(f\"📚 Appeal → Appeal pairs: {appeal_to_appeal}\")\n",
    "print(f\"📚 Verdict → Appeal pairs: {verdict_to_appeal}\")\n",
    "print(f\"❓ Unknown type in verdict_a: {unknown_a}\")\n",
    "print(f\"❓ Unknown type in verdict_b: {unknown_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# def reextract_full_citation_if_prefix_stripped_single_file(csv_path):\n",
    "#     print(f\"🔁 Fixing citations in: {csv_path.name}\")\n",
    "#     try:\n",
    "#         df = pd.read_csv(csv_path)\n",
    "#         if not {\"citation\", \"extracted_text\"}.issubset(df.columns):\n",
    "#             print(f\"⚠️ Skipping {csv_path.name} — missing required columns\")\n",
    "#             return\n",
    "\n",
    "#         new_citations = []\n",
    "\n",
    "#         for citation, text in zip(df[\"citation\"], df[\"extracted_text\"]):\n",
    "#             old_matches = []\n",
    "\n",
    "#             # OLD extraction simulation\n",
    "#             for m in citation_regex.finditer(str(text)):\n",
    "#                 row = m.groups()\n",
    "#                 old_cit = \" \".join(filter(None, row)).strip()\n",
    "#                 old_cit = re.sub(r\"\\s{2,}\", \" \", old_cit)\n",
    "#                 old_cit = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", old_cit)\n",
    "#                 old_cit_stripped = re.sub(r\"^[בוור]\\\"\", \"\\\"\", old_cit)\n",
    "#                 old_cit_stripped = re.sub(r\"^[בוור] \", \"\", old_cit_stripped)\n",
    "\n",
    "#                 if old_cit_stripped == citation:\n",
    "#                     old_matches.append((old_cit, citation))\n",
    "\n",
    "#             # NEW extraction\n",
    "#             fixed_citation = citation\n",
    "#             for m in citation_regex.finditer(str(text)):\n",
    "#                 row = m.groups()\n",
    "#                 full_cit = \" \".join(filter(None, row)).strip()\n",
    "#                 full_cit = re.sub(r\"\\s{2,}\", \" \", full_cit)\n",
    "#                 full_cit = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", full_cit)\n",
    "#                 full_cit = clean_leading_prefix(full_cit)\n",
    "\n",
    "#                 if any(stripped == citation for stripped_full, stripped in old_matches if stripped_full == full_cit):\n",
    "#                     fixed_citation = full_cit\n",
    "#                     if citation != full_cit:\n",
    "#                         print(f\"✅ Updating citation: OLD='{citation}' → NEW='{full_cit}'\")\n",
    "#                     break\n",
    "\n",
    "#             new_citations.append(fixed_citation)\n",
    "\n",
    "#         df[\"citation\"] = new_citations\n",
    "#         df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "#         print(\"✅ File saved\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error processing {csv_path.name}: {e}\")\n",
    "\n",
    "# # ==== Run ====\n",
    "# # target_file = Path(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/verdict_tagged_citations/ת\\\"פ 29454-01-13.csv\")\n",
    "# # reextract_full_citation_if_prefix_stripped_single_file(target_file)\n",
    "# csv_dir = Path(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/appeals_tagged_citations\")\n",
    "# csv_files = list(Path(csv_dir).glob(\"*.csv\"))\n",
    "# print(f\"🔁 Fixing citations using updated logic in {len(csv_files)} files...\")\n",
    "\n",
    "# for csv_file in tqdm(csv_files, desc=\"Fixing citation prefixes\"):\n",
    "#     reextract_full_citation_if_prefix_stripped_single_file(csv_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
