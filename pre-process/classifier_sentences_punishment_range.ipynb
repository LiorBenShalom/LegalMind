{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìè Max tokens: 1287\n",
      "üìâ Min tokens: 7\n",
      "üìä Avg tokens: 88.76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/tagged_punishment_range_sentences.csv\")\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "# Load HeBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "# Tokenize and get lengths\n",
    "token_lengths = [len(tokenizer.encode(t, truncation=False)) for t in texts]\n",
    "\n",
    "# Stats\n",
    "max_len = max(token_lengths)\n",
    "min_len = min(token_lengths)\n",
    "avg_len = sum(token_lengths) / len(token_lengths)\n",
    "\n",
    "print(f\"üìè Max tokens: {max_len}\")\n",
    "print(f\"üìâ Min tokens: {min_len}\")\n",
    "print(f\"üìä Avg tokens: {avg_len:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Trying: adamw, LR=5e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.207119</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190336</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9306 | Precision: 0.9178 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adamw, LR=5e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.180717</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193050</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>0.971831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.202528</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9306 | Precision: 0.9178 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adamw, LR=5e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.205849</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192692</td>\n",
       "      <td>0.925170</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184062</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.177948</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9306 | Precision: 0.9178 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adamw, LR=3e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219205</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.185799</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9231 | Precision: 0.9167 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adamw, LR=3e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228432</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200726</td>\n",
       "      <td>0.925170</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219180</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9241 | Precision: 0.9054 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adamw, LR=3e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.214012</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.188565</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200565</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.178140</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9296 | Precision: 0.9296 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adamw, LR=1e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.234898</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197412</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9014 | Precision: 0.9014 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adamw, LR=1e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225491</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.189335</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193136</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9167 | Precision: 0.9041 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adamw, LR=1e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.210102</td>\n",
       "      <td>0.892086</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.179201</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190428</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192430</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9231 | Precision: 0.9167 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adam, LR=5e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.219011</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197582</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9231 | Precision: 0.9167 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adam, LR=5e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.180013</td>\n",
       "      <td>0.919708</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.886076</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.206251</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9241 | Precision: 0.9054 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adam, LR=5e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173946</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.162915</td>\n",
       "      <td>0.939597</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243815</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.191472</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9296 | Precision: 0.9296 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adam, LR=3e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.245594</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.201920</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9231 | Precision: 0.9167 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: adam, LR=3e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.204276</td>\n",
       "      <td>0.905109</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.873239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.196135</td>\n",
       "      <td>0.931507</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222176</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9241 | Precision: 0.9054 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adam, LR=3e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.213879</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192949</td>\n",
       "      <td>0.937931</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.957746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.203939</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.199345</td>\n",
       "      <td>0.921986</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9220 | Precision: 0.9286 | Recall: 0.9155\n",
      "\n",
      "üîç Trying: adam, LR=1e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.239922</td>\n",
       "      <td>0.907801</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.204167</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.8951 | Precision: 0.8889 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adam, LR=1e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.227121</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192682</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.194901</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.905405</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9241 | Precision: 0.9054 | Recall: 0.9437\n",
      "\n",
      "üîç Trying: adam, LR=1e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.215229</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.183644</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.943662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.195841</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197635</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9231 | Precision: 0.9167 | Recall: 0.9296\n",
      "\n",
      "üîç Trying: sgd, LR=5e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n",
      "\n",
      "üîç Trying: sgd, LR=5e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667544</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.660903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n",
      "\n",
      "üîç Trying: sgd, LR=5e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.637100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.633032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.631708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/liorkob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/liorkob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n",
      "\n",
      "üîç Trying: sgd, LR=3e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n",
      "\n",
      "üîç Trying: sgd, LR=3e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672714</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.042254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668444</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667058</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.014085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0247 | Precision: 0.1000 | Recall: 0.0141\n",
      "\n",
      "üîç Trying: sgd, LR=3e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.649072</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.070423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.641657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n",
      "\n",
      "üîç Trying: sgd, LR=1e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.650001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.649460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.0000 | Precision: 0.0000 | Recall: 0.0000\n",
      "\n",
      "üîç Trying: sgd, LR=1e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.678201</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.254902</td>\n",
       "      <td>0.183099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676709</td>\n",
       "      <td>0.218487</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.183099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676223</td>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.140845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.1770 | Precision: 0.2381 | Recall: 0.1408\n",
      "\n",
      "üîç Trying: sgd, LR=1e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.654146</td>\n",
       "      <td>0.229885</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.140845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.126761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651499</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.098592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651173</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.098592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.1667 | Precision: 0.5385 | Recall: 0.0986\n",
      "\n",
      "üîç Trying: adagrad, LR=5e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212930</td>\n",
       "      <td>0.907801</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.202431</td>\n",
       "      <td>0.907801</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9078 | Precision: 0.9143 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adagrad, LR=5e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.197802</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.182838</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.177313</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9014 | Precision: 0.9014 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adagrad, LR=5e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190216</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172123</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173548</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9155 | Precision: 0.9155 | Recall: 0.9155\n",
      "\n",
      "üîç Trying: adagrad, LR=3e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192830</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193246</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9014 | Precision: 0.9014 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adagrad, LR=3e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193063</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.189606</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.188354</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.8889 | Precision: 0.8767 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adagrad, LR=3e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173967</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.173835</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.167399</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.165802</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9014 | Precision: 0.9014 | Recall: 0.9014\n",
      "\n",
      "üîç Trying: adagrad, LR=1e-05, Epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.321442</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.830986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.282405</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.9130 | Precision: 0.9403 | Recall: 0.8873\n",
      "\n",
      "üîç Trying: adagrad, LR=1e-05, Epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.330328</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>0.774648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.258846</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>0.897059</td>\n",
       "      <td>0.859155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243575</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.8873 | Precision: 0.8873 | Recall: 0.8873\n",
      "\n",
      "üîç Trying: adagrad, LR=1e-05, Epochs=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 00:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298555</td>\n",
       "      <td>0.870229</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.802817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.222720</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.199802</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.194455</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ F1: 0.8690 | Precision: 0.8514 | Recall: 0.8873\n",
      "\n",
      "üèÜ Best model:\n",
      "adamw | LR=5e-05 | Epochs=2 | F1=0.9306 | Precision=0.9178 | Recall=0.9437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    ")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/tagged_punishment_range_sentences.csv\")\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "labels = df[\"Tag\"].astype(int).tolist()\n",
    "\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Dataset\n",
    "class PunishmentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = PunishmentDataset(train_encodings, train_labels)\n",
    "val_dataset = PunishmentDataset(val_encodings, val_labels)\n",
    "\n",
    "# Grid setup\n",
    "optimizers = {\n",
    "    \"adamw\": lambda model, lr: AdamW(model.parameters(), lr=lr),\n",
    "    \"adam\": lambda model, lr: torch.optim.Adam(model.parameters(), lr=lr),\n",
    "    \"sgd\": lambda model, lr: torch.optim.SGD(model.parameters(), lr=lr),\n",
    "    \"adagrad\": lambda model, lr: torch.optim.Adagrad(model.parameters(), lr=lr),\n",
    "}\n",
    "epochs_list = [2, 3, 4]\n",
    "learning_rates = [5e-5, 3e-5, 1e-5]\n",
    "\n",
    "# Best tracker\n",
    "best_f1 = 0\n",
    "best_model_info = \"\"\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    f1 = f1_score(labels, preds)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "# Grid search\n",
    "for opt_name, opt_fn in optimizers.items():\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            print(f\"\\nüîç Trying: {opt_name}, LR={lr}, Epochs={epochs}\")\n",
    "            \n",
    "            model = AutoModelForSequenceClassification.from_pretrained(\"avichr/heBERT\", num_labels=2)\n",
    "            optimizer = opt_fn(model, lr)\n",
    "            \n",
    "            args = TrainingArguments(\n",
    "                output_dir=f\"./results_{opt_name}_{lr}_{epochs}\",\n",
    "                num_train_epochs=epochs,\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=8,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                save_strategy=\"no\",\n",
    "                logging_strategy=\"no\",\n",
    "                report_to=\"none\"\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=val_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                optimizers=(optimizer, None)\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            metrics = trainer.evaluate()\n",
    "            f1, precision, recall = metrics[\"eval_f1\"], metrics[\"eval_precision\"], metrics[\"eval_recall\"]\n",
    "\n",
    "            print(f\"‚úÖ F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_model_info = f\"{opt_name} | LR={lr} | Epochs={epochs} | F1={f1:.4f} | Precision={precision:.4f} | Recall={recall:.4f}\"\n",
    "                model.save_pretrained(\"best_model\")\n",
    "                tokenizer.save_pretrained(\"best_model\")\n",
    "\n",
    "print(f\"\\nüèÜ Best model:\\n{best_model_info}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Text 1:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 4 ◊ó◊ï◊ì◊©◊ô◊ù, ◊ï◊î◊™◊†◊ê◊ô ◊î◊ï◊ê ◊©◊ú◊ê ◊ô◊¢◊ë◊ï◊® ◊õ◊ú ◊¢◊ë◊ô◊®◊î ◊û◊°◊ï◊í ◊¢◊ï◊ï◊ü ◊ú◊§◊ô ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù, ◊ú◊û◊©◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 2:\n",
      "◊¢◊ô◊ï◊ü ◊ë◊§◊°◊ô◊ß◊î ◊û◊ú◊û◊ì ◊ê◊§◊ï◊ê, ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ê◊™ ◊û◊¢◊©◊ô◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊î◊ù, ◊û◊ó◊ô◊ô◊ë ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊õ◊©◊î◊û◊™◊ó◊ù ◊†◊¢ ◊ë◊ô◊ü 12 ◊ú-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊õ◊ú ◊î◊û◊ß◊®◊ô◊ù ◊ë◊î◊ù ◊î◊ï◊©◊™ ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊î◊ì◊ë◊® ◊†◊¢◊©◊î ◊ë◊°◊ò◊ô◊ô◊î ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊ë◊©◊ú ◊î◊ú◊ô◊ö ◊©◊ô◊ß◊ï◊ù ◊û◊©◊û◊¢◊ï◊™◊ô ◊ï◊ë◊¶◊ô◊®◊ï◊£ ◊†◊°◊ô◊ë◊ï◊™ ◊ó◊®◊ô◊í◊ï◊™ ◊†◊ï◊°◊§◊ï◊™. ◊õ◊ê◊û◊ï◊®, ◊ë◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô-◊™\"◊ê) 38450-09-12 ◊ë◊¢◊†◊ô◊ô◊ü ◊°◊ß◊ï◊ò, ◊¢◊ú ◊ê◊£ ◊î◊©◊ô◊ß◊ï◊ù ◊î◊û◊©◊û◊¢◊ï◊™◊ô ◊©◊î◊†◊ê◊©◊ù ◊¢◊ë◊®, ◊ï◊û◊ì◊ï◊ë◊® ◊î◊ô◊î ◊ë◊†◊ê◊©◊ù ◊ë◊ü 61 ◊©◊¶◊®◊ö ◊°◊û◊ô◊ù ◊û◊í◊ô◊ú 45 ◊ë◊©◊ú ◊û◊¶◊ë ◊†◊§◊©◊ô ◊õ◊™◊ï◊¶◊ê◊î ◊û◊†◊ò◊ô◊ô◊™◊ï ◊î◊û◊ô◊†◊ô◊™, ◊ï◊ê◊£ ◊©◊©◊ù ◊î◊ï◊°◊õ◊ù ◊õ◊ô ◊î◊°◊ù ◊†◊®◊õ◊© ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô, ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊ß◊§◊ô◊ì ◊ú◊¶◊ô◊ô◊ü ◊õ◊ô ◊ú◊ê ◊î◊ô◊î ◊ë◊õ◊ú ◊ê◊ú◊ï ◊õ◊ì◊ô ◊ú◊î◊ó◊®◊ô◊í ◊ê◊™ ◊î◊†◊ê◊©◊ù ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊û◊ó◊ô◊ô◊ë ◊¢◊ï◊†◊© ◊û◊ê◊°◊®, ◊ï◊î◊ï◊ê ◊¢◊©◊î ◊ñ◊ê◊™ ◊®◊ß ◊õ◊ô ◊ú◊õ◊ú ◊î◊ê◊û◊ï◊® ◊î◊¶◊ò◊®◊§◊ï ◊©◊ô◊ß◊ï◊ú◊ô◊ù ◊®◊§◊ï◊ê◊ô◊ô◊ù ◊ï◊î◊ï◊û◊†◊ô◊ò◊ê◊®◊ô◊ô◊ù ◊û◊ô◊ï◊ó◊ì◊ô◊ù. ◊ï◊õ◊ê◊û◊ï◊®, ◊û◊ì◊ï◊ë◊® ◊î◊ô◊î ◊ë◊û◊ô ◊©◊¢◊ë◊® ◊õ◊ë◊®◊™ ◊ì◊®◊ö ◊û◊©◊û◊¢◊ï◊™◊ô◊™ ◊ë◊î◊ú◊ô◊ö ◊î◊©◊ô◊ß◊ï◊ù ◊©◊î◊ó◊ú ◊û◊ô◊ï◊ñ◊û◊™◊ï, ◊ï◊í◊ô◊ú◊î ◊û◊ó◊ï◊ô◊ë◊ï◊™ ◊ï◊û◊ï◊ò◊ô◊ë◊¶◊ô◊î ◊í◊ë◊ï◊î◊î ◊ú◊î◊¶◊ú◊ô◊ó ◊ë◊ï. ◊ê◊ô◊†◊ô ◊°◊ë◊ï◊® ◊õ◊ô ◊ë◊¢◊†◊ô◊ô◊†◊†◊ï ◊†◊ô◊™◊ü ◊ú◊ï◊û◊® ◊õ◊ô ◊î◊†◊ê◊©◊ù ◊¢◊ë◊® ◊î◊ú◊ô◊ö ◊©◊ô◊ß◊ï◊û◊ô. ◊ë◊ï◊ï◊ì◊ê◊ô ◊ú◊ê ◊û◊©◊û◊¢◊ï◊™◊ô. ◊ê◊ì◊®◊ë◊î, ◊ë◊™◊°◊ß◊ô◊® ◊¶◊ï◊ô◊ü ◊õ◊ô ◊í◊ù ◊†◊õ◊ï◊ü ◊ú◊û◊ï◊¢◊ì ◊õ◊™◊ô◊ë◊™ ◊î◊™◊°◊ß◊ô◊®, ◊ß◊ô◊ô◊û◊ô◊ù ◊í◊ï◊®◊û◊ô ◊°◊ô◊õ◊ï◊ü ◊ú◊î◊ô◊©◊†◊ï◊™ ◊¢◊ë◊ô◊®◊ï◊™ ◊ë◊¢◊™◊ô◊ì ◊ë◊©◊ú ◊î◊¢◊ï◊ë◊ì◊î ◊õ◊ô ◊î◊†◊ê◊©◊ù ◊ë◊¢◊™◊ï◊™ ◊ú◊ó◊• ◊ï◊û◊¶◊ï◊ß◊î ◊û◊™◊ß◊©◊î ◊ë◊î◊§◊¢◊ú◊™ ◊©◊ô◊ß◊ï◊ú ◊ì◊¢◊™. ◊î◊ô◊ï◊™ ◊î◊†◊ê◊©◊ù ◊û◊ô ◊©◊ê◊ô◊†◊ï ◊û◊ó◊ñ◊ô◊ß ◊ë◊ì◊§◊ï◊°◊ô◊ù ◊¢◊ë◊®◊ô◊ô◊†◊ô◊ô◊ù ◊ï◊ë◊¢◊û◊ì◊ï◊™ ◊§◊®◊ï-◊ó◊ë◊®◊™◊ô◊ï◊™, ◊ê◊ô◊†◊ï ◊û◊ú◊û◊ì ◊¢◊ú ◊î◊ú◊ô◊ö ◊©◊ô◊ß◊ï◊û◊ô. ◊ê◊ì◊®◊ë◊î, ◊ë◊°◊ï◊§◊ï ◊©◊ú ◊î◊™◊°◊ß◊ô◊® ◊ê◊£ ◊¶◊ï◊ô◊ü ◊õ◊ô ◊ë◊î◊¢◊ì◊® ◊î◊ú◊ô◊ö ◊ò◊ô◊§◊ï◊ú◊ô ◊ß◊ô◊ô◊ù ◊°◊ô◊õ◊ï◊ü ◊ú◊î◊ô◊©◊†◊ï◊™ ◊¢◊ë◊ô◊®◊ï◊™ ◊ë◊¢◊™◊ô◊ì. ◊ú◊õ◊ü, ◊ê◊ô◊†◊ô ◊°◊ë◊ï◊® ◊õ◊ô ◊ô◊© ◊ú◊°◊ò◊ï◊™ ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù, ◊ï◊í◊ù ◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊î◊ê◊ô◊©◊ô◊ï◊™ ◊î◊ê◊ó◊®◊ï◊™ ◊©◊ú ◊î◊†◊ê◊©◊ù, ◊ë◊õ◊ú◊ú ◊ñ◊î ◊†◊ò◊ô◊ô◊™◊ï ◊î◊û◊ô◊†◊ô◊™ ◊ï◊û◊¶◊ë◊ï ◊î◊®◊§◊ï◊ê◊ô, ◊í◊ù ◊ê◊ù ◊î◊ï◊ê ◊û◊ï◊®◊õ◊ë, ◊ê◊ô◊†◊ü ◊û◊¶◊ì◊ô◊ß◊ï◊™ ◊ñ◊ê◊™.\n",
      "Real: 1 | Predicted: 0\n",
      "\n",
      "üìù Text 3:\n",
      "◊ë◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô-◊û◊®◊õ◊ñ) 31599-08-20 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ë◊ï◊ñ◊í◊ú◊ï (◊í◊ñ◊® ◊ì◊ô◊†◊ô ◊û◊ô◊ï◊ù 1.3.23) ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊ë◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊ë◊û◊©◊ß◊ú ◊©◊ú 425 ◊ß\"◊í. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊†◊¢ ◊ë◊ô◊ü 45-66 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊î◊†◊ê◊©◊ù ◊†◊ì◊ï◊ü ◊ú◊¢◊†◊ô◊©◊î ◊ë◊ì◊®◊ö ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊û◊ò◊¢◊û◊ô ◊©◊ô◊ß◊ï◊ù ◊ô◊ï◊¶◊ê ◊ì◊ï◊§◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 4:\n",
      "◊ë◊™\"◊§ 33090-01-14 ◊û◊ì\"◊ô ◊†' ◊°◊ú◊ê◊û◊î (10.7.14) ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊û◊°◊í◊®◊™ ◊©◊ú ◊î◊°◊ì◊® ◊ò◊ô◊¢◊ï◊ü ◊ë◊î◊ó◊ñ◊ß◊™ 36 ◊í◊®◊ù ◊î◊®◊ï◊ê◊ô◊ü ◊û◊ó◊ï◊ú◊ß◊ô◊ù ◊ú-60 ◊û◊†◊ï◊™. ◊î◊†◊ê◊©◊ù ◊¶◊¢◊ô◊®, ◊ë◊¢◊ú ◊î◊®◊©◊¢◊î ◊ô◊ó◊ô◊ì◊î ◊ë◊¢◊ë◊ô◊®◊î ◊ñ◊î◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ê◊ô◊û◊• ◊ê◊™ ◊î◊î◊°◊ì◊® ◊ï◊í◊ñ◊® ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊û◊©◊ö 21 ◊ó◊ï◊ì◊©◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 5:\n",
      "◊ß◊†◊° ◊ë◊°◊ö 1,000 ‚Ç™ ◊ê◊ï 10 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊¢◊ì ◊ô◊ï◊ù 1.4.2015.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 6:\n",
      "◊ë◊°◊ö ◊î◊õ◊ú ◊ô◊®◊¶◊î ◊ê◊§◊ï◊ê ◊î◊†◊ê◊©◊ù 28 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊î◊ó◊ú ◊û◊ô◊ï◊ù ◊û◊¢◊¶◊®◊ï (6.7.13). ◊õ◊û◊ï ◊õ◊ü ◊ô◊© ◊ú◊†◊õ◊ï◊™ ◊û◊™◊ß◊ï◊§◊™ ◊î◊û◊ê◊°◊® ◊ê◊™ ◊î◊ô◊û◊ô◊ù ◊ë◊î◊ù ◊î◊ô◊î ◊î◊†◊ê◊©◊ù ◊¢◊¶◊ï◊® ◊ë◊í◊ô◊ü ◊î◊™◊ô◊ß◊ô◊ù ◊©◊¶◊ï◊®◊§◊ï, ◊ì◊î◊ô◊ô◊†◊ï: ◊û◊ô◊ï◊ù 10.6.09 ◊¢◊ì ◊ô◊ï◊ù 12.6.09; ◊ï◊ô◊ï◊ù 7.2.09.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 7:\n",
      "◊ê◊†◊ô ◊í◊ï◊ñ◊® ◊¢◊ú ◊î◊†◊ê◊©◊ù  9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ê◊ï◊™◊ù ◊ô◊®◊¶◊î ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊®◊ï◊™ ◊ë◊ê◊í◊ï◊ì◊î ◊ú◊û◊¢◊ü ◊î◊ß◊©◊ô◊© ◊ë◊®◊ó' ◊ê◊®◊ô◊ê◊ú ◊©◊®◊ï◊ü 9 ◊ë◊ò◊ô◊®◊™ ◊î◊õ◊®◊û◊ú ( 5 ◊ô◊û◊ô◊ù ◊ë◊©◊ë◊ï◊¢) ◊ú◊ú◊ê ◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 8:\n",
      "◊ú◊†◊ï◊õ◊ó ◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢◊î÷º ◊©◊ú ◊¢◊ë◊ô◊®◊™ ◊î◊°◊ó◊® ◊ë◊°◊û◊ô◊ù, ◊©◊ë◊ï◊¶◊¢◊î ◊ë◊°◊û◊ô◊ù ◊©◊î◊©◊§◊¢◊™◊ù ◊í◊ì◊ï◊ú◊î ◊ê◊ö ◊ë◊õ◊û◊ï◊ô◊ï◊™ ◊ß◊ò◊†◊ï◊™ ◊ô◊ó◊°◊ô◊™ ◊ï◊ë◊ô◊ï◊ù ◊ê◊ó◊ì, ◊ï◊ú◊†◊ï◊õ◊ó ◊°◊ï◊í ◊î◊°◊û◊ô◊ù ◊©◊î◊ï◊ó◊ñ◊ß◊ï ◊ï◊õ◊û◊ï◊™◊ù ◊ï◊®◊û◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊û◊¢◊©◊ô◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊†◊¢ ◊ë◊ô◊ü 6 ◊ú-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ê◊¶◊ô◊ô◊ü ◊ë◊î◊ß◊©◊® ◊ñ◊î ◊õ◊ô ◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü ◊†◊ï◊¢◊ì ◊ú◊î◊ë◊†◊ï◊™ ◊ê◊™ ◊©◊ô◊ß◊ï◊ú ◊î◊ì◊¢◊™ ◊î◊©◊ô◊§◊ï◊ò◊ô ◊ë◊¢◊†◊ô◊©◊î, ◊ê◊ö ◊ú◊ê ◊ú◊ë◊ò◊ú◊ï. ◊î◊ï◊ê◊ô◊ú ◊ï◊õ◊ö, ◊ß◊ë◊ô◊¢◊™ ◊û◊™◊ó◊û◊ô ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊©◊û◊™◊ó◊ô◊ú◊ô◊ù ◊ë-7 ◊ê◊ï ◊ë-8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊î◊û◊ï◊†◊¢◊™ ◊û◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ú◊©◊ß◊ï◊ú ◊ú◊î◊ï◊®◊ï◊™ ◊õ◊ô ◊î◊¢◊ï◊†◊© ◊ô◊®◊ï◊¶◊î ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊†◊®◊ê◊ô◊™ ◊ú◊ô ◊ë◊¢◊ô◊ô◊™◊ô◊™. ◊î◊§◊¢◊® ◊û◊ë◊ó◊ô◊†◊™ ◊ê◊ï◊®◊ö ◊™◊ß◊ï◊§◊™ ◊î◊û◊ê◊°◊® ◊ë◊ô◊ü 6 ◊ú-8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊î◊ï◊ê ◊ß◊ò◊ü ◊ë◊û◊ô◊ì◊î ◊õ◊ñ◊ï ◊©◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊ï◊û◊® ◊©◊ì◊ï◊ï◊ß◊ê ◊ê◊ó◊ì ◊û◊ë◊ô◊ü ◊©◊†◊ô ◊î◊¢◊ï◊†◊©◊ô◊ù ◊î◊ï◊ê ◊î◊¢◊ï◊†◊© ◊î◊ô◊ó◊ô◊ì ◊î◊î◊ï◊ú◊ù. ◊ú◊¢◊ï◊û◊™ ◊ñ◊ê◊™, ◊î◊î◊ë◊ì◊ú ◊ë◊ô◊ü ◊®◊ô◊¶◊ï◊ô◊ï ◊©◊ú ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊õ◊ú◊ô◊ê◊î ◊ê◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊î◊ï◊ê ◊û◊î◊ï◊™◊ô. ◊ú◊†◊ï◊õ◊ó ◊û◊°◊ß◊†◊ï◊™◊ô◊î ◊©◊ú ◊î◊ï◊ï◊¢◊ì◊î ◊î◊¶◊ô◊ë◊ï◊®◊ô◊™ ◊ú◊ë◊ó◊ô◊†◊™ ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊ï◊î◊ò◊ô◊§◊ï◊ú ◊ë◊¢◊ë◊®◊ô◊ô◊†◊ô◊ù ◊ë◊®◊ê◊©◊ï◊™ ◊î◊©◊ï◊§◊ò◊™ ◊ë◊ì◊ô◊û◊ï◊° ◊ì◊ú◊ô◊î ◊ì◊ï◊®◊†◊® (◊û◊ê◊ï◊í' 2015) ◊©◊ú◊§◊ô◊î◊ü ◊†◊©◊ô◊ê◊™ ◊¢◊ï◊†◊© ◊ë◊ß◊î◊ô◊ú◊î ◊™◊ï◊®◊û◊™ ◊ô◊ï◊™◊® ◊ú◊©◊ô◊ß◊ï◊ù ◊î◊†◊ê◊©◊ù ◊ï◊û◊ß◊ò◊ô◊†◊î ◊ê◊™ ◊î◊°◊ô◊õ◊ï◊ô ◊©◊ô◊§◊©◊¢ ◊©◊ï◊ë ◊ï◊ê◊ô◊ú◊ï ◊î◊î◊®◊™◊¢◊î ◊î◊û◊ï◊©◊í◊™ ◊û◊®◊ô◊¶◊ï◊ô ◊û◊ê◊°◊® ◊ë◊õ◊ú◊ô◊ê◊î ◊î◊ô◊ê ◊û◊ï◊í◊ë◊ú◊™, ◊ë◊®◊ô ◊õ◊ô ◊ë◊û◊ß◊®◊ô◊ù ◊û◊¢◊ô◊ü ◊ê◊ú◊î ◊®◊¶◊ï◊ô ◊ú◊ß◊ë◊ï◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊©◊ë◊û◊ß◊®◊ô◊ù ◊î◊û◊™◊ê◊ô◊û◊ô◊ù ◊ô◊ê◊§◊©◊® ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ú◊î◊ï◊®◊ï◊™ ◊©◊î◊¢◊ï◊†◊© ◊ô◊®◊ï◊¶◊î ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊¢◊ú ◊õ◊ö ◊ô◊© ◊ú◊î◊ï◊°◊ô◊£ ◊õ◊ô ◊î◊¶◊§◊ô◊§◊ï◊™ ◊î◊®◊ë◊î ◊ë◊ë◊ô◊™ ◊î◊°◊ï◊î◊®, ◊©◊ê◊ô◊†◊ù ◊¢◊ï◊û◊ì◊ô◊ù ◊ë◊î◊ô◊ë◊ò ◊ñ◊î ◊ë◊°◊ò◊†◊ì◊®◊ò◊ô◊ù ◊ë◊ô◊†◊ú◊ê◊ï◊û◊ô◊ô◊ù, ◊û◊ó◊ô◊ô◊ë◊™ ◊ê◊£ ◊î◊ô◊ê ◊¶◊û◊¶◊ï◊ù ◊ë◊õ◊ú◊ô◊ê◊î ◊ë◊û◊ß◊®◊ô◊ù ◊©◊†◊í◊ñ◊®◊ô◊ù ◊¢◊ï◊†◊©◊ô ◊û◊ê◊°◊® ◊ß◊¶◊®◊ô◊ù ◊ô◊ó◊°◊ô◊™ (◊®◊ê◊ï ◊ë◊í\"◊• 1892/14 ◊î◊ê◊í◊ï◊ì◊î ◊ú◊ñ◊õ◊ï◊ô◊ï◊™ ◊î◊ê◊ñ◊®◊ó ◊ë◊ô◊©◊®◊ê◊ú ◊†' ◊î◊©◊® ◊ú◊ë◊ò◊ó◊ï◊ü ◊§◊†◊ô◊ù (2017)). ◊ñ◊ï◊î◊ô ◊î◊°◊ô◊ë◊î ◊ú◊õ◊ö ◊©◊î◊û◊ó◊ï◊ß◊ß ◊ê◊£ ◊ß◊ë◊¢ ◊ë◊î◊ï◊®◊ê◊™ ◊©◊¢◊î (◊©◊ò◊®◊ù ◊†◊õ◊†◊°◊î ◊ú◊™◊ï◊ß◊£) ◊©◊†◊ô◊™◊ü ◊ô◊î◊ô◊î ◊ú◊®◊¶◊ï◊™ ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊í◊ù ◊¢◊ï◊†◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊ë◊ô◊ü 6 ◊ú-9 ◊ó◊ï◊ì◊©◊ô◊ù (◊ê◊ù ◊õ◊ô ◊¢◊ú◊ô ◊ú◊î◊¢◊ô◊® ◊©◊ë◊î◊™◊ó◊©◊ë ◊ë◊î◊©◊ú◊õ◊ï◊™ ◊î◊õ◊ú◊õ◊ú◊ô◊ï◊™ ◊©◊ú ◊®◊ô◊¶◊ï◊ô ◊¢◊ï◊†◊© ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊©◊ë◊û◊î◊ú◊õ◊ï ◊¶◊®◊õ◊ô ◊î◊†◊ô◊ì◊ï◊ü ◊ê◊ô◊†◊ù ◊û◊°◊ï◊§◊ß◊ô◊ù ◊¢◊ú ◊ô◊ì◊ô ◊î◊û◊ì◊ô◊†◊î ◊ï◊ê◊ô◊ü ◊î◊ï◊ê ◊ô◊õ◊ï◊ú ◊ú◊î◊™◊§◊®◊†◊°, ◊ú◊ê ◊ë◊ò◊ï◊ó ◊©◊®◊¶◊ï◊ô ◊ú◊í◊ñ◊ï◊® ◊¢◊ï◊†◊© ◊û◊°◊ï◊í ◊ñ◊î ◊ú◊™◊ß◊ï◊§◊î ◊õ◊î ◊û◊û◊ï◊©◊õ◊™).\n",
      "Real: 1 | Predicted: 0\n",
      "\n",
      "üìù Text 9:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ú◊ê◊ó◊® ◊©◊¢◊ô◊ô◊†◊™◊ô ◊ë◊§◊°◊ô◊ß◊î ◊ê◊ú◊ô◊î ◊î◊§◊†◊ï ◊î◊¶◊ì◊ì◊ô◊ù ◊ï◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô  ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊™◊ô◊ß ◊ñ◊î, ◊†◊¢ ◊ë◊ô◊ü 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú  ◊ú◊ë◊ô◊ü 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 10:\n",
      "◊ú◊ê◊ó◊® ◊©◊©◊ß◊ú◊™◊ô ◊ê◊™ ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ë◊ô◊®◊ï◊™, ◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢÷º◊ü ◊õ◊û◊§◊ï◊®◊ò ◊ú◊¢◊ô◊ú, ◊î◊¢◊®◊õ◊ô◊ù ◊î◊ó◊ë◊®◊™◊ô◊ô◊ù ◊¢◊ú◊ô◊î◊ù ◊ô◊© ◊ú◊î◊í◊ü, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊î◊ù ◊ï◊î◊§◊°◊ô◊ß◊î ◊î◊†◊î◊ï◊í◊î, ◊ê◊†◊ô ◊°◊ë◊ï◊® ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ó◊° ◊ú◊ê◊ô◊®◊ï◊¢ ◊î◊®◊ê◊©◊ï◊ü ◊¶◊®◊ô◊ö ◊ú◊õ◊ú◊ï◊ú ◊®◊õ◊ô◊ë ◊©◊ú ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊ú◊ê ◊ô◊§◊ó◊™ ◊û-  20 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ú◊ê ◊ô◊¢◊ú◊î ◊¢◊ú  48 ◊ó◊ï◊ì◊©◊ô◊ù ◊ë◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù, ◊ï◊ë◊ô◊ó◊° ◊ú◊ê◊ô◊®◊ï◊¢ ◊î◊©◊†◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊û◊™◊ó◊ô◊ú ◊ë◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ï◊¢◊ì ◊ú◊û◊ê◊°◊® ◊©◊ú ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô◊ù ◊ê◊ï◊™◊ï ◊†◊ô◊™◊ü ◊ú◊®◊¶◊ï◊™ ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 11:\n",
      "◊î◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü ◊©◊†◊§◊í◊¢ ◊õ◊™◊ï◊¶◊ê◊î ◊û◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î ◊¢◊†◊ô◊ô◊†◊ï ◊§◊í◊ô◊¢◊î ◊ë◊©◊ú◊ï◊û◊ï ◊ï◊ë◊ò◊ó◊ï◊†◊ï ◊©◊ú ◊î◊¶◊ô◊ë◊ï◊®, ◊î◊¢◊ï◊†◊© ◊î◊ß◊ë◊ï◊¢ ◊ú◊¶◊ô◊ì◊î ◊©◊ú ◊î◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊õ◊ô◊ü ◊î◊ï◊ê ◊ó◊û◊© ◊©◊†◊ï◊™ ◊û◊ê◊°◊®.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 12:\n",
      "◊î◊¢◊ï◊ú◊î ◊û◊§◊°◊ß◊ô ◊î◊ì◊ô◊ü ◊©◊î◊ï◊ë◊ê◊ï ◊ú◊¢◊ô◊ú ◊î◊ï◊ê ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊õ◊ô◊ü/◊ê◊í◊®◊ï◊§◊ü ◊©◊ú◊ê ◊ú◊û◊ò◊®◊î ◊õ◊©◊®◊î, ◊ï◊ñ◊ê◊™ ◊õ◊ê◊©◊® ◊ú◊ê ◊†◊¢◊©◊î ◊©◊ô◊û◊ï◊© ◊§◊ï◊í◊¢◊†◊ô ◊ë◊°◊õ◊ô◊ü, ◊†◊¢ ◊ë◊ô◊ü ◊î◊®◊©◊¢◊î ◊î◊û◊ú◊ï◊ï◊î ◊ë◊©◊ú\"◊¶ ◊ú◊ë◊ô◊ü 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊¶◊ô◊®◊ï◊£ ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô.\n",
      "Real: 0 | Predicted: 1\n",
      "\n",
      "üìù Text 13:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 7 ◊ó◊ï◊ì◊©◊ô◊ù ◊ú◊û◊©◊ö ◊©◊ú◊ï◊© ◊©◊†◊ô◊ù ◊û◊ô◊ï◊ù ◊©◊ó◊®◊ï◊®◊ù ◊©◊ú ◊î◊†◊ê◊©◊û◊ô◊ù ◊û◊û◊ê◊°◊®. ◊î◊™◊†◊ê◊ô ◊î◊ï◊ê ◊©◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊†◊ê◊©◊û◊ô◊ù ◊ú◊ê ◊ô◊¢◊ë◊ï◊® ◊¢◊ë◊ô◊®◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊§◊©◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 14:\n",
      "◊†◊°◊ô◊ë◊ï◊™ ◊û◊ó◊û◊ô◊®◊ï◊™ ◊ê◊ú◊î ◊©◊†◊°◊ß◊®◊ï ◊ú◊¢◊ô◊ú, ◊¢◊ú ◊®◊ß◊¢ ◊î◊§◊°◊ô◊ß◊î ◊î◊†◊ï◊î◊í◊™, ◊õ◊§◊ô ◊î◊†◊ñ◊õ◊® ◊ë◊í◊ñ◊® ◊ì◊ô◊†◊ï ◊©◊ú ◊õ◊ë' ◊î◊©' ◊ë◊ü ◊ò◊ï◊ú◊ô◊ú◊î, ◊û◊ë◊ô◊ê◊ï◊™ ◊ê◊ï◊™◊ô ◊ú◊ß◊ë◊ï◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î ◊©◊ë◊§◊†◊ô◊†◊ï ◊î◊ï◊ê ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú-12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ú◊¶◊ì ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊ß◊†◊° ◊õ◊°◊§◊ô.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 15:\n",
      "◊ë◊¢\"◊§ 6489/20 ◊§◊ï◊ß◊ú◊ï◊†◊°◊ß◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (8.4.2021)  ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊® ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊î◊ß◊û◊™ ◊û◊¢◊ë◊ì◊î, ◊í◊ô◊ì◊ï◊ú ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊©◊ú100  ◊ß\"◊í, ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊î◊ó◊ñ◊ß◊™ ◊õ◊ú◊ô◊ù. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 48-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊ë◊î◊™◊ó◊©◊ë ◊ë◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊ì◊ï◊û◊ï◊™ ◊†◊ì◊ï◊ü ◊ú◊û◊ê◊°◊® ◊ë◊ü 33 ◊ó◊ï◊ì◊©◊ô◊ù;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 16:\n",
      "9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊î ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊î◊†◊ê◊©◊ù ◊ô◊ë◊¶◊¢ ◊ê◊™ ◊¢◊ë◊ï◊ì◊ï◊™ ◊î◊©◊ô◊®◊ï◊™ ◊ë◊û◊°◊í◊®◊™ \"◊©◊ï◊ï◊ô◊ù ◊§\"◊™\" ◊ë◊®◊ó◊ï◊ë ◊ë◊ü ◊¶◊ô◊ï◊ü ◊í◊ú◊ô◊° 34, ◊°◊í◊ï◊ú◊î. ◊î◊õ◊ú ◊ë◊î◊™◊ê◊ù ◊ú◊ê◊û◊ï◊® ◊ë◊ó◊ï◊ï◊™ ◊ì◊¢◊™ ◊î◊û◊û◊ï◊†◊î ◊¢◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊û◊ô◊ï◊ù 14/10/24, ◊ï◊ë◊õ◊§◊ï◊£ ◊ú◊û◊í◊ë◊ú◊ï◊™ ◊î◊û◊†◊ï◊ô◊ï◊™ ◊ë◊ó◊ï◊ï◊™ ◊î◊ì◊¢◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 17:\n",
      "◊®◊¢\"◊§ 9426/04 ◊í◊ï◊®◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (28.10.04). ◊î◊†◊ê◊©◊ù ◊î◊ó◊ñ◊ô◊ß ◊ë◊î◊ñ◊ì◊û◊†◊ï◊™ ◊ê◊ó◊™ 800 ◊í◊®◊ù ◊†◊ò◊ï ◊©◊ú ◊ó◊©◊ô◊©. ◊î◊†◊ê◊©◊ù ◊†◊ì◊ï◊ü ◊¢◊ú ◊ô◊ì◊ô ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ú-10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊™◊ï◊ö ◊î◊§◊¢◊ú◊î ◊©◊ú ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊©◊ú ◊©◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô◊ù, ◊ó◊ú◊ß◊ï ◊ë◊ó◊ï◊§◊£ ◊ï◊ó◊ú◊ß◊ï ◊ë◊û◊¶◊ò◊ë◊®, ◊õ◊ö ◊©◊°◊ö ◊î◊õ◊ú ◊î◊ï◊©◊™◊ï ◊¢◊ú◊ô◊ï 13 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊®◊¢◊ï◊®◊ï ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊†◊ì◊ó◊î ◊ï◊õ◊ü ◊†◊ì◊ó◊™◊î ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊©◊î◊ï◊í◊©◊î ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 18:\n",
      "◊ë◊¢◊§\"◊í (◊û◊ó◊ï◊ñ◊ô ◊ë◊ê◊® ◊©◊ë◊¢) 13427-04-13 ◊ì◊ï◊ì ◊ß◊ì◊ï◊© ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 2.9.2013) ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊¢◊ú ◊ô◊°◊ï◊ì ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊°◊§◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 60.3 ◊í◊®◊ù ◊ï- 1.34 ◊í◊®◊ù ◊ï◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô (0.98 ◊í◊®◊ù ◊ï-19.25 ◊í◊®◊ù). ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ê◊ô◊©◊® ◊ê◊™ ◊î◊¢◊ï◊†◊© ◊©◊î◊ï◊ò◊ú ◊ë◊¢◊®◊õ◊ê◊î ◊î◊ì◊ô◊ï◊†◊ô◊™, ◊ê◊ö ◊ß◊ë◊¢ ◊©◊ó◊ú◊ß ◊û◊î◊û◊ê◊°◊®◊ô◊ù ◊î◊û◊ï◊™◊†◊ô◊ù ◊©◊î◊ï◊§◊¢◊ú◊ï ◊ô◊®◊ï◊¶◊ï ◊ë◊ó◊ï◊§◊£, ◊õ◊ö ◊©◊¢◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊ò◊ú◊ï 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊ú◊ê◊ó◊® ◊î◊§◊¢◊ú◊™ ◊©◊†◊ô ◊û◊ê◊°◊®◊ô◊ù ◊û◊ï◊™◊†◊ô◊ù ◊ó◊ú◊ß◊ù ◊ë◊ó◊ï◊§◊£, ◊î◊ï◊©◊™◊ï ◊¢◊ú ◊î◊†◊ê◊©◊ù 16 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ß◊†◊° ◊õ◊°◊§◊ô ◊ï◊§◊°◊ô◊ú◊™ ◊®◊ô◊©◊ô◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 19:\n",
      "◊ú◊ê◊ó◊® ◊©◊ß◊ô◊ú◊™ ◊õ◊ú◊ú ◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù, ◊¢◊™◊ô◊®◊™◊î ◊î◊¢◊ï◊†◊©◊ô◊™ ◊©◊ú ◊î◊û◊ê◊©◊ô◊û◊î, ◊ó◊ú◊ß◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ë◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù ◊ï◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ê◊†◊ô ◊ß◊ï◊ë◊¢◊™ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 4 ◊ï◊¢◊ì 6.5 ◊©◊†◊ï◊™ ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 20:\n",
      "◊ú◊ê◊ï◊® ◊õ◊ú ◊î◊ê◊û◊ï◊® ◊ú◊¢◊ô◊ú, ◊î◊†◊†◊ô ◊ß◊ï◊ë◊¢ ◊©◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊ê◊ô◊®◊ï◊¢ ◊†◊©◊ï◊ê ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊©◊¢◊†◊ô◊ô◊†◊ï ◊î◊ó◊ñ◊ß◊î ◊©◊ú 200 ◊í◊®◊ù ◊ó◊©◊ô◊© ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊†◊¢ ◊ë◊ô◊ü 6 ◊ú-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 21:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û- 4 ◊©◊†◊ï◊™ ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊ì 7 ◊©◊†◊ï◊™ ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 22:\n",
      "◊î◊†◊ê◊©◊û◊™ ◊™◊ó◊™◊ï◊ù ◊¢◊ú ◊î◊™◊ó◊ô◊ô◊ë◊ï◊™ ◊ë◊°◊ö ◊©◊ú 3,000 ‚Ç™ ◊©◊ú◊ê ◊™◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊î ◊ë◊î ◊î◊ï◊®◊©◊¢◊î. ◊ú◊ê ◊™◊ó◊™◊ù ◊î◊î◊™◊ó◊ô◊ô◊ë◊ï◊™ ◊™◊ô◊ê◊°◊® ◊ú◊û◊©◊ö 5 ◊ô◊û◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 23:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊™◊ô◊ß ◊î◊¢◊ô◊ß◊®◊ô (◊î◊§◊®◊™ ◊î◊ï◊®◊ê◊î ◊ó◊ï◊ß◊ô◊™) ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì ◊ú-6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊™◊ô◊ß ◊î◊û◊¶◊ï◊®◊£ ◊î◊®◊ê◊©◊ï◊ü (◊í◊ô◊ì◊ï◊ú ◊°◊û◊ô◊ù) ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û- 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú- 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊™◊ô◊ß ◊î◊û◊¶◊ï◊®◊£ ◊î◊©◊†◊ô (◊î◊ó◊ñ◊ß◊™ ◊°◊õ◊ô◊ü ◊ï◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™) ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú- 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 1\n",
      "\n",
      "üìù Text 24:\n",
      "◊ë◊õ◊ú ◊î◊†◊ï◊í◊¢ ◊ú◊®◊õ◊ô◊ë ◊î◊§◊°◊ô◊ú◊î ◊ë◊§◊ï◊¢◊ú ◊†◊ï◊õ◊ó ◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊ï◊™, ◊î◊©◊ô◊û◊ï◊© ◊ë◊®◊õ◊ë, ◊ï◊î◊¢◊ï◊ë◊ì◊î ◊©◊î◊†◊ê◊©◊ù ◊†◊î◊í ◊™◊ó◊™ ◊î◊©◊§◊¢◊™ ◊°◊ù (◊î◊í◊ù ◊©◊ú◊ê ◊î◊ï◊ê◊©◊ù ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊†◊î◊ô◊í◊î ◊ë◊©◊õ◊®◊ï◊™/◊™◊ó◊™ ◊î◊©◊§◊¢◊™ ◊°◊û◊ô◊ù) ◊ê◊†◊ô ◊°◊ë◊ï◊® ◊©◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊©  ◊¶◊®◊ô◊ö ◊ú◊õ◊ú◊ï◊ú ◊®◊õ◊ô◊ë ◊©◊ú ◊§◊°◊ô◊ú◊î ◊ë◊§◊ï◊¢◊ú ◊©◊ú◊ê ◊ô◊§◊ó◊™ ◊û-24 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ú◊ê ◊ô◊¢◊ú◊î ◊¢◊ú 48 ◊ó◊ï◊ì◊©◊ô◊ù.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 25:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ê◊©◊® ◊ú◊†◊ê◊©◊ù 1 ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û- 36 ◊ï◊¢◊ì ◊ú- 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊ê◊ô◊ú◊ï ◊ë◊ê◊©◊® ◊ú◊†◊ê◊©◊ù 2 ◊î◊û◊™◊ó◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û- 24 ◊ï◊¢◊ì ◊ú- 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 26:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô, ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ó◊° ◊ú◊¢◊ë◊ô◊®◊ï◊™ ◊î◊°◊ó◊® ◊ï◊õ◊ü ◊¢◊ë◊ô◊®◊ï◊™ ◊î◊î◊ó◊ñ◊ß◊î ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û-18 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊¢◊ì ◊ú-30 ◊ó◊ï◊ì◊©◊ô◊ù ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 27:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú- 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊í◊ô◊ü ◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊ê◊ô◊®◊ï◊¢◊ô◊ù.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 28:\n",
      "◊†◊ï◊õ◊ó ◊î◊ê◊û◊ï◊®, ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊™ ◊î◊°◊§◊ß◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú ◊ñ◊¢◊ô◊® ◊ú◊¢◊ô◊©◊ï◊ü ◊ï◊õ◊ì◊ï◊® ◊ê◊ó◊ì ◊©◊ú ◊ê◊ß◊°◊ò◊ñ◊ô, ◊ú◊ê◊ï◊® ◊î◊†◊°◊ô◊ë◊ï◊™ ◊î◊ß◊ï◊†◊ß◊®◊ò◊ô◊ï◊™ ◊ï◊î◊¢◊ß◊®◊ï◊†◊ï◊™ ◊©◊ë◊ô◊°◊ï◊ì ◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü ◊¶◊®◊ô◊ö ◊ú◊†◊ï◊¢ ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊ï◊ì◊ì◊ô◊ù ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ú-12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊û◊ê◊°◊®◊ô◊ù ◊û◊ï◊™◊†◊ô◊ù, ◊§◊°◊ô◊ú◊™ ◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ï◊¢◊†◊ô◊©◊î ◊õ◊ú◊õ◊ú◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 29:\n",
      "20 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊§◊ï◊¢◊ú, ◊û◊ô◊ï◊ù ◊û◊¢◊¶◊®◊ï ◊ë◊™◊ô◊ß ◊ñ◊î (19.3.24).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 30:\n",
      "◊¢◊†◊ô◊ô◊ü ◊§◊ú◊ï◊†◊ô - ◊ë◊í◊ô◊ü ◊ê◊ó◊ì ◊î◊ê◊ô◊©◊ï◊û◊ô◊ù ◊ë◊õ◊™◊ë ◊ê◊ô◊©◊ï◊ù ◊û◊™◊ï◊ß◊ü ◊©◊î◊ï◊í◊© ◊õ◊†◊í◊ì ◊î◊û◊¢◊®◊¢◊® ◊ï◊ê◊ó◊®◊ô◊ù, ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊ï◊õ◊ü ◊ë◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü. ◊ì◊ï◊ë◊® ◊ë◊û◊õ◊ô◊®◊™ 51 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ú◊°◊ï◊õ◊ü ◊û◊©◊ò◊®◊™◊ô. ◊ë◊í◊ô◊ü ◊ê◊ô◊©◊ï◊ù ◊ê◊ó◊®, ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü. ◊ì◊ï◊ë◊® ◊ë◊û◊õ◊ô◊®◊™ ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú 50.08 ◊í◊®◊ù ◊ï◊õ◊ì◊ï◊® ◊ì◊ï◊í◊û◊ô◊™ ◊©◊ú \"MDMA\" ◊ú◊°◊ï◊õ◊ü. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊†◊¢ ◊ë◊ô◊ü 40 ◊ú-70 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊ú◊ë◊°◊ï◊£ ◊í◊ñ◊® ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® 43 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 31:\n",
      "◊ú◊î◊©◊ú◊û◊™ ◊î◊™◊û◊ï◊†◊î ◊ô◊¶◊ï◊ô◊ü ◊õ◊ô ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù 1, ◊ê◊©◊® ◊ú◊ê ◊î◊ï◊®◊©◊¢ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊ï◊™ ◊û◊ï◊©◊ú◊û◊ï◊™ ◊©◊ú ◊î◊™◊§◊®◊¶◊ï◊™ ◊ú◊ë◊ô◊™ ◊û◊í◊ï◊®◊ô◊ù ◊ï◊†◊ô◊°◊ô◊ï◊ü ◊í◊†◊ô◊ë◊î, ◊ê◊ú◊ê ◊ë◊°◊ô◊ï◊¢, ◊ß◊ë◊¢◊™◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊ú◊í◊ë◊ô◊ï ◊†◊¢ ◊ë◊ô◊ü 6 ◊ó◊ï◊ì◊©◊ô◊ù, ◊ê◊©◊® ◊ë◊û◊ß◊®◊ô◊ù ◊î◊û◊™◊ê◊ô◊û◊ô◊ù ◊ô◊õ◊ï◊ú ◊ï◊ô◊ï◊ò◊ú◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ú◊ë◊ô◊ü 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊õ◊ü ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô, ◊ß◊†◊° ◊ï◊§◊ô◊¶◊ï◊ô ◊ú◊û◊™◊ú◊ï◊†◊†◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 32:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ï◊ê ◊î◊ó◊ú ◊û-13 ◊ï◊¢◊ì ◊ú-30 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 33:\n",
      "◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô -  ◊ú◊ó◊ï◊ë◊™ ◊î◊†◊ê◊©◊ù ◊û◊í◊ï◊ï◊ü ◊î◊®◊©◊¢◊ï◊™ ◊î◊û◊©◊ß◊§◊ï◊™ ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊ï◊¶◊¢◊ï ◊ë◊ô◊ü ◊î◊©◊†◊ô◊ù 2001 - 2010. ◊ë◊ô◊ü ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊ë◊î◊ü ◊î◊ï◊®◊©◊¢ ◊†◊ô◊™◊ü ◊ú◊û◊†◊ï◊™ ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊§◊®◊¢◊î ◊ú◊©◊ï◊ò◊®, ◊î◊ó◊ñ◊ß◊™ ◊®◊õ◊ï◊© ◊ó◊©◊ï◊ì ◊õ◊í◊†◊ï◊ë, ◊î◊ì◊ó◊î ◊ë◊ó◊ß◊ô◊®◊î, ◊©◊ô◊ë◊ï◊© ◊î◊ú◊ô◊õ◊ô ◊û◊©◊§◊ò, ◊ê◊ô◊ï◊û◊ô◊ù ◊ï◊õ◊ü ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊ï◊õ◊ú◊ô◊ù ◊ú◊°◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊î◊†◊ê◊©◊ù ◊®◊ô◊¶◊î ◊ë◊¢◊ë◊® ◊©◊†◊ô ◊û◊ê◊°◊® ◊ß◊¶◊®◊ô◊ù (◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊ï◊©◊ú◊ï◊©◊î ◊ó◊ï◊ì◊©◊ô◊ù). ◊ê◊°◊õ◊ù ◊ï◊ê◊ï◊û◊® ◊õ◊ô ◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ê◊ô◊†◊ï ◊û◊ê◊§◊©◊® ◊û◊™◊ü ◊î◊ß◊ú◊î ◊û◊©◊û◊¢◊ï◊™◊ô◊™, ◊ê◊ö ◊¢◊ù ◊ñ◊ê◊™, ◊ê◊ô◊†◊ï ◊õ◊ï◊ú◊ú ◊¢◊ë◊ô◊®◊ï◊™ ◊°◊û◊ô◊ù ◊ó◊û◊ï◊®◊ï◊™ ◊ê◊ï ◊®◊ô◊¶◊ï◊ô ◊™◊ß◊ï◊§◊ï◊™ ◊û◊ê◊°◊® ◊û◊û◊ï◊©◊õ◊ï◊™ ◊ï◊¢◊ú ◊õ◊ü ◊ê◊ô◊†◊ï ◊û◊¶◊ì◊ô◊ß ◊ê◊£ ◊î◊ó◊û◊®◊î ◊†◊ô◊õ◊®◊™ ◊ë◊¢◊†◊ô◊©◊î. ◊ë◊î◊™◊ô◊ô◊ó◊° ◊ú◊û◊ê◊°◊® ◊î◊û◊ï◊™◊†◊î  - ◊û◊ì◊ï◊ë◊® ◊ë◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊©◊î◊ï◊ò◊ú ◊ë◊©◊†◊™ 2011 ◊î◊û◊©◊ß◊£ ◊¢◊ë◊ô◊®◊ï◊™ ◊°◊û◊ô◊ù ◊©◊ú ◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊©◊ë◊ï◊¶◊¢◊ï ◊ë◊©◊†◊™ 2005, ◊ì◊î◊ô◊ô◊†◊ï ◊ú◊§◊†◊ô ◊õ-9 ◊©◊†◊ô◊ù. ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ê◊ú◊î, ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊ß◊ô◊ô◊û◊™ ◊î◊¶◊ì◊ß◊î ◊ú◊î◊§◊¢◊ú◊™ ◊î◊û◊ê◊°◊® ◊î◊û◊ï◊™◊†◊î ◊ë◊ó◊§◊ô◊§◊î ◊û◊°◊ï◊ô◊û◊™ ◊ú◊¢◊ï◊†◊© ◊î◊û◊ê◊°◊® ◊ê◊©◊® ◊ô◊ï◊ò◊ú ◊ë◊í◊ô◊ü ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊î◊†◊ï◊õ◊ó◊ô◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 34:\n",
      "◊¶◊ï ◊û◊ë◊ó◊ü ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 12 ◊ó◊ï◊ì◊©◊ô◊ù ◊î◊ó◊ú ◊û◊î◊ô◊ï◊ù. ◊î◊†◊ê◊©◊ù ◊û◊ï◊ñ◊î◊® ◊ë◊ó◊ï◊ë◊™ ◊©◊ô◊™◊ï◊£ ◊§◊¢◊ï◊ú◊î ◊ï◊õ◊ô ◊ê◊ù ◊ú◊ê ◊ô◊¢◊©◊î ◊õ◊ü, ◊†◊ô◊™◊ü ◊ô◊î◊ô◊î ◊ú◊ë◊ò◊ú ◊ê◊™ ◊î◊¶◊ï ◊ï◊ú◊í◊ñ◊ï◊® ◊¢◊ú◊ô◊ï ◊¢◊ï◊†◊© ◊ê◊ó◊® ◊™◊ó◊™◊ô◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 35:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö  4 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊ô◊©◊ê ◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊ï◊™ ◊¢◊ú ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊¢◊ï◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 36:\n",
      "◊ë◊î◊™◊ó◊©◊ë ◊ë◊¢◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊©◊ú ◊î◊î◊ú◊ô◊û◊î; ◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù; ◊ë◊î◊™◊ó◊©◊ë ◊ë◊ó◊ï◊û◊®◊™ ◊î◊û◊¢◊©◊ô◊ù ◊ï◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢◊ù; ◊ï◊ë◊î◊™◊ó◊©◊ë ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ë◊û◊ß◊®◊ô◊ù ◊ì◊ï◊û◊ô◊ù; ◊ê◊†◊ô ◊°◊ë◊ï◊®◊î ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊õ◊ú ◊ê◊ó◊ì ◊û◊ê◊®◊ë◊¢◊™ ◊î◊ê◊ô◊©◊ï◊û◊ô◊ù ◊†◊©◊ï◊ê ◊î◊™◊ô◊ß ◊î◊¢◊ô◊ß◊®◊ô ◊†◊¢ ◊û◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ß◊¶◊® (◊©◊†◊ô◊™◊ü ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™), ◊ï◊¢◊ì ◊û◊°◊§◊® ◊ë◊ï◊ì◊ì ◊©◊ú ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊õ◊ú ◊ê◊®◊ë◊¢◊™ ◊î◊ê◊ô◊©◊ï◊û◊ô◊ù ◊†◊©◊ï◊ê ◊î◊™◊ô◊ß ◊î◊¢◊ô◊ß◊®◊ô ◊†◊¢ ◊û◊û◊°◊§◊® ◊ë◊ï◊ì◊ì ◊©◊ú ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú (◊î◊û◊¶◊ï◊ô ◊ë◊®◊£ ◊î◊¢◊ú◊ô◊ï◊ü ◊î◊†◊ô◊™◊ü ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™) ◊ï◊¢◊ì 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 37:\n",
      "8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï ◊ë◊™◊ô◊ß ◊ñ◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 38:\n",
      "◊î◊§◊¢◊ú◊™ ◊î◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊©◊ú ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊™\"◊§ 5242/07 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊®◊ê◊ï◊ë◊ü (23.10.07). ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ô◊ï◊§◊¢◊ú ◊ë◊ó◊ï◊§◊£, ◊õ◊ö ◊©◊°◊ö ◊î◊õ◊ú ◊ô◊®◊¶◊î ◊î◊†◊ê◊©◊ù ◊©◊ú◊ï◊©◊î ◊ï◊ó◊¶◊ô ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 39:\n",
      "◊î◊î◊í◊†◊î ◊î◊§◊†◊™◊î ◊ú◊û◊°◊§◊® ◊§◊°◊ß◊ô ◊ì◊ô◊ü ◊õ◊ê◊©◊® ◊û◊™◊ó◊û◊ô ◊î◊¢◊ï◊†◊© ◊©◊†◊ß◊ë◊¢◊ï ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 6 ◊ï◊¢◊ì 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊î◊¢◊ï◊†◊©◊ô◊ù ◊©◊î◊ï◊ò◊ú◊ï ◊î◊ô◊ï ◊ë◊ò◊ï◊ï◊ó ◊©◊ë◊ô◊ü 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú-35 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 40:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö  3 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊î◊†◊ê◊©◊û◊™ ◊ú◊ê ◊™◊ô◊©◊ê ◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊™◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊§◊®◊™ ◊î◊ï◊®◊ê◊î ◊ó◊ï◊ß◊ô◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 41:\n",
      "◊î◊†◊ê◊©◊ù ◊ô◊©◊ú◊ù ◊ß◊†◊° ◊ë◊°◊ö ◊©◊ú 1,500 ‚Ç™, ◊ê◊ï 15 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 42:\n",
      "◊ë◊™\"◊§ (◊©◊ú◊ï◊ù ◊ê◊ô◊ú◊™) 30518-07-11 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊®◊ü ◊ê◊ë◊®◊î◊ù (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 12.1.2012) ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊¢◊ú ◊ô◊°◊ï◊ì ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊û◊°◊í◊®◊™ ◊î◊°◊ì◊® ◊ì◊ô◊ï◊†◊ô ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊ô◊ô◊ë◊ï◊ê, ◊ô◊ô◊¶◊ï◊ê, ◊û◊°◊ó◊® ◊ï◊î◊°◊§◊ß◊™ ◊°◊ù ◊ï◊ë◊¢◊ë◊®◊™ ◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ë◊õ◊ö ◊©◊ß◊ô◊ë◊ú ◊û◊ê◊ó◊® ◊ë◊©◊™◊ô ◊î◊ñ◊ì◊û◊†◊ï◊ô◊ï◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 100 ◊í◊®◊ù ◊ï◊ë◊û◊©◊ß◊ú 199.3 ◊í◊®◊ù, ◊ï◊ó◊ô◊ú◊ß ◊ê◊ï◊™◊ù ◊ú◊û◊†◊ï◊™ ◊ú◊¶◊ï◊®◊ö ◊î◊§◊¶◊™◊ù. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊ë◊ü 19 ◊ú◊ú◊ê ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊î◊ï◊ò◊ú◊ï 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊û◊ê◊°◊®◊ô◊ù ◊û◊ï◊™◊†◊ô◊ù, ◊¶◊ï ◊û◊ë◊ó◊ü ◊ï◊î◊™◊ó◊ô◊ô◊ë◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 43:\n",
      "◊ë◊¢◊§\"◊í (◊û◊ó' ◊ë\"◊©) 13427-04-13 ◊ì◊ï◊ì ◊ß◊ì◊ï◊© ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 2.9.2013) ◊î◊™◊ß◊ë◊ú ◊ó◊ú◊ß◊ô◊™ ◊¢◊®◊¢◊ï◊®◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊ô◊°◊ï◊ì ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊®◊õ◊ê◊î ◊î◊ì◊ô◊ï◊†◊ô◊™, ◊ë◊î◊°◊§◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 60.3 ◊í◊®◊ù, ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 0.98 ◊í◊®◊ù ◊ï◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú ◊©◊ú 19.25 ◊í◊®◊ù ◊ú◊©◊ô◊û◊ï◊©◊ï ◊î◊¢◊¶◊û◊ô. ◊ë◊¢◊®◊õ◊ê◊î ◊î◊ì◊ô◊ï◊†◊ô◊™ ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊ï◊ú◊ù ◊ú◊õ◊ú ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊ô◊ü 6 ◊ú-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊ë◊¢◊ú ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊î◊ï◊ò◊ú◊ï 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î, ◊ß◊†◊°, ◊î◊™◊ó◊ô◊ô◊ë◊ï◊™, ◊§◊°◊ô◊ú◊™ ◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ï◊î◊ï◊§◊¢◊ú ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 44:\n",
      "◊ú◊ê◊ó◊® ◊©◊©◊ß◊ú◊™◊ô ◊ê◊™ ◊õ◊ú ◊î◊ê◊û◊ï◊® ◊ú◊¢◊ô◊ú, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊ë◊ê◊ô◊®◊ï◊¢ ◊ñ◊î, ◊†◊¢ ◊ë◊ô◊ü ◊©◊ú◊ï◊©◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊®◊ï◊™, ◊ï◊¢◊ì 15 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 45:\n",
      "◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô ◊ó◊ô◊§◊î) 65612-01-23 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊í◊®◊ë◊ï◊ê◊ô◊° (24.7.2024) - ◊î◊°◊†◊í◊ï◊® ◊î◊§◊†◊î ◊ú◊í◊ñ◊® ◊ì◊ô◊ü ◊ñ◊î ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊û◊°' 1, ◊ë◊§◊®◊©◊î ◊ë◊î ◊î◊ï◊§◊¢◊ú ◊î◊°◊ï◊õ◊ü ◊î◊°◊û◊ï◊ô ◊ì◊õ◊ê◊ü. ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù 1 ◊ì◊ï◊ë◊® ◊ë◊©◊ú◊ï◊© ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü, ◊ï◊õ◊ü ◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊ß◊©◊ô◊®◊™ ◊ß◊©◊® ◊ú◊§◊©◊¢. ◊ñ◊ê◊™, ◊ú◊ê◊ó◊® ◊©◊ë◊©◊ú◊ï◊© ◊î◊ñ◊ì◊û◊†◊ï◊ô◊ï◊™ ◊©◊ï◊†◊ï◊™ ◊†◊ê◊©◊ù 1 ◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊î◊û◊õ◊ï◊†◊î \"◊ì◊ï◊°◊î\", ◊ï◊û◊õ◊® ◊ú◊°◊ï◊õ◊ü ◊ë◊°◊ö ◊î◊õ◊ú ◊õ-300 ◊í◊®◊ù ◊©◊ú ◊°◊ù ◊ñ◊î. ◊¢◊ï◊ì, ◊†◊ê◊©◊ù 1 ◊ß◊©◊® ◊ß◊©◊® ◊¢◊ù ◊ê◊ó◊® ◊ú◊§◊ô◊ï ◊î◊ê◊ó◊® ◊ô◊û◊õ◊ï◊® ◊ú◊°◊ï◊õ◊ü ◊°◊ù ◊û◊ê◊ï◊™◊ï ◊°◊ï◊í ◊ë◊û◊©◊ß◊ú ◊©◊ú ◊õ-50 ◊í◊®◊ù ◊ï◊õ◊ü ◊†◊™◊§◊° ◊ë◊®◊©◊ï◊™◊ï 0.72 ◊í◊®◊ù ◊©◊ú ◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊©◊ô◊ï◊¢◊ì ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊ú◊ê◊ó◊® ◊©◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊¢◊û◊ì ◊¢◊ú ◊ò◊ô◊ë◊ï ◊©◊ú ◊°◊ù ◊î\"◊ì◊ï◊°◊î\" ◊ï◊™◊®◊õ◊ï◊ë◊™◊ï ◊î◊©◊ï◊†◊î ◊ë◊§◊®◊ò◊ô ◊î◊ê◊ô◊©◊ï◊ù ◊î◊©◊ï◊†◊ô◊ù, ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊†◊¢ ◊ë◊ô◊ü 40 ◊ú-70 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ú◊ë◊°◊ï◊£, ◊î◊ï◊©◊™ ◊¢◊ú ◊†◊ê◊©◊ù 1 ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊ú 42 ◊ó◊ï◊ì◊©◊ô◊ù, ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 46:\n",
      "◊ë◊®◊¢\"◊§ 894/16  ◊§◊®◊• ◊†' ◊û◊ì\"◊ô (10.2.16) ◊†◊ì◊ï◊ü ◊û◊ß◊®◊î ◊î◊ì◊ï◊û◊î ◊ë◊ô◊ï◊™◊® ◊ú◊¢◊†◊ô◊ô◊ü ◊ì◊†◊ü. ◊ë◊ê◊ï◊™◊ï ◊û◊ß◊®◊î ◊†◊ì◊ó◊™◊î ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊©◊ú ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊ß◊ï◊ß◊ê◊ô◊ü ◊©◊ú◊ê ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô, ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 31.05 ◊í◊®◊ù ◊†◊ò◊ï. ◊î◊†◊ê◊©◊ù ◊†◊¢◊ì◊® ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊ê◊ë ◊ú◊ô◊ú◊ì◊ô◊ù ◊ï◊©◊î◊î ◊ë◊û◊©◊ö ◊™◊ß◊ï◊§◊î ◊ê◊®◊ï◊õ◊î ◊ë◊ê◊ô◊ñ◊ï◊ß ◊ê◊ú◊ß◊ò◊®◊ï◊†◊ô. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ê◊ô◊©◊® ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊©◊†◊ß◊ë◊¢: ◊ë◊ô◊ü 15 ◊ú- 30 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊ê◊™ ◊î◊¢◊ï◊†◊© ◊ê◊©◊® ◊ú◊ß◊ó ◊ë◊ó◊©◊ë◊ï◊ü ◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù ◊ï◊î◊ï◊¢◊û◊ì ◊¢◊ú 15 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 47:\n",
      "◊¢\"◊§ 3790/23 ◊©◊ô◊ô◊†◊® ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (28.9.2023) - ◊î◊û◊¢◊®◊¢◊®◊ô◊ù ◊î◊ï◊®◊©◊¢◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊î◊ß◊©◊ï◊®◊ï◊™ ◊ú◊°◊ó◊® ◊ë◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù. ◊¢◊®◊¢◊ï◊® ◊©◊ú ◊û◊¢◊®◊¢◊® ◊û◊°' 1 ◊†◊ì◊ó◊î ◊ú◊ê◊ó◊® ◊©◊î◊ú◊î ◊ó◊ñ◊® ◊ë◊ï ◊û◊¢◊®◊¢◊ï◊®◊ï ◊ë◊î◊û◊ú◊¶◊™ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü. ◊¢◊®◊¢◊ï◊®◊ï ◊©◊ú ◊û◊¢◊®◊¢◊® ◊û◊°' 2 ◊†◊ì◊ó◊î ◊ú◊í◊ï◊§◊ï ◊ë◊§◊°◊ß ◊î◊ì◊ô◊ü ◊î◊†\"◊ú. ◊¢◊ô◊ï◊ü ◊ë◊í◊ñ◊® ◊ì◊ô◊†◊ï ◊©◊ú ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô (◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô ◊ó◊ô◊§◊î) 60222-03-22), ◊û◊¢◊ú◊î ◊õ◊ô ◊ë◊í◊ì◊®◊ô ◊î◊ê◊ô◊©◊ï◊ù ◊î◊®◊ê◊©◊ï◊ü, ◊û◊¢◊®◊¢◊® 1 ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ô◊ï◊¢ ◊ú◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊û◊¢◊®◊¢◊® 2 ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊ì◊ï◊ë◊® ◊ë◊û◊õ◊ô◊®◊™ 29.6 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ú◊°◊ï◊õ◊ü ◊û◊©◊ò◊®◊™◊ô. ◊ë◊í◊ì◊®◊ô ◊î◊ê◊ô◊©◊ï◊ù ◊î◊©◊†◊ô, ◊û◊¢◊®◊¢◊® 1 ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊ì◊ï◊ë◊® ◊ë◊û◊õ◊ô◊®◊™ 96.8 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ú◊°◊ï◊õ◊ü. ◊ë◊ê◊ô◊©◊ï◊ù ◊î◊©◊ú◊ô◊©◊ô, ◊û◊¢◊®◊¢◊® 1 ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ô◊ï◊¢ ◊ú◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü. ◊ì◊ï◊ë◊® ◊ë◊û◊õ◊ô◊®◊™ 101.3 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ú◊°◊ï◊õ◊ü. ◊ë◊ê◊ô◊©◊ï◊ù ◊î◊®◊ë◊ô◊¢◊ô, ◊û◊¢◊®◊¢◊® 2 ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ô◊ï◊¢ ◊ú◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü. ◊ì◊ï◊ë◊® ◊ë◊û◊õ◊ô◊®◊™ 99.9 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ú◊°◊ï◊õ◊ü. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊û◊¢◊®◊¢◊® 1, ◊î◊†◊¢ ◊ë◊ô◊ü 48 ◊ú-72 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊û◊¢◊®◊¢◊® 2 ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊î◊†◊¢ ◊ë◊ô◊ü 30 ◊ú-48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊ú ◊û◊¢◊®◊¢◊® 1 ◊†◊í◊ñ◊®◊ï 54 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú (◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù) ◊ï◊õ◊ü ◊î◊ï◊§◊¢◊ú ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊©◊ú 9 ◊ó◊ï◊ì◊©◊ô◊ù, ◊õ◊ö ◊©◊ê◊®◊ë◊¢◊î ◊ó◊ï◊ì◊©◊ô◊ù ◊ô◊®◊ï◊¶◊ï ◊ë◊û◊¶◊ò◊ë◊®. ◊¢◊ú ◊û◊¢◊®◊¢◊® 2 ◊†◊í◊ñ◊®◊ï 32 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú (◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù) ◊ï◊õ◊ü ◊î◊ï◊§◊¢◊ú ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊©◊ú 9 ◊ó◊ï◊ì◊©◊ô◊ù, ◊õ◊ö ◊©◊ê◊®◊ë◊¢◊î ◊ó◊ï◊ì◊©◊ô◊ù ◊ô◊®◊ï◊¶◊ï ◊ë◊û◊¶◊ò◊ë◊®.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 48:\n",
      "◊¢\"◊§ 5213/11 ◊ì◊†◊ô◊° ◊ê◊®◊ô◊û◊ô◊ô◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (21.6.12)- ◊©◊ù ◊†◊ì◊ï◊ü ◊î◊û◊¢◊®◊¢◊® ◊ú-51 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊¢◊ú ◊î◊ó◊ì◊®◊™ ◊°◊ù ◊î◊®◊ï◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú 33 ◊í◊®', ◊ó◊©◊ô◊©, ◊ï◊õ◊ì◊ï◊®◊ô MDMA ◊ï◊¶◊ô◊®◊ï◊£ ◊™◊ô◊ß ◊ó◊û◊ï◊® ◊†◊ï◊°◊£ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊ë◊õ◊ú◊ê. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 49:\n",
      "◊û◊ê◊°◊® ◊ú◊û◊©◊ö 10 ◊ó◊ï◊ì◊©◊ô◊ù ◊ê◊ï◊ú◊ù ◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊ô◊©◊ê ◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊™◊ï◊ö ◊©◊ú◊ï◊© ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊™ ◊°◊ù ◊û◊°◊ï◊í ◊§◊©◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 50:\n",
      "◊û◊ê◊°◊® ◊¢◊ú-◊™◊†◊ê◊ô ◊ë◊ü ◊ê◊®◊ë◊¢◊î ◊ó◊ï◊ì◊©◊ô◊ù ◊ú◊û◊©◊ö ◊©◊†◊™◊ô◊ô◊ù ◊û◊ô◊ï◊ù ◊©◊ó◊®◊ï◊®◊ï, ◊©◊ú◊ê ◊ô◊¢◊ë◊ï◊® ◊¢◊ë◊ô◊®◊™ ◊°◊ù ◊û◊°◊ï◊í ◊§◊©◊¢;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 51:\n",
      "9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊ï ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊î◊†◊ê◊©◊ù ◊ô◊®◊¶◊î ◊¢◊ï◊†◊© ◊ñ◊î ◊ë\"◊û◊ï◊¢◊ì◊ï◊ü ◊î◊ô◊ô◊û◊° ◊ú◊ê◊ñ◊®◊ó ◊î◊ï◊ï◊™◊ô◊ß ◊™◊ú ◊ê◊ë◊ô◊ë\" ◊ë◊õ◊™◊ï◊ë◊™ ◊©◊ì◊®◊ï◊™ ◊ó◊õ◊û◊ô ◊ô◊©◊®◊ê◊ú 79, ◊™◊ú ◊ê◊ë◊ô◊ë-◊ô◊§◊ï, ◊î◊ó◊ú ◊û◊ô◊ï◊ù 9.6.25. ◊î◊†◊ê◊©◊ù ◊û◊ï◊ñ◊î◊® ◊õ◊ô ◊ê◊ù ◊ô◊§◊® ◊ê◊™ ◊™◊†◊ê◊ô ◊î◊©◊ô◊®◊ï◊™ ◊î◊ï◊ê ◊¢◊ú◊ï◊ú ◊ú◊®◊¶◊ï◊™ ◊¢◊ï◊†◊© ◊ñ◊î ◊ë◊û◊ê◊°◊® ◊û◊û◊©. ◊ê◊©◊® ◊ú◊î◊ô◊ß◊£ ◊î◊©◊¢◊ï◊™ ◊î◊ù ◊ë◊î◊™◊ê◊ù ◊ú◊ê◊û◊ï◊® ◊ë◊ó◊ï◊ï◊™ ◊î◊ì◊¢◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 52:\n",
      "◊™\"◊§ (◊†◊™') 31134-05-18 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ï◊®◊ñ◊ï◊ü (20.10.2022) (◊î◊ï◊í◊© ◊¢◊ú ◊ô◊ì◊ô ◊î◊î◊í◊†◊î) ‚Äì◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë- 39 ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊ë- 10 ◊¢◊ë◊ô◊®◊ï◊™ ◊™◊ô◊ï◊ï◊ö ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü. ◊î◊¢◊°◊ß◊ê◊ï◊™ ◊ë◊ï◊¶◊¢◊ï ◊ë◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊û◊®◊ô◊ó◊ï◊ê◊†◊î ◊ï◊ß◊†◊ê◊ë◊ô◊°, ◊ë◊û◊©◊ß◊ú ◊©◊ë◊ô◊ü 5 ◊í◊®◊ù ◊ú◊ë◊ô◊ü ◊¢◊©◊®◊ï◊™ ◊í◊®◊û◊ô◊ù ◊ú◊¢◊°◊ß◊î, ◊û◊®◊ë◊ô◊™◊ü 10 ◊í◊®◊ù ◊ú◊¢◊°◊ß◊î. ◊î◊¶◊ì◊ì◊ô◊ù ◊î◊í◊ô◊¢◊ï ◊ú◊î◊°◊õ◊û◊ï◊™ ◊õ◊ô ◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊î◊ê◊ô◊©◊ô◊ï◊™ ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ï◊î◊î◊ú◊ô◊ö ◊î◊ò◊ô◊§◊ï◊ú◊ô ◊©◊¢◊ë◊®, ◊û◊¶◊ì◊ô◊ß◊ô◊ù ◊ó◊®◊ô◊í◊î ◊ú◊ß◊ï◊ú◊î ◊û◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î, ◊ï◊õ◊ô ◊î◊û◊ó◊ú◊ï◊ß◊™ ◊†◊ï◊í◊¢◊™ ◊ú◊î◊ô◊ß◊£ ◊î◊ó◊®◊ô◊í◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊ë◊¢ ◊õ◊ô ◊û◊ì◊ï◊ë◊® ◊ë◊ê◊ó◊ì ◊û◊ê◊ï◊™◊ù ◊û◊ß◊®◊ô◊ù ◊ó◊®◊ô◊í◊ô◊ù ◊î◊û◊¶◊ì◊ô◊ß◊ô◊ù ◊ú◊™◊ü ◊û◊¢◊û◊ì ◊ë◊õ◊ï◊®◊î ◊ú◊©◊ô◊ß◊ï◊ú ◊©◊ô◊ß◊ï◊û◊ô ◊ï◊î◊©◊ô◊™ ◊¢◊ú ◊î◊†◊ê◊©◊ù 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ê◊©◊® ◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ë◊¶◊ì ◊¶◊ï ◊û◊ë◊ó◊ü ◊ú◊û◊©◊ö 18 ◊ó◊ï◊ì◊©◊ô◊ù, ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ï◊°◊§◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 53:\n",
      "◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊û◊©◊ö 6 ◊ó◊ï◊ì◊©◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 54:\n",
      "◊ë◊™\"◊§ 11284-07-12 ◊î◊†\"◊ú, ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊û◊°◊í◊®◊™ ◊î◊°◊ì◊® ◊ò◊ô◊¢◊ï◊ü ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊î ◊©◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊ó◊¶◊®◊ô◊ù ◊ú◊©◊ù ◊©◊ô◊û◊ï◊© ◊ï◊î◊õ◊†◊î ◊©◊ú ◊°◊ù, ◊ë◊õ◊ö ◊©◊ë◊†◊î ◊ë◊ê◊ó◊ì ◊û◊ó◊ì◊®◊ô ◊ë◊ô◊™◊ï ◊ó◊û◊û◊î ◊ï◊ë◊î ◊í◊ô◊ì◊ú 17 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ô◊°. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊©◊ô◊™ ◊¢◊ú ◊î◊†◊ê◊©◊ù 4 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ë◊¶◊ô◊®◊ï◊£ ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 55:\n",
      "◊ë◊¢\"◊§ 5385/16 ◊®◊§◊ê◊ú ◊ë◊ü ◊©◊û◊¢◊ï◊ü ◊†' ◊û\"◊ô (7.3.2017) ◊î◊ï◊®◊©◊¢ ◊î◊û◊¢◊®◊¢◊® ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊ß◊©◊ô◊®◊™ ◊ß◊©◊® ◊ú◊ë◊ô◊¶◊ï◊¢ ◊§◊©◊¢ ◊ï◊†◊ô◊°◊ô◊ï◊ü ◊ú◊©◊ï◊ì, ◊ë◊õ◊ö ◊©◊ß◊©◊® ◊ô◊ó◊ì ◊¢◊ù ◊ê◊ó◊® ◊ú◊©◊ì◊ï◊ì ◊°◊†◊ô◊§◊ô ◊ë◊†◊ß ◊î◊ì◊ï◊ê◊®, ◊ï◊†◊¢◊©◊ï ◊†◊ô◊°◊ô◊ï◊†◊ï◊™ ◊ú◊©◊ï◊ì ◊¢◊ù ◊ê◊ß◊ì◊ó ◊ì◊û◊î ◊ï◊™◊ï◊ö ◊©◊î◊ù ◊®◊¢◊ï◊ú◊ô ◊§◊†◊ô◊ù, ◊©◊†◊ô ◊î◊†◊ô◊°◊ô◊ï◊†◊ï◊™ ◊ú◊ê ◊¢◊ú◊ï ◊õ◊ì◊ô ◊ë◊ô◊¶◊ï◊¢ ◊î◊©◊ï◊ì, ◊ë◊®◊ê◊©◊ï◊ü ◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊î◊ù ◊ú◊î◊ô◊õ◊†◊° ◊ú◊°◊†◊ô◊£, ◊ï◊ë◊©◊†◊ô ◊ú◊ê◊ó◊® ◊©◊ì◊ó◊§◊ï ◊ê◊ô◊©◊î ◊©◊î◊ô◊ô◊™◊î ◊ë◊õ◊†◊ô◊°◊î ◊ï◊†◊õ◊†◊°◊ï ◊ú◊°◊†◊ô◊£, ◊†◊†◊¢◊ú ◊î◊°◊†◊ô◊£ ◊ï◊î◊ï◊ñ◊¢◊ß◊î ◊¢◊ñ◊®◊î. ◊ë◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ë◊ô◊ü 12 ◊ú ‚Äì 36 ◊ó◊ì◊©◊ô ◊û◊ê◊°◊®, ◊†◊í◊ñ◊®◊ï 20 ◊ó◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊î◊ï◊§◊¢◊ú ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ó◊ú◊ß◊ï ◊ë◊û◊¶◊ò◊ë◊® ◊ï◊ó◊ú◊ß◊ï ◊ë◊ó◊ï◊§◊£, ◊õ◊ö ◊©◊î◊¢◊†◊ô◊©◊î ◊î◊õ◊ï◊ú◊ú◊™ ◊î◊ô◊ô◊™◊î ◊©◊ú 24 ◊ó◊ì◊©◊ô ◊û◊ê◊°◊®. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î. ◊í◊ù ◊õ◊ê◊ü ◊û◊ì◊ï◊ë◊® ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ó◊û◊ï◊®◊ï◊™ ◊§◊ó◊ï◊™ ◊û◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î ◊¢◊ú ◊ô◊ì◊ô ◊î◊†◊ê◊©◊ù, ◊ï◊î◊ê◊ú◊ô◊û◊ï◊™ ◊©◊î◊§◊í◊ô◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 56:\n",
      "◊ú◊†◊ê◊©◊ù 3, 3 ◊î◊®◊©◊¢◊ï◊™ ◊ß◊ï◊ì◊û◊ï◊™, ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ë◊ü 5 ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊™\"◊§ 21113-03-17, ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ë◊ü 7 ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊™\"◊§ 61930-07-17 ◊ï◊õ◊ü ◊§◊°◊ô◊ú◊™ ◊®◊ô◊©◊ô◊ï◊ü ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö 6 ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊ê◊ï◊™◊ï ◊™\"◊§.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 57:\n",
      "◊®◊¢\"◊§ 322/15 ◊í'◊ê◊†◊ó ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (22.1.15).  ◊î◊û◊ë◊ß◊© ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊õ◊ô◊ü ◊©◊ú◊ê ◊õ◊ì◊ô◊ü ◊ï◊õ◊ü ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ë◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß ◊ë◊ë◊ô◊™◊ï ◊°◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 214.62 ◊í◊®◊ù ◊ï◊õ◊ü ◊ë◊û◊©◊ß◊ú 192.67 ◊í◊®◊ù, ◊°◊ö ◊î◊õ◊ú 417 ◊í◊®◊ù. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊î◊ò◊ô◊ú ◊¢◊ú◊ô◊ï 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊™◊ï◊ö ◊î◊§◊¢◊ú◊™ ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊©◊ú ◊©◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô◊ù, ◊õ◊ö ◊©◊°◊ö ◊î◊õ◊ú ◊î◊ï◊©◊™◊ï ◊¢◊ú◊ô◊ï 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊®◊¢◊ï◊®◊ï ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊†◊ì◊ó◊î ◊ï◊õ◊ü ◊†◊ì◊ó◊™◊î ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊©◊î◊ï◊í◊©◊î ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 58:\n",
      "◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ê◊ú◊î, ◊î◊†◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊ß◊ò◊í◊ï◊®◊ô◊î ◊ñ◊ï ◊©◊ú ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊ï◊¶◊¢◊ï ◊¢\"◊ô ◊†◊ê◊©◊ù 2, ◊†◊¢ ◊ë◊ô◊ü 5 ◊ï◊¢◊ì 15 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 59:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô, ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ó◊° ◊ú◊¢◊ë◊ô◊®◊ï◊™ ◊î◊°◊ó◊® ◊ï◊õ◊ü ◊¢◊ë◊ô◊®◊ï◊™ ◊î◊î◊ó◊ñ◊ß◊î ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û-16 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊¢◊ì ◊ú-30 ◊ó◊ï◊ì◊©◊ô◊ù ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 60:\n",
      "◊ó◊ô◊ì◊ï◊© ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊©◊ú 7 ◊ó◊ï◊ì◊©◊ô◊ù ◊ê◊©◊® ◊î◊ï◊ò◊ú ◊ë◊û◊°◊í◊®◊™ ◊™\"◊§ (◊®◊û') 1430/06 ◊ú◊û◊©◊ö ◊©◊†◊™◊ô◊ô◊ù ◊û◊î◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 61:\n",
      "◊ë◊¢\"◊§ (◊ó◊ô') 41827-08-10 ◊§◊®◊ô◊ì◊û◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (30.12.2010) ◊ê◊ô◊©◊® ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊¢◊ï◊†◊© ◊©◊ú ◊ó◊û◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊†◊ô◊©◊î ◊†◊ï◊°◊§◊™ ◊©◊ê◊ô◊†◊î ◊¢◊ô◊ß◊®◊ô◊™, ◊ë◊í◊ô◊ü ◊¢◊ë◊ô◊®◊ï◊™ ◊ì◊ï◊û◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô ◊ï◊í◊ô◊ì◊ï◊ú ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊°.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 62:\n",
      "◊ë◊®◊¢\"◊§ 6041/18 ◊õ◊î◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (22.10.18), ◊†◊ì◊ï◊†◊î ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊©◊ú ◊û◊ë◊ß◊© ◊©◊î◊ï◊®◊©◊¢ ◊ë◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü, ◊ï◊í◊†◊ô◊ë◊™ ◊ó◊©◊û◊ú, ◊ú◊ê◊ó◊® ◊©◊í◊ô◊ì◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 39.5 ◊ß\"◊í; ◊ï◊ê◊£ ◊ë◊ô◊¶◊¢ ◊í◊†◊ô◊ë◊™ ◊ó◊©◊û◊ú ◊ë◊©◊ï◊ï◊ô ◊©◊ú 40,000 ‚Ç™. ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ß◊ë◊¢ ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊î◊û◊ë◊ß◊© ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 18 ◊ú-36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊í◊ñ◊® ◊¢◊ú◊ô◊ï 20 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊®◊¢◊ï◊® ◊©◊î◊ï◊í◊© ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊†◊ì◊ó◊î, ◊ï◊õ◊ö ◊í◊ù ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊î◊¢◊®◊¢◊ï◊® ◊©◊î◊ï◊í◊©◊î ◊¢◊ú ◊ô◊ì◊ô ◊î◊û◊ë◊ß◊©.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 63:\n",
      "◊¶◊ï ◊û◊ë◊ó◊ü ‚Äì ◊†◊ô◊™◊ü ◊ë◊ñ◊î ◊¶◊ï ◊û◊ë◊ó◊ü ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 12 ◊ó◊ï◊ì◊©◊ô◊ù ◊õ◊ú◊§◊ô ◊î◊†◊ê◊©◊ù. ◊î◊†◊ê◊©◊ù ◊û◊ó◊ï◊ô◊ô◊ë ◊ú◊©◊™◊£ ◊§◊¢◊ï◊ú◊î ◊¢◊ù ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü, ◊î◊õ◊ú ◊¢◊ú ◊§◊ô ◊î◊†◊ó◊ô◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü. ◊û◊ï◊ë◊î◊® ◊ú◊†◊ê◊©◊ù ◊õ◊ô ◊ë◊ê◊ù ◊ú◊ê ◊ô◊ß◊ô◊ô◊ù ◊¶◊ï ◊ñ◊î ◊†◊ô◊™◊ü ◊ô◊î◊ô◊î ◊ú◊ó◊ñ◊ï◊® ◊ï◊ú◊ì◊ï◊ü ◊û◊ó◊ì◊© ◊ë◊©◊ê◊ú◊™ ◊î◊¢◊ï◊†◊©.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 64:\n",
      "◊¢◊ï◊†◊© ◊î◊û◊ê◊°◊® ◊î◊û◊®◊ë◊ô ◊î◊ß◊ë◊ï◊¢ ◊ë◊ó◊ï◊ß ◊ë◊ô◊ó◊° ◊ú◊¢◊ë◊ô◊®◊ï◊™ ◊†◊©◊ï◊ê ◊î◊ê◊ô◊©◊ï◊û◊ô◊ù ◊¢◊ï◊û◊ì ◊¢◊ú 20 ◊©◊†◊î ◊ú◊û◊¢◊ò ◊ë◊ô◊ó◊°  ◊ú◊¢◊ë◊ô◊®◊î ◊©◊ú ◊†◊ò◊ô◊ú◊™ ◊ó◊©◊û◊ú ◊©◊î◊¢◊ï◊†◊© ◊î◊û◊®◊ë◊ô ◊î◊ß◊ë◊ï◊¢ ◊ë◊¶◊ô◊ì◊î ◊¢◊ï◊û◊ì ◊¢◊ú 3 ◊©◊†◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 65:\n",
      "12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ñ◊ê◊™ ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï ◊ë◊ô◊ü ◊î◊™◊ê◊®◊ô◊õ◊ô◊ù 27.5.13 ◊ú- 15.7.13.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 66:\n",
      "◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊î◊®◊ê◊ï◊ô ◊ú◊û◊¢◊©◊ô◊ù ◊ê◊ú◊î ◊†◊¢ ◊ë◊ô◊ü  14-40  ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 67:\n",
      "◊ú◊¶◊ï◊®◊ö ◊ë◊ó◊ô◊†◊™ ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊†◊ô◊™◊ü ◊ú◊î◊§◊†◊ï◊™ ◊ú◊§◊°◊ß◊ô ◊î◊ì◊ô◊ü ◊î◊ë◊ê◊ô◊ù, ◊ë◊©◊ô◊†◊ï◊ô◊ô◊ù ◊î◊û◊ó◊ï◊ô◊ë◊ô◊ù: ◊®◊¢\"◊§ 2277/21 ◊ê◊ë◊®◊î◊ù ◊ô◊ï◊ó◊†◊†◊ï◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (8.4.2021), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊í◊ô◊ì◊ï◊ú ◊°◊û◊ô◊ù ◊ï◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ë◊û◊©◊ß◊ú ◊ß◊ô◊ú◊ï◊í◊®◊ù ◊ê◊ó◊ì (◊ú◊ê ◊¶◊ï◊ô◊†◊ï ◊û◊°◊§◊® ◊î◊©◊™◊ô◊ú◊ô◊ù) ◊ï◊†◊í◊ñ◊®◊ï ◊¢◊ú◊ô◊ï ◊©◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù; ◊®◊¢\"◊§ 513/21 ◊¢◊ô◊ì◊ü ◊ô◊î◊ï◊©◊¢ ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (4.3.2021), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊©◊í◊ô◊ì◊ú 60 ◊©◊™◊ô◊ú◊ô◊ù ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 10 ◊ß\"◊í ◊ï◊î◊ó◊ñ◊ô◊ß ◊õ◊ú◊ô◊ù ◊î◊û◊©◊û◊©◊ô◊ù ◊ú◊î◊õ◊†◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü, ◊ú◊¢◊©◊®◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊ñ◊ê◊™ ◊ú◊û◊®◊ï◊™ ◊î◊ú◊ô◊ö ◊©◊ô◊ß◊ï◊û◊ô ◊©◊¢◊ë◊®; ◊®◊¢\"◊§ 2151/21 ◊©◊ó◊£ ◊ì◊î◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (7.4.2021), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊ú-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊í◊ô◊ü ◊í◊ô◊ì◊ï◊ú 100 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 23 ◊ß\"◊í. ◊¢◊ë◊®◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊î◊ô◊î ◊†◊ß◊ô ◊ê◊ö ◊î◊™◊°◊ß◊ô◊® ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊î◊ô◊î ◊©◊ú◊ô◊ú◊ô; ◊®◊¢\"◊§ 7009/20 ◊ë◊ï◊®◊ô◊° ◊ô◊©◊®◊ê◊ô◊ú◊ï◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (6.12.2020), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊©◊í◊ô◊ì◊ú 22 ◊ß\"◊í ◊ß◊†◊ë◊ï◊° (◊ú◊ê ◊¶◊ï◊ô◊†◊ï ◊û◊°◊§◊® ◊î◊©◊™◊ô◊ú◊ô◊ù) ◊ú-13 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ó◊®◊£ ◊î◊û◊ú◊¶◊î ◊ó◊ô◊ï◊ë◊ô◊™ ◊©◊ú ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü; ◊¢◊§\"◊í 26985-08-19 ◊ê◊®◊ò◊ô◊ï◊ù ◊©◊ò◊†◊§◊® ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (21.11.2019), ◊ë◊ï ◊î◊ï◊ò◊ú ◊¢◊ú ◊†◊ê◊©◊ù ◊©◊í◊ô◊ì◊ú ◊¢◊©◊®◊ï◊™ ◊©◊™◊ô◊ú◊ô◊ù ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 113 ◊ß\"◊í, ◊¶◊ï ◊©◊ú\"◊¶ ◊ï◊û◊ë◊ó◊ü ◊ï◊ñ◊ê◊™ ◊ë◊©◊ú ◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù; ◊¢◊§\"◊í 32119-08-21 ◊ê◊ì◊ô◊® ◊ô◊ï◊ê◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (28.10.2021), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊©◊í◊ô◊ì◊ú 68 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú 7.56 ◊ß\"◊í ◊†◊ò◊ï ◊ú◊©◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù; ◊¢◊§\"◊í 54910-04-21 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ë◊ü ◊¢◊ñ◊®◊ô (19.7.2021), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊©◊í◊ô◊ì◊ú 100 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊©◊ú ◊õ-17 ◊ß\"◊í ◊ú◊™◊©◊¢◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 68:\n",
      "◊ë◊¢\"◊§ 8224/17 ◊§◊®◊• ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (21.6.18) ◊†◊ì◊ï◊ü ◊¢◊®◊¢◊ï◊® ◊†◊ê◊©◊ù ◊©◊î◊ß◊ô◊ù ◊û◊ó◊°◊ü ◊ï◊î◊ß◊ô◊ù ◊ë◊ï ◊û◊¢◊ë◊ì◊î ◊ú◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊ß◊†◊ë◊ï◊°, ◊ë◊î ◊†◊™◊§◊°◊ï ◊°◊û◊ô◊ù ◊ë◊û◊©◊ß◊ú ◊©◊ú 170 ◊ß\"◊í. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 48-30 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊î◊†◊ê◊©◊ù, ◊í◊ô◊ú◊ï ◊û◊ë◊ï◊í◊®, ◊ú◊ú◊ê ◊¢◊ë◊®, ◊û◊¶◊ë◊ü ◊î◊®◊§◊ï◊ê◊ô ◊©◊ú ◊ë◊†◊ï◊™ ◊û◊©◊§◊ó◊™◊ï ◊û◊ï◊®◊õ◊ë, ◊†◊ì◊ï◊ü ◊ú◊û◊ê◊°◊® ◊ë◊ü 36 ◊ó◊ï◊ì◊©◊ô◊ù. ◊î◊†◊ê◊©◊ù ◊ó◊ñ◊® ◊û◊¢◊®◊¢◊ï◊®◊ï ◊ú◊ê◊ó◊® ◊©◊†◊©◊û◊¢◊ï ◊î◊¢◊®◊ï◊™ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 69:\n",
      "◊ß◊†◊° ◊ë◊°◊ö 10,000 ‚Ç™ ◊ê◊ï 45 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊ë-10 ◊™◊©◊ú◊ï◊û◊ô◊ù ◊©◊ï◊ï◊ô◊ù ◊ï◊®◊¶◊ï◊§◊ô◊ù ◊î◊ó◊ú ◊û◊ô◊ï◊ù 5.8.25.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 70:\n",
      "◊î◊¢◊®◊ö ◊î◊û◊ï◊í◊ü ◊î◊ô◊†◊ï ◊§◊í◊ô◊¢◊î ◊ë◊ë◊®◊ô◊ê◊ï◊™ ◊î◊¶◊ô◊ë◊ï◊® ◊†◊ï◊õ◊ó ◊©◊ô◊û◊ï◊© ◊ë◊°◊ù ◊ï◊î◊§◊í◊ô◊¢◊î ◊î◊ó◊ë◊®◊™◊ô◊™ ◊©◊ô◊ï◊¶◊® ◊î◊©◊ô◊û◊ï◊© ◊ë◊°◊ù ◊ë◊õ◊ú◊ú ◊î◊ó◊ë◊®◊î ◊†◊ï◊õ◊ó ◊î◊§◊©◊ô◊¢◊î ◊©◊†◊ï◊¶◊®◊™ ◊°◊ë◊ô◊ë◊ï. ◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊ë◊û◊ß◊®◊î ◊î◊†◊ï◊õ◊ó◊ô ◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊î◊™◊¢◊ú◊ù ◊û◊õ◊û◊ï◊™ ◊î◊°◊ù ◊î◊ß◊ò◊†◊î ◊©◊î◊†◊ê◊©◊ù ◊ô◊ô◊ë◊ê ◊ê◊®◊¶◊î. ◊õ◊ê◊©◊® ◊ú◊õ◊ö ◊û◊™◊ú◊ï◊ï◊î ◊î◊©◊ô◊û◊ï◊© ◊©◊¢◊ï◊©◊î ◊î◊†◊ê◊©◊ù ◊ë◊©◊ú ◊û◊¶◊ë◊ï ◊î◊®◊§◊ï◊ê◊ô. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊ê◊ô◊®◊ï◊¢◊ô◊ù ◊î◊ï◊ê ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 71:\n",
      "◊î◊†◊ê◊©◊û◊™ ◊™◊¢◊û◊ï◊ì ◊ë◊§◊ô◊ß◊ï◊ó ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊ú◊û◊©◊ö 12 ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊î◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 72:\n",
      "◊ë◊¢\"◊§ 863/18 ◊°◊ë◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (15.11.2018) ◊î◊™◊ß◊ë◊ú ◊¢◊®◊¢◊ï◊® ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊†◊ê◊©◊ù ◊ë◊¢◊ú ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô ◊©◊î◊ß◊ô◊ù, ◊ô◊ó◊ì ◊¢◊ù ◊ê◊ó◊®◊ô◊ù, ◊û◊¢◊ë◊ì◊î ◊ë◊î ◊í◊ô◊ì◊ú◊ï 283 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 92 ◊ß\"◊í. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 52-28 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊í◊ñ◊® ◊¢◊ú◊ô◊ï 42 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊ï◊†◊©◊ï ◊î◊ï◊§◊ó◊™ ◊ï◊î◊ï◊¢◊û◊ì ◊¢◊ú 36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊û◊ò◊¢◊û◊ô◊ù ◊©◊ú ◊ê◊ó◊ô◊ì◊ï◊™ ◊¢◊†◊ô◊©◊î ◊ï◊™◊°◊ß◊ô◊®◊ô◊ù ◊ó◊ô◊ï◊ë◊ô◊ô◊ù;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 73:\n",
      "◊§◊ï◊°◊ú◊™ ◊î◊†◊ê◊©◊ù ◊û◊ß◊ë◊ú ◊ï◊û◊î◊ó◊ñ◊ô◊ß ◊®◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ú◊û◊©◊ö 11 ◊ó◊ï◊ì◊©◊ô◊ù, ◊§◊°◊ô◊ú◊î ◊ë◊§◊ï◊¢◊ú. ◊î◊†◊ê◊©◊ù ◊ô◊§◊ß◊ô◊ì ◊ê◊™ ◊®◊©◊ô◊ï◊†◊ï ◊ë◊û◊ñ◊õ◊ô◊®◊ï◊™ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊¢◊ì ◊ú◊ô◊ï◊ù 3.9.14.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 74:\n",
      "◊õ◊ê◊û◊ï◊®, ◊î◊¶◊ì◊ì◊ô◊ù ◊î◊í◊ô◊¢◊ï ◊ú◊î◊°◊ì◊® ◊ò◊ô◊¢◊ï◊ü ◊ë◊¢◊ú ◊ò◊ï◊ï◊ó ◊¢◊†◊ô◊©◊î ◊û◊ï◊°◊õ◊ù (4-0 ◊©◊†◊ô◊ù), ◊õ◊©◊î◊ù ◊¢◊®◊ô◊ù ◊ú◊õ◊ö ◊©◊î◊®◊£ ◊î◊¢◊ú◊ô◊ï◊ü ◊¢◊ú◊ô◊ï ◊î◊°◊õ◊ô◊û◊ï, ◊†◊û◊ï◊ö ◊û◊î◊®◊£ ◊î◊™◊ó◊™◊ï◊ü ◊©◊ú ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊û◊î◊°◊ï◊í ◊ë◊ï ◊¢◊°◊ß◊ô◊†◊ü ◊†◊ï◊õ◊ó ◊ß◊ô◊ï◊û◊î ◊©◊ú ◊û◊°◊õ◊™ ◊¢◊ï◊ë◊ì◊™◊ô◊™ ◊ô◊ô◊ó◊ï◊ì◊ô◊™ ◊ï◊û◊ï◊®◊õ◊ë◊™, ◊ï◊¢◊ú ◊õ◊ü ◊°◊ë◊®◊ï ◊õ◊ô ◊†◊ô◊™◊ü ◊ú◊í◊ñ◊ï◊® ◊ê◊™ ◊ì◊ô◊†◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ë◊í◊ì◊®◊ï ◊©◊ú ◊î◊°◊ì◊® ◊î◊ò◊ô◊¢◊ï◊ü, ◊í◊ù ◊û◊ë◊ú◊ô ◊ú◊î◊ô◊ì◊®◊© ◊ú◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î.\n",
      "Real: 1 | Predicted: 0\n",
      "\n",
      "üìù Text 75:\n",
      "◊ë◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô-◊™\"◊ê) 318-01-21 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊¢◊¶◊ò◊î (06.05.21), ◊î◊ï◊®◊©◊¢◊ï ◊©◊†◊ô ◊†◊ê◊©◊û◊ô◊ù ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊ô◊ô◊ë◊ï◊ê ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ë◊î◊ô◊ß◊§◊ô◊ù ◊©◊ú 4.3 ◊ú◊ô◊ò◊®◊ô◊ù ◊©◊ú ◊°◊ù ◊û◊°◊ï◊í GBL. ◊î◊†◊ê◊©◊û◊ô◊ù ◊¢◊©◊ï ◊©◊ô◊û◊ï◊© ◊ë◊ñ◊î◊ï◊™ ◊ë◊ì◊ï◊ô◊î ◊ú◊¶◊ï◊®◊ö ◊î◊ñ◊û◊†◊™ ◊î◊°◊ù ◊ï◊†◊ô◊°◊ï ◊ú◊ò◊©◊ò◊© ◊ê◊™ ◊ñ◊ô◊ß◊™◊ù ◊ú◊ó◊ë◊ô◊ú◊î ◊¢◊ú ◊ô◊ì◊ô ◊ë◊ß◊©◊î ◊û◊ô◊ì◊ô◊ì◊™◊ù ◊ú◊ß◊ë◊ú ◊¢◊ë◊ï◊®◊ù ◊ê◊™ ◊î◊ó◊ë◊ô◊ú◊î ◊ë◊ê◊û◊™◊ú◊ï◊™ ◊©◊ï◊†◊ï◊™. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊†◊¢ ◊ë◊ô◊ü 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊†◊ô◊™◊ü ◊ú◊®◊¶◊ï◊™ ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ú◊ë◊ô◊ü 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊ú ◊§◊°◊ß ◊î◊ì◊ô◊ü ◊î◊ï◊í◊© ◊¢◊®◊¢◊ï◊® ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü (◊¢\"◊§ 4346/21 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊¢◊¶◊ò◊î (28.07.21)) ◊ê◊©◊® ◊î◊ó◊û◊ô◊® ◊ê◊™ ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊ê◊ó◊ì ◊î◊†◊ê◊©◊û◊ô◊ù ◊ï◊î◊¢◊û◊ô◊ì◊ï ◊¢◊ú 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊¢◊ï◊†◊©◊î ◊©◊ú ◊î◊†◊ê◊©◊û◊™ ◊î◊©◊†◊ô◊ô◊î ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ú◊ê ◊î◊™◊¢◊®◊ë ◊ú◊†◊ï◊õ◊ó ◊î◊ô◊ï◊™◊î ◊ë◊î◊®◊ô◊ï◊ü, ◊™◊ï◊ö ◊©◊î◊ï◊ê ◊©◊ë ◊ï◊û◊ì◊í◊ô◊© ◊ê◊™ ◊î◊ó◊ï◊û◊®◊î ◊©◊ë◊ô◊ë◊ï◊ê ◊°◊ù ◊ñ◊î ◊ï◊ë◊¢◊ô◊ß◊® ◊†◊ï◊õ◊ó ◊î◊®◊¢◊ï◊™ ◊î◊ó◊ï◊ú◊ï◊™ ◊ê◊ú◊ô◊î◊ü ◊†◊ó◊©◊§◊†◊ï ◊õ◊ó◊ë◊®◊î ◊õ◊™◊ï◊¶◊ê◊î ◊û◊©◊ô◊û◊ï◊© ◊ë◊°◊ù ◊ñ◊î, ◊©◊†◊ó◊©◊ë ◊ô◊ó◊°◊ô◊™ \"◊ó◊ì◊©\", ◊ï◊õ◊õ◊ú◊ú ◊ê◊ô◊ü ◊ú◊ß◊ë◊ï◊¢ ◊¢◊†◊ô◊©◊î ◊©◊ê◊ô◊†◊î ◊õ◊ï◊ú◊ú◊™ ◊®◊õ◊ô◊ë ◊©◊ú ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊û◊ê◊ó◊ï◊®◊ô ◊°◊ï◊®◊í ◊ï◊ë◊®◊ô◊ó (◊®◊ê◊ï ◊í◊ù: ◊¢\"◊§ 667/21 ◊ë◊¢◊†◊ô◊ô◊ü ◊ë◊ü ◊§◊ï◊®◊™).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 76:\n",
      "◊ë◊î◊™◊ó◊©◊ë ◊ë◊ê◊ï◊§◊ô ◊î◊û◊¢◊©◊ô◊ù ◊ï◊ë◊õ◊ö ◊©◊ë◊û◊ß◊®◊î ◊ñ◊î ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù ◊ê◊ô◊†◊î ◊ë◊ì◊®◊í◊™ ◊ó◊ï◊û◊®◊î ◊í◊ë◊ï◊î◊î ◊ï◊ë◊ó◊ì ◊§◊¢◊û◊ô◊ï◊™ ◊©◊ú ◊î◊û◊¢◊©◊î, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢◊™ ◊©◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊†◊¢ ◊ë◊ô◊ü ◊™◊ß◊ï◊§◊™ ◊û◊ê◊°◊® ◊û◊™◊ï◊†◊î ◊©◊ô◊õ◊ï◊ú ◊ï◊™◊®◊ï◊¶◊î ◊ë◊¢\"◊© ◊ú◊ë◊ô◊ü 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 0 | Predicted: 1\n",
      "\n",
      "üìù Text 77:\n",
      "◊î◊†◊ê◊©◊ù ◊ô◊©◊ú◊ù ◊ß◊†◊° ◊ë◊°◊ö ◊©◊ú 2,500 ‚Ç™, ◊ê◊ï 10 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊ë-5 ◊™◊©◊ú◊ï◊û◊ô◊ù ◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊©◊ï◊ï◊ô◊ù ◊ï◊®◊¶◊ï◊§◊ô◊ù, ◊õ◊ê◊©◊® ◊î◊®◊ê◊©◊ï◊ü ◊©◊ë◊î◊ù ◊ú◊ê ◊ô◊ê◊ï◊ó◊® ◊û◊ô◊ï◊ù 1.3.15 ◊ï◊î◊ô◊™◊®◊î ◊ë-1 ◊ú◊õ◊ú ◊ó◊ï◊ì◊© ◊©◊ú◊ê◊ó◊®◊ô◊ï. ◊î◊ô◊î ◊ï◊ê◊ó◊ì ◊î◊™◊©◊ú◊ï◊û◊ô◊ù ◊ú◊ê ◊ô◊©◊ï◊ú◊ù ◊ë◊û◊ï◊¢◊ì, ◊ê◊ñ◊ô, ◊ô◊¢◊û◊ï◊ì ◊û◊ú◊ï◊ê ◊°◊õ◊ï◊ù ◊î◊ß◊†◊° ◊ú◊§◊ô◊®◊¢◊ï◊ü ◊û◊ô◊ì◊ô. ◊î◊ß◊†◊° ◊ô◊ï◊¢◊ë◊® ◊ú◊ò◊ï◊ë◊™ ◊î◊ß◊®◊ü ◊©◊§◊ï◊¢◊ú◊™ ◊ú◊§◊ô ◊°◊¢◊ô◊£ 36◊ó ◊ú◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 78:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊°◊¢◊ô◊£ 40 ◊í(◊ê) ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü ◊î◊™◊©◊ú\"◊ñ ‚Äì 1977, ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ô◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ú◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊©◊ë◊ô◊¶◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊î◊™◊ê◊ù ◊ú◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î, ◊ï◊ú◊©◊ù ◊õ◊ö ◊ô◊™◊ó◊©◊ë ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊©◊†◊§◊í◊¢ ◊û◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î, ◊ë◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊ï, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊î◊ß◊©◊ï◊®◊ï◊™ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊†◊ï◊õ◊ó ◊í◊ï◊ì◊ú ◊î◊û◊¢◊ë◊ì◊î ◊ï◊õ◊û◊ï◊™ ◊î◊°◊ù ◊©◊í◊ï◊ì◊ú◊î ◊ë◊û◊¢◊ë◊ì◊î ◊î◊ê◊ë◊ñ◊ï◊® ◊ï◊î◊î◊õ◊†◊î ◊©◊†◊ì◊®◊©◊î ◊ú◊¶◊ï◊®◊ö ◊î◊ß◊û◊™ ◊î◊û◊¢◊ë◊ì◊î ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊ô◊®◊ï◊¢ ◊î◊ï◊ê 32-48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 79:\n",
      "◊ë◊¢◊†◊ô◊ô◊†◊†◊ï, ◊î◊†◊ê◊©◊ù ◊î◊ï◊ì◊î ◊ï◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊í◊ô◊ì◊ï◊ú ◊°◊ù, ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô ◊ï◊î◊ó◊ñ◊ß◊™ ◊õ◊ú◊ô◊ù. ◊î◊¢◊ï◊†◊© ◊©◊†◊ß◊ë◊¢ ◊ë◊ó◊ï◊ß ◊ú◊õ◊ú ◊ê◊ó◊™ ◊û◊¢◊ë◊ô◊®◊ï◊™ ◊ê◊ú◊î ◊î◊ï◊ê ◊¢◊©◊®◊ô◊ù ◊©◊†◊ï◊™ ◊û◊ê◊°◊®, ◊ï◊ë◊î◊™◊ê◊ù ◊ô◊© ◊ú◊®◊ê◊ï◊™ ◊ê◊™ ◊î◊†◊ê◊©◊ù ◊õ◊û◊ô ◊©◊î◊ï◊®◊©◊¢ ◊ë◊¢◊°◊ß◊™ ◊°◊û◊ô◊ù. ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊ê◊û◊†◊ù ◊ê◊ô◊†◊ï ◊û◊ô◊ô◊ó◊° ◊ú◊†◊ê◊©◊ù ◊î◊§◊ß◊™ ◊®◊ï◊ï◊ó,  ◊ê◊ú◊ê ◊©◊í◊ô◊ì◊ï◊ú ◊©◊ú 870 ◊©◊™◊ô◊ú◊ô◊ù ◊©◊ú ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 149.44 ◊ß\"◊í ◊†◊ò◊ï ◊ï◊î◊™◊†◊î◊ú◊ï◊™◊ï ◊î◊§◊ú◊ô◊ú◊ô◊™ ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ï◊ë◊õ◊ú◊ú ◊ñ◊î ◊í◊ô◊ì◊ï◊ú ◊î◊©◊™◊ô◊ú◊ô◊ù, ◊ï◊î◊û◊©◊ê◊ë◊ô◊ù ◊©◊î◊ï◊©◊ß◊¢◊ï ◊ú◊¶◊ï◊®◊ö ◊õ◊ö,  ◊û◊¶◊ë◊ô◊¢◊ô◊ù  ◊¢◊ú ◊õ◊ï◊ï◊†◊™◊ï ◊ú◊î◊§◊ô◊ß ◊®◊ï◊ï◊ó ◊û◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù. ◊î◊†◊ê◊©◊ù ◊û◊¶◊ì◊ï ◊ú◊ê ◊î◊ë◊ô◊ê ◊õ◊ú ◊®◊ê◊ô◊î ◊ú◊°◊™◊ô◊®◊™ ◊î◊ó◊ñ◊ß◊î ◊ê◊ï ◊î◊û◊°◊ß◊†◊î ◊î◊†\"◊ú, ◊ï◊û◊õ◊ê◊ü ◊©◊û◊ì◊ï◊ë◊® ◊ë◊û◊°◊ß◊†◊î ◊î◊î◊í◊ô◊ï◊†◊ô◊™ ◊î◊ô◊ó◊ô◊ì◊î. ◊®◊ê◊î ◊ú◊¢◊†◊ô◊ô◊ü ◊ñ◊î ◊ì◊†\"◊§ 10402/07 ◊û◊ò◊ô◊° ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊†◊ë◊ï] ◊©◊ù ◊†◊ß◊ë◊¢ ◊õ◊ô \"◊¢◊ú-◊û◊†◊™ ◊ú◊î◊§◊®◊ô◊ö ◊ê◊™ ◊î◊û◊°◊ß◊†◊î ◊î◊ò◊ë◊¢◊ô◊™ ◊î◊†◊ï◊ë◊¢◊™ ◊û◊î◊†◊°◊ô◊ë◊ï◊™ ◊©◊™◊ï◊ê◊®◊ï, ◊¶◊®◊ô◊ö ◊î◊ô◊î ◊î◊¢◊ï◊™◊® ◊ú◊°◊§◊ß ◊î◊°◊ë◊®◊ô◊ù ◊ê◊©◊® ◊ô◊ï◊õ◊ú◊ï ◊ú◊î◊ß◊î◊ï◊™ ◊ê◊™ ◊¢◊ï◊ß◊¶◊ü ◊©◊ú ◊î◊¢◊ï◊ë◊ì◊ï◊™, ◊ï◊õ◊ô ◊û◊©◊ú◊ê ◊¢◊©◊î ◊õ◊ü ◊¢◊ï◊û◊ì◊™ ◊î◊û◊°◊ß◊†◊î ◊î◊†◊ï◊ë◊¢◊™ ◊û◊î◊ü ◊ë◊ß◊®◊ô◊ò◊®◊ô◊ï◊ü ◊î◊ì◊®◊ï◊© ◊©◊ú ◊ú◊û◊¢◊ú◊î ◊û◊°◊§◊ß ◊°◊ë◊ô◊®\".\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 80:\n",
      "14 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 81:\n",
      "◊ê◊†◊ô ◊û◊ï◊®◊î ◊¢◊ú ◊î◊§◊¢◊ú◊™ ◊î◊û◊ê◊°◊® ◊î◊û◊ï◊™◊†◊î ◊ë◊ü 6 ◊ó◊ï◊ì◊©◊ô◊ù ◊õ◊§◊ô ◊©◊î◊ï◊ò◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ë◊™◊ô◊ß  40057-04-12 ◊ë◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ë◊ê◊©◊ß◊ú◊ï◊ü  ◊ë◊ô◊ï◊ù 19.6.12 ◊ï◊ñ◊ê◊™ ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊ê◊ï◊§◊ü ◊ó◊ï◊§◊£ ◊ú◊û◊ê◊°◊® ◊©◊î◊ï◊ò◊ú ◊î◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 82:\n",
      "◊õ◊ö ◊ê◊ï ◊õ◊ö, ◊ê◊ô◊ü ◊ë◊ß◊ë◊ô◊¢◊™ ◊î◊û◊™◊ó◊ù ◊ë◊™◊ô◊ß ◊î◊†\"◊ú ◊õ◊ì◊ô ◊ú◊™◊û◊ï◊ö ◊ë◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊ú◊ï ◊¢◊™◊®◊î ◊î◊î◊í◊†◊î. ◊ú◊¶◊ï◊®◊ö ◊î◊î◊©◊ï◊ï◊ê◊î ◊®◊ê◊ï ◊ú◊û◊©◊ú ◊ë◊™\"◊§ (◊©◊ú◊ï◊ù ◊ë\"◊©) 52661-11-19 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ì◊®◊¢◊ô ◊ï◊ê◊ó' (19.07.2021)  ◊ß◊ë◊¢ ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊ñ◊î ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ü 24 ◊ï◊¢◊ì 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊î◊ü ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù (◊†◊ê◊©◊ù 3)  ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊í◊ô◊ì◊ï◊ú ◊ï◊î◊ó◊ñ◊ß◊™  ◊ï◊°◊û◊ô◊ù ◊ë◊û◊¢◊ë◊ì◊î ◊ë◊¶◊ï◊ï◊™◊ê ◊ó◊ì◊ê, 1136 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú 15 ◊ß\"◊í, ◊ï◊¢◊ï◊ì 1.2 ◊ß\"◊í ◊ß◊†◊ê◊ë◊ô◊°. ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊†◊¢◊ì◊® ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô ◊ï◊ê◊ë ◊ú 4 ◊ô◊ú◊ì◊ô◊ù ◊†◊í◊ñ◊® ◊ú 27 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊®◊¢◊ï◊® ◊©◊î◊í◊ô◊© ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô (◊¢◊§\"◊í 6525-09-21) ◊†◊ì◊ó◊î, ◊ú◊û◊¢◊ò ◊®◊õ◊ô◊ë ◊î◊§◊°◊ô◊ú◊î ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 83:\n",
      "◊¢\"◊§ 1167/21 ◊ò◊ê◊®◊ß ◊ó◊ï◊í'◊ô◊®◊ê◊™ ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (31.05.2021) - ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ú◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊™ ◊©◊ï◊ì ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊û◊ó◊û◊ô◊®◊ï◊™ ◊ë◊¶◊ï◊ï◊™◊ê. ◊ë◊™◊û◊¶◊ô◊™, ◊î◊û◊¢◊®◊¢◊® ◊ï◊ê◊ó◊® ◊î◊¶◊ò◊ô◊ô◊ì◊ï ◊ë◊û◊ë◊®◊í ◊ï◊°◊õ◊ô◊ü ◊ô◊§◊†◊ô◊™ ◊ë◊ê◊û◊¶◊¢◊ï◊™◊ù ◊î◊°◊ô◊®◊ï ◊ê◊™ ◊ú◊ï◊ó◊ô◊™ ◊î◊®◊ô◊©◊ï◊ô ◊û◊î◊®◊õ◊ë, ◊î◊í◊ô◊¢◊ï ◊ë◊©◊¢◊ï◊™ ◊î◊ú◊ô◊ú◊î ◊î◊û◊ê◊ï◊ó◊®◊ï◊™ ◊ú◊ó◊†◊ï◊™ ◊©◊ë◊™◊ó◊†◊™ ◊ì◊ú◊ß ◊õ◊©◊î◊ù ◊®◊¢◊ï◊ú◊ô ◊§◊†◊ô◊ù ◊ï◊ê◊ï◊ó◊ñ◊ô◊ù ◊ë◊û◊ë◊®◊í ◊ï◊ë◊°◊õ◊ô◊ü ◊û◊ê◊ó◊ï◊®◊ô ◊í◊ë◊ù. ◊î◊û◊¢◊®◊¢◊® ◊ê◊ô◊ô◊ù ◊¢◊ú ◊î◊¢◊ï◊ë◊ì ◊ë◊ê◊ï◊û◊®◊ï \"◊™◊ï◊¶◊ô◊ê ◊ê◊™ ◊î◊õ◊°◊£ ◊ê◊ï ◊©◊ê◊†◊ô ◊ô◊ï◊®◊î ◊ë◊ö\". ◊î◊¢◊ï◊ë◊ì ◊§◊™◊ó ◊î◊ß◊ï◊§◊î ◊ï◊û◊°◊® ◊ú◊û◊¢◊®◊¢◊® 1,140 ‚Ç™ ◊ï◊î◊©◊†◊ô◊ô◊ù ◊†◊û◊ú◊ò◊ï. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊î◊†◊¢ ◊ë◊ô◊ü 18-40 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊ò◊ú◊ï 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 84:\n",
      "◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊î ◊ñ◊ï ◊†◊¢ ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ú◊©◊û◊ï◊†◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 85:\n",
      "◊ú◊ê◊ï◊® ◊ñ◊ê◊™, ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊ï◊™ ◊ê◊©◊® ◊ë◊ô◊¶◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢◊ü, ◊õ◊ï◊ú◊ú ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊ô◊ü 19 ◊ú-43 ◊ó◊ï◊ì◊©◊ô◊ù.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 86:\n",
      "◊î◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù ◊©◊†◊§◊í◊¢◊ï ◊û◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊î ◊ñ◊ï, ◊ú◊¶◊ì◊î ◊ß◊ë◊¢ ◊î◊û◊ó◊ï◊ß◊ß ◊¢◊ï◊†◊© ◊û◊®◊ë◊ô ◊©◊ú 20 ◊©◊†◊ï◊™ ◊û◊ê◊°◊®, ◊ë◊ì◊ï◊û◊î ◊ú◊õ◊ú ◊¢◊ë◊ô◊®◊™ ◊°◊û◊ô◊ù, ◊î◊ù ◊î◊í◊†◊î ◊¢◊ú ◊©◊ú◊ï◊ù ◊ï◊ë◊®◊ô◊ê◊ï◊™ ◊î◊¶◊ô◊ë◊ï◊® ◊û◊§◊†◊ô ◊§◊í◊ô◊¢◊™◊ù ◊î◊ß◊©◊î ◊©◊ú ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊ï◊û◊†◊ô◊¢◊™ ◊†◊ñ◊ß◊ô◊ù ◊õ◊ú◊õ◊ú◊ô◊ô◊ù ◊ï◊ó◊ë◊®◊™◊ô◊ô◊ù ◊¢◊ß◊ô◊§◊ô◊ù. ◊î◊î◊ú◊õ◊î ◊î◊§◊°◊ï◊ß◊î ◊¢◊û◊ì◊î ◊¢◊ú ◊î◊î◊õ◊®◊ó ◊ú◊î◊ô◊ê◊ë◊ß ◊ë◊†◊í◊¢ ◊î◊°◊û◊ô◊ù ◊ï◊¢◊ú ◊î◊¶◊ï◊®◊ö ◊ú◊î◊¢◊ë◊ô◊® ◊û◊°◊® ◊û◊®◊™◊ô◊¢ ◊û◊§◊†◊ô ◊¢◊ô◊°◊ï◊ß ◊ë◊î◊°◊§◊ß◊î ◊ï◊ë◊°◊ó◊® ◊ë◊°◊û◊ô◊ù, ◊ë◊ô◊ü ◊î◊ô◊™◊® ◊¢◊ú-◊ô◊ì◊ô ◊î◊ò◊ú◊™ ◊¢◊ï◊†◊©◊ô◊ù ◊û◊ó◊û◊ô◊®◊ô◊ù ◊¢◊ú ◊û◊ô ◊©◊†◊ï◊™◊ü ◊ô◊ì◊ï ◊ú◊û◊¢◊í◊ú ◊î◊§◊¶◊™ ◊î◊°◊û◊ô◊ù ◊ë◊®◊ë◊ô◊ù ◊ï◊ú◊î◊§◊ô◊õ◊™ ◊î◊°◊û◊ô◊ù ◊ú◊ñ◊û◊ô◊†◊ô◊ù ◊ï◊ú◊†◊í◊ô◊©◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 87:\n",
      "◊¢\"◊§ 1548/18 ◊í◊ô◊ê ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (28.11.2018) - ◊§◊°◊ß ◊ì◊ô◊ü ◊ê◊ú◊ô◊ï ◊î◊§◊†◊î ◊î◊°◊†◊í◊ï◊®. ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊ô◊ô◊ë◊ï◊ê ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ú◊ê◊ó◊® ◊©◊ô◊ô◊ë◊ê ◊ú◊ê◊®◊• ◊ó◊ë◊ô◊ú◊î ◊ï◊ë◊™◊ï◊õ◊î 10,000 ◊ñ◊®◊¢◊ô◊ù ◊©◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ê◊ï◊™◊î ◊©◊ú◊ó ◊ê◊ì◊ù ◊ê◊ó◊® ◊ú◊û◊ú◊ï◊ü ◊ë◊ô◊ï◊ï◊ü ◊¢◊ú ◊û◊†◊™ ◊©◊î◊û◊¢◊®◊¢◊® ◊ô◊ê◊°◊ï◊£ ◊ê◊ï◊™◊î ◊û◊©◊ù ◊ï◊ô◊õ◊†◊ô◊°◊î ◊ú◊ê◊®◊•. ◊ñ◊ê◊™, ◊ë◊û◊°◊í◊®◊™ ◊ß◊©◊® ◊©◊†◊®◊ß◊ù ◊ë◊ô◊ü ◊ê◊ï◊™◊ï ◊ê◊ì◊ù ◊ú◊ë◊ô◊ü ◊†◊ê◊©◊ù ◊†◊ï◊°◊£ ◊©◊¢◊û◊ì ◊ú◊ì◊ô◊ü ◊ô◊ó◊ì ◊¢◊ù ◊î◊û◊¢◊®◊¢◊®. ◊ë◊û◊°◊í◊®◊™ ◊î◊°◊ì◊® ◊ò◊ô◊¢◊ï◊ü ◊¢◊ù ◊î◊û◊¢◊®◊¢◊®, ◊°◊ï◊õ◊ù ◊õ◊ô ◊î◊û◊ê◊©◊ô◊û◊î ◊™◊¢◊™◊ï◊® ◊ú◊¢◊ï◊†◊© ◊û◊ß◊°◊ô◊û◊ú◊ô ◊©◊ú 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ê◊ô◊û◊• ◊ê◊™ ◊¢◊û◊ì◊™ ◊î◊û◊ê◊©◊ô◊û◊î ◊ú◊§◊ô◊î ◊î◊û◊™◊ó◊ù ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊î◊û◊¢◊®◊¢◊® \"◊û◊™◊ó◊ô◊ú ◊ë◊¢◊©◊®◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú\" ◊ï◊í◊ñ◊® ◊¢◊ú◊ô◊ï ◊¢◊ï◊†◊© ◊ñ◊î, ◊ú◊¶◊ì ◊û◊ê◊°◊®◊ô◊ù ◊û◊ï◊™◊†◊ô◊ù ◊ï◊ß◊†◊°. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 88:\n",
      "◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊û◊©◊ö 6 ◊ó◊ï◊ì◊©◊ô◊ù\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 89:\n",
      "◊§◊°◊ô◊ú◊î ◊û◊ú◊î◊ó◊ñ◊ô◊ß ◊ï◊û◊ú◊ß◊ë◊ú ◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 5 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ñ◊ê◊™ ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö 3 ◊©◊†◊ô◊ù ◊©◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊¢◊ë◊ï◊® ◊¢◊ë◊ô◊®◊î ◊ë◊†◊ô◊í◊ï◊ì ◊ú◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 90:\n",
      "◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ë◊ô◊®◊ï◊©◊ú◊ô◊ù ◊ß◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ô◊†◊ï◊¢ ◊ë◊ê◊ï◊™◊ï ◊û◊ß◊®◊î, ◊î◊ì◊ï◊û◊î ◊û◊ê◊ï◊ì ◊ú◊û◊ß◊®◊î ◊©◊ë◊§◊†◊ô◊†◊ï, ◊ë◊ô◊ü 9 ◊ú- 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊ô◊¶◊ï◊ô◊ü ◊õ◊ô ◊ë◊ê◊ï◊™◊ï ◊û◊ß◊®◊î ◊î◊ô◊î ◊î◊†◊ê◊©◊ù ◊ë◊¢◊ú ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô ◊û◊õ◊ë◊ô◊ì.\n",
      "Real: 0 | Predicted: 1\n",
      "\n",
      "üìù Text 91:\n",
      "◊ë◊™\"◊§ (◊û◊ó' ◊û◊®◊õ◊ñ) 2456-07-10 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊™◊ë◊ú (8.5.11) ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊¢◊ú-◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊ï◊¢◊ë◊ô◊®◊ï◊™ ◊†◊ï◊°◊§◊ï◊™. ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊î◊ï◊í◊© ◊õ◊†◊í◊ì ◊ê◊®◊ë◊¢◊î ◊†◊ê◊©◊û◊ô◊ù, ◊õ◊ê◊©◊® ◊í◊ñ◊® ◊î◊ì◊ô◊ü ◊û◊™◊ô◊ô◊ó◊° ◊ú◊†◊ê◊©◊ù 3 ◊ë◊ú◊ë◊ì. ◊î◊†◊ê◊©◊ù ◊í◊ô◊ì◊ú ◊°◊ù ◊û◊°◊ï◊í ◊ß◊†◊ê◊ë◊ï◊° ◊ë◊õ◊û◊ï◊™ ◊î◊¢◊ï◊ú◊î ◊¢◊ú 8.9 ◊ß\"◊í. ◊†◊ô◊ì◊ï◊ü ◊ú- 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ß◊†◊° ◊ë◊°◊ö 10,000 ‚Ç™ ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ï◊°◊§◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 92:\n",
      "◊ë◊í◊ô◊ü ◊™\"◊§ 66147-10-13, ◊ê◊†◊ô ◊û◊ï◊®◊î ◊¢◊ú ◊ó◊ô◊ì◊ï◊© ◊î◊û◊ê◊°◊® ◊î◊û◊ï◊™◊†◊î, ◊©◊û◊©◊õ◊ï ◊©◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô◊ù, ◊ê◊©◊® ◊î◊ï◊ò◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ë◊û◊°◊í◊®◊™ ◊™\"◊§ 4578/07, ◊ñ◊ê◊™ ◊ë◊©◊†◊™◊ô◊ô◊ù ◊†◊ï◊°◊§◊ï◊™. ◊ë◊†◊ï◊°◊£, ◊ê◊†◊ô ◊û◊ò◊ô◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊¶◊ï ◊©◊ú\"◊¶ ◊ú◊û◊©◊ö 200 ◊©◊¢◊ï◊™, ◊ñ◊ê◊™ ◊ë◊î◊™◊ê◊ù ◊ú◊™◊õ◊†◊ô◊™ ◊î◊û◊§◊ï◊®◊ò◊™ ◊ë◊™◊°◊ß◊ô◊® ◊©◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü, ◊ï◊õ◊ü ◊¶◊ï ◊û◊ë◊ó◊ü ◊ú◊û◊©◊ö ◊©◊†◊™◊ô◊ô◊ù ◊û◊î◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 93:\n",
      "◊î◊†◊ê◊©◊ù 2 ◊î◊ï◊®◊©◊¢ ◊ú◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊©◊ú◊ï◊© ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊ë◊©◊™◊ô ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊™◊ô◊ï◊ï◊ö ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü. ◊ë◊©◊ú◊ï◊© ◊¢◊ë◊ô◊®◊ï◊™ ◊î◊°◊ó◊® ◊î◊ô◊î ◊û◊ì◊ï◊ë◊® ◊ë◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú ◊©◊ú ◊û◊¢◊ú 90 ◊í◊®◊ù ◊ë◊®◊ï◊ò◊ï ◊©◊†◊û◊õ◊® ◊™◊û◊ï◊®◊™ 2,200 ‚Ç™ ◊ï◊û◊¢◊ú◊î ◊ë◊õ◊ú ◊ê◊ô◊©◊ï◊ù, ◊õ◊ê◊©◊® ◊î◊™◊û◊ï◊®◊î ◊î◊õ◊°◊§◊ô◊™ ◊©◊ï◊ú◊û◊î ◊ú◊ô◊ì◊ô◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù 2. ◊ë◊©◊™◊ô ◊¢◊ë◊ô◊®◊ï◊™ ◊î◊™◊ô◊ï◊ï◊ö ◊î◊ô◊î ◊û◊ì◊ï◊ë◊® ◊ë- 30 ◊õ◊ì◊ï◊®◊ô◊ù ◊©◊úAlfa-PVP  ◊ë◊¢◊°◊ß◊™ ◊™◊ô◊ï◊ï◊ö ◊ê◊ó◊™, ◊ï◊õ◊ü ◊ë- 30 ◊õ◊ì◊ï◊®◊ô◊ù ◊©◊ú Alfa-PVP ◊ë◊ô◊ó◊ì ◊¢◊ù ◊õ- 47 ◊í◊®◊ù ◊†◊ò◊ï ◊ó◊©◊ô◊© ◊ë◊¢◊°◊ß◊™ ◊î◊™◊ô◊ï◊ï◊ö ◊î◊©◊†◊ô◊ô◊î, ◊õ◊ê◊©◊® ◊ë◊õ◊ú ◊¢◊°◊ß◊î ◊©◊ï◊ú◊ù ◊ú◊†◊ê◊©◊ù 2 ◊¢◊¶◊û◊ï ◊°◊õ◊ï◊ù ◊©◊ú 100 ‚Ç™. ◊†◊ê◊©◊ù 2 (◊©◊î◊ô◊î ◊†◊¢◊ì◊® ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô ◊ë◊¢◊™ ◊í◊ñ◊ô◊®◊™ ◊ì◊ô◊†◊ï) ◊†◊ô◊ì◊ï◊ü, ◊ë◊ô◊ü ◊î◊ô◊™◊®, ◊ú- 20 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 94:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊†◊ï◊í◊¢ ◊ú◊ê◊ô◊©◊ï◊ù ◊î◊©◊†◊ô (◊í◊ô◊ì◊ï◊ú ◊ï◊î◊ó◊ñ◊ß◊î) ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊ê◊®◊ë◊¢◊î ◊ó◊ï◊ì◊©◊ô ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú- 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊ê◊ô◊©◊ï◊ù ◊î◊®◊ê◊©◊ï◊ü (◊ô◊ë◊ï◊ê ◊ñ◊®◊¢◊ô ◊°◊ù) ◊î◊ô◊†◊ï ◊û◊¶◊ï ◊©◊ú\"◊¶ ◊ï◊¢◊ì 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 95:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì ◊ú ‚Äì 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 96:\n",
      "◊ê◊†◊ô ◊û◊ï◊®◊î ◊¢◊ú ◊î◊§◊¢◊ú◊™ ◊¢◊ï◊†◊© ◊î◊û◊ê◊°◊® ◊¢◊ú-◊™◊†◊ê◊ô ◊©◊î◊ï◊ò◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ë◊™\"◊§ (◊†◊¶') 48113-10-18, ◊ë◊ü ◊ê◊®◊ë◊¢◊™ ◊î◊ó◊ï◊ì◊©◊ô◊ù, ◊õ◊ö ◊©◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊û◊™◊ï◊õ◊ï ◊ô◊®◊ï◊¶◊ï ◊ë◊ó◊ï◊§◊£ ◊ú◊¢◊ï◊†◊© ◊î◊û◊ê◊°◊® ◊©◊î◊ï◊ò◊ú ◊¢◊ú◊ô◊ï ◊ú◊¢◊ô◊ú ◊ï◊ê◊ô◊ú◊ï ◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊û◊™◊ï◊õ◊ï ◊ô◊®◊ï◊¶◊ï ◊ë◊û◊¶◊ò◊ë◊® ◊ú◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 97:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊ê◊ô◊©◊ï◊û◊ô◊ù ◊î◊û◊ô◊ï◊ó◊°◊ô◊ù ◊ú◊†◊ê◊©◊ù ◊ë◊í◊ô◊ü ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì ◊ú- 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊ë◊ê◊ô◊©◊ï◊ù ◊î◊û◊ô◊ô◊ó◊° ◊ú◊†◊ê◊©◊ù ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì ◊ú- 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 98:\n",
      "◊ë◊ô◊ó◊° ◊ú◊™◊ô◊ß ◊î◊©◊†◊ô, ◊ô◊© ◊ò◊¢◊ù ◊ë◊ò◊¢◊†◊î ◊õ◊ô ◊ß◊ô◊ô◊û◊™ ◊§◊°◊ô◊ß◊î ◊û◊í◊ï◊ï◊†◊™ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊í◊ô◊ì◊ï◊ú, ◊ê◊ö ◊ô◊ó◊ì ◊¢◊ù ◊ñ◊ê◊™, ◊ì◊ï◊û◊†◊ô ◊õ◊ô ◊§◊°◊ß ◊ì◊ô◊†◊ï ◊©◊ú ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ë◊™◊ú ◊ê◊ë◊ô◊ë ◊ë◊¢◊§\"◊í 46738-09-14, ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊¶◊ß◊ë◊©◊ï◊ï◊ô◊ú◊ô ◊ï◊ê◊ó' (31.12.2014) ◊î◊ï◊ê ◊î◊û◊¢◊ï◊ì◊õ◊ü ◊ë◊ô◊ï◊™◊® ◊ï◊†◊ï◊™◊ü ◊ê◊™ ◊î◊ë◊ô◊ò◊ï◊ô ◊î◊¢◊õ◊©◊ï◊ï◊ô ◊ú◊û◊¶◊ô◊ê◊ï◊™ ◊ë◊™-◊ô◊û◊ô◊†◊ï. ◊¢◊û◊ì ◊¢◊ú ◊õ◊ö ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ë◊ê◊ï◊™◊ï ◊§◊°◊ß ◊ì◊ô◊ü, ◊ï◊¶◊ô◊ô◊ü: \"◊î◊û◊¶◊ô◊ê◊ï◊™ ◊©◊ú ◊©◊†◊™ 1975 ◊ê◊ô◊†◊î ◊î◊û◊¶◊ô◊ê◊ï◊™ ◊©◊ú ◊©◊†◊™ 2014 (◊ê◊†◊ï ◊û◊¶◊ï◊ô◊ô◊ù ◊î◊ô◊ï◊ù ◊ë◊ô◊ï◊ù ◊î◊ê◊ó◊®◊ï◊ü ◊©◊ú ◊©◊†◊î ◊ñ◊ï), ◊û◊¶◊ô◊ê◊ï◊™ ◊û◊©◊™◊†◊î, ◊û◊õ◊™◊ô◊ë◊î ◊î◊™◊û◊ï◊ì◊ì◊ï◊™ ◊©◊ï◊†◊î ◊¢◊ù ◊î◊†◊í◊¢ ◊©◊ú ◊í◊ô◊ì◊ï◊ú ◊°◊ù, ◊©◊¢◊ú ◊õ◊ü, ◊°◊§◊ß ◊ê◊ù ◊†◊ô◊™◊ü ◊ú◊ú◊û◊ï◊ì ◊í◊ñ◊ô◊®◊î ◊©◊ï◊ï◊î ◊û◊î◊§◊°◊ô◊ß◊î ◊ê◊ú◊ô◊î ◊î◊§◊†◊î ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊û◊ê\". ◊†◊®◊ê◊î ◊ú◊ô, ◊õ◊ô ◊ô◊© ◊ú◊®◊ê◊ï◊™ ◊ë◊§◊°◊ß-◊ì◊ô◊ü ◊ñ◊î ◊õ◊§◊°◊ß-◊ì◊ô◊ü ◊û◊†◊ó◊î, ◊ï◊¢◊ú ◊õ◊ü, ◊ê◊†◊ô ◊†◊õ◊ï◊ü ◊ú◊ß◊ë◊ú ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊õ◊§◊ô ◊©◊î◊ï◊¶◊¢ ◊¢◊ú ◊ô◊ì◊ô ◊î◊™◊ë◊ô◊¢◊î ◊ë◊™◊ô◊ß ◊î◊©◊†◊ô, ◊ï◊ô◊© ◊ú◊®◊ê◊ï◊™◊ï ◊õ◊†◊í◊ñ◊® ◊û◊§◊°◊ß ◊î◊ì◊ô◊ü ◊ë◊¢◊†◊ô◊ô◊ü ◊¶◊ß◊ë◊©◊ï◊ï◊ô◊ú◊ô, ◊ì◊î◊ô◊ô◊†◊ï - 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊¢◊ì 20 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 0\n",
      "\n",
      "üìù Text 99:\n",
      "7 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ê◊ï◊™◊ù ◊ú◊ê ◊ô◊®◊¶◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù, ◊¢◊ë◊ô◊®◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊§◊©◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 100:\n",
      "◊ú◊í◊ë◊ô ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™, ◊î◊§◊†◊ï ◊î◊¶◊ì◊ì◊ô◊ù ◊ú◊§◊°◊ô◊ß◊î ◊©◊ó◊ú◊ß◊î ◊ê◊ô◊†◊ï ◊®◊ú◊ï◊ï◊†◊ò◊ô ‚Äì ◊ê◊ù ◊û◊©◊ï◊ù ◊©◊†◊ô◊™◊†◊î ◊ò◊®◊ù ◊™◊ô◊ß◊ï◊ü 113 ◊ï◊û◊ê◊ú◊ô◊ï ◊ô◊ï◊¶◊ê ◊©◊ê◊ô◊†◊î ◊û◊§◊®◊ô◊ì◊î ◊ë◊ô◊ü ◊î◊û◊™◊ó◊ù ◊ú◊ë◊ô◊ü ◊ß◊ë◊ô◊¢◊™ ◊î◊¢◊ï◊†◊© ◊ë◊û◊™◊ó◊ù, ◊ê◊ù ◊û◊©◊ï◊ù ◊©◊î◊ô◊ê ◊û◊™◊ô◊ô◊ó◊°◊™ ◊ú◊û◊ß◊®◊ô◊ù ◊ó◊û◊ï◊®◊ô◊ù ◊û◊¢◊†◊ô◊ô◊†◊†◊ï ◊ê◊ï ◊ß◊ú◊ô◊ù ◊û◊û◊†◊ï, ◊ï◊ê◊ù ◊û◊©◊ï◊ù ◊©◊ë◊™◊ô ◊î◊û◊©◊§◊ò ◊ë◊ó◊®◊ï ◊ú◊ó◊®◊ï◊í ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù. ◊ô◊ó◊ì ◊¢◊ù ◊ñ◊ê◊™, ◊û◊û◊õ◊ú◊ï◊ú ◊î◊ì◊ë◊®◊ô◊ù ◊¢◊ï◊ú◊î ◊õ◊ô ◊®◊û◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊†◊¢◊î ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ï◊¢◊ì 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® (◊®' ◊ë◊î◊ß◊©◊® ◊ñ◊î ◊¢\"◊§  (◊û◊ó◊ï◊ñ◊ô ◊ô-◊ù) 21958-07-16 ◊©◊ù ◊ò◊ï◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (9.4.18); ◊™\"◊§ (◊©◊ú◊ï◊ù ◊ë\"◊©) 20576-10-16 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†◊í◊ì ◊ë◊®◊†◊®◊ì◊ô◊†◊ï ◊ï◊ê◊ó' [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (30.1.17); ◊™\"◊§ (◊©◊ú◊ï◊ù - ◊®◊û◊ú◊î) 11704-06-13 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†◊í◊ì ◊í◊®◊©◊ï◊†◊ï◊ë◊ô◊• [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (11.3.14); ◊™.◊§. (◊©◊ú◊ï◊ù ‚Äì ◊®◊û◊ú◊î) 53518-05-13 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊í◊ë◊ê◊ô [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (15.1.15); ◊™.◊§. (◊©◊ú◊ï◊ù ◊ô-◊ù) 54205-05-15 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊û◊©◊ú◊ô [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (4.5.17)).\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 101:\n",
      "◊ë◊¢◊§\"◊í (◊û◊ó' ◊™\"◊ê) 32826-05-11 ◊û◊ô◊®◊ô◊ú◊ê◊©◊ï◊ô◊ú◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (14.11.11), ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊®◊ï ◊©◊ú ◊†◊ê◊©◊ù, ◊ê◊©◊® ◊î◊ï◊®◊©◊¢ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊î ◊©◊ú ◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú ◊©◊ú 1.99 ◊í◊®◊ù, ◊ï◊†◊ì◊ï◊ü ◊ú-7 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 102:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 103:\n",
      "◊ë◊™\"◊§ (◊§\"◊™) 40921-05-10 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ë◊ü ◊¢◊ñ◊®◊ê (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 18.10.2012), ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù, ◊¢◊ú ◊ô◊°◊ï◊ì ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊î◊°◊§◊ß◊™ ◊°◊ù, ◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊î◊ó◊ñ◊ß◊™ ◊õ◊ú◊ô ◊¢◊ô◊©◊ï◊ü, ◊ë◊û◊°◊í◊®◊™ ◊û◊°◊§◊® ◊ê◊ô◊©◊ï◊û◊ô◊ù, ◊ë◊í◊ô◊ü ◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú◊ô◊ù: 1.05 ◊í◊®◊ù, 1.17 ◊í◊®◊ù, 0.21 ◊í◊®◊ù, ◊õ◊ú◊ô ◊û◊ê◊ï◊ú◊™◊® ◊ú◊©◊ô◊û◊ï◊© ◊ë◊°◊û◊ô◊ù ◊ï◊°◊ô◊§◊ß ◊ú◊ê◊ó◊® ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 0.74 ◊í◊®◊ù ◊†◊ò◊ï, ◊ú◊ú◊ê ◊™◊û◊ï◊®◊î. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊ë◊¢◊ú ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊î◊ï◊©◊™◊ï 4 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊î◊ï◊§◊¢◊ú ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊©◊ú 4 ◊ó◊ï◊ì◊©◊ô◊ù ◊ë◊ó◊ï◊§◊£ ◊ï◊ë◊û◊¶◊ò◊ë◊® ◊õ◊ö ◊©◊î◊†◊ê◊©◊ù ◊®◊ô◊¶◊î 5 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊û◊ê◊°◊®◊ô◊ù ◊û◊ï◊™◊†◊ô◊ù ◊ï◊î◊™◊ó◊ô◊ô◊ë◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 104:\n",
      "◊ë◊¢◊§\"◊í (◊û◊ó' ◊™\"◊ê) 17155-07-10 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊†◊°◊ô◊ù (1.11.10) ◊î◊™◊ß◊ë◊ú ◊¢◊®◊¢◊ï◊® ◊î◊û◊ì◊ô◊†◊î ◊¢◊ú ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊í◊ô◊ì◊ï◊ú ◊©◊ú ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊©◊ú ◊ú◊û◊¢◊ú◊î ◊û- 11 ◊ß\"◊í. ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊†◊ê◊©◊ù 1 ◊î◊ï◊¢◊û◊ì ◊¢◊ú 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® (◊ó◊ú◊£ 6 ◊ó◊ï◊ì◊©◊ô◊ù). ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊†◊ê◊©◊ù 2 ◊†◊ï◊™◊® ◊¢◊ú ◊õ◊†◊ï (6 ◊ó◊ï◊ì◊©◊ô ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 105:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 106:\n",
      "◊ú◊í◊ë◊ô ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™, ◊î◊§◊†◊ï ◊î◊¶◊ì◊ì◊ô◊ù ◊ú◊§◊°◊ô◊ß◊î ◊©◊ó◊ú◊ß◊î ◊ê◊ô◊†◊ï ◊®◊ú◊ï◊ï◊†◊ò◊ô ‚Äì ◊ê◊ù ◊û◊©◊ï◊ù ◊©◊†◊ô◊™◊†◊î ◊ò◊®◊ù ◊™◊ô◊ß◊ï◊ü 113 ◊ï◊û◊ê◊ú◊ô◊ï ◊ô◊ï◊¶◊ê ◊©◊ê◊ô◊†◊î ◊û◊§◊®◊ô◊ì◊î ◊ë◊ô◊ü ◊î◊û◊™◊ó◊ù ◊ú◊ë◊ô◊ü ◊ß◊ë◊ô◊¢◊™ ◊î◊¢◊ï◊†◊© ◊ë◊û◊™◊ó◊ù, ◊ê◊ù ◊û◊©◊ï◊ù ◊©◊î◊ô◊ê ◊û◊™◊ô◊ô◊ó◊°◊™ ◊ú◊û◊ß◊®◊ô◊ù ◊ó◊û◊ï◊®◊ô◊ù ◊û◊¢◊†◊ô◊ô◊†◊†◊ï ◊ê◊ï ◊ß◊ú◊ô◊ù ◊û◊û◊†◊ï, ◊ï◊ê◊ù ◊û◊©◊ï◊ù ◊©◊ë◊™◊ô ◊î◊û◊©◊§◊ò ◊ë◊ó◊®◊ï ◊ú◊ó◊®◊ï◊í ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù. ◊ô◊ó◊ì ◊¢◊ù ◊ñ◊ê◊™, ◊û◊û◊õ◊ú◊ï◊ú ◊î◊ì◊ë◊®◊ô◊ù ◊¢◊ï◊ú◊î ◊õ◊ô ◊®◊û◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊†◊¢◊î ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ï◊¢◊ì 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® (◊®' ◊ë◊î◊ß◊©◊® ◊ñ◊î ◊¢\"◊§  (◊û◊ó◊ï◊ñ◊ô ◊ô-◊ù) 21958-07-16 ◊©◊ù ◊ò◊ï◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (9.4.18); ◊™\"◊§ (◊©◊ú◊ï◊ù ◊ë\"◊©) 20576-10-16 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†◊í◊ì ◊ë◊®◊†◊®◊ì◊ô◊†◊ï ◊ï◊ê◊ó' [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (30.1.17); ◊™\"◊§ (◊©◊ú◊ï◊ù - ◊®◊û◊ú◊î) 11704-06-13 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†◊í◊ì ◊í◊®◊©◊ï◊†◊ï◊ë◊ô◊• [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (11.3.14); ◊™.◊§. (◊©◊ú◊ï◊ù ‚Äì ◊®◊û◊ú◊î) 53518-05-13 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊í◊ë◊ê◊ô [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (15.1.15); ◊™.◊§. (◊©◊ú◊ï◊ù ◊ô-◊ù) 54205-05-15 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊û◊©◊ú◊ô [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (4.5.17)).\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 107:\n",
      "◊î◊§◊¢◊ú◊™ ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ë◊ü 3 ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊™◊ô◊ß ◊¢◊†\"◊§ (◊û◊ó' ◊û◊®◊õ◊ñ-◊ú◊ï◊ì) 12016-12-09 ◊¢◊ë◊ì ◊ê◊ú ◊î◊ê◊ì◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (1.6.10).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 108:\n",
      "◊°◊ö ◊î◊õ◊ú ◊ô◊®◊¶◊î ◊î◊†◊ê◊©◊ù  36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï ◊û◊ô◊ï◊ù 10.01.23 ◊ï◊¢◊ì ◊î◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 109:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û -6 ◊ï◊¢◊ì 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊õ◊ú ◊ê◊ô◊®◊ï◊¢ (◊ê◊ô◊©◊ï◊ù).\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 110:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊¢\"◊™ +◊©◊ú\"◊¶ ◊ï◊¢◊ì 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 111:\n",
      "◊ë◊®◊¢\"◊§  7996/12 ◊ê◊ú◊ô◊î◊ï ◊ô◊ï◊°◊£ ◊†' ◊û\"◊ô, ◊†◊ì◊ï◊ü ◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù, ◊ê◊©◊® ◊î◊ï◊®◊©◊¢ ◊ë◊ë◊ô◊¶◊ï◊¢ 4 ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ó◊® ◊ï◊©◊ú ◊™◊ô◊ï◊ï◊ö ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ó◊©◊ô◊©, ◊ë◊õ◊û◊ï◊ô◊ï◊™ ◊ß◊ò◊†◊ï◊™ ◊ô◊ó◊°◊ô◊™ ( ◊¢◊ì 6 ◊í◊®◊ù ◊†◊ò◊ï ◊©◊ú ◊°◊ù ◊ë◊õ◊ú ◊¢◊°◊ß◊î ). ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ß◊ë◊¢, ◊¢◊ï◊ì ◊ò◊®◊ù ◊õ◊†◊ô◊°◊™◊ï ◊ú◊™◊ï◊ß◊£ ◊©◊ú ◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊õ◊ú ◊ê◊ó◊™ ◊û◊û◊õ◊ô◊®◊ï◊™ ◊î◊°◊ù, ◊†◊¢ ◊ë◊ô◊ü 7 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú ‚Äì 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊î◊©◊ô◊™ ◊¢◊ú ◊î◊†◊ê◊©◊ù 21 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ì◊ó◊î ◊ê◊™ ◊¢◊®◊¢◊ï◊® ◊î◊†◊ê◊©◊ù ◊¢◊ú ◊í◊ñ◊® ◊î◊ì◊ô◊ü. ◊ë◊ì◊ó◊ï◊™◊ï ◊ë◊ß◊©◊î ◊ú◊û◊™◊ü ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊®, ◊¢◊ú ◊§◊°◊ß ◊ì◊ô◊†◊ï ◊©◊ú ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô, ◊¶◊ô◊ô◊ü ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊õ◊ì◊ú◊ß◊û◊ü:\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 112:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í) ◊û◊¶◊ê◊™◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊¢◊ë◊ô◊®◊™ ◊î◊™◊ô◊ï◊ï◊ö ◊†◊¢ ◊û◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊¢◊ì ◊ú-12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 113:\n",
      "◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô ◊ë◊ê◊® ◊©◊ë◊¢) 1818-08-20 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ê◊ú◊ó◊û◊ô◊ì◊ô (1.11.2021 ◊ï- 1.7.2021) - ◊î◊†◊ê◊©◊û◊ô◊ù ◊î◊ï◊®◊©◊¢◊ï ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ù ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊î ◊©◊ú ◊ô◊ë◊ï◊ê ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù, ◊ï◊ñ◊ê◊™ ◊™◊ï◊ö ◊î◊°◊™◊ô◊ô◊¢◊ï◊™ ◊ë◊®◊õ◊ë ◊ú◊ë◊ô◊¶◊ï◊¢ ◊§◊©◊¢, ◊ú◊§◊ô ◊°◊¢◊ô◊£ 43 ◊ú◊§◊ß◊ï◊ì◊™ ◊î◊™◊¢◊ë◊ï◊®◊î, ◊ï◊†◊í◊ñ◊®◊ï ◊¢◊ú ◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊ù 3.5 ◊©◊†◊ï◊™ ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™. ◊û◊¢◊ï◊ë◊ì◊ï◊™ ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊¢◊ï◊ú◊î, ◊õ◊ô ◊¢◊ï◊ë◊® ◊ú◊™◊ê◊®◊ô◊ö 22.7.2020 ◊°◊ô◊õ◊û◊ï ◊ë◊ô◊†◊ô◊î◊ù ◊î◊†◊ê◊©◊û◊ô◊ù ◊ï◊ê◊ó◊®◊ô◊ù ◊©◊ñ◊î◊ï◊™◊ù ◊ê◊ô◊†◊î ◊ô◊ì◊ï◊¢◊î ◊ï◊ë◊ô◊†◊ô◊î◊ù ◊í◊ù ◊™◊ï◊©◊ë◊ô ◊û◊¶◊®◊ô◊ù, ◊ú◊ô◊ô◊ë◊ê ◊û◊û◊¶◊®◊ô◊ù ◊ú◊ô◊©◊®◊ê◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊õ◊û◊ï◊™ ◊û◊°◊ó◊®◊ô◊™. ◊ë◊°◊ï◊§◊ï ◊©◊ú ◊ì◊ë◊®, ◊õ◊ï◊ó◊ï◊™ ◊î◊ë◊ô◊ò◊ó◊ï◊ü ◊™◊§◊°◊ï ◊ê◊™ ◊î◊™◊ô◊ß◊ô◊ù ◊©◊î◊õ◊ô◊ú◊ï ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊©◊ú 54.24 ◊ß\"◊í. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊©◊†◊ß◊ë◊¢ ◊ë◊¢◊†◊ô◊ô◊†◊ù ◊©◊ú ◊î◊†◊ê◊©◊û◊ô◊ù ◊†◊¢ ◊ë◊ô◊ü ◊©◊ú◊ï◊© ◊ú◊ó◊û◊© ◊©◊†◊ï◊™ ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 114:\n",
      "◊ë◊î◊™◊ó◊©◊ë ◊ë◊¢◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊©◊ú ◊î◊î◊ú◊ô◊û◊î ◊ï◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊ô◊™◊® ◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù ◊©◊§◊ï◊®◊ò◊ï, ◊ê◊†◊ô ◊°◊ë◊ï◊®◊î ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊ê◊ô◊®◊ï◊¢ ◊†◊©◊ï◊ê ◊î◊™◊ô◊ß ◊î◊û◊¶◊ï◊®◊£ ◊¢◊ú ◊û◊õ◊ú◊ï◊ú ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊î◊†◊õ◊ú◊ú◊ï◊™ ◊ë◊û◊°◊í◊®◊™◊ï, ◊†◊¢ ◊û◊û◊°◊§◊® ◊ë◊ï◊ì◊ì ◊©◊ú ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊ì 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 115:\n",
      "◊¢\"◊§ 2729/22 ◊û◊ô◊õ◊ê◊ú ◊©◊ô◊ò◊®◊ô◊™ ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (3.5.2023) ◊¢◊†◊ô◊ô◊†◊ü ◊©◊ú ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊ë◊î◊ü ◊î◊ï◊®◊©◊¢ ◊û◊ú◊õ◊î ◊ë◊î◊ß◊û◊î ◊ï◊ë◊î◊§◊¢◊ú◊î ◊©◊ú ◊©◊™◊ô ◊û◊¢◊ë◊ì◊ï◊™ ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù ◊ë◊¢◊®◊ô◊ù ◊®◊¢◊†◊†◊î ◊ï◊ó◊ï◊ú◊ï◊ü. ◊ú◊§◊ô ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊î◊û◊™◊ï◊ß◊ü, ◊û◊ú◊õ◊î ◊©◊õ◊® ◊ô◊ó◊ô◊ì◊™ ◊ì◊ô◊ï◊® ◊ë◊ë◊ô◊™ ◊ë◊®◊¢◊†◊†◊î, ◊ï◊î◊ß◊ô◊ù ◊ë◊î ◊û◊¢◊ë◊ì◊î ◊©◊ê◊ï◊™◊î ◊î◊§◊¢◊ô◊ú, ◊ë◊°◊ô◊ï◊¢ ◊©◊ú◊ï◊©◊î ◊†◊ê◊©◊û◊ô◊ù ◊ê◊ó◊®◊ô◊ù, ◊ë◊û◊©◊ö ◊õ◊ó◊û◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ó◊¶◊ô. ◊ñ◊ê◊™, ◊™◊ï◊ö ◊¶◊ô◊ï◊ì ◊î◊û◊¢◊ë◊ì◊î ◊ë◊¶◊ô◊ï◊ì ◊®◊ë ◊î◊õ◊ï◊ú◊ú ◊û◊ñ◊í◊†◊ô◊ù, ◊û◊§◊ï◊ó◊ô◊ù, ◊ê◊ì◊†◊ô◊ï◊™, ◊®◊§◊ú◊ß◊ò◊ï◊®◊ô◊ù ◊ï◊û◊†◊ï◊®◊ï◊™. ◊ë◊û◊°◊í◊®◊™ ◊î◊§◊¢◊ú◊™ ◊û◊¢◊ë◊ì◊î ◊ñ◊ï, ◊û◊ú◊õ◊î ◊ê◊£ ◊†◊ò◊ú ◊©◊ú◊ê ◊õ◊ì◊ô◊ü ◊ó◊©◊û◊ú. ◊ë◊û◊¢◊ë◊ì◊î ◊ë◊®◊¢◊†◊†◊î ◊†◊™◊§◊°◊ï ◊õ-650 ◊©◊™◊ô◊ú◊ô◊ù ◊©◊ú ◊î◊°◊ù ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 131.16 ◊ß\"◊í, ◊©◊©◊ï◊ï◊ô◊ù ◊û◊ï◊¢◊®◊ö ◊ë◊õ-13,116,000 ◊©\"◊ó. ◊ë◊ô◊†◊ô ◊ú◊ë◊ô◊†◊ô, ◊ë◊ô◊ï◊ù 23.7.2020 ◊©◊õ◊® ◊û◊ú◊õ◊î ◊ô◊ó◊ô◊ì◊™ ◊ì◊ô◊ï◊® ◊ë◊ë◊ô◊™ ◊§◊®◊ò◊ô ◊ë◊ó◊ï◊ú◊ï◊ü, ◊î◊§◊¢◊ô◊ú ◊©◊ù ◊û◊¢◊ë◊ì◊î ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù, ◊™◊ï◊ö ◊¶◊ô◊ï◊ì◊î ◊ë◊¶◊ô◊ï◊ì ◊®◊ë. ◊ë◊û◊¢◊ë◊ì◊î ◊ë◊ó◊ï◊ú◊ï◊ü ◊†◊™◊§◊°◊ï ◊õ-347 ◊©◊™◊ô◊ú◊ô◊ù ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 5.14 ◊ß\"◊í ◊ï◊ë◊©◊ï◊ï◊ô ◊û◊ï◊¢◊®◊ö ◊©◊ú ◊õ-514,000 ◊©\"◊ó. ◊ë◊ê◊©◊® ◊ú◊©◊ò◊®◊ô◊™, ◊ë◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊î◊û◊™◊ï◊ß◊ü, ◊©◊ò◊®◊ô◊™ ◊ï◊†◊ê◊©◊ù ◊†◊ï◊°◊£ ◊†◊™◊†◊ï ◊ú◊ê◊ó◊ì ◊î◊†◊ê◊©◊û◊ô◊ù ◊î◊ê◊ó◊®◊ô◊ù ◊ê◊û◊¶◊¢◊ô◊ù ◊ú◊î◊ß◊û◊™ ◊û◊¢◊ë◊ì◊î ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù ◊ë◊§◊™◊ó ◊™◊ß◊ï◊ï◊î, ◊ê◊©◊® ◊§◊¢◊ú◊î ◊ó◊ï◊ì◊©◊ô◊ô◊ù ‚Äì ◊ê◊ñ ◊†◊™◊§◊°◊ï ◊ë◊î ◊õ-791 ◊©◊™◊ô◊ú◊ô◊ù ◊©◊ú ◊î◊°◊ù ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 40.54 ◊ß\"◊í ◊ï◊ë◊©◊ï◊ï◊ô ◊û◊ï◊¢◊®◊ö ◊©◊ú ◊õ-4,054,000 ◊©\"◊ó. ◊î◊§◊® ◊ê◊™ ◊î◊™◊†◊ê◊ô◊ù ◊î◊û◊í◊ë◊ô◊ú◊ô◊ù ◊©◊†◊ß◊ë◊¢◊ï ◊ú◊ï ◊õ◊ö ◊©◊ô◊¶◊ê ◊ú◊ë◊ô◊ß◊ï◊® ◊ë◊û◊¢◊ë◊ì◊î. ◊ú◊§◊ô ◊¢◊ï◊ë◊ì◊ï◊™ ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊†◊ï◊©◊ê ◊î◊î◊ú◊ô◊ö ◊ë◊ô◊®◊ï◊©◊ú◊ô◊ù, ◊ë◊û◊©◊ö ◊õ◊©◊†◊î ◊ï◊¢◊ï◊ì ◊ß◊ï◊ì◊ù ◊ú◊î◊ß◊û◊™ ◊î◊û◊¢◊ë◊ì◊î ◊ë◊§◊™◊ó-◊™◊ß◊ï◊ï◊î ‚Äì ◊î◊ß◊ô◊ù ◊©◊ò◊®◊ô◊™ ◊ï◊î◊§◊¢◊ô◊ú ◊û◊¢◊ë◊ì◊î ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù ◊ë◊û◊ó◊°◊ü ◊î◊ì◊ô◊®◊î ◊î◊©◊õ◊ï◊®◊î ◊©◊ë◊î ◊î◊™◊í◊ï◊®◊® ◊ô◊ó◊ì ◊¢◊ù ◊ê◊©◊™◊ï ◊ï◊©◊†◊ô ◊ô◊ú◊ì◊ô◊ï ◊ë◊í◊ë◊¢◊™ ◊ñ◊ê◊ë. ◊©◊ò◊®◊ô◊™ ◊î◊ó◊ñ◊ô◊ß ◊ï◊î◊™◊ß◊ô◊ü ◊ê◊™ ◊î◊¶◊ô◊ï◊ì ◊î◊ì◊®◊ï◊© ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù, ◊ï◊©◊™◊ú ◊ë◊û◊¢◊ë◊ì◊î ‚Äì ◊ë◊¢◊¶◊û◊ï ◊ê◊ï ◊ë◊ê◊û◊¶◊¢◊ï◊™ ◊ê◊ó◊®◊ô◊ù ‚Äì 400 ◊©◊™◊ô◊ú◊ô ◊°◊ù. ◊ë◊†◊ï◊°◊£ ◊ê◊ó◊°◊ü ◊©◊ò◊®◊ô◊™ ◊°◊ù ◊ë◊û◊°◊ì◊®◊ï◊ü ◊©◊ë◊ì◊ô◊®◊î ◊ï◊ë◊ê◊ó◊ì ◊û◊î◊ó◊ì◊®◊ô◊ù, ◊ï◊õ◊ü ◊†◊ò◊ú ◊©◊ú◊ê ◊õ◊ì◊ô◊ü ◊ó◊©◊û◊ú. ◊ë◊ì◊ô◊®◊î ◊†◊û◊¶◊ê◊ï 70 ◊ß\"◊í ◊©◊ú ◊î◊°◊ù. ◊ß◊ë◊¢ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊©◊ú 36 ◊¢◊ì 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊¢◊ë◊ï◊® ◊û◊õ◊ú◊ï◊ú ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊î◊ü ◊î◊ï◊®◊©◊¢ ◊û◊ú◊õ◊î, ◊ï◊û◊™◊ó◊ù ◊©◊ú 25 ◊¢◊ì 50 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊¢◊ë◊ï◊® ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊î◊ü ◊î◊ï◊®◊©◊¢ ◊©◊ò◊®◊ô◊™. ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù ◊í◊ñ◊® ◊¢◊ú ◊û◊ú◊õ◊î ◊¢◊ï◊†◊© ◊©◊ú 32 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊™. ◊ú◊§◊ô◊õ◊ö ◊î◊ï◊ò◊ú ◊¢◊ú ◊©◊ò◊®◊ô◊™ ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊ú 25 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊™. ◊î◊¢◊®◊¢◊ï◊® ◊©◊ú ◊û◊ú◊õ◊î ◊†◊ì◊ó◊î. ◊î◊¢◊®◊¢◊ï◊® ◊©◊ú ◊©◊ò◊®◊ô◊™ ◊î◊™◊ß◊ë◊ú ◊õ◊ö ◊©◊î◊ï◊§◊ó◊™◊ï ◊ú- 19 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 116:\n",
      "◊ú◊ê◊ó◊® ◊©◊ß◊ô◊ú◊™ ◊õ◊ú◊ú ◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù, ◊†◊°◊ô◊ë◊ï◊™ ◊î◊û◊¢◊©◊ô◊ù ◊ï◊ó◊ú◊ß◊ï ◊î◊ô◊ó◊°◊ô ◊©◊ú ◊î◊†◊ê◊©◊ù, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù ◊ï◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ê◊†◊ô ◊ß◊ï◊ë◊¢◊™ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 40-65 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 117:\n",
      "◊¢◊ú ◊ô◊°◊ï◊ì ◊õ◊ú ◊ê◊ú◊ï, ◊û◊¶◊ê◊™◊ô ◊ú◊ß◊ë◊ï◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 27-50 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 118:\n",
      "◊ê◊©◊® ◊¢◊ú ◊õ◊ü, ◊ú◊ê◊ó◊® ◊©◊ë◊ó◊†◊™◊ô ◊ê◊™ ◊†◊°◊ô◊ë◊ï◊™ ◊û◊¢◊©◊ô ◊î◊†◊ê◊©◊ù, ◊ú◊®◊ë◊ï◊™ ◊ó◊ú◊ß◊ï ◊î◊ô◊ó◊°◊ô, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù ◊ï◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢◊™ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 46-22 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 119:\n",
      "◊†◊ï◊õ◊ó ◊î◊ê◊û◊ï◊®, ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊™ ◊î◊°◊§◊ß◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú ◊ñ◊¢◊ô◊® ◊ú◊¢◊ô◊©◊ï◊ü ◊ï◊õ◊ì◊ï◊® ◊ê◊ó◊ì ◊©◊ú ◊ê◊ß◊°◊ò◊ñ◊ô, ◊ú◊ê◊ï◊® ◊î◊†◊°◊ô◊ë◊ï◊™ ◊î◊ß◊ï◊†◊ß◊®◊ò◊ô◊ï◊™ ◊ï◊î◊¢◊ß◊®◊ï◊†◊ï◊™ ◊©◊ë◊ô◊°◊ï◊ì ◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü ◊¶◊®◊ô◊ö ◊ú◊†◊ï◊¢ ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊ï◊ì◊ì◊ô◊ù ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ú-12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊û◊ê◊°◊®◊ô◊ù ◊û◊ï◊™◊†◊ô◊ù, ◊§◊°◊ô◊ú◊™ ◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ï◊¢◊†◊ô◊©◊î ◊õ◊ú◊õ◊ú◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 120:\n",
      "◊î◊û◊ê◊©◊ô◊û◊î ◊î◊§◊†◊™◊î ◊ú◊§◊°◊ô◊ß◊î ◊õ◊ê◊©◊® ◊û◊™◊ó◊û◊ô ◊î◊¢◊†◊ô◊©◊î ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 18 ◊ú- 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊î◊¢◊ï◊†◊©◊ô◊ù ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 20 ◊ú- 36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 121:\n",
      "◊©◊ú◊ï◊©◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ê◊ï◊™◊ù ◊ô◊ï◊õ◊ú ◊î◊†◊ê◊©◊ù ◊ú◊®◊¶◊ï◊™ ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊õ◊§◊ô ◊©◊ß◊ë◊¢ ◊î◊û◊û◊ï◊†◊î ◊¢◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊î◊©◊ô◊®◊ï◊™ ◊ë◊ó◊ï◊ï◊™-◊ì◊¢◊™◊ï. ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ú◊î◊™◊ô◊ô◊¶◊ë ◊ú◊®◊ô◊¶◊ï◊ô ◊¢◊ï◊†◊©◊ï ◊ë◊ô◊ï◊ù 7.9.2016 ◊¢◊ì ◊î◊©◊¢◊î 10:00 ◊ë◊û◊§◊ß◊ì◊™ ◊û◊ó◊ï◊ñ ◊ì◊®◊ï◊ù, ◊ô◊ó◊ô◊ì◊™ ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ú◊ô◊ì ◊õ◊ú◊ê ◊ë◊ê◊®-◊©◊ë◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 122:\n",
      "◊ú◊ê◊ó◊® ◊©◊†◊ì◊®◊©◊™◊ô ◊ú◊û◊õ◊ú◊ï◊ú ◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù ◊î◊®◊ú◊ï◊ï◊†◊ò◊ô◊ô◊ù ◊ï◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊©◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊©◊†◊™◊ô◊ô◊ù ◊ï◊¢◊ì ◊ú-5 ◊©◊†◊ï◊™ ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ú◊¶◊ì ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊ß◊†◊°.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 123:\n",
      "◊§◊ï◊°◊ú◊™ ◊î◊†◊ê◊©◊ù ◊û◊ß◊ë◊ú ◊ï◊û◊î◊ó◊ñ◊ô◊ß ◊®◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ú◊û◊©◊ö 12 ◊§◊°◊ô◊ú◊î ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊©◊ê ◊ë◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊î ◊¢◊ú ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊ê◊ï ◊†◊î◊ô◊í◊î ◊ë◊ñ◊û◊ü ◊§◊°◊ô◊ú◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 124:\n",
      "48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊î◊ó◊ú ◊û◊ô◊ï◊ù ◊û◊¢◊¶◊®◊ï 8.7.24.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 125:\n",
      "◊ô◊© ◊ú◊î◊ò◊ô◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊ê◊©◊® ◊û◊ó◊ì ◊ô◊î◊ô◊î ◊û◊©◊û◊¢◊ï◊™◊ô ◊ï◊û◊ê◊ô◊ì◊ö ◊ô◊©◊ß◊ú◊ú ◊ê◊™ ◊ê◊ï◊™◊ù ◊†◊™◊ï◊†◊ô◊ù ◊î◊¢◊ï◊û◊ì◊ô◊ù ◊ú◊ñ◊õ◊ï◊™◊ï, ◊ï◊ë◊ô◊†◊ô◊î◊ù, ◊î◊ï◊ì◊ê◊™◊ï, ◊î◊¢◊ì◊® ◊î◊®◊©◊¢◊ï◊™ ◊ß◊ï◊ì◊û◊ï◊™, ◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊î◊ê◊ô◊©◊ô◊ï◊™ ◊ï◊°◊ô◊õ◊ï◊ô◊ô ◊©◊ô◊ß◊ï◊û◊ï ◊î◊í◊ë◊ï◊î◊ô◊ù. ◊ë◊™◊ï◊ö ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊ï◊¢◊ú ◊ô◊°◊ï◊ì ◊õ◊ú◊ú ◊î◊†◊™◊ï◊†◊ô◊ù, ◊†◊®◊ê◊î ◊õ◊ô ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊¶◊®◊ô◊ö ◊ú◊î◊ô◊í◊ñ◊® ◊ë◊®◊£ ◊î◊™◊ó◊™◊ï◊ü ◊©◊ú ◊î◊û◊™◊ó◊ù. ◊ê◊©◊® ◊¢◊ú ◊õ◊ü, ◊ë◊ô◊î◊û\"◊© ◊í◊ñ◊® ◊¢◊ú ◊î◊†◊ê◊©◊ù 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 126:\n",
      "◊ê◊†◊ô ◊í◊ï◊ñ◊® ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú  28 ◊ó◊ï◊ì◊©◊ô◊ù ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï ◊û◊™◊ê◊®◊ô◊ö 31/12/22 ◊ï◊¢◊ì ◊î◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 127:\n",
      "◊û◊ò◊ë◊¢ ◊î◊ì◊ë◊®◊ô◊ù, ◊û◊ß◊ï◊ù ◊ë◊ï ◊û◊ì◊ï◊ë◊® ◊ë◊©◊†◊ô ◊ê◊ß◊ì◊ó◊ô◊ù, ◊î◊®◊ô ◊©◊ô◊© ◊ë◊õ◊ö ◊ê◊ë◊ó◊†◊î ◊©◊ú ◊û◊û◊© ◊ê◊ú ◊û◊ï◊ú ◊ó◊ú◊ß ◊†◊ô◊õ◊® ◊û◊î◊§◊°◊ô◊ß◊î ◊©◊î◊ï◊ë◊ê◊î ◊ú◊¢◊ô◊ú. ◊û◊†◊í◊ì, ◊û◊ß◊ï◊ù ◊ë◊ï ◊ë◊®◊ô ◊õ◊ô ◊î◊û◊ì◊ï◊ë◊® ◊ë◊û◊¢◊©◊î ◊¢◊ë◊ô◊®◊î ◊ê◊ó◊ì, ◊ì◊ï◊û◊î ◊õ◊ô ◊ê◊ô◊ü ◊û◊ß◊ï◊ù ◊ú◊¢◊®◊ï◊ö ◊ó◊ô◊©◊ï◊ë ◊ê◊®◊ô◊™◊û◊ò◊ô ◊©◊ú ◊î◊¶◊ò◊ë◊®◊ï◊™ ◊©◊†◊ô ◊û◊™◊ó◊û◊ô◊ù ◊¢◊ú ◊§◊ô ◊î◊§◊°◊ô◊ß◊î ◊î◊†◊ï◊î◊í◊™ ◊ë◊ô◊ó◊° ◊ú◊†◊©◊ô◊ê◊™ ◊ê◊ß◊ì◊ó ◊ë◊ï◊ì◊ì, ◊î◊í◊ù ◊©◊û◊†◊í◊ì, ◊î◊¢◊ï◊ë◊ì◊î ◊©◊û◊ì◊ï◊ë◊® ◊ë◊©◊†◊ô ◊ê◊ß◊ì◊ó◊ô◊ù ◊û◊î◊ï◊ï◊î ◊©◊ô◊ß◊ï◊ú ◊û◊î◊ï◊™◊ô ◊ë◊ß◊ë◊ô◊¢◊™ ◊î◊û◊™◊ó◊ù. ◊ë◊î◊ô◊†◊™◊ü ◊î◊ê◊û◊ï◊® ◊ú◊¢◊ô◊ú ◊ï◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊†◊°◊ô◊ë◊ï◊™ ◊î◊ß◊©◊ï◊®◊ï◊™ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î, ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊¢◊ú ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊†◊ï◊¢ ◊ë◊ô◊ü 29 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊ë◊ô◊ü 53 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 128:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í) ◊û◊¶◊ê◊™◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊¢◊ë◊ô◊®◊î ◊ê◊ï◊™◊î ◊ë◊ô◊¶◊¢ ◊î◊†◊ê◊©◊ù ◊†◊¢ ◊ë◊ô◊ü ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ß◊¶◊® ◊ï◊¢◊ì ◊ú-3 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 129:\n",
      "◊¢\"◊§ 6056/18  ◊ê◊ì◊®◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï, 16.05.2019), ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊® ◊©◊ú ◊î◊†◊ê◊©◊ù ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊©, 38 ◊ó◊ï◊ì◊©◊ô◊ù. ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊™ ◊©◊ï◊ì, ◊ú◊ê◊ó◊® ◊©◊í◊†◊ë ◊û◊ô◊ì◊ô◊î ◊©◊ú ◊ß◊©◊ô◊©◊î ◊ë◊™ 90 ◊ê◊™ ◊î◊©◊ß◊ô◊™ ◊ë◊î ◊ê◊ó◊ñ◊î, ◊ë◊™◊ï◊õ◊î ◊î◊ô◊ï ◊û◊¶◊®◊õ◊ô◊ù ◊©◊®◊õ◊©◊î ◊ï◊õ◊ü ◊ê◊™ ◊ê◊®◊†◊ß◊î ◊©◊î◊õ◊ô◊ú 200 ‚Ç™. ◊õ◊™◊ï◊¶◊ê◊î ◊û◊û◊¢◊©◊ô◊ï, ◊†◊§◊ú◊î ◊î◊ß◊©◊ô◊©◊î ◊ê◊®◊¶◊î, ◊†◊ó◊ë◊ú◊î ◊ë◊®◊ê◊©◊î ◊ï◊†◊ñ◊ß◊ß◊î ◊ú◊ò◊ô◊§◊ï◊ú ◊®◊§◊ï◊ê◊ô.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 130:\n",
      "◊û◊ê◊°◊® ◊ú◊û◊©◊ö 6 ◊ó◊ï◊ì◊©◊ô◊ù, ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 131:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö  4 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊ô◊©◊ê ◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊ô◊ï◊ù ◊©◊ó◊®◊ï◊®◊ï ◊û◊î◊û◊ê◊°◊® ◊¢◊ë◊ô◊®◊ï◊™ ◊¢◊ú ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í ◊¢◊ï◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 132:\n",
      "◊ë◊õ◊ú ◊î◊†◊ï◊í◊¢ ◊ú◊†◊ê◊©◊ù 2 (◊ê◊®◊ë◊¢ ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü, ◊õ◊ê◊©◊® ◊¢◊°◊ß◊ô◊†◊ü ◊ë◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú ◊©◊ú ◊¢◊ì ◊©◊†◊ô ◊í◊®◊ù ◊†◊ò◊ï, ◊ê◊ï ◊©◊ú◊ï◊© ◊ò◊ë◊ú◊ô◊ï◊™ ◊©◊ú MDMA, ◊™◊û◊ï◊®◊™ 100-150 ‚Ç™ ◊ë◊õ◊ú ◊û◊ß◊®◊î)- ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊†◊¢ ◊û◊û◊ê◊°◊® ◊ë◊ï◊ì◊ì ◊©◊ú ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊ì 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊ô◊ï◊™◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 133:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ë◊®◊£ ◊î◊†◊û◊ï◊ö ◊ï◊¢◊ì ◊ú- 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 134:\n",
      "◊¢\"◊§ 717/22 ◊°◊ú◊ê◊û◊î ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (1.2.2023) - ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊ú◊ê◊ó◊® ◊†◊ô◊î◊ï◊ú ◊î◊ï◊õ◊ó◊ï◊™ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊ë◊õ◊ö ◊©◊í◊ô◊ì◊ú ◊ë◊ó◊û◊û◊î ◊¶◊û◊ï◊ì ◊ú◊ë◊ô◊™◊ï ◊õ-740 ◊©◊™◊ô◊ú◊ô◊ù ◊©◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊°, ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 114 ◊ß\"◊í. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊†◊¢ ◊ë◊ô◊ü 30 ◊ú-54 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊î◊©◊ô◊™ ◊¢◊ú ◊î◊†◊ê◊©◊ù 45 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊™◊ï◊ö ◊î◊§◊¢◊ú◊™ ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô, ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™. ◊¢◊®◊¢◊ï◊® ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊© ◊î◊™◊ß◊ë◊ú, ◊õ◊ö ◊©◊î◊ï◊©◊™◊ï ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ë◊°◊ï◊§◊ï ◊©◊ú ◊ì◊ë◊® 36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊©◊ß◊ë◊¢ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 135:\n",
      "◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊û◊©◊ö 20 ◊ó◊ï◊ì◊©◊ô◊ù ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï- 16.7.2013 ◊¢◊ì 2.9.2013.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 136:\n",
      "◊ú◊ê◊ó◊® ◊©◊†◊ì◊®◊©◊™◊ô ◊ú◊û◊õ◊ú◊ï◊ú ◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù ◊î◊®◊ú◊ï◊ï◊†◊ò◊ô◊ô◊ù ◊ï◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40◊í(◊ê) ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü), ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ú◊†◊°◊ô◊ë◊ï◊™ ◊î◊û◊ß◊®◊î ◊©◊ú◊§◊†◊ô◊†◊ï, ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊î◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊î◊†◊¢ ◊ë◊ô◊ü 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ê◊©◊® ◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ú◊ë◊ô◊ü 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 137:\n",
      "28  ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï ◊ë◊™◊ô◊ß ◊ñ◊î ◊ë◊î◊™◊ê◊ù ◊ú◊®◊ô◊©◊ï◊û◊ô ◊©◊ë\"◊° (◊ú◊û◊¢◊ò ◊î◊™◊ß◊ï◊§◊î ◊©◊©◊î◊î ◊ë◊™◊†◊ê◊ô ◊ê◊ô◊ñ◊ï◊ß).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 138:\n",
      "8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ê◊ï◊™◊ù ◊ú◊ê ◊ô◊®◊¶◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊©◊ó◊®◊ï◊®◊ï, ◊¢◊ë◊ô◊®◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊§◊©◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 139:\n",
      "◊ë◊¢◊†◊ô◊ô◊ü ◊ò◊©◊ô◊ô◊ë ◊î◊†\"◊ú, ◊†◊í◊ñ◊®◊ï ◊¢◊ú ◊†◊ê◊©◊ù ◊ë◊ü 37, ◊ë◊¢◊ú ◊¢◊ë◊® ◊†◊ß◊ô, ◊î◊°◊ï◊ë◊ú ◊û◊ó◊ú◊™ ◊†◊§◊©, ◊©◊©◊ì◊ì ◊ò◊ú◊§◊ï◊ü ◊†◊ô◊ô◊ì, ◊ê◊ö ◊†◊™◊§◊° ◊ë◊°◊û◊ï◊ö ◊ú◊õ◊ö, ◊ï◊î◊ò◊ú◊§◊ï◊ü ◊î◊ï◊ó◊ñ◊® ◊ú◊ë◊¢◊ú◊ô◊ï, 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 140:\n",
      "◊õ◊ö ◊ë◊™.◊§. 3001-08-12 ◊ì◊ï◊ë◊® ◊ë◊†◊ê◊©◊ù, ◊ë◊¢◊ú ◊î◊®◊©◊¢◊î ◊ß◊ï◊ì◊û◊™ ◊©◊î◊™◊ô◊ô◊©◊†◊î, ◊ê◊©◊® ◊ô◊ô◊¶◊® ◊ï◊î◊ó◊ñ◊ô◊ß ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú 35.89 ◊ß\"◊í, ◊ï◊†◊í◊ñ◊®◊ï ◊¢◊ú◊ô◊ï 13 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊™◊ï◊ö ◊©◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊î◊ï◊ú◊ù ◊©◊ú ◊ë◊ô◊ü 10 ◊ú-30 ◊ó◊ï◊ì◊© ◊û◊ê◊°◊®; ◊™.◊§. 51673-05-13 ◊¢◊†◊ô◊ô◊†◊ï ◊ô◊ô◊¶◊ï◊® ◊ï◊î◊§◊ß◊™ 763.67 ◊í◊®◊ù ◊ß◊†◊ê◊ë◊ï◊° ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ë◊ú◊ë◊ì; ◊™.◊§. 6800-09-13 ◊¢◊†◊ô◊ô◊†◊ï \"◊û◊¢◊ô◊ì◊î ◊ó◊ì ◊§◊¢◊û◊ô◊™\" ◊©◊ú ◊†◊ê◊©◊ù ◊ê◊©◊® ◊í◊ô◊ì◊ú ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú 963.3 ◊í◊®◊ù ◊ë◊ú◊ë◊ì, ◊õ◊ê◊©◊® ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊©◊ô ◊©◊ú ◊ë◊ô◊ü 6 ◊ú-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊í◊ñ◊® ◊¢◊ú◊ô◊ï 4 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊®◊ï◊™ ◊ë◊©◊ú ◊©◊ô◊™◊ï◊£ ◊§◊¢◊ï◊ú◊î ◊¢◊ù ◊î◊î◊ú◊ô◊ö ◊î◊©◊ô◊ß◊ï◊û◊ô; ◊™.◊§. 11831-01-12 ◊¢◊†◊ô◊ô◊†◊ï ◊ë◊†◊ê◊©◊û◊™ ◊ê◊©◊® ◊ë◊ô◊¶◊¢◊î 3 ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ó◊® ◊ë◊ó◊©◊ô◊© ◊ï◊î◊ó◊ñ◊ô◊ß◊î ◊°◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊¢◊ú ◊î◊†◊ê◊©◊û◊™ ◊î◊ï◊ò◊ú ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ë◊ú◊ë◊ì, ◊ë◊°◊ò◊ô◊ô◊î ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊®◊ê◊ï◊ô, ◊ë◊©◊ú ◊î◊¢◊ï◊ë◊ì◊î ◊©◊ë◊©◊†◊™◊ô◊ô◊ù ◊©◊ß◊ì◊û◊ï ◊ú◊í◊ñ◊® ◊î◊ì◊ô◊ü ◊¢◊ë◊®◊î ◊î◊†◊ê◊©◊û◊™ ◊™◊î◊ú◊ô◊ö ◊©◊ô◊ß◊ï◊û◊ô ◊ó◊ô◊ï◊ë◊ô; ◊™.◊§. 32330-10-10 ◊¢◊†◊ô◊ô◊†◊ï ◊ë◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊ï◊î◊ó◊ñ◊ß◊™◊ï ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊ë◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß ◊ë◊ó◊¶◊® ◊ë◊ô◊™◊ï ◊ë-5 ◊©◊ô◊ó◊ô ◊°◊ù ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊°, ◊ë◊í◊ï◊ë◊î 1.5-2 ◊û◊ò◊®◊ô◊ù ◊ï◊ë◊û◊©◊ß◊ú 4.650 ◊ß\"◊í. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊ë◊ü 34 ◊†◊©◊ï◊ô ◊ï◊ê◊ë ◊ú◊ô◊ú◊ì, ◊ê◊©◊® ◊î◊©◊™◊ú◊ë ◊ë◊ß◊ë◊ï◊¶◊î ◊ò◊ô◊§◊ï◊ú◊ô◊™, ◊û◊°◊® ◊ë◊ì◊ô◊ß◊ï◊™ ◊©◊™◊ü ◊†◊ß◊ô◊ï◊™ ◊ï◊ë◊û◊ß◊ë◊ô◊ú ◊û◊¶◊ï◊ô ◊ë◊ò◊ô◊§◊ï◊ú ◊§◊®◊ò◊†◊ô, ◊†◊í◊ñ◊®◊ï 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊®◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 141:\n",
      "◊ë◊î◊™◊ó◊©◊ë ◊ë◊¢◊ß◊®◊ï◊ü ◊î◊î◊ú◊ô◊û◊î ◊î◊û◊î◊ï◊ï◊î ◊¢◊ß◊®◊ï◊ü ◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î; ◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊¢◊ï◊¶◊û◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù; ◊ë◊î◊™◊ó◊©◊ë ◊ë◊ó◊ï◊û◊®◊™ ◊î◊¢◊ë◊ô◊®◊î ◊ï◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢◊î; ◊ï◊õ◊ü ◊ë◊î◊™◊ó◊©◊ë ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ë◊í◊ô◊ü ◊î◊¢◊ë◊ô◊®◊î ◊î◊†◊ì◊ï◊†◊î; ◊ê◊†◊ô ◊°◊ë◊ï◊®◊î ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊†◊¢ ◊û◊û◊ê◊°◊® ◊ß◊¶◊® ◊ë◊§◊ï◊¢◊ú (◊©◊ë◊†◊°◊ô◊ë◊ï◊™ ◊û◊™◊ê◊ô◊û◊ï◊™, ◊†◊ô◊™◊ü ◊ú◊©◊ß◊ï◊ú ◊ê◊™ ◊®◊ô◊¶◊ï◊ô◊ï÷π ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™) ◊ï◊¢◊ì 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 142:\n",
      "◊ú◊§◊ô◊õ◊ö, ◊õ◊û◊¶◊ï◊ï◊™ ◊™◊ô◊ß◊ï◊ü 113 ◊ï◊ë◊î◊™◊ê◊ù ◊ú◊†◊°◊ô◊ë◊ï◊™ ◊û◊¢◊©◊ô ◊î◊†◊ê◊©◊ù, ◊î◊†◊†◊ô ◊ß◊ï◊ë◊¢◊™ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊î◊î◊ï◊ú◊ù ◊ú◊û◊¢◊©◊ô◊ù ◊©◊ë◊ô◊¶◊¢ ◊î◊†◊ê◊©◊ù ◊†◊¢ ◊ë◊ô◊ü 12-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 143:\n",
      "◊ë◊¢\"◊§ 3658/14 ◊ô◊ú◊ô◊°◊ô◊ô◊ë ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (23.1.2015) ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊™ ◊©◊ï◊ì ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊û◊ó◊û◊ô◊®◊ï◊™, ◊ú◊ê◊ó◊® ◊©◊ë◊û◊®◊õ◊ñ ◊ß◊†◊ô◊ï◊™ ◊ë◊ë◊ê◊® ◊©◊ë◊¢ ◊©◊ì◊ì ◊ê◊ô◊©◊î ◊ï◊ê◊ô◊ô◊ù ◊¢◊ú◊ô◊î ◊ë◊¢◊ñ◊®◊™ ◊°◊õ◊ô◊ü ◊û◊ò◊ë◊ó ◊ê◊®◊ï◊õ◊î ◊ï◊ó◊ì◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 3 ◊ú-6 ◊©◊†◊ï◊™ ◊û◊ê◊°◊®, ◊ï◊í◊ñ◊® ◊¢◊ú◊ô◊ï 4 ◊©◊†◊ï◊™ ◊û◊ê◊°◊®, ◊ñ◊ê◊™ ◊¢◊ú ◊®◊ß◊¢ ◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô ◊î◊û◊©◊û◊¢◊ï◊™◊ô, ◊ï◊î◊†◊°◊ô◊ë◊ï◊™ ◊î◊û◊ó◊û◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ë◊ô◊¶◊ï◊¢. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ì◊ó◊î ◊ê◊™ ◊î◊¢◊®◊¢◊ï◊® ◊ï◊î◊ì◊í◊ô◊© ◊ê◊™ ◊î◊§◊í◊ô◊¢◊î ◊î◊†◊§◊©◊ô◊™ ◊î◊ß◊©◊î ◊©◊†◊í◊®◊û◊î ◊ú◊û◊™◊ú◊ï◊†◊†◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 144:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö 8 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊ô◊©◊ê ◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊ô◊ï◊ù ◊©◊ó◊®◊ï◊®◊ï ◊û◊î◊û◊ê◊°◊® ◊¢◊ë◊ô◊®◊ï◊™ ◊¢◊ú ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í ◊§◊©◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 145:\n",
      "◊ß◊†◊° ◊õ◊°◊§◊ô ◊ë◊°◊ö 25,000‚Ç™ ◊ê◊ï 250 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊û◊™◊ï◊ö ◊î◊î◊§◊ß◊ì◊î ◊ë◊ß◊ï◊§◊™ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 146:\n",
      "◊ú◊ê◊ó◊® ◊©◊©◊ß◊ú◊™◊ô ◊ê◊™ ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊û◊ï◊í◊ü, ◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î ◊î◊¢◊ë◊ô◊®◊î ◊ï◊î◊§◊°◊ô◊ß◊î ◊î◊†◊ï◊î◊í◊™, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊ë◊û◊ß◊®◊î ◊ñ◊î ◊†◊¢ ◊ë◊ô◊ü ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ú◊ë◊ô◊ü ◊©◊ë◊¢◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 147:\n",
      "◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊ü ◊ó◊û◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô◊ù, ◊ë◊†◊ô◊õ◊ï◊ô ◊ô◊û◊ô ◊û◊¢◊¶◊®◊ï ◊û◊ô◊ï◊ù 12.9.13 ◊ï◊¢◊ì 11.12.13, ◊ï◊ê◊§◊ô◊ú◊ï ◊ô◊®◊ê◊ï ◊®◊ô◊©◊ï◊û◊ô ◊©◊ë\"◊° ◊ê◊ó◊®◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 148:\n",
      "◊¢\"◊§ 971/21 ◊¢◊û◊†◊ï◊ê◊ú ◊ê◊û◊ê◊¶'◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú  (31.3.21)  ◊©◊ù ◊ê◊ô◊©◊® ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊©◊†◊ß◊ë◊¢ ◊¢◊ú ◊ô◊ì◊ô ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊û◊ó◊ï◊ñ◊ô ◊©◊ú 24 ◊¢◊ì 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊õ◊ê◊©◊® ◊ì◊ï◊ë◊® ◊©◊ù ◊¢◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 678.866 ◊í◊®◊ù ◊†◊ò◊ï (◊®◊ê◊î ◊ë◊ò◊ô◊¢◊ï◊†◊ô ◊î◊†◊ê◊©◊ù ◊§◊°◊ß ◊ì◊ô◊†◊ï ◊©◊ú ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ë◊™\"◊§ 11220-12-18).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 149:\n",
      "◊¢\"◊§ 6300/21 ◊†◊™◊ü ◊™◊ï◊®◊í'◊û◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (◊†◊ë◊ï 23.2.2022) - ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ú◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊™ ◊©◊ï◊ì ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊û◊ó◊û◊ô◊®◊ï◊™. ◊ë◊™◊û◊¶◊ô◊™, ◊î◊û◊¢◊®◊¢◊® ◊î◊¶◊ò◊ô◊ô◊ì ◊ô◊ó◊ì ◊¢◊ù ◊ë◊™ ◊ñ◊ï◊í◊ï ◊ë◊õ◊ú◊ô ◊†◊©◊ß ◊î◊†◊ó◊ñ◊ô◊ù ◊ú◊ê◊û◊ô◊™◊ô◊ô◊ù, ◊†◊õ◊†◊°◊ï ◊ú◊°◊ï◊§◊®◊û◊®◊ß◊ò ◊®◊¢◊ï◊ú◊ô ◊§◊†◊ô◊ù, ◊î◊û◊¢◊®◊¢◊® ◊§◊†◊î ◊ê◊ú ◊î◊ß◊ï◊§◊ê◊ô◊™, ◊©◊ú◊£ ◊õ◊ú◊ô ◊î◊†◊ó◊ñ◊î ◊ú◊î◊ô◊ï◊™ ◊ê◊ß◊ì◊ó ◊ï◊î◊ï◊®◊î ◊ú◊î ◊ú◊î◊¢◊ë◊ô◊® ◊ú◊ô◊ì◊ô◊ï ◊ê◊™ ◊î◊õ◊°◊£ ◊û◊î◊ß◊ï◊§◊î. ◊ë◊†◊ô ◊î◊ñ◊ï◊í ◊ô◊¶◊ê◊ï ◊û◊î◊°◊ï◊§◊®◊û◊®◊ß◊ò ◊¢◊ù ◊©◊ú◊ú ◊ë◊°◊ö 3,000 ◊©\"◊ó, ◊ï◊ê◊ó◊®◊ô◊î◊ù ◊î◊û◊™◊ú◊ï◊†◊ü ◊©◊î◊ó◊ñ◊ô◊ß ◊ê◊ß◊ì◊ó ◊ë◊®◊ô◊©◊ô◊ï◊ü, ◊ì◊®◊ö ◊î◊ê◊ß◊ì◊ó ◊ï◊ß◊®◊ê ◊ú◊û◊¢◊®◊¢◊® ◊ú◊¢◊¶◊ï◊®. ◊î◊û◊¢◊®◊¢◊® ◊õ◊ô◊ï◊ï◊ü ◊õ◊ú◊§◊ô ◊î◊û◊™◊ú◊ï◊†◊ü ◊ê◊™ ◊î◊ó◊§◊• ◊©◊†◊ó◊ñ◊î ◊ú◊î◊ô◊ï◊™ ◊ê◊ß◊ì◊ó, ◊î◊û◊™◊ú◊ï◊†◊ü ◊ô◊®◊î ◊ë◊û◊¢◊®◊¢◊® ◊ï◊ñ◊î ◊†◊§◊¶◊¢ ◊ß◊©◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊î◊†◊¢ ◊ë◊ô◊ü 18-48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊ò◊ú◊ï 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊®◊ë◊ï◊™ ◊î◊§◊¢◊ú◊™ ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊©◊ú 6 ◊ó◊ï◊ì◊©◊ô◊ù, ◊ó◊¶◊ô◊ï ◊ë◊û◊¶◊ò◊ë◊®, ◊ï◊õ◊ü ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊™. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 150:\n",
      "◊ë◊®◊¢\"◊§ 747/14 ◊ú◊ï◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (11.2.14), ◊ì◊ó◊î ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ê◊™ ◊ë◊ß◊©◊™ ◊¢◊®◊¢◊ï◊® ◊©◊ú ◊†◊ê◊©◊ù ◊ê◊©◊® ◊î◊ï◊®◊©◊¢ ◊ë◊©◊™◊ô ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊î◊†◊ê◊©◊ù ◊î◊ó◊ñ◊ô◊ß 5 ◊í◊®◊ù ◊î◊®◊ï◊ê◊ô◊ü ◊ï◊¶◊ô◊®◊£ ◊™◊ô◊ß ◊©◊ë◊ï ◊î◊ó◊ñ◊ô◊ß 0.095 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü. ◊î◊†◊ê◊©◊ù ◊ë◊ü 50, ◊†◊ò◊ú ◊ê◊ó◊®◊ô◊ï◊™ ◊ï◊î◊ó◊ú ◊ë◊ò◊ô◊§◊ï◊ú ◊ë◊ô◊ó◊ô◊ì◊î ◊ú◊†◊§◊í◊¢◊ô ◊î◊™◊û◊õ◊®◊ï◊ô◊ï◊™ ◊ï◊©◊ï◊ú◊ë ◊ë◊ò◊ô◊§◊ï◊ú ◊§◊®◊ò◊†◊ô. ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊î◊û◊ú◊ô◊• ◊ú◊î◊¢◊û◊ô◊ì◊ï ◊ë◊§◊ô◊ß◊ï◊ó ◊ú◊¶◊ì ◊¢◊ï◊†◊© ◊©◊ú ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô. ◊†◊ô◊ì◊ï◊ü ◊ú - 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 151:\n",
      "◊û◊ß◊ï◊ë◊ú ◊¢◊ú◊ô ◊ò◊ô◊¢◊ï◊ü ◊ë\"◊õ ◊î◊û◊ê◊©◊ô◊û◊î ◊ë◊ì◊ë◊® ◊î◊¢◊®◊õ◊ô◊ù ◊î◊ó◊ë◊®◊™◊ô◊ô◊ù ◊©◊†◊§◊í◊¢◊ï ◊¢◊ß◊ë ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊¢◊ú ◊ô◊ì◊ô ◊î◊†◊ê◊©◊ù. ◊ô◊ó◊ì ◊¢◊ù ◊ñ◊ê◊™, ◊ë◊ê◊©◊® ◊ú◊ß◊ë◊ô◊¢◊™ ◊û◊™◊ó◊û◊ô ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊û◊ô◊ù ◊ô◊© ◊ú◊î◊ë◊ó◊ô◊ü ◊ë◊ô◊ü ◊©◊†◊ô ◊î◊ê◊ô◊©◊ï◊û◊ô◊ù. ◊ë◊ê◊©◊® ◊ú◊ê◊ô◊©◊ï◊ù ◊î◊©◊†◊ô ‚Äì ◊ë◊î◊™◊ó◊©◊ë ◊ë◊¢◊ï◊ë◊ì◊î ◊©◊û◊ì◊ï◊ë◊® ◊ë◊°◊ó◊® ◊ë◊°◊û◊ô◊ù ◊ß◊©◊ô◊ù, ◊ï◊õ◊ü ◊ë◊î◊™◊ó◊©◊ë ◊ë◊õ◊û◊ï◊™ ◊î◊°◊û◊ô◊ù, ◊ë◊°◊õ◊ï◊ù ◊î◊õ◊°◊£ ◊î◊û◊©◊û◊¢◊ï◊™◊ô ◊©◊©◊ï◊ú◊ù ◊ë◊¢◊ë◊ï◊®◊ù ◊ï◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊§◊°◊ô◊ß◊î ◊ë◊û◊ß◊®◊ô◊ù ◊ì◊ï◊û◊ô◊ù ◊ú◊í◊ë◊ô ◊°◊û◊ô◊ù ◊ß◊©◊ô◊ù ◊ê◊ó◊®◊ô◊ù ‚Äì ◊û◊ß◊ï◊ë◊ú ◊¢◊ú◊ô◊ô ◊ò◊ô◊¢◊ï◊ü ◊ë\"◊õ ◊î◊û◊ê◊©◊ô◊û◊î ◊ú◊ß◊ë◊ô◊¢◊™ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ë◊ô◊ü 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊ë◊ô◊ü 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 152:\n",
      "◊¢\"◊§ 306/24 ◊ê◊ú◊ô◊©◊¢ ◊ê◊ô◊ô◊ì◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (01.05.24)  ◊î◊û◊¢◊®◊¢◊® ◊†◊ì◊ï◊ü ◊ë◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ú- 14 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ß◊†◊° ◊ë◊°◊ö 10,000 ‚Ç™ ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù, ◊ú◊ê◊ó◊® ◊©◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊™ ◊°◊ù, ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊î◊ó◊ñ◊ß◊™ ◊ó◊¶◊®◊ô◊ù ◊ú◊©◊ù ◊î◊õ◊†◊™ ◊°◊ù, ◊î◊ó◊ñ◊ß◊™ ◊õ◊ú◊ô◊ù ◊î◊û◊©◊û◊©◊ô◊ù ◊ú◊î◊õ◊†◊™ ◊°◊ù ◊ï◊†◊ò◊ô◊ú◊™ ◊ó◊©◊û◊ú. ◊î◊û◊¢◊®◊¢◊® ◊©◊õ◊® ◊ë◊ô◊™ ◊ï◊î◊©◊™◊û◊© ◊ë◊î ◊õ◊û◊¢◊ë◊ì◊î ◊ú◊í◊ô◊ì◊ï◊ú ◊°◊û◊ô◊ù ◊ë◊û◊©◊ö ◊õ- 3.5 ◊ó◊ï◊ì◊©◊ô◊ù. ◊î◊û◊¢◊®◊¢◊® ◊î◊ó◊ñ◊ô◊ß ◊ë◊û◊¢◊ë◊ì◊î ◊¶◊ô◊ï◊ì ◊ô◊ô◊¢◊ï◊ì◊ô ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊û◊ô◊ù ◊ï◊í◊ô◊ì◊ú 364 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 150 ◊ß\"◊í. ◊õ◊ü ◊î◊™◊ó◊ë◊® ◊ú◊®◊©◊™ ◊ó◊©◊û◊ú ◊©◊ú◊ê ◊õ◊ì◊ô◊ü ◊ï◊î◊©◊™◊û◊© ◊ë◊ó◊©◊û◊ú ◊î◊©◊ô◊ô◊ö ◊ú◊ó◊ë◊®◊™ ◊î◊ó◊©◊û◊ú. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊ï◊ú◊ù ◊©◊†◊¢ ◊ë◊ô◊ü 24 - 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊†◊í◊ñ◊® ◊ì◊ô◊†◊ï ◊õ◊ê◊û◊ï◊® ◊ú◊¢◊ô◊ú, ◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊î◊ï◊ì◊ê◊™◊ï ◊î◊û◊ô◊ì◊ô◊™, ◊©◊ô◊™◊ï◊£ ◊î◊§◊¢◊ï◊ú◊î ◊î◊û◊ú◊ê ◊¢◊ù ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊ï◊î◊§◊í◊†◊™ ◊û◊ï◊ò◊ô◊ë◊¶◊ô◊î ◊õ◊†◊î ◊ï◊ê◊û◊™◊ô◊™ ◊ú◊©◊ô◊†◊ï◊ô. ◊î◊û◊¢◊®◊¢◊® ◊ó◊ñ◊® ◊ë◊ï ◊û◊î◊¢◊®◊¢◊ï◊® ◊ë◊î◊û◊ú◊¶◊™ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 153:\n",
      "◊©◊ú◊ï◊©◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊î◊ó◊ú ◊û◊ô◊ï◊ù 7.5.15, ◊ë◊û◊®◊õ◊ñ ◊ô◊ï◊ù ‚Äì ◊õ◊§◊® ◊©◊ú◊ù ◊ë◊™◊ú-◊ê◊ë◊ô◊ë. ◊î◊†◊ê◊©◊ù ◊û◊ï◊ñ◊î◊® ◊õ◊ô ◊ê◊ô-◊¶◊ô◊ï◊™ ◊ú◊î◊ï◊®◊ê◊ï◊™ ◊î◊û◊û◊ï◊†◊î ◊¢◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊î◊©◊ô◊®◊ï◊™ ◊ê◊ï ◊î◊û◊û◊ï◊†◊ô◊ù ◊ë◊û◊ß◊ï◊ù ◊î◊¢◊ë◊ï◊ì◊î ◊ô◊ï◊ë◊ô◊ú ◊ú◊î◊§◊°◊ß◊™ ◊®◊ô◊¶◊ï◊ô ◊î◊¢◊ë◊ï◊ì◊ï◊™ ◊ï◊î◊û◊©◊ö ◊®◊ô◊¶◊ï◊ô ◊î◊û◊ê◊°◊® ◊û◊ê◊ó◊ï◊®◊ô ◊°◊ï◊®◊í ◊ï◊ë◊®◊ô◊ó.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 154:\n",
      "◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 4.5 ◊ó◊ï◊ì◊©◊ô◊ù. ◊™◊ß◊ï◊§◊™ ◊î◊û◊ê◊°◊® ◊™◊®◊ï◊¶◊î ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ë◊î◊™◊ê◊ù ◊ú◊ó◊ï◊ï◊™ ◊ì◊¢◊™ ◊î◊û◊û◊ï◊†◊î ◊û◊ô◊ï◊ù 25.2.2015. ◊î◊†◊ê◊©◊ù ◊û◊ï◊ñ◊î◊® ◊õ◊ô ◊õ◊ú ◊î◊§◊®◊î ◊ë◊™◊†◊ê◊ô ◊¢◊ë◊ï◊ì◊ï◊™ ◊î◊©◊ô◊®◊ï◊™ ◊ô◊õ◊ï◊ú◊î ◊ï◊™◊ë◊ô◊ê ◊ú◊î◊§◊ß◊¢◊™◊ü ◊î◊û◊†◊î◊ú◊ô◊™ ◊ï◊ú◊®◊ô◊¶◊ï◊ô ◊î◊¢◊ï◊†◊© ◊ë◊û◊™◊ß◊ü ◊õ◊ú◊ô◊ê◊î. ◊î◊ï◊ë◊î◊® ◊ú◊†◊ê◊©◊ù ◊õ◊ô ◊û◊ß◊ï◊ù ◊î◊©◊û◊™◊ï ◊ú◊®◊ô◊¶◊ï◊ô ◊¢◊ë◊ï◊ì◊ï◊™ ◊î◊©◊ô◊®◊ï◊™ ◊î◊ô◊†◊ï ◊ë\"◊¢◊û◊ï◊™◊™ ◊ú◊™◊™ ◊ë◊ê◊ô◊ú◊™\", ◊î◊ó◊ú ◊û◊ô◊ï◊ù 17.6.2015, ◊ë◊î◊™◊ê◊ù ◊ú◊ó◊ï◊ï◊™ ◊ì◊¢◊™ ◊î◊û◊û◊ï◊†◊î. ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ú◊î◊™◊ô◊ô◊¶◊ë ◊ë◊û◊ï◊¢◊ì ◊ñ◊î, ◊ë◊©◊¢◊î 8:00, ◊ë◊û◊©◊®◊ì◊ô ◊î◊û◊û◊ï◊†◊î ◊©◊ú◊ô◊ì ◊õ◊ú◊ê ◊ê◊©◊ú ◊ë◊ë◊ê◊® ◊©◊ë◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 155:\n",
      "◊ú◊ê◊ï◊® ◊î◊û◊§◊ï◊®◊ò ◊ú◊¢◊ô◊ú ◊ë◊ì◊ë◊® ◊î◊¢◊®◊õ◊ô◊ù ◊î◊ó◊ë◊®◊™◊ô◊ô◊ù ◊©◊†◊§◊í◊¢◊ï ◊û◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊ï◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊î◊ù, ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊ï◊™, ◊ê◊†◊ô ◊°◊ë◊ï◊® ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊û◊¢◊©◊ô◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊†◊¢ ◊ë◊ô◊ü 26 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊ë◊ô◊ü 56 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 156:\n",
      "◊ë◊î◊™◊ó◊©◊ë ◊ë◊û◊õ◊ú◊ï◊ú ◊î◊†◊°◊ô◊ë◊ï◊™ ◊©◊û◊†◊ô◊™◊ô ◊ú◊¢◊ô◊ú, ◊î◊ß◊©◊ï◊®◊ï◊™ ◊ú◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î, ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ë◊ô◊®◊î, ◊ï◊î◊¢◊®◊õ◊ô◊ù ◊î◊ó◊ë◊®◊™◊ô◊ô◊ù ◊©◊†◊§◊í◊¢◊ï ◊õ◊™◊ï◊¶◊ê◊î ◊û◊ë◊ô◊¶◊ï◊¢◊î, ◊ï◊ú◊ê◊ó◊® ◊©◊¢◊û◊ì◊™◊ô ◊¢◊ú ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ì◊ï◊û◊ï◊™, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊î◊®◊ê◊ï◊ô ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊î◊û◊ß◊®◊î ◊ì◊†◊ü, ◊†◊¢ ◊ë◊ô◊ü 18 ◊ú- 42 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ñ◊ê◊™ ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 157:\n",
      "3\t◊¢\"◊§ 526/14 ◊§◊ú◊ï◊†◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (12.3.14). ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊†◊©◊ß ◊ï◊°◊ó◊® ◊ë◊†◊©◊ß. ◊î◊û◊¢◊®◊¢◊® ◊û◊õ◊® ◊ú◊°◊ï◊õ◊ü ◊û◊©◊ò◊®◊™◊ô ◊ê◊ß◊ì◊ó ◊™◊û◊ï◊®◊™ ◊°◊ö ◊©◊ú 14,000 ‚Ç™. ◊û◊™◊°◊ß◊ô◊® ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊¢◊ú◊î ◊©◊î◊û◊¢◊®◊¢◊® ◊†◊î◊í ◊ú◊î◊©◊™◊û◊© ◊ï◊ú◊°◊ó◊ï◊® ◊ë◊°◊û◊ô◊ù ◊ë◊¢◊ë◊® ◊ï◊õ◊ô ◊î◊ô◊î ◊ë◊¢◊™ ◊û◊™◊ü ◊í◊ñ◊® ◊î◊ì◊ô◊ü ◊ë◊ò◊ô◊§◊ï◊ú ◊ë◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü, ◊ê◊©◊® ◊î◊û◊ú◊ô◊• ◊ú◊î◊°◊™◊§◊ß ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊ñ◊ê◊™ ◊†◊ï◊õ◊ó ◊î◊î◊ú◊ô◊ö ◊î◊ò◊ô◊§◊ï◊ú◊ô ◊î◊ó◊ô◊ï◊ë◊ô ◊ë◊ï ◊î◊ó◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊í◊ñ◊® ◊¢◊ú◊ô◊ï 22 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊î◊§◊ó◊ô◊™ ◊ê◊™ ◊™◊ß◊ï◊§◊™ ◊î◊û◊ê◊°◊® ◊ú-16 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ñ◊ê◊™ ◊û◊ê◊ó◊® ◊ï◊î◊û◊¢◊®◊¢◊® ◊û◊í◊ú◊î ◊§◊ï◊ò◊†◊¶◊ô◊ê◊ú ◊©◊ô◊ß◊ï◊û◊ô ◊û◊û◊©◊ô, ◊ú◊®◊ë◊ï◊™ ◊¢◊ú ◊ô◊ì◊ô ◊î◊ë◊†◊™ ◊î◊ô◊™◊®◊ï◊†◊ï◊™ ◊ë◊†◊ô◊î◊ï◊ú ◊ê◊ï◊®◊ó ◊ó◊ô◊ô◊ù ◊†◊ï◊®◊û◊ò◊ô◊ë◊ô ◊û◊ë◊ú◊ô ◊ú◊¢◊©◊ï◊™ ◊©◊ô◊û◊ï◊© ◊ë◊°◊û◊ô◊ù. ◊ë◊û◊ô◊ú◊ô◊ù ◊ê◊ó◊®◊ï◊™, ◊®◊õ◊ô◊ë ◊î◊û◊ê◊°◊® ◊û◊û◊© ◊ë◊õ◊ú ◊ñ◊ê◊™ ◊†◊ï◊™◊® ◊ë◊¢◊ô◊†◊ï, ◊¢◊ú ◊ê◊£ ◊î◊©◊ô◊ß◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 158:\n",
      "◊¢\"◊§ 306/24 ◊ê◊ú◊ô◊©◊¢ ◊ê◊ô◊ô◊ì◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (01.05.24)  ◊î◊û◊¢◊®◊¢◊® ◊†◊ì◊ï◊ü ◊ë◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ú- 14 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ß◊†◊° ◊ë◊°◊ö 10,000 ‚Ç™ ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù, ◊ú◊ê◊ó◊® ◊©◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊™ ◊°◊ù, ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊î◊ó◊ñ◊ß◊™ ◊ó◊¶◊®◊ô◊ù ◊ú◊©◊ù ◊î◊õ◊†◊™ ◊°◊ù, ◊î◊ó◊ñ◊ß◊™ ◊õ◊ú◊ô◊ù ◊î◊û◊©◊û◊©◊ô◊ù ◊ú◊î◊õ◊†◊™ ◊°◊ù ◊ï◊†◊ò◊ô◊ú◊™ ◊ó◊©◊û◊ú. ◊î◊û◊¢◊®◊¢◊® ◊©◊õ◊® ◊ë◊ô◊™ ◊ï◊î◊©◊™◊û◊© ◊ë◊î ◊õ◊û◊¢◊ë◊ì◊î ◊ú◊í◊ô◊ì◊ï◊ú ◊°◊û◊ô◊ù ◊ë◊û◊©◊ö ◊õ- 3.5 ◊ó◊ï◊ì◊©◊ô◊ù. ◊î◊û◊¢◊®◊¢◊® ◊î◊ó◊ñ◊ô◊ß ◊ë◊û◊¢◊ë◊ì◊î ◊¶◊ô◊ï◊ì ◊ô◊ô◊¢◊ï◊ì◊ô ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊û◊ô◊ù ◊ï◊í◊ô◊ì◊ú 364 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú  150 ◊ß\"◊í. ◊õ◊ü ◊î◊™◊ó◊ë◊® ◊ú◊®◊©◊™ ◊ó◊©◊û◊ú ◊©◊ú◊ê ◊õ◊ì◊ô◊ü ◊ï◊î◊©◊™◊û◊© ◊ë◊ó◊©◊û◊ú ◊î◊©◊ô◊ô◊ö ◊ú◊ó◊ë◊®◊™ ◊î◊ó◊©◊û◊ú. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊ï◊ú◊ù ◊©◊†◊¢ ◊ë◊ô◊ü 24 - 48  ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊†◊í◊ñ◊® ◊ì◊ô◊†◊ï ◊õ◊ê◊û◊ï◊® ◊ú◊¢◊ô◊ú, ◊ë◊©◊ô◊ù ◊ú◊ë ◊ú◊î◊ï◊ì◊ê◊™◊ï ◊î◊û◊ô◊ì◊ô◊™, ◊©◊ô◊™◊ï◊£ ◊î◊§◊¢◊ï◊ú◊î ◊î◊û◊ú◊ê ◊¢◊ù ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊ï◊î◊§◊í◊†◊™ ◊û◊ï◊ò◊ô◊ë◊¶◊ô◊î ◊õ◊†◊î ◊ï◊ê◊û◊™◊ô◊™ ◊ú◊©◊ô◊†◊ï◊ô. ◊î◊û◊¢◊®◊¢◊® ◊ó◊ñ◊® ◊ë◊ï ◊û◊î◊¢◊®◊¢◊ï◊® ◊ë◊î◊û◊ú◊¶◊™ ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 159:\n",
      "◊ß◊†◊° ◊ë◊°◊ö 2,000 ‚Ç™ ◊ê◊ï ◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊û◊ê◊°◊® ◊†◊ï◊°◊§◊ô◊ù ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊¢◊ì ◊ú◊ô◊ï◊ù 1.9.15;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 160:\n",
      "◊ë◊ó◊ô◊†◊™ ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊ß◊†◊ë◊ï◊° ◊ë◊û◊¢◊ë◊ì◊î ◊ë◊û◊©◊ß◊ú◊ô◊ù ◊ì◊ï◊û◊ô◊ù ◊û◊¢◊ú◊î ◊õ◊ô ◊û◊†◊¢◊ì ◊î◊¢◊†◊ô◊©◊î ◊õ◊ï◊ú◊ú ◊ë◊®◊ï◊ë◊ï ◊¢◊ï◊†◊©◊ô ◊û◊ê◊°◊® ◊î◊†◊¢◊ô◊ù ◊ë◊ô◊ü 22-48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®:\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 161:\n",
      "◊ß◊†◊° ◊ë◊°◊ö 2,000 ‚Ç™ ◊ê◊ï 20 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊¢◊ì ◊ï◊ú◊ê ◊ô◊ê◊ï◊ó◊® ◊û◊ô◊ï◊ù 1.3.15.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 162:\n",
      "◊ë◊™\"◊§ ( ◊†◊™◊†◊ô◊î ) 16717-03-13 ◊û\"◊ô ◊†' ◊ê◊†◊í◊ì◊î, ◊†◊ì◊ï◊ü ◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù, ◊©◊î◊ï◊®◊©◊¢ ◊ë◊°◊ó◊® ◊ë◊°◊û◊ô◊ù, ◊ë◊û◊°◊í◊®◊™ ◊î◊§◊¢◊ú◊™ ◊î◊°◊ï◊õ◊ü ◊î◊û◊©◊ò◊®◊™◊ô ◊î◊°◊û◊ï◊ô, ◊ê◊©◊® ◊î◊ï◊§◊¢◊ú ◊û◊ï◊ú ◊ô◊©◊î ◊ï◊î◊†◊ê◊©◊ù. ◊ë◊ê◊ï◊™◊ï ◊û◊ß◊®◊î ◊ì◊ï◊ë◊® ◊ë◊†◊ê◊©◊ù, ◊ê◊©◊® ◊û◊õ◊® ◊ú◊°◊ï◊õ◊ü ◊ë◊©◊™◊ô ◊î◊ñ◊ì◊û◊†◊ï◊ô◊ï◊™, ◊ë◊î◊§◊®◊© ◊©◊ú 5 ◊ô◊û◊ô◊ù, ◊°◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊©, ◊ë◊û◊©◊ß◊ú ◊©◊ú 12 ◊í◊®◊ù ◊†◊ò◊ï, ◊ï◊ñ◊ê◊™ ◊™◊û◊ï◊®◊™ 400 ‚Ç™ ◊ë◊¢◊ì ◊õ◊ú ◊û◊õ◊ô◊®◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊ë◊í◊ô◊ü ◊õ◊ú ◊ê◊ó◊™ ◊û◊ü ◊î◊¢◊ë◊ô◊®◊ï◊™, ◊†◊¢ ◊ë◊ô◊ü 5 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊ë◊ô◊ü 11 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 163:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 6 ◊ó◊ï◊ì◊©◊ô◊ù. ◊î◊†◊ê◊©◊ù ◊ô◊ô◊©◊ê ◊ë◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ù ◊ë◊™◊ß◊ï◊§◊î ◊©◊ú ◊©◊ú◊ï◊© ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊ô◊ë◊¶◊¢ ◊õ◊ú ◊¢◊ë◊ô◊®◊î ◊û◊°◊ï◊í ◊¢◊ï◊ï◊ü ◊ú◊§◊ô ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 164:\n",
      "◊¢◊ï◊ì ◊ô◊ï◊ë◊î◊®, ◊õ◊ô ◊û◊ï◊°◊õ◊ù ◊ú◊û◊¢◊©◊î ◊¢◊ú ◊î◊¶◊ì◊ì◊ô◊ù ◊©◊ô◊© ◊û◊ß◊ï◊ù ◊ú◊ß◊ë◊ï◊¢ ◊û◊™◊ó◊ù ◊†◊§◊®◊ì ◊ú◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊û◊¢◊©◊ô◊ù ◊ë◊î◊ù ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù. ◊î◊î◊í◊†◊î ◊î◊§◊†◊™◊î ◊ú◊§◊°◊ô◊ß◊î ◊ë◊û◊°◊í◊®◊™◊î ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊©◊ú 8-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊ï◊ê◊ï◊ú◊ù ◊ë◊ó◊ô◊†◊™ ◊§◊°◊ô◊ß◊î ◊ñ◊ï ◊û◊ú◊û◊ì◊™ ◊õ◊ô ◊ê◊ô◊ü ◊î◊û◊ì◊ï◊ë◊® ◊ë◊û◊ß◊®◊ô◊ù ◊ë◊î◊ù ◊°◊ó◊®◊ï ◊î◊†◊ê◊©◊û◊ô◊ù ◊ë◊õ◊û◊ï◊ô◊ï◊™ ◊©◊ú ◊õ◊ó◊¶◊ô ◊ß\"◊í ◊°◊ù, ◊õ◊ë◊û◊ß◊®◊î ◊©◊ë◊§◊†◊ô◊ô, ◊ê◊ú◊ê ◊ë◊õ◊û◊ï◊ô◊ï◊™ ◊ß◊ò◊†◊ï◊™ ◊û◊õ◊ö ◊û◊©◊û◊¢◊ï◊™◊ô◊™. ◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ë◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊¢◊ú ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ú◊ß◊ë◊ï◊¢ ◊õ◊ê◊û◊ï◊® ◊û◊™◊ó◊ù ◊ê◊©◊® ◊ô◊™◊ó◊©◊ë ◊í◊ù ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊û◊¢◊©◊ô◊ù, ◊ï◊ë◊õ◊ú◊ú ◊ñ◊î ◊ô◊© ◊ú◊î◊™◊ó◊©◊ë ◊ë◊û◊©◊ß◊ú ◊î◊°◊ù ◊ë◊ï ◊°◊ó◊®◊ï ◊†◊ê◊©◊û◊ô◊ù. ◊ê◊ô◊ü ◊î◊û◊ì◊ï◊ë◊® ◊ë◊ß◊ë◊ô◊¢◊î ◊í◊†◊®◊ô◊™ ◊©◊ú ◊û◊™◊ó◊ù ◊õ◊ú◊ú◊ô ◊ú◊õ◊ú ◊¢◊ë◊ô◊®◊™ ◊°◊ó◊® ◊ë◊°◊ù, ◊ë◊ê◊©◊® ◊î◊ï◊ê, ◊ï◊ú◊§◊ô◊õ◊ö ◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊í◊ñ◊ï◊® ◊í◊ñ◊ô◊®◊î ◊©◊ï◊ï◊î ◊û◊î◊û◊™◊ó◊û◊ô◊ù ◊©◊†◊ß◊ë◊¢◊ï ◊ë◊û◊ß◊®◊ô◊ù ◊ë◊î◊ù ◊î◊°◊ó◊® ◊î◊ô◊î ◊ë◊õ◊û◊ï◊ô◊ï◊™ ◊©◊ú ◊í◊®◊û◊ô◊ù ◊ë◊ï◊ì◊ì◊ô◊ù.\n",
      "Real: 0 | Predicted: 1\n",
      "\n",
      "üìù Text 165:\n",
      "◊§. (◊õ\"◊°) 2045/09 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊¢◊ì◊ï◊ô [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (21.3.2010) ◊†◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊©◊í◊ô◊ì◊ú ◊ë◊ó◊¶◊® ◊ë◊ô◊™◊ï 25 ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ï◊î◊ó◊ñ◊ô◊ß ◊°◊ù ◊†◊ï◊°◊£ ◊ë◊ë◊ô◊™◊ï ◊ú◊©◊ô◊û◊ï◊©◊ï ◊î◊¢◊¶◊û◊ô. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊í◊ñ◊® ◊¢◊ú◊ô◊ï ◊¢◊ï◊†◊© ◊©◊ú 26 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊™◊ï◊ö ◊î◊ë◊ê◊î ◊ë◊ó◊©◊ë◊ï◊ü ◊©◊ú ◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 166:\n",
      "◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊†◊¢ ◊û◊û◊û◊°◊§◊® ◊ó◊ì-◊°◊§◊®◊™◊ô ◊í◊ë◊ï◊î ◊©◊ú ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊ì 28 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 167:\n",
      "◊™\"◊§ 51123-01-12 ◊û◊ì\"◊ô ◊†' ◊ó◊ï◊ò◊ï◊®◊ô◊ê◊†◊°◊ß◊ô (◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï, 23.3.14): ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊î◊™◊ê◊ù ◊ú◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú 97.25 ◊í◊®◊ù ◊†◊ò◊ï ◊ï◊ß◊ï◊ë◊ô◊î ◊©◊ú ◊î◊°◊ù ◊î◊ê◊û◊ï◊® ◊ë◊û◊©◊ß◊ú 2.33 ◊í◊®◊ù ◊†◊ò◊ï ‚Äì ◊ï◊î◊õ◊ú ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊†◊ß◊ë◊¢, ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊ë◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊õ◊ú◊î ◊ë-8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊í◊ñ◊® ◊ì◊ô◊†◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ú-3 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊î◊§◊¢◊ô◊ú ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ë◊ü 3 ◊ó◊ï◊ì◊©◊ô◊ù ◊ë◊û◊¶◊ò◊ë◊® ‚Äì ◊ï◊î◊ï◊®◊î ◊¢◊ú ◊®◊ô◊¶◊ï◊ô ◊î◊¢◊ï◊†◊© ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊ú◊¶◊ì ◊ñ◊ê◊™ ◊†◊í◊ñ◊®◊ï ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô, ◊ß◊†◊° ◊ï◊§◊°◊ô◊ú◊™ ◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 168:\n",
      "◊ú◊ê◊ó◊® ◊©◊ë◊ó◊†◊™◊ô ◊ê◊™ ◊†◊°◊ô◊ë◊ï◊™ ◊û◊¢◊©◊ô ◊î◊†◊ê◊©◊ù, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊û◊ï◊í◊†◊ô◊ù ◊ï◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™, ◊ê◊†◊ô ◊ß◊ï◊ë◊¢◊™ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 48-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊†◊ô◊©◊î ◊©◊ê◊ô◊†◊î ◊õ◊ï◊ú◊ú◊™ ◊û◊ê◊°◊® ◊û◊û◊© ◊™◊ë◊ï◊ê ◊ê◊ö ◊ë◊û◊ß◊®◊ô ◊©◊ô◊ß◊ï◊ù ◊ï◊ë◊ó◊®◊ô◊í◊î ◊û◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ê◊ú◊ï.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 169:\n",
      "◊ë◊¢◊§\"◊í (◊û◊ó' ◊û◊®◊õ◊ñ-◊ú◊ï◊ì) 20785-11-13 ◊ñ◊ô◊™◊ï◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (29.12.13), ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊®◊ï ◊©◊ú ◊†◊ê◊©◊ù, ◊†◊¢◊ì◊® ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊ê◊©◊® ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú 38 ◊í◊®◊ù, ◊ï◊†◊ì◊ï◊ü ◊ú- 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú (◊®' ◊í◊ù ◊™\"◊§ (◊§\"◊™) 16240-11-11 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ñ◊ô◊™◊ï◊ü [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (30.9.13)).\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 170:\n",
      "◊™\"◊§ 17410-08-13 ◊û◊ì\"◊ô ◊†' ◊ê◊ú◊ß◊ú◊¢◊ô ◊î◊†◊ê◊©◊ù ◊®◊õ◊© ◊°◊ù ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊°, ◊ë◊û◊©◊ß◊ú ◊©◊ú 30-100 ◊í◊®◊ù ◊ï◊©◊ô◊ú◊ù ◊ë◊ô◊ü 2,000 ◊ú- 3,000 ‚Ç™ ◊ë◊î◊û◊©◊ö ◊û◊õ◊® ◊ê◊™ ◊î◊°◊ù ◊ú◊ê◊ó◊®◊ô◊ù. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊ú 9-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ë◊©◊ú ◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù ◊†◊ì◊ï◊ü ◊î◊†◊ê◊©◊ù ◊ú-6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢\"◊©.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 171:\n",
      "◊ß◊†◊° ◊õ◊°◊§◊ô ◊ë◊°◊ö ◊©◊ú 4,000 ‚Ç™ ◊ê◊ï 40 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊™◊û◊ï◊®◊™◊ï. ◊î◊ß◊†◊° ◊ô◊©◊ï◊ú◊ù ◊ë- 10 ◊™◊©◊ú◊ï◊û◊ô◊ù ◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊©◊ï◊ï◊ô◊ù ◊ï◊®◊¶◊ï◊§◊ô◊ù, ◊©◊î◊®◊ê◊©◊ï◊ü ◊©◊ë◊î◊ù ◊ë◊ô◊ï◊ù 1.9.15. ◊ú◊ê ◊ô◊©◊ï◊ú◊ù ◊™◊©◊ú◊ï◊ù ◊õ◊ú◊©◊î◊ï ◊ë◊û◊ï◊¢◊ì◊ï, ◊™◊¢◊û◊ï◊ì ◊î◊ô◊™◊®◊î ◊ú◊§◊ô◊®◊¢◊ï◊ü ◊û◊ô◊ô◊ì◊ô.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 172:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ó◊° ◊ú◊¢◊ë◊ô◊®◊ï◊™ ◊î◊°◊ó◊® ◊ï◊õ◊ü ◊¢◊ë◊ô◊®◊™ ◊î◊î◊ó◊ñ◊ß◊î ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û-8 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊¢◊ì ◊ú-24 ◊ó◊ï◊ì◊©◊ô◊ù ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 173:\n",
      "◊©◊ï◊™◊§◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ú◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù, ◊û◊î◊®◊ê◊ü  ◊¢◊®◊û◊ô◊ü , ◊î◊ï◊®◊©◊¢ ◊ë3 ◊¢◊ë◊ô◊®◊ï◊™ ◊°◊ó◊® ◊ë◊°◊ù. ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊©◊†◊ß◊ë◊¢ ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊ú◊õ◊ú ◊î◊ê◊ô◊©◊ï◊û◊ô◊ù ◊¢◊û◊ì ◊¢◊ú 9-24 ◊ó◊ï◊ì◊©◊ô◊ù. ◊†◊ê◊©◊ù ◊ñ◊î ◊†◊ì◊ï◊ü ◊ë◊§◊†◊ô  ◊ë◊¶◊ô◊®◊ï◊£ ◊™◊ô◊ß◊ô◊ù ◊†◊ï◊°◊§◊ô◊ù ◊ú◊®◊ë◊ï◊™ ◊î◊ó◊ñ◊ß◊™ ◊†◊©◊ß ◊ú 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 174:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊ï◊ë◊¢ ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊ë◊™◊ô◊ß ◊ñ◊î ◊ë◊ó◊ï◊û◊®◊™ ◊¢◊ë◊ô◊®◊î ◊§◊ó◊ï◊™◊î ◊†◊ï◊õ◊ó ◊™◊ô◊ß◊ï◊ü ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊ë◊¶◊ï◊®◊î ◊û◊©◊û◊¢◊ï◊™◊ô◊™ ◊ï◊¢◊ô◊û◊î ◊î◊§◊ó◊™◊î ◊ë◊®◊û◊™ ◊î◊û◊°◊ï◊õ◊†◊ï◊™. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì ◊ú- 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 175:\n",
      "◊ë◊™\"◊§ (◊ê◊©') 28897-04-14 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊† ◊û◊ñ◊ú◊® (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 26.8.2014), ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊©◊™◊ô ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊ê◊ô◊°◊ï◊® ◊û◊õ◊ô◊®◊î ◊ô◊ô◊ë◊ï◊ê ◊ï◊ô◊ô◊¶◊ï◊® ◊ó◊ï◊û◊® ◊ê◊°◊ï◊®. ◊ë◊õ◊ö ◊©◊û◊õ◊® ◊ó◊ï◊û◊® ◊ê◊°◊ï◊® ◊û◊°◊ï◊í PB-22 ◊ú◊©◊ï◊ò◊® ◊™◊û◊ï◊®◊™ 100 ‚Ç™. ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊ò◊ú◊ï 11 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊ó◊ô◊ú◊ï◊ò 3,000 ‚Ç™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 176:\n",
      "◊ë◊ô◊ó◊° ◊ú◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ï-MDMA ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊ê◊§◊†◊î ◊ú◊û◊ß◊®◊ô◊ù ◊î◊ë◊ê◊ô◊ù: ◊®◊¢\"◊§ 7572/12 ◊î◊ï◊ñ◊ô◊ô◊ú ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (23.10.2012) ◊ë◊ï ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô ◊ï◊î◊§◊®◊¢◊î ◊ú◊©◊ï◊ò◊®, ◊ë◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß 8 ◊ô◊ó◊ô◊ì◊ï◊™ ◊©◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊î◊®◊ï◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú ◊õ-5.3 ◊í◊®◊ù ◊†◊ò◊ï ◊ï◊†◊í◊ñ◊®◊ï ◊¢◊ú◊ô◊ï 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú; ◊®◊¢\"◊§ 1122-17 ◊ê◊ú◊ï◊ü ◊í◊ï◊ú◊ì◊©◊ò◊ô◊ô◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (5.7.2017), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊î◊ó◊ñ◊ß◊™ 5.28 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊õ◊©◊î◊ï◊ê ◊®◊ï◊õ◊ë ◊¢◊ú ◊ß◊ò◊†◊ï◊¢ ◊™◊ó◊™ ◊î◊©◊§◊¢◊™ ◊°◊û◊ô◊ù ◊ï◊ë◊û◊ï◊¢◊ì ◊†◊ï◊°◊£ ◊†◊™◊§◊° ◊û◊ó◊ñ◊ô◊ß 9 ◊û◊†◊ï◊™ ◊©◊ú MDMA ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 5.5 ◊í◊®◊ù ◊ï◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú 13 ◊í◊®◊ù. ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊ó◊û◊® ◊û◊©◊ô◊©◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ú◊¢◊©◊®◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú; ◊¢◊§\"◊í 65932-02-20 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ú◊ï◊ú◊ï (16.6.2020), ◊ë◊ï ◊î◊ï◊ó◊û◊® ◊¢◊ï◊†◊©◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊ï◊î◊ï◊¢◊û◊ì ◊¢◊ú 32 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊©◊ú ◊î◊ó◊ñ◊ß◊™ 15.8 ◊í◊®' ◊ß◊ï◊ß◊ê◊ô◊ü ◊ï◊ë◊™◊ô◊ß ◊ê◊ó◊® ◊õ-34 ◊í◊®' ◊ß◊ï◊ß◊ê◊ô◊ü ◊ï◊õ-5 ◊í◊®' ◊ß◊†◊ë◊ï◊°; ◊¢◊§\"◊í 37749-10-22 ◊ë◊ê◊°◊ù ◊ê◊°◊û◊ê◊¢◊ô◊ú ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (1.3.2023), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊î◊ó◊ñ◊ß◊™ 23 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü, ◊õ◊©◊ó◊û◊ô◊©◊î ◊û◊™◊ï◊õ◊ù ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô, ◊ï◊ë◊î◊ó◊ñ◊ß◊™ ◊õ◊ú◊ô◊ù ◊ï◊©◊ô◊ì◊ï◊ú ◊ú◊î◊©◊û◊ì◊™ ◊®◊ê◊ô◊î ◊ï◊†◊ô◊ì◊ï◊ü ◊ú-15 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú; ◊¢◊§\"◊í 11305-09-22 ◊û◊ó◊û◊ì ◊°◊ú◊ê◊û◊ô◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (22.9.2022), ◊ë◊ï ◊†◊ß◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊ë◊©◊ú ◊î◊ó◊ñ◊ß◊™ 10 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ï-180 ◊õ◊ì◊ï◊®◊ô MDMA ◊¢◊ï◊ú◊î ◊¢◊ú ◊©◊†◊™ ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊ó◊ú◊ß◊ï ◊î◊™◊ó◊™◊ï◊ü; ◊¢◊§\"◊í 26655-04-14 ◊ó◊û◊ì◊ê◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (10.9.2014), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú 5.24 ◊í◊®◊ù ◊ï◊†◊í◊ñ◊®◊ï ◊¢◊ú◊ô◊ï 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú; ◊¢◊§\"◊í 61351-01-19 ◊ñ◊ï◊ë◊ô◊ì◊ê◊™ ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (7.3.2019), ◊ë◊ï ◊†◊ô◊ì◊ï◊ü ◊†◊ê◊©◊ù ◊ú◊¢◊©◊®◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊©◊ú ◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß 19.22 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü. ◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊©◊†◊ß◊ë◊¢ ◊ë◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊î◊ô◊î 10-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊®◊¢◊ï◊® ◊î◊†◊ê◊©◊ù ◊†◊ì◊ó◊î ◊™◊ï◊ö ◊©◊¶◊ï◊ô◊ü ◊©◊î◊¢◊ï◊†◊© ◊î◊ï◊ê ◊¢◊ú ◊î◊¶◊ì ◊î◊†◊û◊ï◊ö; ◊ë◊ô◊ó◊° ◊ú◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊ó◊©◊ô◊©/◊ß◊†◊ë◊ï◊° ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô ◊ê◊§◊†◊î ◊ú◊ì◊ë◊®◊ô◊ù ◊©◊¶◊ï◊ô◊†◊ï ◊ë◊™.◊§ (◊ß◊®◊ô◊ï◊™)  23498-04-22 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊°◊¢◊ì (17.7.2024) ◊ï◊ú◊ê◊°◊û◊õ◊™◊ê◊ï◊™ ◊©◊î◊ï◊ë◊ê◊ï ◊©◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 177:\n",
      "◊ë◊¢\"◊§ 5522/20 ◊ó◊ú◊ô◊ô◊ó◊ú ◊†' ◊û\"◊ô (24.2.21) ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊í◊ñ◊® ◊ì◊ô◊ü ◊©◊õ◊ú◊ú ◊¢◊ï◊†◊© ◊©◊ú 36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊î◊ï◊©◊™ ◊¢◊ú ◊û◊¢◊®◊¢◊® ◊©◊ë◊ô◊ó◊ì ◊¢◊ù ◊©◊†◊ô ◊ê◊ó◊®◊ô◊ù ◊†◊©◊ê ◊¢◊û◊ï ◊õ◊ú◊ô ◊†◊©◊ß ◊ë◊®◊õ◊ë ◊ê◊©◊® ◊û◊ô ◊û◊ë◊ô◊ü ◊†◊ï◊°◊¢◊ô◊ï ◊ë◊ô◊¶◊¢ ◊ô◊®◊ô ◊ë◊ê◊ï◊ï◊ô◊® ◊ï◊ë◊î◊û◊©◊ö ◊†◊û◊ú◊ò ◊ë◊®◊ô◊¶◊î ◊û◊©◊ï◊ò◊®◊ô◊ù ◊ë◊î◊ù ◊î◊ë◊ó◊ô◊ü ◊™◊ï◊ö ◊©◊î◊©◊ú◊ô◊ö ◊ê◊™ ◊õ◊ú◊ô ◊î◊†◊©◊ß ◊ë◊û◊†◊ï◊°◊™◊ï. ◊ë◊í◊ñ◊® ◊î◊ì◊ô◊ü ◊û◊ï◊©◊ê ◊î◊¢◊®◊¢◊ï◊® ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊†◊¢ ◊ë◊ô◊ü 24 ◊ú◊ë◊ô◊ü 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊¶◊ô◊ô◊ü ◊õ◊ô: \"◊ú◊û◊®◊ë◊î ◊î◊¶◊¢◊®, ◊ó◊®◊£ ◊î◊ê◊ô◊°◊ï◊® ◊©◊ë◊ì◊ô◊ü ◊î◊§◊õ◊î ◊™◊ï◊§◊¢◊™ ◊î◊©◊ô◊û◊ï◊© ◊ë◊†◊©◊ß ◊ó◊ù ◊ú◊û◊õ◊™ ◊û◊ì◊ô◊†◊î. ◊ë◊¢◊ß◊ë◊ï◊™ ◊ñ◊ê◊™, ◊ú◊©◊ù ◊î◊®◊™◊¢◊î, ◊†◊ô◊õ◊®◊™ ◊ë◊§◊°◊ô◊ß◊î ◊û◊í◊û◊î ◊©◊ú ◊î◊ó◊û◊®◊î ◊î◊ì◊®◊í◊™◊ô◊™ ◊ë◊¢◊†◊ô◊©◊î\". ◊†◊ß◊ë◊¢ ◊õ◊ô ◊ú◊ê ◊†◊§◊ú◊î ◊©◊í◊í◊î ◊ë◊ß◊ë◊ô◊¢◊™ ◊î◊û◊™◊ó◊ù ◊™◊ï◊ö ◊©◊¶◊ï◊ô◊ü ◊û◊§◊ï◊®◊©◊ï◊™ ◊õ◊ô ◊ñ◊î ◊®◊ú◊ï◊ï◊†◊ò◊ô ◊õ◊ê◊©◊® ◊ú◊ï◊ß◊ó◊ô◊ù ◊ë◊ó◊©◊ë◊ï◊ü ◊©◊î◊û◊¢◊®◊¢◊® ◊ú◊ê ◊î◊ô◊î ◊ñ◊î ◊©◊ú◊ó◊• ◊¢◊ú ◊î◊î◊ì◊ß.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 178:\n",
      "◊¢◊§\"◊í 42358-10-14 ◊í◊ô◊ê ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (18.02.15) ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊ë◊ë◊ô◊™ ◊û◊©◊§◊ò ◊ß◊û◊ê ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊í◊ô◊ì◊ï◊ú, ◊ô◊ô◊¶◊ï◊® ◊ï◊î◊õ◊†◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù, ◊¢◊ú ◊§◊ô ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊í◊ô◊ì◊ú ◊î◊†◊ê◊©◊ù ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ê◊ë◊ô◊° ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 5,535 ◊ß\"◊í ◊ï◊ó◊©◊ô◊© ◊ë◊û◊©◊ß◊ú ◊©◊ú 1.40 ◊í◊®◊ù ◊†◊ò◊ï, ◊û◊ì◊ï◊ë◊® ◊ë◊û◊¢◊ë◊ì◊™ ◊°◊ù ◊©◊î◊™◊†◊î◊ú◊î ◊ë◊ì◊ô◊®◊™◊ï ◊©◊ú ◊î◊û◊¢◊®◊¢◊®. ◊î◊†◊ê◊©◊ù ◊î◊ó◊ñ◊ô◊ß ◊ë◊õ◊ú◊ô◊ù ◊ú◊í◊ô◊ì◊ï◊ú ◊î◊°◊ù, ◊ë◊°◊§◊® ◊ë◊©◊ù \"◊û◊®◊ô◊ó◊ï◊ê◊†◊î\" ◊î◊õ◊ï◊ú◊ú ◊™◊û◊ï◊†◊ï◊™ ◊ï◊î◊°◊ë◊®◊ô◊ù ◊ú◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊ï◊õ◊ü ◊û◊õ◊©◊ï◊®: ◊§◊ô◊ú◊ò◊® ◊ê◊ï◊ï◊ô◊®, ◊§◊ô◊ú◊ò◊® ◊û◊ô◊ù, ◊©◊†◊¢◊ô ◊ó◊©◊û◊ú, ◊ú◊ï◊ó◊ï◊™ ◊ó◊©◊û◊ú, ◊õ◊ë◊ú◊ô◊ù, ◊û◊§◊¶◊ú◊ô◊ù, 6 ◊ô◊ó◊ô◊ì◊ï◊™ ◊©◊ú ◊™◊¢◊ú◊ï◊™ ◊ê◊ï◊ï◊ô◊®, ◊û◊§◊ï◊ó◊ô◊ù, ◊û◊ê◊ï◊ï◊®◊®. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊ë◊ô◊ü 7 ◊ú- 20 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊ò◊ô◊ú ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊ü 8 ◊ó◊ï◊ì◊©◊ô◊ù, ◊î◊ï◊§◊¢◊ú ◊û◊¢\"◊™ ◊õ◊ö ◊©◊°◊î\"◊õ ◊®◊ô◊¶◊î ◊î◊†◊ê◊©◊ù 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊û◊¢\"◊™. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊î◊™◊ô◊ô◊ó◊° ◊ú◊û◊™◊ó◊ù ◊ï◊ß◊ë◊¢ ◊õ◊ô ◊î◊ï◊ê ◊û◊™◊ó◊ù ◊®◊ê◊ï◊ô. ◊î◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 179:\n",
      "◊©◊©◊î ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö 3 ◊©◊†◊ô◊ù ◊©◊ú◊ê ◊ô◊¢◊ë◊ï◊® ◊¢◊ú ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù [◊†◊ï◊°◊ó ◊ó◊ì◊©], ◊î◊™◊©◊ú\"◊í-1973.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 180:\n",
      "◊û◊õ◊ú ◊î◊û◊ß◊ï◊ë◊•, ◊ë◊î◊™◊ó◊©◊ë ◊ë◊¢◊®◊õ◊ô◊ù ◊î◊ó◊ë◊®◊™◊ô◊ô◊ù ◊©◊†◊§◊í◊¢◊ï ◊û◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î, ◊ë◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊î◊ù, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î, ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊î◊ß◊©◊ï◊®◊ï◊™ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î ◊ï◊î◊ô◊ó◊° ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊î ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ï◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊©◊ô◊© ◊ú◊î◊ò◊ô◊ú ◊¢◊ú◊ô◊ï, ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊î ◊©◊ë◊ï◊¶◊¢◊î ◊¢◊ú ◊ô◊ì◊ô ◊î◊†◊ê◊©◊ù ◊†◊¢ ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊ú◊û◊©◊ö 6 ◊ó◊ï◊ì◊©◊ô◊ù ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊î ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú-12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 181:\n",
      "2\t◊®◊¢\"◊§ 5354/12 ◊ß◊ï◊ë◊® ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï] (12.7.12). ◊î◊û◊ë◊ß◊© ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú 37.96 ◊í◊®◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊™◊ï ◊¢◊¶◊û◊ô◊™. ◊õ◊û◊ï ◊õ◊ü, ◊†◊í◊û◊ú ◊û◊°◊û◊ô◊ù ◊ú◊ê◊ó◊® ◊©◊ë◊û◊©◊ö ◊©◊†◊ô◊ù ◊î◊ô◊î ◊û◊õ◊ï◊® ◊ú◊°◊û◊ô◊ù. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ß◊ë◊¢ ◊©◊î◊û◊™◊ó◊ù ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊†◊¢ ◊ë◊ô◊ü 18 ◊ú- 36 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ì◊ü ◊ê◊ï◊™◊ï ◊ú- 14 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® (◊î◊ô◊†◊ï 4 ◊ó◊ï◊ì◊©◊ô◊ù ◊§◊ó◊ï◊™ ◊û◊î◊í◊ë◊ï◊ú ◊î◊™◊ó◊™◊ï◊ü ◊©◊ú ◊î◊û◊™◊ó◊ù). ◊¢◊®◊¢◊ï◊®◊ï ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊†◊ì◊ó◊î ◊ï◊õ◊ü ◊†◊ì◊ó◊™◊î ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊©◊î◊í◊ô◊© ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü. ◊õ◊ë◊ï◊ì ◊î◊©◊ï◊§◊ò ◊©◊ï◊î◊ù ◊ß◊ë◊¢ ◊ë◊§◊°◊ß◊î 11 ◊ú◊î◊ó◊ú◊ò◊™◊ï ◊©◊ê◊ô◊ü ◊ë◊™◊î◊ú◊ô◊ö ◊î◊©◊ô◊ß◊ï◊ù, ◊û◊ï◊¶◊ú◊ó ◊õ◊õ◊ú ◊©◊ô◊î◊ô◊î, ◊õ◊ì◊ô ◊ú◊ê◊ô◊ô◊ü ◊ê◊™ ◊ó◊ï◊û◊®◊™ ◊î◊û◊¢◊©◊ô◊ù ◊ï◊ê◊™ ◊î◊¶◊ï◊®◊ö ◊ú◊î◊¢◊†◊ô◊© ◊ê◊™ ◊î◊û◊ë◊ß◊© ◊ë◊û◊ê◊°◊® ◊û◊ê◊ó◊ï◊®◊ô ◊°◊ï◊®◊í ◊ï◊ë◊®◊ô◊ó.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 182:\n",
      "30 ◊ô◊û◊ô ◊û◊ê◊°◊® ◊ï◊ñ◊ê◊™ ◊¢◊ú ◊™◊†◊ê◊ô  ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊î◊†◊ê◊©◊ù  ◊™◊ï◊ö ◊™◊ß◊ï◊§◊î ◊ë◊™ 36 ◊ó◊ï◊ì◊©◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊î ◊©◊ú ◊©◊ô◊û◊ï◊© ◊ë◊°◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 183:\n",
      "5129371◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô - ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 12 ◊ó◊ï◊ì◊©◊ô◊ù. ◊î◊†◊ê◊©◊ù ◊ô◊ô◊©◊ê ◊ë◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ù ◊ë◊™◊ß◊ï◊§◊î ◊©◊ú ◊©◊ú◊ï◊© ◊©◊†◊ô◊ù ◊û◊ô◊ï◊ù ◊©◊ó◊®◊ï◊®◊ï ◊ô◊¢◊ë◊ï◊® ◊¢◊ú ◊¢◊ë◊ô◊®◊î ◊û◊°◊ï◊í ◊§◊©◊¢ ◊ú◊§◊ô ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù (◊†◊ï◊°◊ó ◊ó◊ì◊©), ◊î◊™◊©◊ú\"◊í ‚Äì 1973.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 184:\n",
      "◊¢◊†◊ô◊ô◊ü ◊§◊®◊• - ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊í◊ñ◊® ◊ì◊ô◊ü ◊ë◊í◊ì◊®◊ï ◊î◊ï◊©◊™ ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® ◊¢◊ï◊†◊© ◊©◊ú 16 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™, ◊ú◊ê◊ó◊® ◊©◊î◊ï◊®◊©◊¢ ◊ë◊ß◊©◊ô◊®◊™ ◊ß◊©◊® ◊ú◊§◊©◊¢ ◊ï◊ë◊ô◊ë◊ï◊ê ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊û◊™◊ê◊û◊§◊ò◊û◊ô◊ü ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 1.865 ◊ß\"◊í ◊†◊ò◊ï. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ê◊©◊® ◊†◊¢ ◊ë◊ô◊ü 21 ◊ú- 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ê◊ö ◊î◊ó◊ú◊ô◊ò, ◊™◊ï◊ö ◊ó◊®◊ô◊í◊î ◊û◊î◊û◊™◊ó◊ù ◊ï◊ë◊©◊ú ◊î◊û◊ú◊¶◊™ ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü, ◊ú◊î◊©◊ô◊™ ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® 16 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ú◊¶◊ô◊ô◊ü, ◊õ◊ô ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊î◊û◊ú◊ô◊• ◊©◊ú◊ê ◊ú◊í◊ñ◊ï◊® ◊¢◊ú ◊î◊û◊¢◊®◊¢◊® ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 185:\n",
      "◊ë◊û◊°◊í◊®◊™ ◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù ◊ú◊ß◊ï◊ú◊ê ◊ú◊¢◊ï◊†◊©, ◊î◊û◊ê◊°◊® ◊î◊û◊ï◊™◊†◊î ◊©◊™◊ú◊ï◊ô ◊ï◊¢◊ï◊û◊ì ◊õ◊†◊í◊ì ◊î◊†◊ê◊©◊ù, ◊ô◊ï◊§◊¢◊ú ◊õ◊ö ◊©◊ó◊ï◊ì◊© ◊ê◊ó◊ì ◊ô◊î◊ô◊î ◊ë◊ó◊ï◊§◊£ ◊ï◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊ë◊û◊¶◊ò◊ë◊® ◊ú◊¢◊ï◊†◊© ◊©◊ê◊ï◊™◊ï ◊ê◊ò◊ô◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 186:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊ê◊®◊ï◊¢ ◊î◊ó◊ñ◊ß◊™ ◊î◊°◊ù ◊ú◊†◊ï◊õ◊ó ◊°◊ï◊í ◊î◊°◊ù ◊ï◊õ◊û◊ï◊™◊ï ◊û◊ê◊°◊® ◊ú◊™◊ß◊ï◊§◊î ◊ß◊¶◊®◊î ◊ï◊¢◊ì 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊ë◊í◊ô◊ü ◊ê◊ô◊®◊ï◊¢ ◊î◊°◊ó◊® ◊ë◊°◊û◊ô◊ù ◊†◊ï◊õ◊ó ◊°◊ï◊í ◊î◊°◊ù ◊ï◊õ◊û◊ï◊™◊ï ◊û◊ê◊°◊® ◊ú◊™◊ß◊ï◊§◊î ◊©◊ô◊õ◊ï◊ú ◊ï◊™◊®◊ï◊¶◊î ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú- 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 187:\n",
      "◊î◊û◊ê◊©◊ô◊û◊î ◊î◊§◊†◊™◊î ◊ú◊§◊°◊ô◊ß◊î ◊õ◊ê◊©◊® ◊û◊™◊ó◊û◊ô ◊î◊¢◊†◊ô◊©◊î ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 18 ◊ú-56 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊î◊¢◊ï◊†◊©◊ô◊ù ◊©◊î◊ï◊ò◊ú◊ï  ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 24 ◊ú- 40 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 188:\n",
      "◊¢\"◊§ 6878/22 ◊¶'◊ë◊†◊ô◊ï◊ß ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (1.3.2023) - ◊î◊û◊¢◊®◊¢◊® ◊î◊ï◊®◊©◊¢ ◊¢◊ú ◊ô◊°◊ï◊ì ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊î◊ó◊ñ◊ß◊î ◊ë◊¶◊ï◊ï◊™◊ê ◊ó◊ì◊ê ◊©◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊°◊ó◊® ◊ë◊°◊ù ◊û◊°◊ï◊õ◊ü, ◊ú◊ê◊ó◊® ◊©◊î◊ó◊ñ◊ô◊ß ◊ô◊ó◊ì ◊¢◊ù ◊ê◊ó◊®◊™, 100 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü, ◊ï◊¢◊ï◊ì ◊°◊û◊ô◊ù ◊†◊ï◊°◊§◊ô◊ù ◊û◊°◊ï◊í◊ô◊ù ◊©◊ï◊†◊ô◊ù ◊ë◊õ◊û◊ï◊ô◊ï◊™ ◊©◊ê◊ô◊†◊ü ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊ë◊†◊ï◊°◊£, ◊î◊ó◊ñ◊ô◊ß◊î ◊î◊ê◊ó◊®◊™ ◊ë◊ì◊ô◊®◊™◊î ◊û◊©◊ß◊ú◊ô◊ù ◊ê◊ú◊ß◊ò◊®◊ï◊†◊ô◊ô◊ù, ◊ï◊¶◊ô◊ï◊ì ◊î◊û◊©◊û◊© ◊ú◊¶◊®◊ô◊õ◊™ ◊°◊û◊ô◊ù ◊õ◊í◊ï◊ü ◊õ◊§◊ô◊ï◊™, ◊û◊ñ◊®◊ß◊ô◊ù ◊ï◊û◊ì◊ë◊ß◊ï◊™ ◊¢◊ú◊ô◊î◊ü ◊®◊©◊ï◊û◊ô◊ù ◊°◊ï◊í◊ô ◊î◊°◊û◊ô◊ù. ◊õ◊û◊ï-◊õ◊ü, ◊°◊ó◊® ◊î◊†◊ê◊©◊ù ◊ë-3 ◊î◊ñ◊ì◊û◊†◊ï◊ô◊ï◊™ ◊©◊ï◊†◊ï◊™ ◊ë◊ê◊û◊¶◊¢◊ï◊™ ◊ê◊§◊ú◊ô◊ß◊¶◊ô◊ô◊™ \"◊ò◊ú◊í◊®◊ù\" ◊ë◊°◊û◊ô◊ù: 5 ◊í◊®◊ù ◊ß◊†◊ê◊ë◊ô◊°; ◊í◊®◊ù ◊ê◊ó◊ì ◊©◊ú ◊ß◊ï◊ß◊ê◊ô◊ü; ◊í◊®◊ù ◊ê◊ó◊ì ◊©◊ú ◊ß◊†◊ê◊ë◊ô◊°. ◊ë◊ô◊ó◊° ◊ú◊ê◊ô◊©◊ï◊ù ◊î◊®◊ê◊©◊ï◊ü ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 30 ◊ú-60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊î◊ï◊ò◊ú◊ï ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊¢◊ï◊†◊© ◊©◊ú 20 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊™◊ï◊ö ◊°◊ò◊ô◊ô◊î ◊û◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù. ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊© ◊ú◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊î◊™◊ß◊ë◊ú, ◊õ◊ö ◊©◊î◊ï◊©◊™ ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊¢◊ï◊†◊© ◊©◊ú 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊§◊ï◊¢◊ú, ◊ï◊ñ◊ê◊™ ◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù ◊ë◊ú◊ë◊ì ◊ï◊™◊ï◊ö ◊ê◊ô◊©◊ï◊® ◊î◊û◊™◊ó◊ù ◊©◊†◊ß◊ë◊¢ ◊ë◊ë◊ô◊™ ◊û◊©◊§◊ò ◊ß◊û◊ê.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 189:\n",
      "◊ë◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô-◊™\"◊ê) 52341-07-20 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ú◊ë (20.5.21) ◊†◊ì◊ï◊ü ◊î◊†◊ê◊©◊ù ◊ë◊í◊ô◊ü ◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊ë◊û◊©◊ß◊ú 250 ◊ß\"◊í ◊ú◊û◊ê◊°◊® ◊ë◊ü 28 ◊ó◊ï◊ì◊©◊ô◊ù. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊†◊¢ ◊ë◊ô◊ü 33-55 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊û◊¶◊ê ◊ú◊ó◊®◊ï◊í ◊ß◊û◊¢◊î ◊ú◊ß◊ï◊ú◊ê ◊û◊ò◊¢◊û◊ô ◊©◊ô◊ß◊ï◊ù;\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 190:\n",
      "◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö  9 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊î◊†◊ê◊©◊û◊™ ◊ú◊ê ◊™◊ô◊©◊ê ◊¢◊ï◊†◊© ◊ñ◊î ◊ê◊ú◊ê ◊ê◊ù ◊™◊¢◊ë◊ï◊® ◊ë◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊î◊ô◊ï◊ù ◊¢◊ë◊ô◊®◊ï◊™ ◊¢◊ú ◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊§◊©◊¢.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 191:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊î ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 192:\n",
      "◊ë◊™◊§ (◊õ◊§◊® ◊°◊ë◊ê) 2207/09 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊û◊ï◊ó◊û◊ì ◊°◊û◊ê◊®◊î ◊ë◊ü ◊ê◊ô◊ï◊ë (20.11.2011) ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù, ◊ë◊¢◊ú ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô ◊ë◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß 0.20 ◊í◊®◊ù ◊ó◊©◊ô◊©. ◊î◊ï◊ò◊ú◊ï ◊¢◊ú◊ô◊ï ◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊î◊ï◊§◊¢◊ú ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊ô◊ù ◊ë◊ü ◊ó◊ï◊ì◊©◊ô◊ô◊ù ◊ë◊ó◊ï◊§◊£.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 193:\n",
      "◊™\"◊§ (◊õ\"◊°) 15402-09-11 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊©◊ê◊ë◊ô (23.12.14) ◊¢◊ú ◊§◊ô ◊¢◊ï◊ë◊ì◊ï◊™ ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊î◊û◊™◊ï◊ß◊ü ◊î◊©◊™◊û◊© ◊î◊†◊ê◊©◊ù ◊ë◊ô◊ó◊ì ◊¢◊ù ◊ê◊ó◊®, ◊ú◊ú◊ê ◊®◊©◊ï◊™ ◊ë◊û◊©◊ê◊ô◊™ ◊û◊¢◊®◊ë◊ú ◊î◊ë◊ò◊ï◊ü. ◊ë◊î◊û◊©◊ö, ◊ú◊ê ◊©◊¢◊î ◊î◊ê◊ó◊® ◊ú◊ß◊®◊ô◊ê◊ï◊™ ◊©◊ï◊ò◊® ◊ú◊¢◊¶◊ï◊®, ◊ë◊õ◊ï◊ï◊†◊î ◊ú◊î◊§◊®◊ô◊¢ ◊ú◊ï ◊ê◊ï ◊ú◊î◊õ◊©◊ô◊ú◊ï ◊ë◊™◊§◊ß◊ô◊ì◊ï. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊ú ◊û◊¢\"◊™ ◊ï◊¢◊ì 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊î◊†◊ê◊©◊ù ◊†◊ì◊ï◊ü ◊ú◊û◊¢\"◊™ ◊ï◊î◊™◊ó◊ô◊ô◊ë◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 194:\n",
      "◊§◊°◊ô◊ú◊î ◊ë◊§◊ï◊¢◊ú ◊û◊ú◊ß◊ë◊ú ◊ê◊ï ◊û◊ú◊î◊ó◊ñ◊ô◊ß ◊ë◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ú◊û◊©◊ö 6 ◊ó◊ï◊ì◊©◊ô◊ù, ◊î◊ó◊ú ◊û◊ô◊ï◊ù 1.4.15 . ◊î◊†◊ê◊©◊ù ◊ô◊§◊ß◊ô◊ì ◊ê◊™ ◊®◊ô◊©◊ô◊ï◊†◊ï ◊ê◊ï ◊î◊¶◊î◊®◊î ◊û◊™◊ê◊ô◊û◊î ◊ë◊û◊ñ◊õ◊ô◊®◊ï◊™ ◊ë◊ô◊™-◊î◊û◊©◊§◊ò ◊ë◊°◊û◊ï◊ö ◊ú◊§◊†◊ô ◊î◊û◊ï◊¢◊ì ◊î◊ê◊û◊ï◊®.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 195:\n",
      "◊™\"◊§ (◊™\"◊ê) 73652-01-18 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊û◊õ◊ê◊ï◊ô (12/8/2021) ◊ë◊û◊°◊í◊®◊™◊ï ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊©◊™◊ô ◊¢◊ë◊ô◊®◊ï◊™ ◊°◊ó◊® ◊ë◊°◊û◊ô◊ù. ◊ë◊í◊ô◊ü ◊î◊¢◊°◊ß◊î ◊î◊®◊ê◊©◊ï◊†◊î, ◊ë◊û◊°◊í◊®◊™◊î ◊û◊°◊® ◊ú◊°◊ï◊õ◊ü ◊õ- 20 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊™◊û◊ï◊®◊™ 9,000 ‚Ç™, ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊©◊†◊¢ ◊ë◊ô◊ü 20 ◊ú- 47 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®; ◊ï◊ë◊í◊ô◊ü ◊î◊¢◊°◊ß◊î ◊î◊©◊†◊ô◊ô◊î, ◊ë◊û◊°◊í◊®◊™◊î ◊û◊°◊® ◊ú◊°◊ï◊õ◊ü 49 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊™◊û◊ï◊®◊™ 22,500 ‚Ç™, ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊©◊†◊¢ ◊ë◊ô◊ü 30 ◊¢◊ì 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊î◊°◊õ◊ô◊û◊î ◊î◊™◊ë◊ô◊¢◊î ◊ú◊ß◊ô◊ï◊û◊î ◊©◊ú ◊î◊¶◊ì◊ß◊î ◊ú◊°◊ò◊ô◊ô◊î ◊û◊û◊™◊ó◊û◊ô ◊î◊¢◊†◊ô◊©◊î ◊ê◊ö ◊¢◊™◊®◊î ◊ú◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊û◊ê◊ó◊ï◊®◊ô ◊°◊ï◊®◊í ◊ï◊ë◊®◊ô◊ó. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊°◊ß◊® ◊ê◊™ ◊î◊î◊ú◊ô◊ö ◊î◊©◊ô◊ß◊ï◊û◊ô ◊©◊¢◊ë◊® ◊î◊†◊ê◊©◊ù, ◊î◊™◊ó◊©◊ë ◊ë◊™◊ß◊ï◊§◊™ ◊û◊¢◊¶◊®◊ï ◊ï◊î◊ò◊ô◊ú ◊¢◊ú◊ô◊ï 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 196:\n",
      "3 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ê◊ï◊™◊ù ◊ú◊ê ◊ô◊®◊¶◊î ◊ê◊ú◊ê ◊ê◊ù ◊ô◊¢◊ë◊ï◊® ◊™◊ï◊ö 3 ◊©◊†◊ô◊ù ◊û◊©◊ó◊®◊ï◊®◊ï, ◊¢◊ë◊ô◊®◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊¢◊ï◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 197:\n",
      "◊®◊¢\"◊§ 5382/22 ◊ì◊°◊ô◊î ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (28.8.22), ◊ë◊û◊°◊í◊®◊™◊ï ◊†◊ì◊ó◊™◊î ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊§◊°◊ß ◊ì◊ô◊†◊ï ◊©◊ú ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊û◊ó◊ï◊ñ◊ô ◊ë◊û◊°◊í◊®◊™ ◊¢◊®◊¢◊ï◊® ◊î◊†◊ê◊©◊ù ◊¢◊ú ◊í◊ñ◊® ◊ì◊ô◊†◊ï ◊©◊ú ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ï◊ú◊ê◊ó◊® ◊©◊î◊©◊ô◊™ ◊¢◊ú◊ô◊ï 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊õ◊ü ◊î◊§◊¢◊ú◊™ ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ë◊ü ◊ó◊ï◊ì◊© ◊ë◊û◊¶◊ò◊ë◊® ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù ◊†◊ï◊°◊§◊ô◊ù, ◊õ◊ê◊©◊® ◊ë◊¢◊†◊ô◊ô◊ü ◊î◊ó◊ñ◊ß◊™ ◊î◊°◊ù ◊î◊û◊°◊ï◊õ◊ü ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊ô◊ü 8 ◊ú-18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ô◊ï◊¢◊® ◊õ◊ô ◊ë◊ê◊ï◊™◊ï ◊û◊ß◊®◊î ◊†◊™◊§◊°◊ï ◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊©◊ú 40.83 ◊í◊®◊ù ◊ï◊¢◊ï◊ì ◊û◊ê◊ï◊™◊ï ◊°◊ï◊í ◊ë◊û◊ß◊ï◊ù ◊ê◊ó◊® ◊ë◊û◊©◊ß◊ú ◊©◊ú 244 ◊í◊®◊ù ◊ï◊õ◊ü ◊ë◊û◊ß◊ï◊ù ◊©◊ú◊ô◊©◊ô ◊ë◊û◊©◊ß◊ú ◊©◊ú 31.53 ◊í◊®◊ù, ◊ï◊õ◊ü ◊†◊™◊§◊° ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊ï◊ß◊ê◊ô◊ü ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 3.38 ◊í◊®◊ù ◊†◊ò◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 198:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô\"◊í ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊î◊™◊©◊ú\"◊ñ-1977 (◊ú◊î◊ú◊ü: \"◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü\"), ◊ï◊ú◊ê◊ó◊® ◊©◊ß◊ú◊ï◊ú ◊û◊õ◊ú◊ï◊ú ◊î◊†◊°◊ô◊ë◊ï◊™ ◊ï◊î◊©◊ô◊ß◊ï◊ú◊ô◊ù ◊ë◊û◊ß◊®◊î ◊ì◊†◊ê, ◊ê◊†◊ô ◊°◊ë◊ï◊® ◊©◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ê◊™ ◊î◊û◊¢◊©◊ô◊ù ◊†◊¢ ◊ë◊ô◊ü 18 ◊ú-48  ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 199:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ï◊ê ◊î◊ó◊ú ◊û-20 ◊ï◊¢◊ì ◊ú-46 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 200:\n",
      "◊™\"◊§ (◊û◊ó◊ï◊ñ◊ô ◊û◊®◊õ◊ñ) 17677-04-17 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ì◊ï◊õ◊ü (2.11.2017) ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊ô◊ô◊¶◊ï◊® ◊ï◊î◊õ◊†◊™ ◊°◊ù ◊ß◊†◊ë◊ï◊° ◊ë◊û◊¢◊ë◊ì◊î ◊©◊î◊ß◊ô◊ù ◊ï◊ë◊†◊ò◊ô◊ú◊™ ◊ó◊©◊û◊ú, ◊ë◊û◊¢◊ë◊ì◊î ◊†◊û◊¶◊ê◊ï 235 ◊¢◊¶◊ô◊¶◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊í◊ì◊ú◊ô◊ù ◊©◊ï◊†◊ô◊ù ◊ï◊¢◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊û◊ô◊ï◊ë◊©◊ô◊ù, ◊ë◊õ◊û◊ï◊™ ◊õ◊ï◊ú◊ú◊™ ◊©◊ú 72.8 ◊ß\"◊í. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 46-22 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ú◊ó◊ï◊ë◊™ ◊î◊†◊ê◊©◊ù ◊î◊®◊©◊¢◊î ◊ê◊ó◊™ ◊ô◊©◊†◊î ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊°◊û◊ô◊ù ◊ï◊†◊©◊ß, ◊†◊ì◊ï◊ü ◊ú◊û◊ê◊°◊® ◊ë◊ü 27 ◊ó◊ï◊ì◊©◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 201:\n",
      "◊ë\"◊õ ◊î◊†◊ê◊©◊ù ◊î◊§◊†◊î ◊ú◊û◊°◊§◊® ◊§◊°◊ß◊ô ◊ì◊ô◊ü ◊õ◊ê◊©◊® ◊î◊û◊™◊ó◊û◊ô◊ù ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 24 ◊ï◊¢◊ì 48 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊î◊¢◊ï◊†◊©◊ô◊ù ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 14 ◊ï◊¢◊ì 32 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 202:\n",
      "◊ë◊™\"◊§ (◊™\"◊ê) 12462-11-14 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ò◊ú ◊ñ◊ï◊î◊® (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 8.9.2016), ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù, ◊¢◊ú ◊ô◊°◊ï◊ì ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊û◊°◊í◊®◊™ ◊î◊°◊ì◊® ◊ò◊ô◊¢◊ï◊ü, ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊ê◊ô◊°◊ï◊® ◊ô◊ô◊¶◊ï◊® ◊ó◊ï◊û◊® ◊ê◊°◊ï◊® ◊ë◊î◊§◊¶◊î, ◊ë◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß ◊ë◊®◊õ◊ë◊ï ◊ó◊ï◊û◊® ◊û◊°◊õ◊ü ◊û◊°◊ï◊í PB-22 ◊ë◊û◊©◊ß◊ú 5.79 ◊ß\"◊í ◊†◊ò◊ï, ◊ë◊û◊ò◊®◊î ◊ú◊î◊§◊ô◊¶◊ï. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊ï◊ú◊ù ◊©◊†◊¢ ◊ë◊ô◊ü ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊¢◊ì 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊¢◊ú ◊ê◊£ ◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô ◊ï◊ë◊©◊ú ◊î◊ú◊ô◊ö ◊î◊©◊ô◊ß◊ï◊û◊ô ◊ê◊ï◊™◊ï ◊¢◊ë◊®, ◊î◊ï◊ò◊ú◊ï 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊ß◊†◊° ◊ë◊°◊ö ◊©◊ú 3,000 ‚Ç™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 203:\n",
      "◊ë◊®◊¢\"◊§ 7572/12 ◊î◊ñ◊ô◊ô◊ú  ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (23.10.12), ◊†◊ì◊ó◊™◊î ◊ë◊ß◊©◊™ ◊¢◊®◊¢◊ï◊® ◊©◊ú ◊†◊ê◊©◊ù ◊ê◊©◊® ◊î◊ï◊®◊©◊¢ ◊ë◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊ë◊î◊§◊®◊¢◊î ◊ú◊©◊ï◊ò◊® ◊ï◊†◊ô◊ì◊ï◊ü ◊ú - 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù. ◊î◊†◊ê◊©◊ù ◊î◊ó◊ñ◊ô◊ß 5.3 ◊í◊®◊ù ◊î◊®◊ï◊ê◊ô◊ü, ◊ï◊õ◊ê◊©◊® ◊î◊ë◊ó◊ô◊ü ◊ë◊©◊ï◊ò◊® ◊û◊™◊ß◊®◊ë ◊ú◊¢◊ë◊®◊ï, ◊ñ◊®◊ß ◊ê◊™ ◊î◊°◊ù ◊ï◊†◊û◊ú◊ò. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ß◊ë◊¢ ◊õ◊ô ◊†◊ï◊õ◊ó ◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô ◊©◊ú ◊î◊†◊ê◊©◊ù ◊ï◊î◊¢◊ì◊® ◊©◊ô◊™◊ï◊£ ◊§◊¢◊ï◊ú◊î ◊¢◊ù ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊ï◊û◊® ◊õ◊ô ◊ô◊© ◊ó◊ï◊û◊®◊î ◊û◊ô◊ï◊ó◊ì◊™ ◊ë◊¢◊ï◊†◊©.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 204:\n",
      "◊§◊°◊ô◊ú◊î ◊û◊ú◊î◊ó◊ñ◊ô◊ß ◊ï◊û◊ú◊ß◊ë◊ú ◊®◊ô◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 6 ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊ñ◊ê◊™ ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö 3 ◊©◊†◊ô◊ù ◊©◊î◊†◊ê◊©◊ù ◊ú◊ê ◊ô◊¢◊ë◊ï◊® ◊¢◊ë◊ô◊®◊î ◊ë◊†◊ô◊í◊ï◊ì ◊ú◊§◊ß◊ï◊ì◊™ ◊î◊°◊û◊ô◊ù ◊î◊û◊°◊ï◊õ◊†◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 205:\n",
      "◊ë◊™\"◊§ (◊©◊ú◊ï◊ù ◊ô-◊ù) 58075-11-22 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ñ◊®◊ô◊î◊ü (31.7.2023) ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊û◊°◊§◊® ◊®◊ë ◊©◊ú ◊ê◊ô◊©◊ï◊û◊ô◊ù ◊ï◊™◊ô◊ß◊ô◊ù ◊©◊¶◊ï◊®◊§◊ï. ◊î◊ê◊ô◊©◊ï◊ù ◊î◊©◊†◊ô ◊ë◊™◊ô◊ß ◊î◊¢◊ô◊ß◊®◊ô ◊î◊ï◊ê ◊î◊®◊ú◊ï◊ï◊†◊ò◊ô ◊ú◊¢◊†◊ô◊ô◊†◊ï, ◊©◊ù ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊©◊ô◊û◊ï◊© ◊ë◊õ◊®◊ò◊ô◊° ◊ó◊ô◊ï◊ë ◊©◊ê◊ô◊†◊ï ◊©◊ô◊ô◊ö ◊ú◊ï ◊ï◊ë◊ë◊ô◊¶◊ï◊¢ 10 ◊¢◊°◊ß◊ê◊ï◊™ ◊ë◊°◊õ◊ï◊ù ◊û◊¶◊ò◊ë◊® ◊©◊ú 1,206 ‚Ç™. ◊ë◊¢◊ë◊ï◊® ◊ê◊ô◊©◊ï◊ù ◊ñ◊î ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü ◊û◊°◊§◊® ◊ó◊ï◊ì◊©◊ô◊ù ◊ú◊ë◊ô◊ü 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ú◊¶◊ì ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ï◊ß◊†◊°.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 206:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û- 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì ◊ú- 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 207:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì ◊ú- 7 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 208:\n",
      "◊ë◊™\"◊§ (◊ë\"◊©) 15892-12-13 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊§◊ê◊®◊í' (◊†◊ô◊™◊ü ◊ë◊ô◊ï◊ù 24.3.2014), ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊ë◊û◊°◊í◊®◊™ ◊î◊°◊ì◊® ◊°◊í◊ï◊®, ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊°◊ó◊® ◊ë◊ó◊ï◊û◊® ◊û◊°◊õ◊ü ◊û◊°◊ï◊í 25-b-nbome ◊ï◊î◊ó◊ñ◊ß◊™ 1 ◊í◊®◊ù ◊ß◊ï◊ß◊ê◊ô◊ü ◊ï- 1.8 ◊í◊®◊ù ◊ó◊©◊ô◊© ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊ë◊¢◊ú ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô, ◊†◊í◊ñ◊®◊ï  12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊î◊õ◊ï◊ú◊ú◊ô◊ù ◊î◊§◊¢◊ú◊î ◊©◊ú 3 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ë◊ó◊ï◊§◊£.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 209:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊™◊ô◊ß ◊î◊¢◊ô◊ß◊®◊ô ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊¢◊©◊®◊î ◊ó◊ï◊ì◊©◊ô◊ù ◊ï◊¢◊ì ◊ú- 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊ë◊õ◊ú ◊ê◊ó◊ì ◊û◊î◊™◊ô◊ß◊ô◊ù ◊î◊û◊¶◊ï◊®◊§◊ô◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ï◊¢◊ì 7 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 210:\n",
      "◊ë◊í◊ô◊ü ◊™\"◊§ 24394-12-14 ‚Äì 10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊û◊†◊ô◊ô◊†◊ù ◊û◊ô◊ï◊ù ◊û◊¢◊¶◊®◊ï, 26.11.14.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 211:\n",
      "◊ú◊°◊ô◊õ◊ï◊ù- ◊ú◊ê◊ï◊® ◊û◊õ◊ú◊ï◊ú ◊î◊†◊™◊ï◊†◊ô◊ù ◊ï◊ë◊î◊™◊ó◊©◊ë ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊ï◊î◊í◊™ ◊ë◊°◊ï◊í ◊ñ◊î ◊©◊ú ◊¢◊ë◊ô◊®◊ï◊™ ◊ë◊†◊°◊ô◊ë◊ï◊™ ◊î◊ó◊ñ◊ß◊î ◊ì◊ï◊û◊ï◊™ ◊õ◊õ◊ú ◊î◊†◊ô◊™◊ü, ◊®◊ê◊ô◊™◊ô ◊ú◊ß◊ë◊ï◊¢ ◊õ◊ô ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊†◊¢ ◊ë◊ô◊ü 24 ◊ú- 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ú◊¶◊ì ◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊ô◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 212:\n",
      "◊î◊î◊™◊ó◊ô◊ô◊ë◊ï◊™ ◊™◊ô◊ó◊™◊ù ◊ë◊û◊ñ◊õ◊ô◊®◊ï◊™ ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊¢◊ï◊ì ◊î◊ô◊ï◊ù. ◊ú◊ê ◊ô◊ó◊™◊ï◊ù ◊î◊†◊ê◊©◊ù ◊õ◊ê◊û◊ï◊®, ◊ô◊ê◊°◊® ◊ú◊û◊©◊ö 15 ◊ô◊ï◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 213:\n",
      "◊ë◊®◊¢\"◊§ 174/21 ◊°◊ï◊ô◊°◊î ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú (25.2.21), ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊ï◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊ú◊ê◊ó◊® ◊©◊î◊ß◊ô◊ù ◊û◊¢◊ë◊ì◊î ◊ï◊í◊ô◊ì◊ú ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 38 ◊ß\"◊í. ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊î◊©◊ú◊ï◊ù ◊ß◊ë◊¢ ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊î◊û◊ë◊ß◊© ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊©◊†◊¢ ◊ë◊ô◊ü 10 ◊ú-28 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ó◊®◊í ◊û◊î◊û◊™◊ó◊ù ◊î◊†\"◊ú ◊ë◊©◊ú ◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù ◊ï◊î◊©◊ô◊™ ◊¢◊ú◊ô◊ï 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ú◊®◊ô◊¶◊ï◊ô ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ß◊ô◊ë◊ú ◊ê◊™ ◊¢◊®◊¢◊ï◊® ◊î◊û◊ì◊ô◊†◊î, ◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 15 ◊ú-30 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊û◊©◊ô◊ß◊ï◊ú◊ô ◊©◊ô◊ß◊ï◊ù ◊î◊ò◊ô◊ú ◊¢◊ú ◊î◊û◊ë◊ß◊© 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊©◊î◊í◊ô◊© ◊î◊û◊ë◊ß◊© ◊†◊ì◊ó◊™◊î ◊¢◊ú ◊ô◊ì◊ô ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 214:\n",
      "◊î◊§◊°◊ô◊ß◊î ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊ë◊î◊ü ◊î◊ï◊®◊©◊¢ ◊î◊†◊ê◊©◊ù ◊û◊¶◊ë◊ô◊¢◊î ◊¢◊ú ◊ß◊©◊™ ◊¢◊†◊ô◊©◊î ◊®◊ó◊ë◊î, ◊õ◊ú ◊û◊ß◊®◊î ◊ï◊†◊°◊ô◊ë◊ï◊™◊ô◊ï. ◊ë◊¢\"◊§ 5807/17 ◊ì◊®◊ó◊ô ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú, ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊°◊ô◊ï◊¢ ◊ú◊í◊ô◊ì◊ï◊ú ◊°◊ù, ◊°◊ô◊ï◊¢ ◊ú◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊†◊î◊ô◊í◊î ◊ë◊©◊õ◊®◊ï◊™. ◊î◊†◊ê◊©◊ù ◊°◊ô◊ô◊¢ ◊ú◊†◊ê◊©◊ù ◊ê◊ó◊® ◊ë◊î◊ß◊û◊™ ◊û◊¢◊ë◊ì◊™ ◊°◊û◊ô◊ù ◊©◊ë◊î ◊õ- 76 ◊ß\"◊í ◊ß◊†◊ë◊ï◊°, ◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊ï◊¶◊¢◊ï ◊¢◊ú ◊§◊†◊ô ◊™◊ß◊ï◊§◊î ◊ê◊®◊ï◊õ◊î, ◊©◊ë◊î ◊†◊¢◊®◊ö ◊™◊õ◊†◊ï◊ü ◊û◊ï◊ß◊ì◊ù ◊†◊®◊ó◊ë. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊©◊ë◊ô◊ü 12 ◊ú- 24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ï◊¢◊ú ◊î◊†◊ê◊©◊ù ◊†◊í◊ñ◊®◊ï 15 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊© ◊†◊ì◊ó◊î. ◊ë◊¢\"◊§ 4474/19 ◊ê◊ë◊ï ◊°◊ë◊ô◊ú◊î ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ô◊ï◊¢ ◊ú◊í◊ô◊ì◊ï◊ú ◊©◊™◊ô◊ú◊ô ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 109 ◊ß\"◊í, ◊ë◊õ◊ö ◊©◊î◊í◊ô◊¢ ◊ú◊©◊ì◊î, ◊ë◊©◊™◊ô ◊î◊ñ◊ì◊û◊†◊ï◊ô◊ï◊™ ◊©◊ï◊†◊ï◊™, ◊¢◊ú ◊û◊†◊™ ◊ú◊ò◊§◊ú ◊ë◊©◊™◊ô◊ú◊ô◊ù ◊ï◊ú◊î◊©◊ß◊ï◊™◊ù. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊û◊ó◊ï◊ñ◊ô ◊ú◊ê ◊û◊¶◊ê ◊ú◊†◊õ◊ï◊ü ◊ú◊ß◊ë◊ï◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊ï◊ú◊ù, ◊î◊ï◊ê◊ô◊ú ◊ï◊î◊û◊ê◊©◊ô◊û◊î ◊î◊í◊ë◊ô◊ú◊î ◊¢◊¶◊û◊î ◊ú◊¢◊ï◊†◊© ◊©◊ú 15 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ï◊î◊ò◊ô◊ú ◊¢◊ú ◊î◊†◊ê◊©◊ù 11 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊© ◊î◊™◊ß◊ë◊ú, ◊ï◊¢◊ï◊†◊©◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊¢◊û◊ì ◊¢◊ú 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊ï ◊ë◊ì◊®◊ö ◊©◊ú ◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ñ◊ê◊™, ◊ë◊ô◊ü ◊î◊ô◊™◊® ◊ë◊î◊™◊ó◊©◊ë ◊ë◊í◊ô◊ú◊ï ◊î◊¶◊¢◊ô◊®, ◊™◊ß◊ï◊§◊™ ◊û◊¢◊¶◊®◊ï, ◊ï◊ó◊ñ◊®◊™◊ï ◊ú◊û◊°◊ú◊ï◊ú ◊ó◊ô◊ô◊ù ◊†◊ï◊®◊û◊ò◊ô◊ë◊ô. ◊ë◊¢\"◊§ 7243/17 ◊õ◊• ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊ß◊©◊ô◊®◊™ ◊ß◊©◊® ◊ú◊ë◊ô◊¶◊ï◊¢ ◊§◊©◊¢, ◊©◊ô◊ë◊ï◊© ◊û◊î◊ú◊õ◊ô ◊û◊©◊§◊ò, ◊ï◊õ◊ü ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊°◊ô◊ï◊¢ ◊ú◊ô◊ô◊¶◊ï◊®, ◊î◊õ◊†◊î ◊ï◊î◊§◊ß◊î ◊©◊ú ◊°◊ù ◊ß◊†◊ë◊ï◊°, ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 137 ◊ß\"◊í, ◊ë◊õ◊ö ◊©◊°◊ô◊ô◊¢ ◊ë◊©◊õ◊ô◊®◊™ ◊û◊™◊ó◊ù ◊î◊õ◊ï◊ú◊ú ◊ô◊ó◊ô◊ì◊™ ◊í◊í ◊í◊ì◊ï◊ú◊î ◊ú◊¶◊ï◊®◊ö ◊î◊ß◊û◊™ ◊û◊¢◊ë◊ì◊™ ◊°◊û◊ô◊ù. ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü 12 ◊ú-24 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊ú ◊î◊†◊ê◊©◊ù, ◊¶◊¢◊ô◊® ◊ú◊ú◊ê ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô ◊ß◊ï◊ì◊ù ◊ê◊©◊® ◊°◊ô◊ô◊¢ ◊ú◊ô◊ú◊ì◊ô◊ù ◊ï◊†◊ï◊¢◊® ◊ë◊°◊ô◊õ◊ï◊ü ◊ò◊®◊ù ◊û◊¢◊¶◊®◊ï, ◊î◊ï◊ò◊ú◊ï 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊© ◊î◊™◊ß◊ë◊ú ◊ï◊¢◊ï◊†◊©◊ï ◊î◊ï◊¢◊û◊ì ◊¢◊ú 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™. ◊ë◊®◊¢\"◊§ 5214/08 ◊ë◊ü ◊ê◊î◊®◊ï◊ü ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊°◊ô◊ï◊¢ ◊ú◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ï◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊§◊®◊¢◊î ◊ú◊©◊ï◊ò◊® ◊ë◊û◊ô◊ú◊ï◊ô ◊™◊§◊ß◊ô◊ì◊ï. ◊ñ◊ê◊™ ◊ú◊ê◊ó◊® ◊©◊°◊ô◊ô◊¢ ◊ú◊ê◊ó◊® ◊ú◊î◊ó◊ñ◊ô◊ß ◊ë◊©◊ò◊ó ◊§◊™◊ï◊ó ◊°◊ù ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú 52.8 ◊ß\"◊í. ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊ò◊ú◊ï 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù, ◊ñ◊ê◊™ ◊ó◊®◊£ ◊î◊û◊ú◊¶◊™ ◊©◊ô◊®◊ï◊™ ◊î◊û◊ë◊ó◊ü ◊ú◊ë◊ò◊ú ◊î◊®◊©◊¢◊™◊ï. ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊© ◊î◊™◊ß◊ë◊ú ◊ë◊ê◊ï◊§◊ü ◊©◊¢◊ï◊†◊©◊ï ◊î◊ï◊¢◊û◊ì ◊¢◊ú 8 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ß◊©◊™ ◊®◊©◊ï◊™ ◊¢◊®◊¢◊ï◊® ◊†◊ì◊ó◊™◊î.◊ë◊¢\"◊§ 6031/18 ◊ê◊ú◊¢◊°◊ù ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú  ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊°◊ô◊ï◊¢ ◊ú◊í◊ô◊ì◊ï◊ú ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊ß◊†◊ë◊ï◊° ◊ë◊û◊©◊ß◊ú ◊©◊ú 57.05 ◊ß\"◊í ◊ï◊û◊™◊ü ◊î◊ô◊™◊® ◊ú◊ó◊¶◊®◊ô◊ù . ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊î◊ï◊ò◊ú◊ï 11 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ß◊ô◊ë◊ú ◊ê◊™ ◊¢◊®◊¢◊ï◊®◊ï ◊¢◊ú ◊ó◊ï◊û◊®◊™ ◊î◊¢◊ï◊†◊©, ◊ï◊ñ◊ê◊™ ◊ë◊ô◊ü ◊î◊ô◊™◊® ◊ú◊ê◊ï◊® ◊¢◊ë◊®◊ï ◊î◊§◊ú◊ô◊ú◊ô ◊î◊†◊ß◊ô, ◊û◊¶◊ë◊î ◊î◊ë◊®◊ô◊ê◊ï◊™◊ô ◊©◊ú ◊ë◊™◊ï ◊ï◊î◊û◊ú◊¶◊™ ◊©◊ô◊®◊ï◊™, ◊ï◊¢◊ï◊†◊©◊ï ◊î◊ï◊¢◊û◊ì ◊¢◊ú 6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 215:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß (◊°◊¢◊ô◊£ 40 ◊ô◊í) ◊û◊¶◊ê◊™◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊¢◊ë◊ô◊®◊™ ◊î◊î◊ó◊ñ◊ß◊î ◊©◊ú◊ê ◊ú◊©◊ô◊û◊ï◊© ◊¢◊¶◊û◊ô ◊î◊ï◊ê ◊ë◊ô◊ü ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊ú- 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 216:\n",
      "◊™\"◊§ (◊ô-◊ù) 16926-04-16 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ô◊ï◊ê◊ú ◊õ◊£   ◊î◊†◊ê◊©◊ù ◊î◊ï◊®◊©◊¢ ◊ë◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊û◊ô◊ù ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™ ◊ë◊û◊©◊ß◊ú ◊©◊ú 20.109 ◊í◊®'. ◊†◊í◊ñ◊®◊ï ◊¢◊ú◊ô◊ï 3 ◊ó◊ï◊ì' ◊û◊¢\"◊™ ◊ï◊ß◊†◊°.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 217:\n",
      "6 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®, ◊ê◊©◊® ◊ô◊®◊ï◊¶◊ï ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™, ◊ñ◊ê◊™ ◊ë◊î◊™◊ê◊ù ◊ú◊ó◊ï◊ï◊™ ◊ì◊¢◊™ ◊î◊û◊û◊ï◊†◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 218:\n",
      "◊©◊†◊ô ◊î◊û◊ê◊°◊®◊ô◊ù ◊î◊û◊ï◊™◊†◊ô◊ù ◊ë◊†◊ô 6 ◊ï-12 ◊ó◊ï◊ì◊©◊ô◊ù, ◊©◊†◊í◊ñ◊®◊ï ◊¢◊ú ◊î◊†◊ê◊©◊ù ◊ë◊™.◊§. (◊©◊ú◊ï◊ù ◊™\"◊ê) 5084/05 (◊®' ◊¢/3), ◊û◊ï◊§◊¢◊ú◊ô◊ù ◊ë◊ó◊ï◊§◊£ ◊î◊ê◊ó◊ì ◊ú◊û◊©◊†◊î◊ï ◊ï◊ë◊û◊¶◊ò◊ë◊® ◊ú◊¢◊ï◊†◊© ◊î◊†\"◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 219:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊°◊¢◊ô◊£ 40 ◊í(◊ê) ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü ◊î◊™◊©◊ú\"◊ñ ‚Äì 1977, ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ô◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊ï◊ú◊ù ◊ú◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊©◊ë◊ô◊¶◊¢ ◊î◊†◊ê◊©◊ù ◊ë◊î◊™◊ê◊ù ◊ú◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î, ◊ï◊ú◊©◊ù ◊õ◊ö ◊ô◊™◊ó◊©◊ë ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊©◊†◊§◊í◊¢ ◊û◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î, ◊ë◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊ï, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊î◊ß◊©◊ï◊®◊ï◊™ ◊ë◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊ë◊û◊ß◊®◊î ◊î◊†◊ï◊õ◊ó◊ô ◊î◊û◊™◊ó◊ù ◊¶◊®◊ô◊ö ◊ú◊î◊™◊ó◊©◊ë ◊ë◊õ◊ö ◊©◊û◊ì◊ï◊ë◊® ◊ë◊ê◊ô◊®◊ï◊¢ ◊ê◊ó◊ì ◊©◊ú ◊°◊ó◊® ◊ë◊°◊ù ◊ë◊ï ◊†◊û◊õ◊® ◊°◊ù ◊û◊°◊ï◊í MDMA ◊î◊†◊ó◊©◊ë ◊°◊ù ◊ß◊©◊î ◊ë◊õ◊û◊ï◊™ ◊ú◊ê ◊û◊ë◊ï◊ò◊ú◊™. ◊û◊†◊í◊ì ◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊î◊™◊¢◊ú◊ù ◊û◊î◊¢◊ï◊ë◊ì◊î ◊õ◊ô ◊î◊†◊ê◊©◊ù ◊†◊õ◊†◊° ◊ú◊™◊û◊ï◊†◊™ ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊ë◊û◊ï◊¢◊ì ◊û◊°◊ô◊®◊™ ◊î◊°◊ù ◊ï◊ú◊ê ◊î◊ï◊§◊ô◊¢ ◊ë◊©◊ú◊ë ◊™◊õ◊†◊ï◊ü ◊ï◊í◊ô◊ë◊ï◊© ◊î◊¢◊°◊ß◊î. ◊ë◊†◊ï◊°◊£ ◊î◊ó◊ñ◊ô◊ß ◊î◊†◊ê◊©◊ù ◊ë◊°◊û◊ô◊ù ◊û◊°◊ï◊õ◊†◊ô◊ù ◊û◊°◊ï◊í◊ô◊ù ◊©◊ï◊†◊ô◊ù ◊ì◊ë◊® ◊î◊û◊ú◊û◊ì ◊¢◊ú ◊†◊í◊ô◊©◊ï◊™ ◊î◊†◊ê◊©◊ù ◊ú◊°◊ù. ◊ô◊ó◊ì ◊¢◊ù ◊ñ◊ê◊™, ◊õ◊û◊ï◊™ ◊î◊°◊ù ◊©◊†◊™◊§◊°◊î ◊ê◊ô◊†◊î ◊í◊ì◊ï◊ú◊î. ◊ú◊ê ◊û◊¶◊ê◊™◊ô ◊û◊ß◊ï◊ù ◊ú◊ß◊ë◊ï◊¢ ◊û◊™◊ó◊û◊ô◊ù ◊†◊§◊®◊ì◊ô◊ù ◊ú◊¢◊ë◊®◊™ ◊î◊°◊ó◊® ◊ï◊¢◊ë◊®◊™ ◊î◊î◊ó◊ñ◊ß◊î. ◊ê◊ï◊û◊†◊ù ◊û◊ì◊ï◊ë◊® ◊ë◊û◊¢◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù ◊õ◊î◊í◊ì◊®◊™◊ù ◊ë◊î◊ú◊õ◊™ ◊í'◊ê◊ë◊® (◊¢\"◊§ 4910/13 ◊í'◊ê◊ë◊® ◊†' ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú [◊§◊ï◊®◊°◊ù ◊ë◊†◊ë◊ï], (29.10.14)), ◊ê◊ï◊ú◊ù ◊ß◊ô◊ô◊ù ◊ß◊©◊® ◊®◊¢◊ô◊ï◊†◊ô ◊ë◊ô◊ü ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊ï◊ú◊õ◊ü ◊†◊ô◊™◊ü ◊ú◊í◊ë◊© ◊û◊™◊ó◊ù ◊ê◊ó◊ì ◊ú◊õ◊ú◊ú ◊î◊û◊¢◊©◊ô◊ù ◊ë◊ê◊ô◊®◊ï◊¢ ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊ô◊®◊ï◊¢ ◊î◊ï◊ê 24-36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊¢◊†◊ô◊©◊î ◊†◊ú◊ï◊ï◊™.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 220:\n",
      "◊ë◊™\"◊§ (◊®◊û') ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊ê◊ô◊ô◊ñ◊ô◊ü (26.6.11), ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊¢◊ú ◊§◊ô ◊î◊ï◊ì◊ê◊™◊ï, ◊ë◊ë◊ô◊¶◊ï◊¢ ◊¢◊ë◊ô◊®◊î ◊©◊ú ◊î◊ó◊ñ◊ß◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™. ◊î◊†◊ê◊©◊ù ◊î◊ó◊ñ◊ô◊ß ◊ë◊©◊ë◊¢ ◊ê◊®◊ô◊ñ◊ï◊™ ◊î◊û◊õ◊ô◊ú◊ï◊™ ◊°◊ù ◊û◊°◊ï◊õ◊ü ◊û◊°◊ï◊í ◊î◊®◊ï◊ê◊ô◊ü, ◊ë◊û◊©◊ß◊ú ◊õ◊ï◊ú◊ú ◊©◊ú 6.04 ◊í◊®◊ù ◊†◊ò◊ï. ◊†◊ô◊ì◊ï◊ü ◊ú- 4 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊ú◊¶◊ì ◊¢◊ï◊†◊©◊ô◊ù ◊†◊ú◊ï◊ï◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 221:\n",
      "◊ë◊®◊¢\"◊§ 8325/13 ◊°◊ô◊ê◊ó ◊†' ◊û◊ì\"◊ô (8.1.14) ◊ì◊ï◊ë◊® ◊ë◊†◊ê◊©◊ù ◊©◊î◊ï◊®◊©◊¢ ◊ë◊î◊ó◊ñ◊ß◊™ 39 ◊í◊®◊ù ◊î◊®◊ï◊ê◊ô◊ü ◊©◊ú◊ê ◊ú◊¶◊®◊ô◊õ◊î ◊¢◊¶◊û◊ô◊™, ◊ï◊õ◊ü ◊î◊§◊®◊¢◊î ◊ú◊©◊ï◊ò◊®. ◊ë◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊ß◊û◊ê, ◊î◊ï◊ò◊ú◊ï 42 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊©◊õ◊ú◊ú◊ï ◊î◊§◊¢◊ú◊™ ◊û◊ê◊°◊®◊ô◊ù ◊¢◊ú ◊™◊†◊ê◊ô ◊ú◊û◊©◊ö ◊©◊†◊î. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊ì◊ó◊î ◊ê◊™ ◊ë◊ß◊©◊™ ◊î◊®◊©◊ï◊™ ◊ú◊¢◊®◊¢◊®.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 222:\n",
      "◊í◊ù ◊ú◊ê◊ó◊® ◊©◊†◊©◊ß◊ú◊™ ◊™◊ß◊ï◊§◊™ ◊î◊û◊¢◊¶◊® ◊ï◊î◊™◊ß◊ï◊§◊î ◊ë◊î ◊î◊ô◊î ◊î◊†◊ê◊©◊ù ◊ë◊™◊î◊ú◊ô◊õ◊ô◊ù ◊î◊©◊ï◊†◊ô◊ù ◊õ◊ê◊û◊ï◊®, ◊¢◊†◊ô◊©◊î ◊©◊ú 9 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ô◊© ◊ë◊î ◊õ◊ì◊ô ◊î◊ß◊ú◊î ◊©◊ú ◊û◊û◊© ◊ï◊ô◊®◊ô◊ì◊î ◊û◊©◊û◊¢◊ï◊™◊ô◊™ ◊ê◊ú ◊û◊™◊ó◊™ ◊ú◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 223:\n",
      "◊ë◊™\"◊§ (◊©◊ú◊ï◊ù ◊ë\"◊©) 56865-02-22 ◊û◊ì◊ô◊†◊™ ◊ô◊©◊®◊ê◊ú ◊†' ◊†◊ó◊©◊ï◊ü (11.6.2023) ◊î◊ï◊®◊©◊¢ ◊†◊ê◊©◊ù ◊¢◊ú ◊ë◊°◊ô◊° ◊î◊ï◊ì◊ê◊™◊ï ◊ë◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ú ◊í◊†◊ô◊ë◊î ◊ú◊§◊ô ◊°◊¢◊ô◊£ 384 ◊ú◊ó◊ï◊ß ◊ï◊ê◊ô◊ï◊û◊ô◊ù ◊ú◊§◊ô ◊°◊¢◊ô◊£ 192 ◊ú◊ó◊ï◊ß. ◊ú◊§◊ô ◊¢◊ï◊ë◊ì◊ï◊™ ◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù, ◊î◊†◊ê◊©◊ù ◊î◊í◊ô◊¢ ◊ú◊ë◊ô◊™ ◊¢◊°◊ß ◊ï◊ì◊®◊© ◊û◊î◊¢◊ï◊ë◊ì◊™ ◊©◊™◊§◊™◊ó ◊ê◊™ ◊î◊ß◊ï◊§◊î ◊ï◊™◊û◊°◊ï◊® ◊ú◊ï ◊õ◊°◊£, ◊ï◊ú◊ê◊ó◊® ◊©◊û◊°◊®◊î ◊ú◊ï 240 ‚Ç™, ◊ê◊ô◊ô◊ù ◊¢◊ú◊ô◊î ◊ë◊ê◊ï◊û◊®◊ï \"◊ê◊ú ◊™◊§◊†◊ô ◊ú◊û◊©◊ò◊®◊î, ◊ê◊†◊ô ◊ê◊ñ◊õ◊ï◊® ◊ê◊ï◊™◊ö\". ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊†◊ô◊©◊î ◊î◊†◊¢ ◊ë◊ô◊ü ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊©◊ô◊õ◊ï◊ú ◊ï◊ô◊®◊ï◊¶◊î ◊ë◊¢◊ë◊ï◊ì◊ï◊™ ◊©◊ô◊®◊ï◊™ ◊ï◊¢◊ì 18 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 224:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊î ◊ë◊¢\"◊© ◊ï◊¢◊ì ◊ú- 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 225:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊õ◊ú◊ú◊ô ◊î◊ë◊†◊ô◊ô◊™ ◊¢◊†◊ô◊©◊î ◊ê◊†◊ô ◊ß◊ï◊ë◊¢ ◊©◊û◊™◊ó◊ù ◊î◊¢◊†◊ô◊©◊î ◊™\"◊§ 54876-02-13 ◊†◊¢ ◊ë◊ô◊ü ◊û◊ê◊°◊® ◊û◊ï◊™◊†◊î ◊ú-5 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 226:\n",
      "◊ë◊û◊ß◊®◊î ◊©◊ë◊§◊†◊ô, ◊°◊ï◊í ◊î◊°◊û◊ô◊ù ◊ï◊õ◊û◊ï◊™◊ù, ◊õ◊§◊ô ◊©◊î◊ï◊ó◊ñ◊ß◊ï ◊¢◊ú ◊ô◊ì◊ô ◊î◊†◊ê◊©◊ù, ◊û◊ó◊ô◊ô◊ë◊ô◊ù ◊¢◊†◊ô◊©◊î ◊û◊û◊©◊ô◊™ ◊û◊ê◊ó◊ï◊®◊ô ◊°◊ï◊®◊í ◊ï◊ë◊®◊ô◊ó. ◊ô◊ï◊ñ◊õ◊® ◊©◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊®◊ú◊ï◊ï◊†◊ò◊ô ◊ú◊¢◊ë◊ô◊®◊ï◊™ ◊©◊ë◊ï◊¶◊¢◊ï ◊¢◊ú ◊ô◊ì◊ô ◊î◊†◊ê◊©◊ù, ◊î◊ï◊ê ◊û◊™◊ó◊ù ◊©◊ó◊ú◊ß◊ï ◊î◊™◊ó◊™◊ï◊ü ◊û◊™◊ó◊ô◊ú ◊û-10 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ï◊ë◊¢◊ï◊ì ◊©◊ó◊ú◊ß◊ï ◊î◊¢◊ú◊ô◊ï◊ü ◊û◊°◊™◊ô◊ô◊ù ◊ë-36 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®. ◊û◊ì◊ï◊ë◊® ◊ë◊û◊™◊ó◊ù ◊©◊î◊í◊ë◊ï◊ú ◊î◊™◊ó◊™◊ï◊ü ◊©◊ú◊ï ◊û◊™◊ó◊ô◊ú ◊û◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ú◊ê ◊û◊ß◊ï◊ë◊ú◊™ ◊¢◊ú◊ô◊ô ◊ò◊¢◊†◊™ ◊î◊î◊í◊†◊î ◊©◊©◊ô◊ß◊ï◊ú◊ô ◊î◊©◊ô◊ß◊ï◊ù ◊û◊¶◊ì◊ô◊ß◊ô◊ù ◊°◊ò◊ô◊î ◊û◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊¢◊ì ◊õ◊ì◊ô ◊ê◊ô ◊î◊ò◊ú◊™ ◊û◊ê◊°◊® ◊û◊ê◊ó◊ï◊®◊ô ◊°◊ï◊®◊í ◊ï◊ë◊®◊ô◊ó, ◊ï◊ñ◊ê◊™ ◊ú◊ê ◊õ◊ú ◊©◊õ◊ü, ◊ú◊ê◊ï◊® ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊©◊î◊ô◊†◊ï ◊ó◊ë ◊î◊§◊¢◊ú◊î ◊ë◊û◊ß◊®◊î ◊©◊ë◊§◊†◊ô. ◊ú◊ê ◊î◊™◊®◊©◊û◊™◊ô ◊©◊î◊û◊ß◊®◊î ◊©◊ë◊§◊†◊ô ◊î◊ô◊†◊ï ◊û◊°◊ï◊í ◊î◊û◊ß◊®◊ô◊ù ◊î◊ó◊®◊ô◊í◊ô◊ù ◊©◊û◊¶◊ì◊ô◊ß◊ô◊ù ◊î◊ê◊®◊õ◊™ ◊î◊û◊ê◊°◊® ◊¢◊ú ◊™◊†◊ê◊ô ◊û◊õ◊ó ◊°◊û◊õ◊ï◊™◊ï ◊î◊û◊ô◊ï◊ó◊ì◊™ ◊©◊ú ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊©◊ß◊ë◊ï◊¢◊î ◊ë◊°◊¢◊ô◊£ 85 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ï◊ñ◊ê◊™ ◊ú◊ê◊ï◊® ◊®◊ô◊ë◊ï◊ô ◊î◊¢◊ë◊ô◊®◊ï◊™ ◊©◊û◊ï◊§◊ô◊¢◊ï◊™ ◊ë◊õ◊™◊ë ◊î◊ê◊ô◊©◊ï◊ù ◊©◊ë◊õ◊ï◊ú◊ü ◊û◊ì◊ï◊ë◊® ◊ë◊¢◊ë◊ô◊®◊™ ◊°◊û◊ô◊ù ◊û◊°◊ï◊í ◊§◊©◊¢ ◊©◊ú◊¶◊ô◊ì◊î ◊¢◊ï◊†◊© ◊û◊ê◊°◊® ◊û◊ß◊°◊ô◊û◊ê◊ú◊ô ◊©◊ú 20 ◊©◊†◊î.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 227:\n",
      "◊ë◊¢\"◊§ 5681/23 ◊ó◊ï◊ï◊ê ◊†' ◊û\"◊ô (20.12.23) ◊†◊ì◊ó◊î ◊¢◊®◊¢◊ï◊® ◊¢◊ú ◊í◊ñ◊® ◊ì◊ô◊ü ◊©◊õ◊ú◊ú ◊¢◊ï◊†◊© ◊©◊ú 35 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ë◊¢◊†◊ô◊ô◊†◊ï ◊©◊ú ◊û◊¢◊®◊¢◊® ◊©◊î◊ï◊®◊©◊¢ ◊ë◊õ◊ö ◊©◊î◊ó◊ñ◊ô◊ß ◊ë◊ê◊ß◊ì◊ó (◊ê◊ß◊ì◊ó ◊î◊ñ◊†◊ß◊î ◊û◊ï◊°◊ë) ◊ò◊¢◊ï◊ü ◊ë◊û◊ó◊°◊†◊ô◊™ ◊™◊ï◊ê◊û◊™ ◊ï◊ë◊î ◊©◊ô◊©◊î ◊õ◊ì◊ï◊®◊ô◊ù ◊ï◊ê◊©◊® ◊†◊¢◊¶◊® ◊õ◊©◊î◊ï◊ê ◊û◊î◊ú◊ö ◊û◊ó◊ï◊• ◊ú◊ë◊ô◊™◊ï ◊õ◊©◊î◊ê◊ß◊ì◊ó ◊î◊ò◊¢◊ï◊ü ◊û◊ï◊°◊™◊® ◊ë◊ë◊í◊ì◊ô◊ï. ◊ë◊ê◊ï◊™◊ï ◊í◊ñ◊® ◊ì◊ô◊ü ◊†◊ß◊ë◊¢ ◊û◊™◊ó◊ù ◊¢◊ï◊†◊© ◊î◊†◊¢ ◊ë◊ô◊ü 24 ◊ú◊ë◊ô◊ü 50 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú. ◊ë◊ô◊™ ◊î◊û◊©◊§◊ò ◊î◊¢◊ú◊ô◊ï◊ü ◊¶◊ô◊ô◊ü, ◊ë◊ì◊ï◊ó◊ï◊™ ◊ê◊™ ◊î◊¢◊®◊¢◊ï◊® ◊õ◊ô: \"◊¢◊ë◊ô◊®◊ï◊™ ◊î◊†◊©◊ß ◊ë◊ê◊©◊® ◊î◊ü ◊î◊§◊õ◊ï ◊ú◊û◊õ◊™ ◊û◊ì◊ô◊†◊î. ◊ó◊ï◊û◊®◊™◊ü ◊î◊ô◊™◊ô◊®◊î ◊ï◊î◊©◊§◊¢◊™◊ü ◊¢◊ú ◊™◊ó◊ï◊©◊™ ◊î◊ë◊ò◊ó◊ï◊ü ◊©◊ú ◊õ◊ú◊ú ◊ê◊ñ◊®◊ó◊ô ◊î◊û◊ì◊ô◊†◊î, ◊û◊ó◊ô◊ô◊ë◊ï◊™ ◊î◊ò◊ú◊™ ◊¢◊†◊ô◊©◊î ◊û◊©◊û◊¢◊ï◊™◊ô◊™ ◊í◊ù ◊¢◊ú ◊†◊ê◊©◊ù ◊†◊¢◊ì◊® ◊¢◊ë◊® ◊§◊ú◊ô◊ú◊ô [...] ◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊î◊ó◊û◊®◊î ◊ë◊ô◊ó◊° ◊ú◊õ◊ú◊ú ◊¢◊ë◊ô◊®◊ï◊™ ◊î◊†◊©◊ß, ◊ê◊©◊® ◊†◊ô◊õ◊®◊™ ◊ë◊§◊°◊ô◊ß◊™◊ï ◊©◊ú ◊ë◊ô◊™ ◊û◊©◊§◊ò ◊ñ◊î ◊ë◊ê◊î ◊ú◊ô◊ì◊ô ◊ë◊ô◊ò◊ï◊ô [...] ◊ë◊û◊°◊í◊®◊™ ◊™◊ô◊ß◊ï◊ü ◊û◊°' 140 ◊ú◊ó◊ï◊ß [...] ◊ï◊ê◊û◊†◊ù ◊†◊ô◊õ◊®◊™ ◊ë◊©◊†◊ô◊ù ◊î◊ê◊ó◊®◊ï◊†◊ï◊™ ◊û◊í◊û◊™ ◊î◊ó◊û◊®◊î ◊ë◊¢◊†◊ô◊©◊î ◊ú◊©◊ù ◊û◊ô◊í◊ï◊® ◊¢◊ë◊ô◊®◊ï◊™ ◊ê◊ú◊ï...\".\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 228:\n",
      "◊î. ◊§◊°◊ô◊ú◊î ◊ë◊§◊ï◊¢◊ú ◊û◊ß◊ë◊ú ◊ê◊ï ◊û◊î◊ó◊ñ◊ô◊ß ◊®◊©◊ô◊ï◊ü ◊†◊î◊ô◊í◊î ◊ú◊™◊ß◊ï◊§◊î ◊©◊ú 3 ◊ó◊ï◊ì◊©◊ô◊ù.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 229:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü, ◊ô◊© ◊ú◊ß◊ë◊ï◊¢, ◊ë◊ò◊®◊ù ◊í◊ñ◊ô◊®◊™ ◊î◊ì◊ô◊ü ◊ê◊™ ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù, ◊™◊ï◊ö ◊î◊™◊ó◊©◊ë◊ï◊™ ◊ë◊¢◊ô◊ß◊®◊ï◊ü ◊î◊û◊†◊ó◊î ◊ë◊¢◊†◊ô◊©◊î, ◊©◊î◊ï◊ê ◊ß◊ô◊ï◊û◊ï ◊©◊ú ◊ô◊ó◊° ◊î◊ï◊ú◊ù ◊ë◊ô◊ü ◊ó◊ï◊û◊®◊™ ◊û◊¢◊©◊î ◊î◊¢◊ë◊ô◊®◊î ◊ë◊†◊°◊ô◊ë◊ï◊™◊ô◊ï ◊ï◊û◊ô◊ì◊™ ◊ê◊©◊û◊ï ◊©◊ú ◊î◊†◊ê◊©◊ù. ◊ë◊ô◊ü ◊°◊ï◊í ◊ï◊û◊ô◊ì◊™ ◊î◊¢◊ï◊†◊© ◊î◊û◊ï◊ò◊ú ◊¢◊ú◊ô◊ï, ◊û◊ô◊ì◊™ ◊î◊§◊í◊ô◊¢◊î ◊ë◊¢◊®◊ö ◊î◊ó◊ë◊®◊™◊ô ◊î◊û◊ï◊í◊ü, ◊ë◊û◊ì◊ô◊†◊ô◊ï◊™ ◊î◊¢◊†◊ô◊©◊î ◊î◊†◊î◊ï◊í◊î ◊ï◊ë◊†◊°◊ô◊ë◊ï◊™ ◊ë◊ô◊¶◊ï◊¢ ◊î◊¢◊ë◊ô◊®◊î. ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ë◊í◊ô◊ü ◊î◊ê◊®◊ï◊¢ ◊î◊ï◊ê ◊û◊ê◊°◊® ◊©◊ô◊®◊ï◊¶◊î ◊ë◊¢\"◊© ◊ï◊¢◊ì ◊ú- 12 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊®.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 230:\n",
      "◊î◊û◊ê◊©◊ô◊û◊î ◊î◊§◊†◊™◊î ◊ú◊û◊°◊§◊® ◊§◊°◊ß◊ô ◊ì◊ô◊ü ◊õ◊ê◊©◊® ◊û◊™◊ó◊û◊ô ◊î◊¢◊ï◊†◊© ◊©◊†◊ß◊ë◊¢◊ï ◊†◊¢◊ô◊ù ◊ë◊ô◊ü 24 ◊ï◊¢◊ì 72 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú ◊ï◊î◊¢◊ï◊†◊©◊ô◊ù ◊©◊î◊ï◊ò◊ú◊ï  ◊î◊ô◊ï ◊ë◊ò◊ï◊ï◊ó ◊©◊ë◊ô◊ü 30  ◊ú- 60 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 231:\n",
      "◊ë◊î◊™◊ê◊ù ◊ú◊™◊ô◊ß◊ï◊ü 113 ◊ú◊ó◊ï◊ß ◊î◊¢◊ï◊†◊©◊ô◊ü (◊°◊¢◊ô◊£ 40 ◊ô◊í'), ◊°◊ë◊ï◊®◊†◊ô ◊õ◊ô ◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊î◊ô◊†◊ï ◊î◊ó◊ú ◊û -20 ◊ï◊¢◊ì 40  ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú.\n",
      "Real: 1 | Predicted: 1\n",
      "\n",
      "üìù Text 232:\n",
      "◊î◊¢◊ï◊†◊©◊ô◊ù ◊ë◊™\"◊§ 9346-07-12 ◊ï◊™\"◊§ 51135-11-13 ◊ô◊®◊ï◊¶◊ï ◊ë◊ó◊ï◊§◊£ ◊ñ◊î ◊ú◊ñ◊î, ◊ï◊ë◊û◊¶◊ò◊ë◊® ◊ú◊¢◊ï◊†◊© ◊©◊û◊ï◊ò◊ú ◊ë◊í◊ô◊ü ◊™\"◊§ 24394-12-14 (◊°◊î\"◊õ 11 ◊ó◊ï◊ì◊©◊ô◊ù ◊ë◊í◊ô◊ü ◊™◊ô◊ß◊ô◊ù ◊ê◊ú◊î). ◊î◊û◊ê◊°◊®◊ô◊ù ◊¢◊ú ◊™◊†◊ê◊ô ◊ô◊ï◊§◊¢◊ú◊ï ◊ë◊ó◊ï◊§◊£ ◊ñ◊î ◊ú◊ñ◊î, ◊ï◊ë◊û◊¶◊ò◊ë◊® ◊ú◊¢◊ï◊†◊© ◊©◊û◊ï◊ò◊ú ◊ë◊í◊ô◊ü ◊™◊ô◊ß ◊ñ◊î, ◊õ◊ö ◊©◊°◊ö ◊î◊õ◊ú ◊ô◊®◊¶◊î ◊î◊†◊ê◊©◊ù 14 ◊ó◊ï◊ì◊©◊ô ◊û◊ê◊°◊® ◊ë◊§◊ï◊¢◊ú, ◊û◊ô◊ï◊ù ◊û◊¢◊¶◊®◊ï.\n",
      "Real: 0 | Predicted: 0\n",
      "\n",
      "üìù Text 233:\n",
      "3\t◊û◊™◊ó◊ù ◊î◊¢◊ï◊†◊© ◊î◊î◊ï◊ú◊ù ◊ú◊¢◊ë◊ô◊®◊î ◊©◊ú ◊û◊õ◊ô◊®◊™ ◊û◊©◊ß◊î ◊û◊©◊õ◊® ◊ú◊ß◊ò◊ô◊ü\n",
      "Real: 0 | Predicted: 1\n",
      "\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9750    0.9630    0.9689       162\n",
      "           1     0.9178    0.9437    0.9306        71\n",
      "\n",
      "    accuracy                         0.9571       233\n",
      "   macro avg     0.9464    0.9533    0.9497       233\n",
      "weighted avg     0.9576    0.9571    0.9572       233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Load model and tokenizer\n",
    "model_path = \"best_model\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# 2. Tokenize validation texts again\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Predict in batches\n",
    "with torch.no_grad():\n",
    "    outputs = model(**val_encodings)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "# 4. Print predictions vs. ground truth\n",
    "for i, (text, real, pred) in enumerate(zip(val_texts, val_labels, predictions)):\n",
    "    print(f\"\\nüìù Text {i+1}:\\n{text}\\nReal: {real} | Predicted: {pred}\")\n",
    "\n",
    "# 5. Optional: Classification report\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(val_labels, predictions, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
