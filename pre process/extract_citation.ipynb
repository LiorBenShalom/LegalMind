{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 11786-06-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 13632-08-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 20051-12-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 21139-04-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 22830-12-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 17856-06-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 16420-10-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 1995-03-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 31034-06-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 229-08-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 26417-03-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 28615-08-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 31661-09-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 3295-04-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 33009-05-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 32659-05-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 32803-12-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 32889-12-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 3742-07-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 34003-04-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 395-05-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 34027-04-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 34413-09-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 38154-09-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 40313-09-15.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 45561-04-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 45713-07-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 41855-07-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 42291-02-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 42697-05-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 45747-05-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 47161-08-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 48435-02-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 49416-04-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 49772-11-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 49995-09-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 50551-03-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 50556-08-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 51038-02-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 52479-01-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 520-07-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 522-05-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 52219-06-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 57618-05-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 55670-06-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 613-04-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 55908-09-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 51523-10-17.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 59812-02-16.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 630-02-18.csv\n",
      "Combined citations extracted and saved to: /home/liorkob/thesis/lcp/data/filtered_citations_csv_2018/filtered_ת\"פ 633-05-17.csv\n",
      "no_parts_files\n",
      "ת\"פ 16420-10-16.csv part exists but no citation found\n",
      "ת\"פ 31661-09-17.csv part exists but no citation found\n",
      "ת\"פ 32659-05-18.csv part exists but no citation found\n",
      "ת\"פ 32803-12-17.csv no part exists\n",
      "ת\"פ 34413-09-17.csv no part exists\n",
      "ת\"פ 42697-05-18.csv no part exists\n",
      "ת\"פ 51038-02-17.csv no part exists\n",
      "ת\"פ 55908-09-17.csv part exists but no citation found\n",
      "ת\"פ 633-05-17.csv no part exists\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "required_parts = [\n",
    "    \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\",\n",
    "    \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\", \"מדיניות הענישה\"\n",
    "]\n",
    "\n",
    "citation_patterns = [\n",
    "    'ע\"פ', 'ת\"פ', 'עפ\"ג', 'ע״פ', 'ת״פ', 'עפ״ג'\n",
    "]\n",
    "\n",
    "def extract_citations_with_full_parts(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract citations from the given CSV file, including all rows starting from occurrences of required parts.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file generated by `docToCsv`.\n",
    "    - output_path (str): Path to save the extracted CSV with citations.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if no rows match the required parts, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        part_flag=False\n",
    "\n",
    "        # Filter rows where 'part' matches exactly any of the required_parts\n",
    "        primary_matches = df[\n",
    "            df['part'].isin(required_parts) & \n",
    "            df['text'].str.contains('|'.join(citation_patterns), na=False)\n",
    "        ]\n",
    "\n",
    "        # Extended logic: Extract all rows starting from each required_part and search for citations\n",
    "        extended_matches = []\n",
    "        for part in required_parts:\n",
    "            part_indices = df[df['part'].str.contains(part, na=False)].index.tolist()\n",
    "            for idx in part_indices:\n",
    "                part_flag=True\n",
    "                # Include rows from the current part onward\n",
    "                extended_rows = df.loc[idx:]\n",
    "                # Filter rows that contain citations\n",
    "                citation_rows = extended_rows[extended_rows['text'].str.contains('|'.join(citation_patterns), na=False)]\n",
    "                extended_matches.extend(citation_rows.to_dict('records'))\n",
    "\n",
    "        # Combine primary and extended matches\n",
    "        combined_matches = pd.DataFrame(primary_matches.to_dict('records') + extended_matches).drop_duplicates()\n",
    "\n",
    "        # Save the combined results\n",
    "        combined_matches.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Combined citations extracted and saved to: {output_path}\")\n",
    "\n",
    "        # Return True if no matches were found\n",
    "        return combined_matches.empty,part_flag\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_path}: {e}\")\n",
    "        return True  # Consider the file as missing required parts in case of an error\n",
    "def batch_extract_citations(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Batch process all CSV files in a directory for citation extraction.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dir (str): Directory containing CSV files.\n",
    "    - output_dir (str): Directory to save the filtered citation CSVs.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of verdict file names where none of the required parts exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    no_parts_verdicts = {}\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('.csv'):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_path = os.path.join(output_dir, f\"filtered_{file}\")\n",
    "            missing_parts,part_flag = extract_citations_with_full_parts(input_path, output_path)\n",
    "            \n",
    "            if missing_parts:\n",
    "                if part_flag:\n",
    "                    no_parts_verdicts[file] = \"part exists but no citation found\"\n",
    "                else:\n",
    "                    no_parts_verdicts[file] = \"no part exists\"\n",
    "    return no_parts_verdicts\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_dir = \"/home/liorkob/thesis/lcp/data/docx_csv_2018\"  # Directory where `docToCsv` saves CSV files\n",
    "    output_csv_dir = \"/home/liorkob/thesis/lcp/data/filtered_citations_csv_2018\"\n",
    "    no_parts_files = batch_extract_citations(input_csv_dir, output_csv_dir)\n",
    "    print(\"no_parts_files\")\n",
    "    for v, k in no_parts_files.items():\n",
    "        print(v, k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_citations_with_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import openai  \n",
    "\n",
    "OPENAI_API_KEY = \"your-api-key\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Required parts to filter\n",
    "required_parts = [\n",
    "    \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\",\n",
    "    \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\", \"מדיניות הענישה\"\n",
    "]\n",
    "\n",
    "# Patterns to identify legal citations\n",
    "citation_patterns = [\n",
    "    'ע\"פ', 'ת\"פ', 'עפ\"ג', 'ע״פ', 'ת״פ', 'עפ״ג'\n",
    "]\n",
    "\n",
    "def call_gpt_to_split_text(text):\n",
    "    \"\"\"\n",
    "    Calls GPT API to split a paragraph containing multiple citations into separate paragraphs.\n",
    "    \n",
    "    Parameters:\n",
    "    - text (str): The paragraph with multiple citations.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of split paragraphs, each containing only one citation.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The following Hebrew paragraph contains multiple legal citations. Split it into separate paragraphs so that each one contains exactly one citation without modifying the text:\n",
    "    \n",
    "    {text}\n",
    "    \n",
    "    Return only the split paragraphs as a list.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "\n",
    "        # Extract and return the GPT response (list of paragraphs)\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"].strip().split(\"\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling GPT API: {e}\")\n",
    "        return [text]  # If API call fails, return the original text unchanged\n",
    "\n",
    "def extract_citations_with_gpt(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Extracts citations and ensures each paragraph has only one citation by using GPT API.\n",
    "    \n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file containing text data.\n",
    "    - output_path (str): Path to save the processed CSV.\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if no citations found, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        part_flag = False\n",
    "\n",
    "        # Filter rows where 'part' is relevant and 'text' contains citations\n",
    "        filtered_rows = df[df['part'].isin(required_parts) & df['text'].str.contains('|'.join(citation_patterns), na=False)]\n",
    "\n",
    "        processed_rows = []\n",
    "        for _, row in filtered_rows.iterrows():\n",
    "            text = row[\"text\"]\n",
    "            citation_count = sum(text.count(pattern) for pattern in citation_patterns)\n",
    "\n",
    "            if citation_count > 1:  # If multiple citations exist, split using GPT\n",
    "                part_flag = True\n",
    "                split_texts = call_gpt_to_split_text(text)\n",
    "                for split_text in split_texts:\n",
    "                    new_row = row.copy()\n",
    "                    new_row[\"text\"] = split_text\n",
    "                    processed_rows.append(new_row)\n",
    "            else:\n",
    "                processed_rows.append(row)\n",
    "\n",
    "        # Save results\n",
    "        processed_df = pd.DataFrame(processed_rows)\n",
    "        processed_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Citations processed and saved to: {output_path}\")\n",
    "\n",
    "        return processed_df.empty, part_flag\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_path}: {e}\")\n",
    "        return True, False\n",
    "\n",
    "def batch_extract_citations_with_gpt(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Batch processes all CSV files in a directory for citation extraction and paragraph splitting.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dir (str): Directory containing CSV files.\n",
    "    - output_dir (str): Directory to save the processed citation CSVs.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of verdict file names where no citations were found.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    no_parts_verdicts = {}\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_path = os.path.join(output_dir, f\"filtered_{file}\")\n",
    "            missing_parts, part_flag = extract_citations_with_gpt(input_path, output_path)\n",
    "\n",
    "            if missing_parts:\n",
    "                if part_flag:\n",
    "                    no_parts_verdicts[file] = \"Part exists but no citation found\"\n",
    "                else:\n",
    "                    no_parts_verdicts[file] = \"No part exists\"\n",
    "    \n",
    "    return no_parts_verdicts\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_dir = \"/home/liorkob/thesis/lcp/data/docx_csv_2018\"\n",
    "    output_csv_dir = \"/home/liorkob/thesis/lcp/data/filtered_citations_csv_2018\"\n",
    "    no_parts_files = batch_extract_citations_with_gpt(input_csv_dir, output_csv_dir)\n",
    "\n",
    "    print(\"No parts files:\")\n",
    "    for file, reason in no_parts_files.items():\n",
    "        print(file, reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_citations_with_dictalm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load DictaLM model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\"dicta-il/dictalm2.0-instruct\", torch_dtype=torch.bfloat16, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dicta-il/dictalm2.0-instruct\")\n",
    "\n",
    "# Initialize DictaLM generator\n",
    "dictalm_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Required parts to filter\n",
    "required_parts = [\n",
    "    \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\",\n",
    "    \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\", \"מדיניות הענישה\"\n",
    "]\n",
    "\n",
    "# Patterns to identify legal citations\n",
    "citation_patterns = [\n",
    "    'ע\"פ', 'ת\"פ', 'עפ\"ג', 'ע״פ', 'ת״פ', 'עפ״ג'\n",
    "]\n",
    "\n",
    "def query_dictalm(text):\n",
    "    \"\"\"\n",
    "    Calls DictaLM to split paragraphs with multiple citations.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The paragraph with multiple citations.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of split paragraphs, each containing only one citation.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"הטקסט הבא מכיל מספר הפניות לפסקי דין. \"\n",
    "        \"עליך לפצל אותו כך שכל פסקה תכיל הפניה אחת בלבד, בלי לשנות את התוכן או הניסוח:\\n\\n\"\n",
    "        f\"טקסט: {text}\\n\"\n",
    "        \"תשובה:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = dictalm_generator(prompt, max_new_tokens=1024, num_return_sequences=1)\n",
    "        generated_text = response[0][\"generated_text\"].strip()\n",
    "        return generated_text.split(\"\\n\\n\")  # Split into separate paragraphs\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Switching to CPU due to GPU memory constraints.\")\n",
    "        return [text]  # Return original text if failure\n",
    "\n",
    "def extract_citations_with_dictalm(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Extracts citations and ensures each paragraph has only one citation using DictaLM.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the CSV file containing text data.\n",
    "    - output_path (str): Path to save the processed CSV.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if no citations found, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        part_flag = False\n",
    "\n",
    "        # Filter rows where 'part' is relevant and 'text' contains citations\n",
    "        filtered_rows = df[df['part'].isin(required_parts) & df['text'].str.contains('|'.join(citation_patterns), na=False)]\n",
    "\n",
    "        processed_rows = []\n",
    "        for _, row in filtered_rows.iterrows():\n",
    "            text = row[\"text\"]\n",
    "            citation_count = sum(text.count(pattern) for pattern in citation_patterns)\n",
    "\n",
    "            if citation_count > 1:  # If multiple citations exist, split using DictaLM\n",
    "                part_flag = True\n",
    "                split_texts = query_dictalm(text)\n",
    "                for split_text in split_texts:\n",
    "                    new_row = row.copy()\n",
    "                    new_row[\"text\"] = split_text\n",
    "                    processed_rows.append(new_row)\n",
    "            else:\n",
    "                processed_rows.append(row)\n",
    "\n",
    "        # Save results\n",
    "        processed_df = pd.DataFrame(processed_rows)\n",
    "        processed_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Citations processed and saved to: {output_path}\")\n",
    "\n",
    "        return processed_df.empty, part_flag\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {csv_path}: {e}\")\n",
    "        return True, False\n",
    "\n",
    "def batch_extract_citations_with_dictalm(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Batch processes all CSV files in a directory for citation extraction and paragraph splitting.\n",
    "\n",
    "    Parameters:\n",
    "    - input_dir (str): Directory containing CSV files.\n",
    "    - output_dir (str): Directory to save the processed citation CSVs.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of verdict file names where no citations were found.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    no_parts_verdicts = {}\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".csv\"):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_path = os.path.join(output_dir, f\"filtered_{file}\")\n",
    "            missing_parts, part_flag = extract_citations_with_dictalm(input_path, output_path)\n",
    "\n",
    "            if missing_parts:\n",
    "                if part_flag:\n",
    "                    no_parts_verdicts[file] = \"Part exists but no citation found\"\n",
    "                else:\n",
    "                    no_parts_verdicts[file] = \"No part exists\"\n",
    "    \n",
    "    return no_parts_verdicts\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_dir = \"/home/liorkob/thesis/lcp/data/docx_csv_2018\"\n",
    "    output_csv_dir = \"/home/liorkob/thesis/lcp/data/filtered_citations_csv_2018\"\n",
    "    no_parts_files = batch_extract_citations_with_dictalm(input_csv_dir, output_csv_dir)\n",
    "\n",
    "    print(\"No parts files:\")\n",
    "    for file, reason in no_parts_files.items():\n",
    "        print(file, reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
