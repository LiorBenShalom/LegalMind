{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "print(docx.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT DOC TO DOCX!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x LibreOffice_7.6.4_Linux_x86-64.AppImage\n",
    "!./LibreOffice_7.6.4_Linux_x86-64.AppImage --headless --convert-to docx your_file.doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load libreoffice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT DOC TO DOCX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CONVERT DOC TO DOCX!!!\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def force_quit_libreoffice():\n",
    "    \"\"\"Force quit LibreOffice by killing all soffice processes.\"\"\"\n",
    "    try:\n",
    "        # Kill all LibreOffice (soffice) processes\n",
    "        subprocess.run([\"pkill\", \"-f\", \"soffice\"], check=True)\n",
    "        print(\"Force quit LibreOffice successfully.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"No LibreOffice processes were found to quit.\")\n",
    "\n",
    "def convert_doc_to_docx(input_path, output_path):\n",
    "    print(f\"Starting conversion of {input_path} to {output_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Run the conversion command and capture output and errors\n",
    "        result = subprocess.run(\n",
    "            ['unoconv', '-v', '-f', 'docx', '-o', output_path, input_path],\n",
    "            stdout=subprocess.PIPE,  # Capture standard output\n",
    "            stderr=subprocess.PIPE,  # Capture standard error\n",
    "            timeout=600  # Timeout after 10 minutes (adjust as needed)\n",
    "        )\n",
    "\n",
    "        # Check if the command was successful (exit code 0)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error converting {input_path}: {result.stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"Successfully converted {input_path} to {output_path}\")\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Conversion of {input_path} timed out after 10 minutes\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting {input_path}: {e}\")\n",
    "\n",
    "    # Force quit LibreOffice to free up resources after each conversion\n",
    "    force_quit_libreoffice()\n",
    "\n",
    "def convert_all_docs_in_dir(directory_path):\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.doc'):\n",
    "            input_file = os.path.join(directory_path, filename)\n",
    "            output_file = os.path.join(directory_path, f\"{os.path.splitext(filename)[0]}.docx\")\n",
    "\n",
    "            # Convert the file and save it with the new .docx extension\n",
    "            convert_doc_to_docx(input_file, output_file)\n",
    "\n",
    "            # Optional: Delete the original .doc file after conversion\n",
    "            os.remove(input_file)\n",
    "\n",
    "            print(f\"Converted {filename} to {os.path.basename(output_file)}\")\n",
    "\n",
    "# Example usage:\n",
    "doc_directory ='/home/liorkob/M.Sc/thesis/data/drugs/drugs doc'\n",
    "convert_all_docs_in_dir(doc_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade python-docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 18.2MB/s]                    \n",
      "2025-03-31 10:24:37 INFO: Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-03-31 10:24:37 INFO: Downloading default packages for language: he (Hebrew) ...\n",
      "2025-03-31 10:24:44 INFO: File exists: /home/liorkob/stanza_resources/he/default.zip\n",
      "2025-03-31 10:24:47 INFO: Finished downloading models and saved to /home/liorkob/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('he')  # run only once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 10:24:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 424kB [00:00, 8.48MB/s]                    \n",
      "2025-03-31 10:24:49 INFO: Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-03-31 10:24:59 INFO: Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | iahlt_charlm      |\n",
      "=================================\n",
      "\n",
      "2025-03-31 10:24:59 INFO: Using device: cpu\n",
      "2025-03-31 10:24:59 INFO: Loading: tokenize\n",
      "2025-03-31 10:25:11 INFO: Loading: mwt\n",
      "2025-03-31 10:25:11 INFO: Loading: pos\n",
      "2025-03-31 10:25:15 INFO: Loading: lemma\n",
      "2025-03-31 10:25:16 INFO: Loading: depparse\n",
      "2025-03-31 10:25:17 INFO: Loading: ner\n",
      "2025-03-31 10:25:20 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/judgeEnv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "import re\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx import Document\n",
    "\n",
    "from docx.table import _Cell, Table\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "# nlp = stanza.Pipeline('he')\n",
    "nlp = stanza.Pipeline('he', use_gpu=False)\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Modify property of Paragraph.text to include hyperlink text\n",
    "Paragraph.text = property(lambda self: get_paragraph_text(self))\n",
    "\n",
    "def get_paragraph_text(paragraph) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from paragraph, including hyperlink text.\n",
    "    \"\"\"\n",
    "    def get_xml_tag(element):\n",
    "        return \"%s:%s\" % (element.prefix, re.match(\"{.*}(.*)\", element.tag).group(1))\n",
    "\n",
    "    text_content = ''\n",
    "    run_count = 0\n",
    "    for child in paragraph._p:\n",
    "        tag = get_xml_tag(child)\n",
    "        if tag == \"w:r\":\n",
    "            text_content += paragraph.runs[run_count].text\n",
    "            run_count += 1\n",
    "        if tag == \"w:hyperlink\":\n",
    "            for sub_child in child:\n",
    "                if get_xml_tag(sub_child) == \"w:r\":\n",
    "                    text_content += sub_child.text\n",
    "    return text_content\n",
    "\n",
    "\n",
    "def is_paragraph_bold(block) -> bool:\n",
    "    if block.style and block.style.font:\n",
    "        if block.style.font.bold:  # Check if bold is part of the style\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_block_bold(block) -> bool:\n",
    "    # Check if the paragraph style indicates a bold style (e.g., \"כותרת\")\n",
    "    if block.style and block.style.name in [\"כותרת\", \"Heading\", \"Title\"]:  \n",
    "        return True\n",
    "\n",
    "    # Check if the style font is bold\n",
    "    if block.style and block.style.font and block.style.font.bold:\n",
    "        return True\n",
    "\n",
    "    # # Check if any run is bold\n",
    "    # if block.runs:\n",
    "    #     for run in block.runs:\n",
    "    #         if run.bold or (run.font and run.font.bold):\n",
    "    #             return True\n",
    "    return False\n",
    "def is_run_bold(run) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a run is bold, including inherited and complex script (cs_bold) styles.\n",
    "    \"\"\"\n",
    "    if run.bold is not None:\n",
    "        return run.bold\n",
    "    if run.font and run.font.bold is not None:\n",
    "        return run.font.bold\n",
    "    if run.font and run.font.cs_bold is not None:\n",
    "        return run.font.cs_bold  # Check for complex script bold\n",
    "    return False\n",
    "\n",
    "def is_block_styled(block) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the entire block/paragraph text is fully bold or fully underlined,\n",
    "    while handling:\n",
    "    - Allow the first run to differ in style if it is a prefix (e.g., 'א.', '1.', 'א)', '1)').\n",
    "    - Skip empty or non-alphanumeric runs.\n",
    "    - Allow trailing punctuation with different styling.\n",
    "    \"\"\"\n",
    "    if hasattr(block, \"runs\") and block.runs:\n",
    "        # Combine text from all meaningful runs\n",
    "        combined_text = \" \".join(run.text.strip() for run in block.runs if run.text.strip()).strip()\n",
    "        \n",
    "        # Handle empty text\n",
    "        if not combined_text:\n",
    "            return False\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(combined_text.split())\n",
    "        if word_count < 4:\n",
    "            print(combined_text)\n",
    "            return True  # Return True if there are fewer than 3 words\n",
    "\n",
    "\n",
    "        # Identify meaningful runs: Ignore runs that are empty or contain only spaces/non-alphanumeric characters\n",
    "        meaningful_runs = [run for run in block.runs if run.text.strip() and any(c.isalnum() for c in run.text)]\n",
    "\n",
    "        if not meaningful_runs:\n",
    "            return False\n",
    "\n",
    "        # Check if the first run is a prefix (e.g., \"א.\", \"1.\", \"א)\", \"1)\")\n",
    "        first_run_text = meaningful_runs[0].text.strip()\n",
    "        is_prefix = bool(re.match(r'^[\\u0590-\\u05FF]\\.|^[\\u0590-\\u05FF]\\)|^\\d+\\.|^\\d+\\)', first_run_text))\n",
    "\n",
    "        # Allow the first run to differ in style if it's a valid prefix\n",
    "        runs_to_check = meaningful_runs[1:] if is_prefix else meaningful_runs\n",
    "\n",
    "        # Check if all remaining runs are styled as bold or underlined\n",
    "        all_bold =is_block_bold(block) or all(is_run_bold(run) or run.text in [\":\", \".\", \",\"] for run in runs_to_check)\n",
    "        all_underlined = all(run.underline is True or run.text in [\":\", \".\", \",\"] for run in runs_to_check)\n",
    "\n",
    "        # Allow for trailing punctuation to differ in style\n",
    "        if combined_text[-1] in [\":\", \".\", \",\"]:\n",
    "            return all_bold or all_underlined\n",
    "        else:\n",
    "            return is_block_bold(block) or all(is_run_bold(run) or run.underline is True for run in runs_to_check)\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def iterate_block_items(parent):\n",
    "    \"\"\"\n",
    "    Iterate over paragraphs and tables in a document or cell.\n",
    "    \"\"\"\n",
    "    if hasattr(parent, \"element\") and hasattr(parent.element, \"body\"):\n",
    "        parent_element = parent.element.body\n",
    "    elif hasattr(parent, \"_tc\"):\n",
    "        parent_element = parent._tc\n",
    "    else:\n",
    "        print(f\"Unsupported parent type: {type(parent)}\")\n",
    "        return\n",
    "\n",
    "    for child in parent_element.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            table = Table(child, parent)\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    yield from iterate_block_items(cell)\n",
    "\n",
    "def extract_part_after_number_or_hebrew_letter(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text following a pattern of number or Hebrew letter.\n",
    "    \"\"\"\n",
    "    pattern = r'^(?:[0-9\\u05D0-\\u05EA]+)\\.\\s*(.*)'\n",
    "    match = re.search(pattern, sentence)\n",
    "    return match.group(1).strip() if match else sentence\n",
    "\n",
    "def count_patterns_in_block(block) -> int:\n",
    "    \"\"\"\n",
    "    Count the number-dot or dot-number patterns in a block.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s*(?:\\.\\d+|\\d+\\.)'\n",
    "    return len(re.findall(pattern, block.text))\n",
    "\n",
    "def count_consecutive_blocks_starting_with_number(blocks) -> int:\n",
    "    \"\"\"\n",
    "    Count consecutive blocks starting with a number or Hebrew letter.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for block in blocks:\n",
    "        if 'הנאשם' in block.text:\n",
    "            return 1\n",
    "        count += count_patterns_in_block(block)\n",
    "        if 'חקיקה שאוזכרה' in block.text:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def extract_name_after_word(text: str, word: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the words following a given word up to the end of the sentence.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(fr'{word}(?:,)?\\s*([\\u0590-\\u05FF\\s\\'\\(\\)-]+)')\n",
    "    match = pattern.search(text)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "def extract_violations(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract violations from the text based on a pre-defined pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = re.findall(r\"(?:סעיף|סעיפים|ס'|סע')\\s*\\d+\\s*(?:\\([\\s\\S]*?\\))?.*?(?=\\s*(?:ב|ל)(?:חוק|פקודת))\\s*(?:ב|ל)(?:חוק|פקודת)\\s*ה?(?:עונשין|כניסה לישראל|סמים\\s+המסוכנים|\\w+)?\", text)\n",
    "    # matches = re.findall(r\"(?:סעיף|סעיפים|ס'|סע')\\s*\\d+\\s*(?:\\([\\s\\S]*?\\))?.*?(?=\\s*(?:ב|ל)(?:חוק|פקודת))\\s*(?:ב|ל)(?:חוק|פקודת)\\s*ה?(?:עונשין|כניסה לישראל|סמים\\s+המסוכנים|[^\\[]+)?\", text)\n",
    "\n",
    "    matches = [match.strip() for match in matches]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RENAME DOCX TO VERICT NUMBER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RENAME DOCX TO VERICT NUMBER!!\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from docx import Document\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize spaces and special characters in Hebrew text\"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text).replace(\"\\u00A0\", \" \").strip()\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('/',\"∕\")).strip()\n",
    "\n",
    "# Citation patterns dictionary\n",
    "citation_patterns = {\n",
    "    'ע\"פ': r'ע\"פ\\s*(\\d+/\\d+)',\n",
    "    'ת\"פ': r'ת\"פ\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ\"ג': r'עפ\"ג\\s*(\\d+/\\d+)',\n",
    "}\n",
    "\n",
    "# Function to read the first 4 rows and extract the citation pattern\n",
    "def extract_citation_from_docx(docx_path):\n",
    "    # Open the docx file\n",
    "    doc = Document(docx_path)\n",
    "    \n",
    "    # Read the first 4 rows (paragraphs)\n",
    "    first_rows =\"\"\n",
    "    i=0\n",
    "    for block in iterate_block_items(doc):\n",
    "        if i==10:\n",
    "            break\n",
    "        print(block.text)\n",
    "        first_rows += normalize_text(block.text) + \" \"\n",
    "        i+=1\n",
    "    \n",
    "    # Try to match each citation pattern\n",
    "    for label, pattern in citation_patterns.items():\n",
    "        match = re.search(pattern, first_rows)\n",
    "        if match:\n",
    "            return match.group(0)\n",
    "    return None\n",
    "\n",
    "docx_dir ='/home/liorkob/thesis/lcp/data/docx_citations_2019'\n",
    "\n",
    "# Iterate through all .docx files in the directory\n",
    "for filename in os.listdir(docx_dir):\n",
    "    if filename.endswith('.docx'):\n",
    "        file_path = os.path.join(docx_dir, filename)\n",
    "        \n",
    "        # Extract citation from the first 4 rows\n",
    "        citation_name = extract_citation_from_docx(file_path)\n",
    "        \n",
    "        if citation_name:\n",
    "            # Create the new filename\n",
    "            print(\"citation_name:\",citation_name)\n",
    "            new_filename = normalize_case_name(citation_name) + '.docx'\n",
    "            new_file_path = os.path.join(docx_dir, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f'Renamed \"{filename}\" to \"{new_filename}\"')\n",
    "        else:\n",
    "            print(f'No citation found in \"{filename}\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract verdict from appeals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "# Citation patterns\n",
    "citation_patterns = {\n",
    "    'ת\"פ': r'ת\"פ\\s*(\\d+[-/]\\d+)',\n",
    "    'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+)',\n",
    "    'ע\"פ': r'ע\"פ\\s*(\\d+[-/]\\d+)',\n",
    "    'בתי\"פ': r'בתי\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "    'תי\"פ': r'תי\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "    'ת\\.פ\\.': r'ת\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "    'בת\\.פ\\.': r'בת\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "    'תיק': r'תיק\\s*(\\d+[-/]\\d+)',\n",
    "}\n",
    "\n",
    "# Normalize Hebrew text\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize spaces and special characters in Hebrew text.\"\"\"\n",
    "    return unicodedata.normalize(\"NFKC\", text).replace(\"\\u00A0\", \" \").strip()\n",
    "\n",
    "# Normalize case names\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('/', \"∕\")).strip()\n",
    "\n",
    "# Extract hyperlinks from DOCX\n",
    "def getLinkedText(soup):\n",
    "    links = []\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            parts = tag.text.split('\"')\n",
    "            if len(parts) > 1:  # Ensure the URL exists before accessing index 1\n",
    "                url = parts[1]\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: No valid URL found in HYPERLINK tag: {tag.text}\")\n",
    "                url = None  # Assign None if URL is missing\n",
    "\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None and maybe_text.text.strip() != \"\":\n",
    "                    text += maybe_text.text.strip()\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None and maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                    break\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "    return links\n",
    "def getURLs(soup, links):\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            for rel in soup.find_all(\"Relationship\"):\n",
    "                if rel[\"Id\"] == link[\"id\"]:\n",
    "                    link[\"href\"] = rel[\"Target\"]\n",
    "    return links\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_hyperlinks(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from a .docx file and returns a dictionary \n",
    "    where the linked text is mapped to its corresponding URL.\n",
    "    \"\"\"\n",
    "    # Open the .docx file as a zip archive\n",
    "    try:\n",
    "        archive = zipfile.ZipFile(docx_path, \"r\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ Error: Cannot open {docx_path} (Bad ZIP format)\")\n",
    "        return {}\n",
    "\n",
    "    # Extract main document XML\n",
    "    try:\n",
    "        file_data = archive.read(\"word/document.xml\")\n",
    "        doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "        linked_text = getLinkedText(doc_soup)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No document.xml found in {docx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Extract hyperlink relationships from _rels/document.xml.rels\n",
    "    try:\n",
    "        url_data = archive.read(\"word/_rels/document.xml.rels\")\n",
    "        url_soup = BeautifulSoup(url_data, \"xml\")\n",
    "        links_with_urls = getURLs(url_soup, linked_text)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No _rels/document.xml.rels found in {docx_path}\")\n",
    "        links_with_urls = linked_text\n",
    "\n",
    "    # Extract footnotes (if available)\n",
    "    try:\n",
    "        footnote_data = archive.read(\"word/footnotes.xml\")\n",
    "        footnote_soup = BeautifulSoup(footnote_data, \"xml\")\n",
    "        footnote_links = getLinkedText(footnote_soup)\n",
    "\n",
    "        footnote_url_data = archive.read(\"word/_rels/footnotes.xml.rels\")\n",
    "        footnote_url_soup = BeautifulSoup(footnote_url_data, \"xml\")\n",
    "        footnote_links_with_urls = getURLs(footnote_url_soup, footnote_links)\n",
    "\n",
    "        # Merge footnote links\n",
    "        links_with_urls += footnote_links_with_urls\n",
    "    except KeyError:\n",
    "        pass  # No footnotes found, continue\n",
    "\n",
    "    # Convert extracted links to a dictionary: {linked_text: URL}\n",
    "    return {link[\"text\"]: link.get(\"href\", None) for link in links_with_urls}\n",
    "\n",
    "# GPT-based verdict extraction\n",
    "def extract_verdict_with_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "Given the text of a legal appeal document, identify and extract **only** the referenced verdict that the appeal was filed against. \n",
    "\n",
    "- The verdict typically appears in sentences mentioning 'ערעור על' (appeal on) followed by a reference to a previous court decision.\n",
    "- **Return only the case reference** (e.g., ת\"פ 53715-12-15) without any additional text.\n",
    "- **If multiple verdicts are mentioned, return only the first valid one.**\n",
    "- If no referenced verdict is found, return \"No verdict found\".\n",
    "\n",
    "**Input Text:**\n",
    "{text}\n",
    "\n",
    "**Extracted verdict name (only the case reference):**\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract legal references accurately.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_verdict_from_appeal(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    first_rows = \"\"\n",
    "    i = 0\n",
    "    \n",
    "    for block in iterate_block_items(doc):\n",
    "        if i == 100 or \"בשם המערער\" in first_rows or \"בשם המערערת\" in first_rows:\n",
    "            break\n",
    "        \n",
    "        first_rows += normalize_text(block.text) + \" \"\n",
    "        i += 1\n",
    "    \n",
    "    verdict = extract_verdict_with_gpt(first_rows)\n",
    "    \n",
    "    print(\"ask gpt:\", first_rows)\n",
    "    print(\"gpt say:\", verdict)\n",
    "    \n",
    "    if \"בשם המערער\" not in first_rows and \"בשם המערערת\" not in first_rows :\n",
    "        print(docx_path,\"Rows without 'בשם המערער':\", first_rows)\n",
    "    \n",
    "    return verdict\n",
    "\n",
    "# Processing all DOCX files\n",
    "def process_docx_files(docx_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(docx_dir):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(docx_dir, filename)\n",
    "            appeal_case = os.path.splitext(filename)[0]\n",
    "\n",
    "            print(f\"Processing: {appeal_case}\")\n",
    "\n",
    "            # Extract referenced verdict\n",
    "            verdict = extract_verdict_from_appeal(file_path)\n",
    "            \n",
    "            # Extract URLs\n",
    "            links = extract_hyperlinks(file_path)\n",
    "            # print(\" Extracted Links:\")\n",
    "            # for key, value in links.items():\n",
    "            #     print(f\"{key} → {value}\")\n",
    "\n",
    "            url = links.get(verdict, \"\")\n",
    "            print(f\"Extracted verdict:\", verdict)\n",
    "            print(f\"Extracted url:\", url)\n",
    "\n",
    "\n",
    "            results.append({\n",
    "                \"verdict\": verdict,\n",
    "                \"appeal\": appeal_case,\n",
    "                \"url\": url\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame and save\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Results saved to {output_csv}\")\n",
    "\n",
    "# Run script\n",
    "years = [2018,2019,2020]\n",
    "for year in years:\n",
    "    docx_dir = f'/home/liorkob/thesis/lcp/data/docx_citations_{year}'\n",
    "    output_csv = f\"/home/liorkob/thesis/lcp/data/docx_citations_{year}/verdict_appeal.csv\"\n",
    "\n",
    "    process_docx_files(docx_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 10:26:11,505 - INFO - Processing directory: /home/liorkob/M.Sc/thesis/data/drugs/docx\n",
      "2025-03-31 10:26:11,507 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-04-31343-39.csv\n",
      "2025-03-31 10:26:11,508 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-21-03-10139-230.csv\n",
      "2025-03-31 10:26:11,509 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-03-29863-814.csv\n",
      "2025-03-31 10:26:11,510 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-20-12-1338-335.csv\n",
      "2025-03-31 10:26:11,511 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-21-11-35998-405.csv\n",
      "2025-03-31 10:26:11,511 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-01-47042-277.csv\n",
      "2025-03-31 10:26:11,513 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-21-12-42162-424.csv\n",
      "2025-03-31 10:26:11,514 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-01-45199-299.csv\n",
      "2025-03-31 10:26:11,515 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-03-60484-11.csv\n",
      "2025-03-31 10:26:11,516 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-03-60934-640.csv\n",
      "2025-03-31 10:26:11,517 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-03-60484-807.csv\n",
      "2025-03-31 10:26:11,518 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-04-7867-732.csv\n",
      "2025-03-31 10:26:11,519 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-05-48194-28.csv\n",
      "2025-03-31 10:26:11,520 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-06-54208-991.csv\n",
      "2025-03-31 10:26:11,520 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-08-15884-889.csv\n",
      "2025-03-31 10:26:11,522 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-08-15747-700.csv\n",
      "2025-03-31 10:26:11,522 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-07-53625-139.csv\n",
      "2025-03-31 10:26:11,523 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-07-32175-327.csv\n",
      "2025-03-31 10:26:11,524 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-11-1990-79.csv\n",
      "2025-03-31 10:26:11,525 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-01-46786-206.csv\n",
      "2025-03-31 10:26:11,526 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-10-36801-783.csv\n",
      "2025-03-31 10:26:11,527 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-01-53640-602.csv\n",
      "2025-03-31 10:26:11,528 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-22-12-47987-640.csv\n",
      "2025-03-31 10:26:11,529 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-05-49875-541.csv\n",
      "2025-03-31 10:26:11,530 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-01-65592-826.csv\n",
      "2025-03-31 10:26:11,531 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-05-13598-78.csv\n",
      "2025-03-31 10:26:11,532 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-04-8444-574.csv\n",
      "2025-03-31 10:26:11,533 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-08-41833-827.csv\n",
      "2025-03-31 10:26:11,534 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-05-52567-993.csv\n",
      "2025-03-31 10:26:11,535 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-06-15727-546.csv\n",
      "2025-03-31 10:26:11,537 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-11-21137-454.csv\n",
      "2025-03-31 10:26:11,537 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-23-12-14553-571.csv\n",
      "2025-03-31 10:26:11,538 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-24-07-35088-529.csv\n",
      "2025-03-31 10:26:11,539 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-16-12-19437-616.csv\n",
      "2025-03-31 10:26:11,540 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-02-58269-573.csv\n",
      "2025-03-31 10:26:11,541 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-06-45102-509.csv\n",
      "2025-03-31 10:26:11,542 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-24-07-35088-627.csv\n",
      "2025-03-31 10:26:11,543 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-05-41247-961.csv\n",
      "2025-03-31 10:26:11,544 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-17-10-12320-80.csv\n",
      "2025-03-31 10:26:11,545 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-11-6047-641.csv\n",
      "2025-03-31 10:26:11,546 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-06-12380-808.csv\n",
      "2025-03-31 10:26:11,547 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-02-48152-11.csv\n",
      "2025-03-31 10:26:11,548 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-12-65763-975.csv\n",
      "2025-03-31 10:26:11,548 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-06-57459-683.csv\n",
      "2025-03-31 10:26:11,549 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-07-8440-55.csv\n",
      "2025-03-31 10:26:11,550 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-05-28746-251.csv\n",
      "2025-03-31 10:26:11,551 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-08-24302-570.csv\n",
      "2025-03-31 10:26:11,551 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-09-43084-871.csv\n",
      "2025-03-31 10:26:11,553 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-02-39690-282.csv\n",
      "2025-03-31 10:26:11,555 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-20-10-29591-812.csv\n",
      "2025-03-31 10:26:11,559 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-02-40908-370.csv\n",
      "2025-03-31 10:26:11,561 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-03-10393-815.csv\n",
      "2025-03-31 10:26:11,562 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-03-56850-36.csv\n",
      "2025-03-31 10:26:11,563 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-03-7171-604.csv\n",
      "2025-03-31 10:26:11,563 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-09-42879-201.csv\n",
      "2025-03-31 10:26:11,564 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-04-3526-322.csv\n",
      "2025-03-31 10:26:11,565 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-08-36546-752.csv\n",
      "2025-03-31 10:26:11,567 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-06-986-611.csv\n",
      "2025-03-31 10:26:11,568 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-10-30884-544.csv\n",
      "2025-03-31 10:26:11,569 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-11-47780-86.csv\n",
      "2025-03-31 10:26:11,570 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-12-59578-330.csv\n",
      "2025-03-31 10:26:11,571 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-01-16014-223.csv\n",
      "2025-03-31 10:26:11,572 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-21-11-9851-17.csv\n",
      "2025-03-31 10:26:11,573 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-01-21142-171.csv\n",
      "2025-03-31 10:26:11,573 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-01-30506-413.csv\n",
      "2025-03-31 10:26:11,574 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-01-33898-590.csv\n",
      "2025-03-31 10:26:11,575 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-03-40650-599.csv\n",
      "2025-03-31 10:26:11,576 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-03-62924-190.csv\n",
      "2025-03-31 10:26:11,577 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-03-66877-310.csv\n",
      "2025-03-31 10:26:11,578 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-05-36033-112.csv\n",
      "2025-03-31 10:26:11,579 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-05-39259-133.csv\n",
      "2025-03-31 10:26:11,580 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-03-66922-347.csv\n",
      "2025-03-31 10:26:11,581 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-08-1062-886.csv\n",
      "2025-03-31 10:26:11,582 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-11-37606-732.csv\n",
      "2025-03-31 10:26:11,583 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-08-21724-809.csv\n",
      "2025-03-31 10:26:11,584 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-11-64232-785.csv\n",
      "2025-03-31 10:26:11,584 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-09-6749-871.csv\n",
      "2025-03-31 10:26:11,585 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-22-12-20664-221.csv\n",
      "2025-03-31 10:26:11,586 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-01-18076-300.csv\n",
      "2025-03-31 10:26:11,587 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-01-30353-140.csv\n",
      "2025-03-31 10:26:11,588 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-02-21629-964.csv\n",
      "2025-03-31 10:26:11,589 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-02-41988-839.csv\n",
      "2025-03-31 10:26:11,590 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-02-4621-383.csv\n",
      "2025-03-31 10:26:11,591 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-02-57179-8.csv\n",
      "2025-03-31 10:26:11,592 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-04-9457-33.csv\n",
      "2025-03-31 10:26:11,593 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-02-67009-724.csv\n",
      "2025-03-31 10:26:11,594 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-03-20257-157.csv\n",
      "2025-03-31 10:26:11,595 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-04-7086-117.csv\n",
      "2025-03-31 10:26:11,596 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-04-9457-915.csv\n",
      "2025-03-31 10:26:11,596 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-05-72468-401.csv\n",
      "2025-03-31 10:26:11,597 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-06-28122-243.csv\n",
      "2025-03-31 10:26:11,598 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-07-61816-773.csv\n",
      "2025-03-31 10:26:11,599 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-08-30009-79.csv\n",
      "2025-03-31 10:26:11,600 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-09-35299-260.csv\n",
      "2025-03-31 10:26:11,601 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-10-13373-857.csv\n",
      "2025-03-31 10:26:11,602 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-11-49510-943.csv\n",
      "2025-03-31 10:26:11,604 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-11-8306-514.csv\n",
      "2025-03-31 10:26:11,605 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-24-01-16800-726.csv\n",
      "2025-03-31 10:26:11,606 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-12-13704-479.csv\n",
      "2025-03-31 10:26:11,607 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-23-12-26004-465.csv\n",
      "2025-03-31 10:26:11,608 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-24-01-26596-264.csv\n",
      "2025-03-31 10:26:11,609 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-24-01-3818-394.csv\n",
      "2025-03-31 10:26:11,610 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-24-01-9804-702.csv\n",
      "2025-03-31 10:26:11,611 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-24-05-46424-585.csv\n",
      "2025-03-31 10:26:11,611 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-24-05-28565-553.csv\n",
      "2025-03-31 10:26:11,612 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/m03000037-875.csv\n",
      "2025-03-31 10:26:11,613 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/m08000207-175.csv\n",
      "2025-03-31 10:26:11,614 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-10-07-2892-123.csv\n",
      "2025-03-31 10:26:11,615 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-10-11-18588-3.csv\n",
      "2025-03-31 10:26:11,616 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-13-06-56235-154.csv\n",
      "2025-03-31 10:26:11,617 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-13-10-737-239.csv\n",
      "2025-03-31 10:26:11,618 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-01-16958-980.csv\n",
      "2025-03-31 10:26:11,619 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-01-17778-22.csv\n",
      "2025-03-31 10:26:11,620 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-07-9343-44.csv\n",
      "2025-03-31 10:26:11,621 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-07-44226-55.csv\n",
      "2025-03-31 10:26:11,622 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-09-23142-592.csv\n",
      "2025-03-31 10:26:11,623 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-09-54272-333.csv\n",
      "2025-03-31 10:26:11,624 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-14-10-18510-163.csv\n",
      "2025-03-31 10:26:11,625 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-15-01-33508-80.csv\n",
      "2025-03-31 10:26:11,626 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-15-03-55487-480.csv\n",
      "2025-03-31 10:26:11,626 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-15-09-24514-351.csv\n",
      "2025-03-31 10:26:11,627 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-15-11-31122-886.csv\n",
      "2025-03-31 10:26:11,629 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-15-11-46896-519.csv\n",
      "2025-03-31 10:26:11,630 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-01-37551-462.csv\n",
      "2025-03-31 10:26:11,631 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-02-9109-304.csv\n",
      "2025-03-31 10:26:11,632 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-06-6788-21.csv\n",
      "2025-03-31 10:26:11,633 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-06-52031-406.csv\n",
      "2025-03-31 10:26:11,634 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-07-11608-225.csv\n",
      "2025-03-31 10:26:11,635 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-07-11720-448.csv\n",
      "2025-03-31 10:26:11,636 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-09-22679-134.csv\n",
      "2025-03-31 10:26:11,637 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-09-57384-189.csv\n",
      "2025-03-31 10:26:11,638 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-10-22609-304.csv\n",
      "2025-03-31 10:26:11,639 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-11-58033-22.csv\n",
      "2025-03-31 10:26:11,640 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-11-63255-338.csv\n",
      "2025-03-31 10:26:11,641 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-12-701-772.csv\n",
      "2025-03-31 10:26:11,643 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-12-19902-389.csv\n",
      "2025-03-31 10:26:11,644 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-16-12-26620-433.csv\n",
      "2025-03-31 10:26:11,645 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-01-620-279.csv\n",
      "2025-03-31 10:26:11,647 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-01-59971-328.csv\n",
      "2025-03-31 10:26:11,648 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-02-51451-22.csv\n",
      "2025-03-31 10:26:11,649 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-03-30876-747.csv\n",
      "2025-03-31 10:26:11,650 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-04-17677-800.csv\n",
      "2025-03-31 10:26:11,651 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-09-40239-862.csv\n",
      "2025-03-31 10:26:11,652 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-11-7629-317.csv\n",
      "2025-03-31 10:26:11,653 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-17-11-37176-540.csv\n",
      "2025-03-31 10:26:11,654 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-01-73652-357.csv\n",
      "2025-03-31 10:26:11,655 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-02-5680-55.csv\n",
      "2025-03-31 10:26:11,656 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-02-5775-459.csv\n",
      "2025-03-31 10:26:11,657 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-02-58507-317.csv\n",
      "2025-03-31 10:26:11,658 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-04-16826-842.csv\n",
      "2025-03-31 10:26:11,658 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-04-46786-521.csv\n",
      "2025-03-31 10:26:11,660 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-05-39201-55.csv\n",
      "2025-03-31 10:26:11,661 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-05-69278-836.csv\n",
      "2025-03-31 10:26:11,661 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-06-6997-910.csv\n",
      "2025-03-31 10:26:11,663 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-07-67342-121.csv\n",
      "2025-03-31 10:26:11,664 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-08-8864-290.csv\n",
      "2025-03-31 10:26:11,665 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-08-58591-978.csv\n",
      "2025-03-31 10:26:11,666 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-09-809-412.csv\n",
      "2025-03-31 10:26:11,667 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-10-39573-636.csv\n",
      "2025-03-31 10:26:11,668 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-18-11-42681-44.csv\n",
      "2025-03-31 10:26:11,669 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-01-68942-383.csv\n",
      "2025-03-31 10:26:11,670 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-01-68958-71.csv\n",
      "2025-03-31 10:26:11,671 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-02-45480-398.csv\n",
      "2025-03-31 10:26:11,672 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-02-70401-748.csv\n",
      "2025-03-31 10:26:11,673 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-04-14488-104.csv\n",
      "2025-03-31 10:26:11,674 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-04-14578-148.csv\n",
      "2025-03-31 10:26:11,674 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-19-07-4435-107.csv\n",
      "2025-03-31 10:26:11,675 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-20-02-33338-33.csv\n",
      "2025-03-31 10:26:11,676 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-20-07-66437-927.csv\n",
      "2025-03-31 10:26:11,677 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-20-09-5778-327.csv\n",
      "2025-03-31 10:26:11,678 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/ME-20-12-6678-48.csv\n",
      "2025-03-31 10:26:11,679 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-13-09-29504-694.csv\n",
      "2025-03-31 10:26:11,680 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-14-04-19985-55.csv\n",
      "2025-03-31 10:26:11,681 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-15-05-1276-44.csv\n",
      "2025-03-31 10:26:11,681 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-15-05-13336-101.csv\n",
      "2025-03-31 10:26:11,682 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-16-09-64418-33.csv\n",
      "2025-03-31 10:26:11,683 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-17-03-34607-993.csv\n",
      "2025-03-31 10:26:11,684 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-17-07-16759-919.csv\n",
      "2025-03-31 10:26:11,685 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-03-50607-561.csv\n",
      "2025-03-31 10:26:11,686 - INFO - Output already exists, skipping: /home/liorkob/M.Sc/thesis/data/drugs/docx_csv/SH-19-11-18218-11.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "###PRE-PROCESS\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import stanza\n",
    "# from docx import Document\n",
    "import logging\n",
    "\n",
    "\"\"\"\n",
    "This script processes `.docx` verdict files, extracting text from them, identifying and classifying specific sections of the document. It saves the results into CSV files with the extracted text, sections, and metadata for further analysis.\n",
    "\n",
    "### Key Functionalities:\n",
    "\n",
    "1. **Text Extraction and Preprocessing**:\n",
    "   - The script iterates through paragraphs in `.docx` files, using custom functions from `utils.py` to identify specific sections based on formatting (e.g., bold text).\n",
    "   - The extracted sections are stored in a dictionary along with the corresponding full sentences from the document.\n",
    "\n",
    "2. **Part Identification**:\n",
    "   - It processes bolded blocks of text as distinct \"parts\" or sections (e.g., titles or key sections) and appends them to a list.\n",
    "   - For each sentence, the script associates it with both the most recent part (stored as `part_single`) and a concatenation of all previous parts (stored as `part_concatenated`).\n",
    "\n",
    "3. **NLP Processing**:\n",
    "   - The Hebrew Stanza NLP pipeline is used to split the text into sentences, which are then stored in the output alongside the associated document sections.\n",
    "   - The script also applies filters to skip short paragraphs and unwanted patterns (e.g., references to certain case types).\n",
    "\n",
    "4. **Error Handling and Logging**:\n",
    "   - The script uses Python’s `logging` module to provide informative logs, including handling errors if a document can't be opened or processed.\n",
    "   - It catches and logs any exceptions during the processing of files.\n",
    "\n",
    "5. **CSV Output**:\n",
    "   - For each `.docx` file, the extracted data (including text, section titles, and concatenated sections) is saved to a CSV file.\n",
    "\n",
    "6. **Recursive Directory Processing**:\n",
    "   - The script recursively processes `.docx` files in a specified root directory (`selenium_downloads\\מרב גרינברג`), saving the results for each file in a corresponding output directory (`outputs\\merav_grinberg_preproccsed`).\n",
    "\n",
    "### Main Functions:\n",
    "\n",
    "- **doc_to_csv(doc_path: str, result_path: str)**:\n",
    "   - Processes a single `.docx` file, extracting text and metadata.\n",
    "   - Saves the results to a CSV file if a result path is provided.\n",
    "\n",
    "- **run()**:\n",
    "   - Iterates through all `.docx` files in the root directory.\n",
    "   - For each file, it calls `doc_to_csv` and saves the resulting DataFrame as a CSV.\n",
    "\n",
    "### Usage:\n",
    "The script is executed via the `run()` function, which processes all files in the specified directory. It logs the status and outputs CSV files containing preprocessed data for each document.\n",
    "\"\"\"\n",
    "number_pattern = re.compile(r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "''', re.VERBOSE)\n",
    "\n",
    "def should_split_sentence(sentence: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether a sentence should be split.\n",
    "    - A sentence should NOT be split if it contains a citation (matches `number_pattern`).\n",
    "    \"\"\"\n",
    "    return not number_pattern.search(sentence)\n",
    "\n",
    "def validate_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        print(\"The file is valid.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating document: {e}\")\n",
    "        return False\n",
    "    \n",
    "def docToCsv(doc_path: str = None):\n",
    "    \"\"\"\n",
    "    Converts a DOCX document to a CSV format by extracting relevant parts of the document \n",
    "    based on specified conditions like block boldness or specific patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - doc_path (str, optional): The path to the DOCX document. Defaults to None.\n",
    "\n",
    "    Steps:\n",
    "    1. Initialize data dictionary to hold extracted content.\n",
    "    2. Open and iterate through the provided DOCX document.\n",
    "    3. Filter out unnecessary blocks.\n",
    "    4. Determine if the current block is a title or content.\n",
    "    5. If it's content, tokenize it using the Stanza library.\n",
    "    6. Add the extracted content to the data dictionary.\n",
    "    7. Convert the data dictionary to a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A Pandas DataFrame containing the extracted text from the DOCX document with columns 'text' and 'part'.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {'verdict': [],'text': [], 'part': []}\n",
    "    data['verdict']=os.path.splitext(os.path.basename(doc_path))[0]\n",
    "    doc = Document(doc_path)\n",
    "    part = 'nothing' \n",
    "\n",
    "    # for paragraph in doc.paragraphs:\n",
    "\n",
    "\n",
    "    for block in iterate_block_items(doc): # Updated usage\n",
    "        flag = False\n",
    "\n",
    "        # if len(block.text) <= 1 or 'ע\"פ' in block.text or 'ת\"פ' in block.text or 'עפ\"ג' in block.text:\n",
    "        #     continue\n",
    "        # if   \"מחמד כנעאנה\" in block.text :\n",
    "        #     i=0\n",
    "# and not re.match(r'^\\d', block.text) and not re.match(r'[\\u0590-\\u05FF][^.)*]*[.)]', block.text)\n",
    "        if is_block_styled(block) and len(block.text.split(' ')) < 10:\n",
    "            # התאמה לתחילת כותרת - מספר או אות בעברית עם נקודה/סוגריים\n",
    "            if re.match(r'^(?:\\d+[.)]|[\\u0590-\\u05FF][.)])', block.text):\n",
    "                # הסר את החלק התואם מהתחלה\n",
    "                part = re.sub(r'^(?:\\d+[.)]|[\\u0590-\\u05FF][.)])', '', block.text).strip()\n",
    "            else:\n",
    "                # אם לא עונה לתנאים, העתק את הטקסט כפי שהוא\n",
    "                part = block.text\n",
    "        else:\n",
    "            extracted_part_text = extract_part_after_number_or_hebrew_letter(block.text)\n",
    "            \n",
    "            # Preserve paragraph integrity while handling sentence splitting\n",
    "            if len(extracted_part_text.split()) < 10 or not should_split_sentence(extracted_part_text):\n",
    "                text = extracted_part_text  # Keep paragraph as-is if it's short or contains a citation\n",
    "            else:\n",
    "                sentences = nlp(extracted_part_text)\n",
    "                filtered_sentences = []\n",
    "                temp_sentence = \"\"\n",
    "\n",
    "                # Reconstruct text while avoiding citation splits and handling quotes\n",
    "                for sentence in sentences.sentences:\n",
    "                    text = sentence.text.strip()\n",
    "\n",
    "                    # Handle quotation blocks\n",
    "                    if text.startswith('\"'):\n",
    "                        flag = True\n",
    "                        continue\n",
    "                    if text.endswith('\".') or text.endswith('\"'):\n",
    "                        flag = False\n",
    "                        continue\n",
    "                    if flag:\n",
    "                        continue\n",
    "\n",
    "                    # Skip text if it matches the section title (part)\n",
    "                    if text == part:\n",
    "                        continue\n",
    "\n",
    "                    # Merge sentences to prevent citation splits\n",
    "                    if should_split_sentence(text):\n",
    "                        if temp_sentence:\n",
    "                            filtered_sentences.append(temp_sentence.strip())\n",
    "                            temp_sentence = \"\"\n",
    "                        filtered_sentences.append(text)\n",
    "                    else:\n",
    "                        temp_sentence += \" \" + text  # Merge citation sentence to previous\n",
    "\n",
    "                if temp_sentence:\n",
    "                    filtered_sentences.append(temp_sentence.strip())\n",
    "\n",
    "                text = \" \".join(filtered_sentences)  # Keep full paragraph if needed\n",
    "\n",
    "            if text.strip():  # Avoid empty lines\n",
    "                data['text'].append(text)\n",
    "                data['part'].append(part)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def run():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    root_directory = \"/home/liorkob/M.Sc/thesis/data/drugs/docx\"\n",
    "    output_dir = \"/home/liorkob/M.Sc/thesis/data/drugs/docx_csv\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(root_directory):\n",
    "        logging.info(f\"Processing directory: {root}\")\n",
    "        for file in files:\n",
    "            if not file.lower().endswith('.docx'):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_path = os.path.join(output_dir, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                logging.info(f\"Output already exists, skipping: {output_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = docToCsv(doc_path=input_path)\n",
    "                df.to_csv(output_path, index=False)\n",
    "                logging.info(f\"Processed and saved: {output_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def verify_verdict_parts_from_csv(output_directory, required_parts):\n",
    "    \"\"\"\n",
    "    Verifies the presence of required parts in each CSV file generated from verdict processing.\n",
    "\n",
    "    Parameters:\n",
    "    - output_directory (str): Directory containing the output CSV files.\n",
    "    - required_parts (list): List of strings representing the required parts.\n",
    "\n",
    "    Output:\n",
    "    - Prints the parts for each verdict.\n",
    "    - Identifies and lists verdicts where none of the required parts are found.\n",
    "    \"\"\"\n",
    "    verdicts_with_no_parts = []  # Store verdicts where none of the parts exist\n",
    "\n",
    "    for file in os.listdir(output_directory):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(output_directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        verdict_name = os.path.splitext(file)[0]\n",
    "        \n",
    "        print(f\"Verifying Verdict: {verdict_name}\")\n",
    "        \n",
    "        # Extract unique parts from the DataFrame\n",
    "        verdict_parts = df['part'].dropna().astype(str).unique()  # Ensure all parts are strings\n",
    "        \n",
    "        # Print all parts for the verdict\n",
    "        print(\"  Parts in the verdict:\")\n",
    "        for part in verdict_parts:\n",
    "            print(f\"    - {part}\")\n",
    "        \n",
    "        # Check if none of the required parts exist\n",
    "        if not any(any(required in part for part in verdict_parts) for required in required_parts):\n",
    "            verdicts_with_no_parts.append((verdict_name,verdict_parts))  # Add to the list of problematic verdicts\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Print verdicts with no matching parts\n",
    "    if verdicts_with_no_parts:\n",
    "        print(\"Verdicts with no matching parts:\")\n",
    "        for verdict,parts in verdicts_with_no_parts:\n",
    "            print(f\"  - {verdict}\")\n",
    "            print(f\"parts: {parts}\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"All verdicts have at least one matching part.\")\n",
    "\n",
    "# Define the directory containing the output CSV files\n",
    "output_directory = \"/home/liorkob/thesis/lcp/data/docx_csv_2020\"\n",
    "\n",
    "# Define the required parts (partial matching supported)\n",
    "required_parts = [\"אחידות בענישה\",\"מתחם הענישה\",\"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\", \"הענישה הנוהגת\",\"ענישה נוהגת\", \"מתחם העונש\" ,\"מתחם עונש\",\"מדיניות הענישה\" \"והכרעה\", \"ההרשעה\",\"מדיניות הענישה הנהוגה\"]\n",
    "# required_parts=[\"הכרעת הדין\", \"אישום\" ,\"רקע\" ,\"כללי\" ,\"כתב אישום\",\"כתב האישום\"]\n",
    "# Run the verification\n",
    "verify_verdict_parts_from_csv(output_directory, required_parts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judgeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
