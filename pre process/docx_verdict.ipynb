{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "print(docx.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT DOC TO DOCX!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x LibreOffice_7.6.4_Linux_x86-64.AppImage\n",
    "!./LibreOffice_7.6.4_Linux_x86-64.AppImage --headless --convert-to docx your_file.doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load libreoffice\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVERT DOC TO DOCX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CONVERT DOC TO DOCX!!!\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def force_quit_libreoffice():\n",
    "    \"\"\"Force quit LibreOffice by killing all soffice processes.\"\"\"\n",
    "    try:\n",
    "        # Kill all LibreOffice (soffice) processes\n",
    "        subprocess.run([\"pkill\", \"-f\", \"soffice\"], check=True)\n",
    "        print(\"Force quit LibreOffice successfully.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"No LibreOffice processes were found to quit.\")\n",
    "\n",
    "def convert_doc_to_docx(input_path, output_path):\n",
    "    print(f\"Starting conversion of {input_path} to {output_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Run the conversion command and capture output and errors\n",
    "        result = subprocess.run(\n",
    "            ['unoconv', '-v', '-f', 'docx', '-o', output_path, input_path],\n",
    "            stdout=subprocess.PIPE,  # Capture standard output\n",
    "            stderr=subprocess.PIPE,  # Capture standard error\n",
    "            timeout=600  # Timeout after 10 minutes (adjust as needed)\n",
    "        )\n",
    "\n",
    "        # Check if the command was successful (exit code 0)\n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error converting {input_path}: {result.stderr.decode()}\")\n",
    "        else:\n",
    "            print(f\"Successfully converted {input_path} to {output_path}\")\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"Conversion of {input_path} timed out after 10 minutes\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while converting {input_path}: {e}\")\n",
    "\n",
    "    # Force quit LibreOffice to free up resources after each conversion\n",
    "    force_quit_libreoffice()\n",
    "\n",
    "def convert_all_docs_in_dir(directory_path):\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.doc'):\n",
    "            input_file = os.path.join(directory_path, filename)\n",
    "            output_file = os.path.join(directory_path, f\"{os.path.splitext(filename)[0]}.docx\")\n",
    "\n",
    "            # Convert the file and save it with the new .docx extension\n",
    "            convert_doc_to_docx(input_file, output_file)\n",
    "\n",
    "            # Optional: Delete the original .doc file after conversion\n",
    "            os.remove(input_file)\n",
    "\n",
    "            print(f\"Converted {filename} to {os.path.basename(output_file)}\")\n",
    "\n",
    "# Example usage:\n",
    "doc_directory ='/home/liorkob/M.Sc/thesis/data/drugs/drugs doc'\n",
    "convert_all_docs_in_dir(doc_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install stanza\n",
    "# import stanza\n",
    "# stanza.download('he') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "# nlp = stanza.Pipeline('he', use_gpu=False)\n",
    "nlp = stanza.Pipeline('he', processors='tokenize', use_gpu=True)\n",
    "\n",
    "import re\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx import Document\n",
    "\n",
    "from docx.table import _Cell, Table\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Modify property of Paragraph.text to include hyperlink text\n",
    "Paragraph.text = property(lambda self: get_paragraph_text(self))\n",
    "\n",
    "def get_paragraph_text(paragraph) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from paragraph, including hyperlink text.\n",
    "    \"\"\"\n",
    "    def get_xml_tag(element):\n",
    "        return \"%s:%s\" % (element.prefix, re.match(\"{.*}(.*)\", element.tag).group(1))\n",
    "\n",
    "    text_content = ''\n",
    "    run_count = 0\n",
    "    for child in paragraph._p:\n",
    "        tag = get_xml_tag(child)\n",
    "        if tag == \"w:r\":\n",
    "            text_content += paragraph.runs[run_count].text\n",
    "            run_count += 1\n",
    "        if tag == \"w:hyperlink\":\n",
    "            for sub_child in child:\n",
    "                if get_xml_tag(sub_child) == \"w:r\":\n",
    "                    text_content += sub_child.text\n",
    "    return text_content\n",
    "\n",
    "\n",
    "def is_paragraph_bold(block) -> bool:\n",
    "    if block.style and block.style.font:\n",
    "        if block.style.font.bold:  # Check if bold is part of the style\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_block_bold(block) -> bool:\n",
    "    # Check if the paragraph style indicates a bold style (e.g., \"כותרת\")\n",
    "    if block.style and block.style.name in [\"כותרת\", \"Heading\", \"Title\"]:  \n",
    "        return True\n",
    "\n",
    "    # Check if the style font is bold\n",
    "    if block.style and block.style.font and block.style.font.bold:\n",
    "        return True\n",
    "\n",
    "    # # Check if any run is bold\n",
    "    # if block.runs:\n",
    "    #     for run in block.runs:\n",
    "    #         if run.bold or (run.font and run.font.bold):\n",
    "    #             return True\n",
    "    return False\n",
    "def is_run_bold(run) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a run is bold, including inherited and complex script (cs_bold) styles.\n",
    "    \"\"\"\n",
    "    if run.bold is not None:\n",
    "        return run.bold\n",
    "    if run.font and run.font.bold is not None:\n",
    "        return run.font.bold\n",
    "    if run.font and run.font.cs_bold is not None:\n",
    "        return run.font.cs_bold  # Check for complex script bold\n",
    "    return False\n",
    "\n",
    "def is_block_styled(block) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the entire block/paragraph text is fully bold or fully underlined,\n",
    "    while handling:\n",
    "    - Allow the first run to differ in style if it is a prefix (e.g., 'א.', '1.', 'א)', '1)').\n",
    "    - Skip empty or non-alphanumeric runs.\n",
    "    - Allow trailing punctuation with different styling.\n",
    "    \"\"\"\n",
    "    if hasattr(block, \"runs\") and block.runs:\n",
    "        # Combine text from all meaningful runs\n",
    "        combined_text = \" \".join(run.text.strip() for run in block.runs if run.text.strip()).strip()\n",
    "        \n",
    "        # Handle empty text\n",
    "        if not combined_text:\n",
    "            return False\n",
    "        \n",
    "        # Check word count\n",
    "        word_count = len(combined_text.split())\n",
    "        if word_count < 4:\n",
    "            # print(combined_text)\n",
    "            return True  # Return True if there are fewer than 3 words\n",
    "\n",
    "\n",
    "        # Identify meaningful runs: Ignore runs that are empty or contain only spaces/non-alphanumeric characters\n",
    "        meaningful_runs = [run for run in block.runs if run.text.strip() and any(c.isalnum() for c in run.text)]\n",
    "\n",
    "        if not meaningful_runs:\n",
    "            return False\n",
    "\n",
    "        # Check if the first run is a prefix (e.g., \"א.\", \"1.\", \"א)\", \"1)\")\n",
    "        first_run_text = meaningful_runs[0].text.strip()\n",
    "        is_prefix = bool(re.match(r'^[\\u0590-\\u05FF]\\.|^[\\u0590-\\u05FF]\\)|^\\d+\\.|^\\d+\\)', first_run_text))\n",
    "\n",
    "        # Allow the first run to differ in style if it's a valid prefix\n",
    "        runs_to_check = meaningful_runs[1:] if is_prefix else meaningful_runs\n",
    "\n",
    "        # Check if all remaining runs are styled as bold or underlined\n",
    "        all_bold =is_block_bold(block) or all(is_run_bold(run) or run.text in [\":\", \".\", \",\"] for run in runs_to_check)\n",
    "        all_underlined = all(run.underline is True or run.text in [\":\", \".\", \",\"] for run in runs_to_check)\n",
    "\n",
    "        # Allow for trailing punctuation to differ in style\n",
    "        if combined_text[-1] in [\":\", \".\", \",\"]:\n",
    "            return all_bold or all_underlined\n",
    "        else:\n",
    "            return is_block_bold(block) or all(is_run_bold(run) or run.underline is True for run in runs_to_check)\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def iterate_block_items(parent):\n",
    "    \"\"\"\n",
    "    Iterate over paragraphs and tables in a document or cell.\n",
    "    \"\"\"\n",
    "    if hasattr(parent, \"element\") and hasattr(parent.element, \"body\"):\n",
    "        parent_element = parent.element.body\n",
    "    elif hasattr(parent, \"_tc\"):\n",
    "        parent_element = parent._tc\n",
    "    else:\n",
    "        print(f\"Unsupported parent type: {type(parent)}\")\n",
    "        return\n",
    "\n",
    "    for child in parent_element.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            yield Paragraph(child, parent)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            table = Table(child, parent)\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "                    yield from iterate_block_items(cell)\n",
    "\n",
    "def extract_part_after_number_or_hebrew_letter(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text following a pattern of number or Hebrew letter.\n",
    "    \"\"\"\n",
    "    pattern = r'^(?:[0-9\\u05D0-\\u05EA]+)\\.\\s*(.*)'\n",
    "    match = re.search(pattern, sentence)\n",
    "    return match.group(1).strip() if match else sentence\n",
    "\n",
    "def count_patterns_in_block(block) -> int:\n",
    "    \"\"\"\n",
    "    Count the number-dot or dot-number patterns in a block.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s*(?:\\.\\d+|\\d+\\.)'\n",
    "    return len(re.findall(pattern, block.text))\n",
    "\n",
    "def count_consecutive_blocks_starting_with_number(blocks) -> int:\n",
    "    \"\"\"\n",
    "    Count consecutive blocks starting with a number or Hebrew letter.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for block in blocks:\n",
    "        if 'הנאשם' in block.text:\n",
    "            return 1\n",
    "        count += count_patterns_in_block(block)\n",
    "        if 'חקיקה שאוזכרה' in block.text:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def extract_name_after_word(text: str, word: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the words following a given word up to the end of the sentence.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(fr'{word}(?:,)?\\s*([\\u0590-\\u05FF\\s\\'\\(\\)-]+)')\n",
    "    match = pattern.search(text)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "def extract_violations(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract violations from the text based on a pre-defined pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = re.findall(r\"(?:סעיף|סעיפים|ס'|סע')\\s*\\d+\\s*(?:\\([\\s\\S]*?\\))?.*?(?=\\s*(?:ב|ל)(?:חוק|פקודת))\\s*(?:ב|ל)(?:חוק|פקודת)\\s*ה?(?:עונשין|כניסה לישראל|סמים\\s+המסוכנים|\\w+)?\", text)\n",
    "    # matches = re.findall(r\"(?:סעיף|סעיפים|ס'|סע')\\s*\\d+\\s*(?:\\([\\s\\S]*?\\))?.*?(?=\\s*(?:ב|ל)(?:חוק|פקודת))\\s*(?:ב|ל)(?:חוק|פקודת)\\s*ה?(?:עונשין|כניסה לישראל|סמים\\s+המסוכנים|[^\\[]+)?\", text)\n",
    "\n",
    "    matches = [match.strip() for match in matches]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RENAME DOCX TO VERICT NUMBER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# List of legal acronyms (same as yours)\n",
    "acronyms = [\n",
    "    \"אב\", \"אבע\", \"אימוצ\", \"אמצ\", \"אפ\", \"אפח\", \"את\", \"אתפ\", \"באפ\", \"באש\", \"בבנ\", \"בגצ\", \"בדא\", \"בדמ\",\n",
    "    \"בדמש\", \"בהנ\", \"בהע\", \"בהש\", \"בידמ\", \"בידע\", \"בל\", \"בלמ\", \"במ\", \"בעא\", \"בעח\", \"בעמ\", \"בעק\", \"בפ\",\n",
    "    \"בפמ\", \"בפת\", \"בצא\", \"בצהמ\", \"בק\", \"בקמ\", \"בקשה\", \"ברמ\", \"ברע\", \"ברעפ\", \"ברש\", \"בש\", \"בשא\",\n",
    "    \"בשגצ\", \"בשהת\", \"בשז\", \"בשמ\", \"בשע\", \"בשפ\", \"בתת\", \"גזז\", \"גמר\", \"גפ\", \"דבע\", \"דח\", \"דט\", \"דיונ\",\n",
    "    \"דמ\", \"דמר\", \"דמש\", \"דנ\", \"דנא\", \"דנגצ\", \"דנמ\", \"דנפ\", \"הד\", \"הדפ\", \"הוצלפ\", \"הט\", \"הכ\", \"המ\",\n",
    "    \"המד\", \"הממ\", \"המע\", \"המש\", \"הנ\", \"הסת\", \"הע\", \"העז\", \"הפ\", \"הפב\", \"הפמ\", \"הצמ\", \"הש\", \"השא\",\n",
    "    \"השגצ\", \"השפ\", \"השר\", \"הת\", \"וחק\", \"וע\", \"ושמ\", \"ושק\", \"ושר\", \"זי\", \"חא\", \"חבר\", \"חד\", \"חדא\",\n",
    "    \"חדלפ\", \"חדלת\", \"חדמ\", \"חדפ\", \"חהע\", \"חי\", \"חנ\", \"חסמ\", \"חעמ\", \"חעק\", \"חש\", \"יוש\", \"ייתא\", \"ימא\",\n",
    "    \"יס\", \"כצ\", \"מ\", \"מא\", \"מבכ\", \"מבס\", \"מונופולינ\", \"מזג\", \"מח\", \"מחוז\", \"מחע\", \"מט\", \"מטכל\", \"מי\",\n",
    "    \"מיב\", \"מכ\", \"ממ\", \"מס\", \"מסט\", \"מעי\", \"מעת\", \"מקמ\", \"מרכז\", \"מת\", \"נ\", \"נב\", \"נבא\", \"נמ\", \"נמב\",\n",
    "    \"נעד\", \"נער\", \"סבא\", \"סע\", \"סעש\", \"סק\", \"סקכ\", \"ע\", \"עא\", \"עאח\", \"עאפ\", \"עב\", \"עבאפ\", \"עבז\", \"עבח\",\n",
    "    \"עבי\", \"עבל\", \"עבמצ\", \"עבעח\", \"עבפ\", \"עבר\", \"עבשהת\", \"עגר\", \"עדי\", \"עדמ\", \"עהג\", \"עהס\", \"עהפ\",\n",
    "    \"עו\", \"עורפ\", \"עז\", \"עח\", \"עחא\", \"עחדלפ\", \"עחדפ\", \"עחדת\", \"עחהס\", \"עחע\", \"עחק\", \"עחר\", \"עכב\",\n",
    "    \"על\", \"עלא\", \"עלבש\", \"עלח\", \"עלע\", \"עמ\", \"עמא\", \"עמה\", \"עמז\", \"עמח\", \"עמי\", \"עמלע\", \"עממ\", \"עמנ\",\n",
    "    \"עמפ\", \"עמצ\", \"עמק\", \"עמרמ\", \"עמש\", \"עמשמ\", \"עמת\", \"ענ\", \"ענא\", \"ענמ\", \"ענמא\", \"ענמש\", \"ענפ\",\n",
    "    \"עסא\", \"עסק\", \"עע\", \"עעא\", \"עעמ\", \"עער\", \"עעתא\", \"עפ\", \"עפא\", \"עפג\", \"עפהג\", \"עפמ\", \"עפמק\",\n",
    "    \"עפנ\", \"עפס\", \"עפספ\", \"עפע\", \"עפר\", \"עפת\", \"עצמ\", \"עק\", \"עקג\", \"עקמ\", \"עקנ\", \"עקפ\", \"ער\", \"ערא\",\n",
    "    \"ערגצ\", \"ערמ\", \"ערעור\", \"ערפ\", \"ערר\", \"עש\", \"עשא\", \"עשמ\", \"עשר\", \"עשת\", \"עשתש\", \"עת\", \"עתא\",\n",
    "    \"עתמ\", \"עתפב\", \"עתצ\", \"פא\", \"פה\", \"פל\", \"פלא\", \"פמ\", \"פמר\", \"פעמ\", \"פקח\", \"פר\", \"פרק\", \"פשז\",\n",
    "    \"פשר\", \"פת\", \"צא\", \"צבנ\", \"צה\", \"צו\", \"צח\", \"צמ\", \"קג\", \"קפ\", \"רחדפ\", \"רמש\", \"רע\", \"רעא\", \"רעב\",\n",
    "    \"רעבס\", \"רעו\", \"רעמ\", \"רעס\", \"רעפ\", \"רעפא\", \"רעצ\", \"רער\", \"רערצ\", \"רעש\", \"רעתא\", \"רצפ\", \"רתק\",\n",
    "    \"ש\", \"שבד\", \"שמ\", \"שמי\", \"שנא\", \"שע\", \"שעמ\", \"שק\", \"שש\", \"תא\", \"תאדמ\", \"תאח\", \"תאמ\", \"תאק\", \"תב\",\n",
    "    \"תבכ\", \"תבע\", \"תג\", \"תגא\", \"תד\", \"תדא\", \"תהג\", \"תהנ\", \"תהס\", \"תוב\", \"תוח\", \"תח\", \"תחפ\", \"תחת\",\n",
    "    \"תט\", \"תי\", \"תכ\", \"תלא\", \"תלב\", \"תלהמ\", \"תלפ\", \"תלתמ\", \"תמ\", \"תמהח\", \"תממ\", \"תמק\", \"תמר\",\n",
    "    \"תמש\", \"תנג\", \"תנז\", \"תע\", \"תעא\", \"תעז\", \"תפ\", \"תפב\", \"תפח\", \"תפחע\", \"תפכ\", \"תפמ\", \"תפע\",\n",
    "    \"תפק\", \"תצ\", \"תק\", \"תקח\", \"תקמ\", \"תרמ\", \"תת\", \"תתח\", \"תתע\", \"תתעא\", \"תתק\"\n",
    "]\n",
    "\n",
    "def create_acronym_variants(acronyms):\n",
    "    acronym_variants = []\n",
    "    for a in acronyms:\n",
    "        if len(a) > 1:\n",
    "            # Case 1: Original acronym with quotes/dots before last letter\n",
    "            base_acronym = a\n",
    "            if a.startswith('ב') or a.startswith('ו') or a.startswith('ה'):\n",
    "                # Also add variant without the prefix letter\n",
    "                base_acronym = a[1:]\n",
    "            \n",
    "            # For each acronym (both with and without prefix)\n",
    "            for acr in [a, base_acronym]:\n",
    "                if len(acr) > 1:\n",
    "                    # Standard quote/dot before last letter\n",
    "                    quoted = rf\"{acr[:-1]}[\\\"'״]{acr[-1]}\"\n",
    "                    with_dot = rf\"{acr[:-1]}\\.{acr[-1]}\"\n",
    "                    acronym_variants.append(f\"(?:{quoted}|{with_dot})\")\n",
    "                    \n",
    "                    # Add dot-separated variant\n",
    "                    dots_between = '\\.'.join(list(acr))\n",
    "                    acronym_variants.append(dots_between)\n",
    "                    acronym_variants.append(acr)  # Add this line\n",
    "\n",
    "    \n",
    "    return '|'.join(acronym_variants)\n",
    "        \n",
    "acronym_pattern = create_acronym_variants(acronyms)\n",
    "\n",
    "# Ensure the numbers follow the correct format\n",
    "number_pattern = r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "'''\n",
    "citation_pattern = fr'''\n",
    "    (?<!\\w)                      # Ensure no letter before\n",
    "    ([א-ת]?)                     # Optional single Hebrew prefix letter (but no isolated matches)\n",
    "    ({acronym_pattern})           # Captures acronym (short & long)\n",
    "    \\.?                          # Optional dot after acronym\n",
    "    \\s*                          # Optional spaces\n",
    "    (\\((.*?)\\))?                  # Optional court location in parentheses\n",
    "    \\s*[-/]?\\s*                  # Required space or separator before case number\n",
    "    ({number_pattern})            # Captures case number formats\n",
    "    (?!\\w)                       # Ensure no letter after\n",
    "'''.strip()\n",
    "\n",
    "# Compile regex with verbose flag for readability\n",
    "citation_regex = re.compile(citation_pattern, re.VERBOSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import re\n",
    "\n",
    "def clean_hebrew_verdict_text(text: str) -> str:\n",
    "    # Remove duplicate phrases (e.g. repeated 'בית משפט השלום בקריות')\n",
    "    parts = list(dict.fromkeys(text.split('<<')))\n",
    "    cleaned = '<<'.join(parts)\n",
    "\n",
    "    # Remove nested/multiple angle brackets\n",
    "    cleaned = re.sub(r'[<]{2,}', '<', cleaned)\n",
    "    cleaned = re.sub(r'[>]{2,}', '>', cleaned)\n",
    "\n",
    "    # Remove empty brackets or stray symbols\n",
    "    cleaned = re.sub(r'<\\s*>', '', cleaned)\n",
    "    cleaned = re.sub(r'[<>]', '', cleaned)\n",
    "\n",
    "    # Remove escape characters\n",
    "    cleaned = cleaned.replace(\"\\\\'\", \"'\").replace('\\\\', '')\n",
    "\n",
    "    # Replace double \"נגד נגד\" with single\n",
    "    cleaned = re.sub(r'נגד\\s+נגד', 'נגד', cleaned)\n",
    "\n",
    "    # Normalize spaces\n",
    "    cleaned = re.sub(r'\\s{2,}', ' ', cleaned).strip()\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# --- Normalize & regex helper functions ---\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    return text.replace(\"\\u00A0\", \" \").replace(\"\\u200f\", \"\").strip()\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('/', \"∕\")).strip()\n",
    "\n",
    "# --- Citation extraction ---\n",
    "def extract_citation_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    first_rows = \"\"\n",
    "    i = 0\n",
    "    header = doc.sections[0].header\n",
    "    head=\"\"\n",
    "    for paragraph in header.paragraphs:\n",
    "        head+=paragraph.text\n",
    "\n",
    "\n",
    "    for block in iterate_block_items(doc):\n",
    "        if i == 10:\n",
    "            break\n",
    "        first_rows += normalize_text(block.text) + \" \"\n",
    "    \n",
    "        if normalize_text(block.text) != \"\":\n",
    "            i += 1\n",
    "    first_rows=clean_hebrew_verdict_text(first_rows)\n",
    "    # match = citation_regex.search(first_rows)\n",
    "    match =citation_regex.search(head)\n",
    "    if match:\n",
    "        citation = \" \".join(map(str, filter(pd.notna, match.groups()))).strip()\n",
    "        if citation and citation[0] in \"בוור\":\n",
    "            citation = citation[1:].lstrip()\n",
    "        if re.match(r\"^על \\d+$\", citation):\n",
    "            return None\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        return citation\n",
    "    return None\n",
    "\n",
    "# --- CSV file ---\n",
    "# csv_path = \"/home/liorkob/M.Sc/thesis/data/drugs/similarity_gt_drugs.csv\"\n",
    "# csv_path_2 = \"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\"\n",
    "\n",
    "# df = pd.read_csv(csv_path)\n",
    "# df_2 = pd.read_csv(csv_path_2)\n",
    "\n",
    "# --- File renaming ---\n",
    "docx_dir = '/home/liorkob/M.Sc/thesis/data/5k/docx'\n",
    "\n",
    "for filename in os.listdir(docx_dir):\n",
    "    if filename.endswith('.docx'):\n",
    "        file_path = os.path.join(docx_dir, filename)\n",
    "        file_stem = filename.rsplit(\".\", 1)[0]  # remove .docx\n",
    "        # if file_stem != \"SH-08-231-870\":\n",
    "        #     continue\n",
    "        citation_name = extract_citation_from_docx(file_path)\n",
    "\n",
    "        if citation_name:\n",
    "            new_filename = normalize_case_name(citation_name) + '.docx'\n",
    "            new_file_path = os.path.join(docx_dir, new_filename)\n",
    "            \n",
    "            # Rename file\n",
    "            os.rename(file_path, new_file_path)\n",
    "            print(f'Renamed \"{filename}\" → \"{new_filename}\"')\n",
    "\n",
    "            new_stem = new_filename.rsplit(\".\", 1)[0]  # remove .docx\n",
    "\n",
    "        else:\n",
    "            print(f'No citation found in \"{filename}\"')\n",
    "\n",
    "# --- Save updated CSV ---\n",
    "# df.to_csv(csv_path, index=False)\n",
    "# print(\"CSV updated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split: verdict and appeals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to your folder with renamed .docx files\n",
    "docx_dir = '/home/liorkob/M.Sc/thesis/data/5k/docx'\n",
    "verdict_dir = os.path.join(docx_dir, 'verdict')\n",
    "appeals_dir = os.path.join(docx_dir, 'appeals')\n",
    "\n",
    "# Create target directories if they don't exist\n",
    "os.makedirs(verdict_dir, exist_ok=True)\n",
    "os.makedirs(appeals_dir, exist_ok=True)\n",
    "\n",
    "# Go through each file and move it to the appropriate folder\n",
    "for filename in os.listdir(docx_dir):\n",
    "    if not filename.endswith('.docx'):\n",
    "        continue\n",
    "\n",
    "    # Full source file path\n",
    "    src_path = os.path.join(docx_dir, filename)\n",
    "\n",
    "    # Skip if already moved\n",
    "    if os.path.exists(os.path.join(verdict_dir, filename)) or os.path.exists(os.path.join(appeals_dir, filename)):\n",
    "        continue\n",
    "\n",
    "    # Check first letter to determine type\n",
    "    first_letter = filename[0]\n",
    "    if first_letter == 'ת':  # תיק פלילי → verdict\n",
    "        dst_path = os.path.join(verdict_dir, filename)\n",
    "    elif first_letter == 'ע':  # ערעור → appeal\n",
    "        dst_path = os.path.join(appeals_dir, filename)\n",
    "    else:\n",
    "        print(f'Skipping \"{filename}\": unknown type')\n",
    "        continue\n",
    "\n",
    "    # Move the file\n",
    "    os.rename(src_path, dst_path)\n",
    "    print(f'Moved \"{filename}\" to \"{dst_path}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from docx import Document\n",
    "\n",
    "# doc = Document(\"/home/liorkob/M.Sc/thesis/data/5k/docx/22003050-C08.docx\")\n",
    "# header = doc.sections[0].header\n",
    "# for paragraph in header.paragraphs:\n",
    "#     print(paragraph.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract verdict from appeals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from openai import OpenAI\n",
    "from docx import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "# Citation patterns\n",
    "# citation_patterns = {\n",
    "#     'ת\"פ': r'ת\"פ\\s*(\\d+[-/]\\d+)',\n",
    "#     'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+)',\n",
    "#     'ע\"פ': r'ע\"פ\\s*(\\d+[-/]\\d+)',\n",
    "#     'בתי\"פ': r'בתי\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "#     'תי\"פ': r'תי\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "#     'ת\\.פ\\.': r'ת\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "#     'בת\\.פ\\.': r'בת\\.פ\\.\\s*(\\d+[-/]\\d+)',\n",
    "#     'תיק': r'תיק\\s*(\\d+[-/]\\d+)',\n",
    "# }\n",
    "\n",
    "# # Normalize Hebrew text\n",
    "# def normalize_text(text):\n",
    "#     \"\"\"Normalize spaces and special characters in Hebrew text.\"\"\"\n",
    "#     return unicodedata.normalize(\"NFKC\", text).replace(\"\\u00A0\", \" \").strip()\n",
    "\n",
    "# # Normalize case names\n",
    "# def normalize_case_name(case_name):\n",
    "#     \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "#     return re.sub(r'\\s+', ' ', case_name.replace('/', \"∕\")).strip()\n",
    "\n",
    "# Extract hyperlinks from DOCX\n",
    "def getLinkedText(soup):\n",
    "    links = []\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            parts = tag.text.split('\"')\n",
    "            if len(parts) > 1:  # Ensure the URL exists before accessing index 1\n",
    "                url = parts[1]\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: No valid URL found in HYPERLINK tag: {tag.text}\")\n",
    "                url = None  # Assign None if URL is missing\n",
    "\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None and maybe_text.text.strip() != \"\":\n",
    "                    text += maybe_text.text.strip()\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None and maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                    break\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "    return links\n",
    "def getURLs(soup, links):\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            for rel in soup.find_all(\"Relationship\"):\n",
    "                if rel[\"Id\"] == link[\"id\"]:\n",
    "                    link[\"href\"] = rel[\"Target\"]\n",
    "    return links\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_hyperlinks(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from a .docx file and returns a dictionary \n",
    "    where the linked text is mapped to its corresponding URL.\n",
    "    \"\"\"\n",
    "    # Open the .docx file as a zip archive\n",
    "    try:\n",
    "        archive = zipfile.ZipFile(docx_path, \"r\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ Error: Cannot open {docx_path} (Bad ZIP format)\")\n",
    "        return {}\n",
    "\n",
    "    # Extract main document XML\n",
    "    try:\n",
    "        file_data = archive.read(\"word/document.xml\")\n",
    "        doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "        linked_text = getLinkedText(doc_soup)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No document.xml found in {docx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Extract hyperlink relationships from _rels/document.xml.rels\n",
    "    try:\n",
    "        url_data = archive.read(\"word/_rels/document.xml.rels\")\n",
    "        url_soup = BeautifulSoup(url_data, \"xml\")\n",
    "        links_with_urls = getURLs(url_soup, linked_text)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No _rels/document.xml.rels found in {docx_path}\")\n",
    "        links_with_urls = linked_text\n",
    "\n",
    "    # Extract footnotes (if available)\n",
    "    try:\n",
    "        footnote_data = archive.read(\"word/footnotes.xml\")\n",
    "        footnote_soup = BeautifulSoup(footnote_data, \"xml\")\n",
    "        footnote_links = getLinkedText(footnote_soup)\n",
    "\n",
    "        footnote_url_data = archive.read(\"word/_rels/footnotes.xml.rels\")\n",
    "        footnote_url_soup = BeautifulSoup(footnote_url_data, \"xml\")\n",
    "        footnote_links_with_urls = getURLs(footnote_url_soup, footnote_links)\n",
    "\n",
    "        # Merge footnote links\n",
    "        links_with_urls += footnote_links_with_urls\n",
    "    except KeyError:\n",
    "        pass  # No footnotes found, continue\n",
    "\n",
    "    # Convert extracted links to a dictionary: {linked_text: URL}\n",
    "    return {link[\"text\"]: link.get(\"href\", None) for link in links_with_urls}\n",
    "\n",
    "# GPT-based verdict extraction\n",
    "def extract_verdict_with_gpt(text):\n",
    "    prompt = f\"\"\"\n",
    "Given the text of a legal appeal document, identify and extract **only** the referenced verdict that the appeal was filed against. \n",
    "\n",
    "- The verdict typically appears in sentences mentioning 'ערעור על' (appeal on) followed by a reference to a previous court decision.\n",
    "- **Return only the case reference** (e.g., ת\"פ 53715-12-15) without any additional text.\n",
    "- **If multiple verdicts are mentioned, return only the first valid one.**\n",
    "- If no referenced verdict is found, return \"No verdict found\".\n",
    "\n",
    "**Input Text:**\n",
    "{text}\n",
    "\n",
    "**Extracted verdict name (only the case reference):**\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract legal references accurately.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def extract_verdict_from_appeal(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    first_rows = \"\"\n",
    "    i = 0\n",
    "\n",
    "    for block in iterate_block_items(doc):\n",
    "        if i == 100 or \"בשם המערער\" in first_rows or \"בשם המערערת\" in first_rows:\n",
    "            break\n",
    "\n",
    "        first_rows += normalize_text(block.text) + \" \"\n",
    "        i += 1\n",
    "\n",
    "    # First try: extract using citation regex\n",
    "    match = citation_regex.search(first_rows)\n",
    "    if match:\n",
    "        citation = \" \".join(map(str, filter(pd.notna, match.groups()))).strip()\n",
    "        if citation and citation[0] in \"בוור\":\n",
    "            citation = citation[1:].lstrip()\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        return citation\n",
    "\n",
    "    # Fallback: use GPT if no citation found\n",
    "    print(\"Fallback to GPT for:\", docx_path)\n",
    "    return extract_verdict_with_gpt(first_rows)\n",
    "\n",
    "# Processing all DOCX files\n",
    "def process_docx_files(docx_dir, output_csv):\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(docx_dir):\n",
    "        if filename.endswith('.docx'):\n",
    "            file_path = os.path.join(docx_dir, filename)\n",
    "            appeal_case = os.path.splitext(filename)[0]\n",
    "\n",
    "            print(f\"Processing: {appeal_case}\")\n",
    "\n",
    "            # Extract referenced verdict\n",
    "            verdict = extract_verdict_from_appeal(file_path)\n",
    "            \n",
    "            # Extract URLs\n",
    "            links = extract_hyperlinks(file_path)\n",
    "            # print(\" Extracted Links:\")\n",
    "            # for key, value in links.items():\n",
    "            #     print(f\"{key} → {value}\")\n",
    "\n",
    "            url = links.get(verdict, \"\")\n",
    "            print(f\"Extracted verdict:\", verdict)\n",
    "            print(f\"Extracted url:\", url)\n",
    "\n",
    "\n",
    "            results.append({\n",
    "                \"verdict\": verdict,\n",
    "                \"appeal\": appeal_case,\n",
    "                \"url\": url\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame and save\n",
    "    final_df = pd.DataFrame(results)\n",
    "    final_df.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Results saved to {output_csv}\")\n",
    "\n",
    "docx_dir = f'/home/liorkob/thesis/lcp/data/docx_citations'\n",
    "output_csv = f\"/home/liorkob/thesis/lcp/data/docx_citations/verdict_appeal.csv\"\n",
    "\n",
    "process_docx_files(docx_dir, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PRE-PROCESS\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import stanza\n",
    "# from docx import Document\n",
    "import logging\n",
    "\n",
    "\"\"\"\n",
    "This script processes `.docx` verdict files, extracting text from them, identifying and classifying specific sections of the document. It saves the results into CSV files with the extracted text, sections, and metadata for further analysis.\n",
    "\n",
    "### Key Functionalities:\n",
    "\n",
    "1. **Text Extraction and Preprocessing**:\n",
    "   - The script iterates through paragraphs in `.docx` files, using custom functions from `utils.py` to identify specific sections based on formatting (e.g., bold text).\n",
    "   - The extracted sections are stored in a dictionary along with the corresponding full sentences from the document.\n",
    "\n",
    "2. **Part Identification**:\n",
    "   - It processes bolded blocks of text as distinct \"parts\" or sections (e.g., titles or key sections) and appends them to a list.\n",
    "   - For each sentence, the script associates it with both the most recent part (stored as `part_single`) and a concatenation of all previous parts (stored as `part_concatenated`).\n",
    "\n",
    "3. **NLP Processing**:\n",
    "   - The Hebrew Stanza NLP pipeline is used to split the text into sentences, which are then stored in the output alongside the associated document sections.\n",
    "   - The script also applies filters to skip short paragraphs and unwanted patterns (e.g., references to certain case types).\n",
    "\n",
    "4. **Error Handling and Logging**:\n",
    "   - The script uses Python’s `logging` module to provide informative logs, including handling errors if a document can't be opened or processed.\n",
    "   - It catches and logs any exceptions during the processing of files.\n",
    "\n",
    "5. **CSV Output**:\n",
    "   - For each `.docx` file, the extracted data (including text, section titles, and concatenated sections) is saved to a CSV file.\n",
    "\n",
    "6. **Recursive Directory Processing**:\n",
    "   - The script recursively processes `.docx` files in a specified root directory (`selenium_downloads\\מרב גרינברג`), saving the results for each file in a corresponding output directory (`outputs\\merav_grinberg_preproccsed`).\n",
    "\n",
    "### Main Functions:\n",
    "\n",
    "- **doc_to_csv(doc_path: str, result_path: str)**:\n",
    "   - Processes a single `.docx` file, extracting text and metadata.\n",
    "   - Saves the results to a CSV file if a result path is provided.\n",
    "\n",
    "- **run()**:\n",
    "   - Iterates through all `.docx` files in the root directory.\n",
    "   - For each file, it calls `doc_to_csv` and saves the resulting DataFrame as a CSV.\n",
    "\n",
    "### Usage:\n",
    "The script is executed via the `run()` function, which processes all files in the specified directory. It logs the status and outputs CSV files containing preprocessed data for each document.\n",
    "\"\"\"\n",
    "number_pattern = re.compile(r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "''', re.VERBOSE)\n",
    "\n",
    "def should_split_sentence(sentence: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determine whether a sentence should be split.\n",
    "    - A sentence should NOT be split if it contains a citation (matches `number_pattern`).\n",
    "    \"\"\"\n",
    "    return not number_pattern.search(sentence)\n",
    "\n",
    "def validate_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        print(\"The file is valid.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating document: {e}\")\n",
    "        return False\n",
    "    \n",
    "def docToCsv(doc_path: str = None):\n",
    "    \"\"\"\n",
    "    Converts a DOCX document to a CSV format by extracting relevant parts of the document \n",
    "    based on specified conditions like block boldness or specific patterns.\n",
    "\n",
    "    Parameters:\n",
    "    - doc_path (str, optional): The path to the DOCX document. Defaults to None.\n",
    "\n",
    "    Steps:\n",
    "    1. Initialize data dictionary to hold extracted content.\n",
    "    2. Open and iterate through the provided DOCX document.\n",
    "    3. Filter out unnecessary blocks.\n",
    "    4. Determine if the current block is a title or content.\n",
    "    5. If it's content, tokenize it using the Stanza library.\n",
    "    6. Add the extracted content to the data dictionary.\n",
    "    7. Convert the data dictionary to a Pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A Pandas DataFrame containing the extracted text from the DOCX document with columns 'text' and 'part'.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {'verdict': [],'text': [], 'part': []}\n",
    "    data['verdict']=os.path.splitext(os.path.basename(doc_path))[0]\n",
    "    doc = Document(doc_path)\n",
    "    part = 'nothing' \n",
    "\n",
    "    # for paragraph in doc.paragraphs:\n",
    "\n",
    "\n",
    "    for block in iterate_block_items(doc): # Updated usage\n",
    "        flag = False\n",
    "\n",
    "        # if len(block.text) <= 1 or 'ע\"פ' in block.text or 'ת\"פ' in block.text or 'עפ\"ג' in block.text:\n",
    "        #     continue\n",
    "        # if   \"מחמד כנעאנה\" in block.text :\n",
    "        #     i=0\n",
    "# and not re.match(r'^\\d', block.text) and not re.match(r'[\\u0590-\\u05FF][^.)*]*[.)]', block.text)\n",
    "        if is_block_styled(block) and len(block.text.split(' ')) < 10:\n",
    "            # התאמה לתחילת כותרת - מספר או אות בעברית עם נקודה/סוגריים\n",
    "            if re.match(r'^(?:\\d+[.)]|[\\u0590-\\u05FF][.)])', block.text):\n",
    "                # הסר את החלק התואם מהתחלה\n",
    "                part = re.sub(r'^(?:\\d+[.)]|[\\u0590-\\u05FF][.)])', '', block.text).strip()\n",
    "            else:\n",
    "                # אם לא עונה לתנאים, העתק את הטקסט כפי שהוא\n",
    "                part = block.text\n",
    "        else:\n",
    "            extracted_part_text = extract_part_after_number_or_hebrew_letter(block.text)\n",
    "            \n",
    "            # Preserve paragraph integrity while handling sentence splitting\n",
    "            if len(extracted_part_text.split()) < 10 or not should_split_sentence(extracted_part_text):\n",
    "                text = extracted_part_text  # Keep paragraph as-is if it's short or contains a citation\n",
    "            else:\n",
    "                sentences = nlp(extracted_part_text)\n",
    "                filtered_sentences = []\n",
    "                temp_sentence = \"\"\n",
    "\n",
    "                # Reconstruct text while avoiding citation splits and handling quotes\n",
    "                for sentence in sentences.sentences:\n",
    "                    text = sentence.text.strip()\n",
    "\n",
    "                    # Handle quotation blocks\n",
    "                    if text.startswith('\"'):\n",
    "                        flag = True\n",
    "                        continue\n",
    "                    if text.endswith('\".') or text.endswith('\"'):\n",
    "                        flag = False\n",
    "                        continue\n",
    "                    if flag:\n",
    "                        continue\n",
    "\n",
    "                    # Skip text if it matches the section title (part)\n",
    "                    if text == part:\n",
    "                        continue\n",
    "\n",
    "                    # Merge sentences to prevent citation splits\n",
    "                    if should_split_sentence(text):\n",
    "                        if temp_sentence:\n",
    "                            filtered_sentences.append(temp_sentence.strip())\n",
    "                            temp_sentence = \"\"\n",
    "                        filtered_sentences.append(text)\n",
    "                    else:\n",
    "                        temp_sentence += \" \" + text  # Merge citation sentence to previous\n",
    "\n",
    "                if temp_sentence:\n",
    "                    filtered_sentences.append(temp_sentence.strip())\n",
    "\n",
    "                text = \" \".join(filtered_sentences)  # Keep full paragraph if needed\n",
    "\n",
    "            if text.strip():  # Avoid empty lines\n",
    "                data['text'].append(text)\n",
    "                data['part'].append(part)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def run():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    root_directory = \"/home/liorkob/M.Sc/thesis/data/5k/docx/verdict\"\n",
    "    output_dir = \"/home/liorkob/M.Sc/thesis/data/5k/verdict_csv\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for root, _, files in os.walk(root_directory):\n",
    "        logging.info(f\"Processing directory: {root}\")\n",
    "        for file in tqdm(files, desc=f\"Processing files in {root}\"):\n",
    "            if not file.lower().endswith('.docx'):\n",
    "                continue\n",
    "\n",
    "            input_path = os.path.join(root, file)\n",
    "            output_path = os.path.join(output_dir, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "\n",
    "            if os.path.exists(output_path):\n",
    "                logging.info(f\"Output already exists, skipping: {output_path}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df = docToCsv(doc_path=input_path)\n",
    "                df.to_csv(output_path, index=False)\n",
    "                logging.info(f\"Processed and saved: {output_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def verify_verdict_parts_from_csv(output_directory, required_parts):\n",
    "    \"\"\"\n",
    "    Verifies the presence of required parts in each CSV file generated from verdict processing.\n",
    "\n",
    "    Parameters:\n",
    "    - output_directory (str): Directory containing the output CSV files.\n",
    "    - required_parts (list): List of strings representing the required parts.\n",
    "\n",
    "    Output:\n",
    "    - Prints the parts for each verdict.\n",
    "    - Identifies and lists verdicts where none of the required parts are found.\n",
    "    \"\"\"\n",
    "    verdicts_with_no_parts = []  # Store verdicts where none of the parts exist\n",
    "\n",
    "    for file in os.listdir(output_directory):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(output_directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        verdict_name = os.path.splitext(file)[0]\n",
    "        \n",
    "        print(f\"Verifying Verdict: {verdict_name}\")\n",
    "        \n",
    "        # Extract unique parts from the DataFrame\n",
    "        verdict_parts = df['part'].dropna().astype(str).unique()  # Ensure all parts are strings\n",
    "        \n",
    "        # Print all parts for the verdict\n",
    "        print(\"  Parts in the verdict:\")\n",
    "        for part in verdict_parts:\n",
    "            print(f\"    - {part}\")\n",
    "        \n",
    "        # Check if none of the required parts exist\n",
    "        if not any(any(required in part for part in verdict_parts) for required in required_parts):\n",
    "            verdicts_with_no_parts.append((verdict_name,verdict_parts))  # Add to the list of problematic verdicts\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Print verdicts with no matching parts\n",
    "    if verdicts_with_no_parts:\n",
    "        print(\"Verdicts with no matching parts:\")\n",
    "        for verdict,parts in verdicts_with_no_parts:\n",
    "            print(f\"  - {verdict}\")\n",
    "            print(f\"parts: {parts}\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"All verdicts have at least one matching part.\")\n",
    "\n",
    "# Define the directory containing the output CSV files\n",
    "output_directory = \"/home/liorkob/M.Sc/thesis/data/5k/appeals_csv\"\n",
    "\n",
    "# Define the required parts (partial matching supported)\n",
    "required_parts = [\"אחידות בענישה\",\"מתחם הענישה\",\"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\", \"הענישה הנוהגת\",\"ענישה נוהגת\", \"מתחם העונש\" ,\"מתחם עונש\",\"מדיניות הענישה\" \"והכרעה\", \"ההרשעה\",\"מדיניות הענישה הנהוגה\"]\n",
    "# required_parts=[\"הכרעת הדין\", \"אישום\" ,\"רקע\" ,\"כללי\" ,\"כתב אישום\",\"כתב האישום\"]\n",
    "# Run the verification\n",
    "verify_verdict_parts_from_csv(output_directory, required_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chack multy defents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from docx import Document\n",
    "\n",
    "# Set paths\n",
    "docx_directory = \"/home/liorkob/M.Sc/thesis/data/5k/docx/verdict\"\n",
    "\n",
    "\n",
    "# Initialize counter\n",
    "mention_count = 0\n",
    "mention_1_count = 0\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\",\"השתלשלות\", \"ג ז ר\",  \"ד י ן\"\n",
    "]\n",
    "\n",
    "# Process files\n",
    "file_list = [f for f in os.listdir(docx_directory) if f.lower().endswith(\".docx\")]\n",
    "\n",
    "# Process each DOCX file\n",
    "for filename in tqdm(file_list, desc=\"Checking for 'הנאשמים' in DOCX files\"):\n",
    "    try:\n",
    "        file_path = os.path.join(docx_directory, filename)\n",
    "        doc = Document(file_path)\n",
    "        first_rows = \"\"\n",
    "        i = 0\n",
    "        header = doc.sections[0].header\n",
    "        for paragraph in header.paragraphs:\n",
    "            first_rows+=paragraph.text\n",
    "\n",
    "\n",
    "        for block in iterate_block_items(doc):\n",
    "            if i == 25:\n",
    "                break\n",
    "            first_rows += block.text + \" \"\n",
    "\n",
    "            import re  # Make sure this is at the top\n",
    "\n",
    "            if re.search('|'.join(START_PARTS), block.text, flags=re.IGNORECASE):\n",
    "                break\n",
    "\n",
    "            if block.text != \"\":\n",
    "                i += 1\n",
    "        # Check for \"הנאשמים\"\n",
    "        if \"הנאשמים:\" in first_rows or \"נאשמים:\" in first_rows or \"הנאשמים :\" in first_rows or \"נאשמים :\" in first_rows:\n",
    "            mention_count += 1\n",
    "            print(f\"\\n📌 Found in: {filename}\")\n",
    "            print(first_rows)\n",
    "\n",
    "\n",
    "        # Check for \"הנאשם\"\n",
    "        elif \"הנאשם\" in first_rows or \"נאשם\" in first_rows:\n",
    "            mention_1_count += 1\n",
    "            # print(f\"\\n📌 Found in: {filename}\")\n",
    "            # print(first_rows)\n",
    "\n",
    "        else :\n",
    "            print(\"NOTHING FOUND\")\n",
    "            print(first_rows)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {filename}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\\n🔍 Total files containing 'הנאשמים' before start part: {mention_count}\")\n",
    "# Summary\n",
    "print(f\"\\n🔍 Total files containing 'הנאשם' before start part: {mention_1_count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
