{
  "best_metric": 1.2173517942428589,
  "best_model_checkpoint": "./hebert-mlm-3k-drugs/checkpoint-29880",
  "epoch": 18.0,
  "eval_steps": 500,
  "global_step": 29880,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.060240963855421686,
      "grad_norm": 8.803086280822754,
      "learning_rate": 4.984939759036145e-05,
      "loss": 1.6535,
      "step": 100
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 9.007942199707031,
      "learning_rate": 4.9698795180722894e-05,
      "loss": 1.5459,
      "step": 200
    },
    {
      "epoch": 0.18072289156626506,
      "grad_norm": 9.217738151550293,
      "learning_rate": 4.954819277108434e-05,
      "loss": 1.5965,
      "step": 300
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 9.606913566589355,
      "learning_rate": 4.9397590361445786e-05,
      "loss": 1.4896,
      "step": 400
    },
    {
      "epoch": 0.30120481927710846,
      "grad_norm": 15.598121643066406,
      "learning_rate": 4.924698795180723e-05,
      "loss": 1.6513,
      "step": 500
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 11.538981437683105,
      "learning_rate": 4.909638554216868e-05,
      "loss": 1.4131,
      "step": 600
    },
    {
      "epoch": 0.42168674698795183,
      "grad_norm": 10.106720924377441,
      "learning_rate": 4.8945783132530124e-05,
      "loss": 1.3316,
      "step": 700
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 7.27126407623291,
      "learning_rate": 4.879518072289157e-05,
      "loss": 1.3698,
      "step": 800
    },
    {
      "epoch": 0.5421686746987951,
      "grad_norm": 20.37376594543457,
      "learning_rate": 4.8644578313253016e-05,
      "loss": 1.5538,
      "step": 900
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 30.435779571533203,
      "learning_rate": 4.8493975903614455e-05,
      "loss": 1.5615,
      "step": 1000
    },
    {
      "epoch": 0.6626506024096386,
      "grad_norm": 18.967607498168945,
      "learning_rate": 4.834337349397591e-05,
      "loss": 1.2658,
      "step": 1100
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 8.046201705932617,
      "learning_rate": 4.8192771084337354e-05,
      "loss": 1.4415,
      "step": 1200
    },
    {
      "epoch": 0.7831325301204819,
      "grad_norm": 10.312341690063477,
      "learning_rate": 4.804216867469879e-05,
      "loss": 1.3672,
      "step": 1300
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 6.239132404327393,
      "learning_rate": 4.7891566265060246e-05,
      "loss": 1.4871,
      "step": 1400
    },
    {
      "epoch": 0.9036144578313253,
      "grad_norm": 19.926042556762695,
      "learning_rate": 4.774096385542169e-05,
      "loss": 1.7289,
      "step": 1500
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 20.98212432861328,
      "learning_rate": 4.759036144578313e-05,
      "loss": 2.289,
      "step": 1600
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.0887703895568848,
      "eval_runtime": 8.4149,
      "eval_samples_per_second": 208.202,
      "eval_steps_per_second": 26.025,
      "step": 1660
    },
    {
      "epoch": 1.0240963855421688,
      "grad_norm": 13.504544258117676,
      "learning_rate": 4.7439759036144584e-05,
      "loss": 2.3227,
      "step": 1700
    },
    {
      "epoch": 1.0843373493975903,
      "grad_norm": 16.044055938720703,
      "learning_rate": 4.728915662650602e-05,
      "loss": 2.2723,
      "step": 1800
    },
    {
      "epoch": 1.144578313253012,
      "grad_norm": 9.964897155761719,
      "learning_rate": 4.713855421686747e-05,
      "loss": 2.1762,
      "step": 1900
    },
    {
      "epoch": 1.2048192771084336,
      "grad_norm": 8.579399108886719,
      "learning_rate": 4.698795180722892e-05,
      "loss": 2.1904,
      "step": 2000
    },
    {
      "epoch": 1.2650602409638554,
      "grad_norm": 9.94161605834961,
      "learning_rate": 4.683734939759036e-05,
      "loss": 2.3237,
      "step": 2100
    },
    {
      "epoch": 1.3253012048192772,
      "grad_norm": 12.977394104003906,
      "learning_rate": 4.668674698795181e-05,
      "loss": 2.2622,
      "step": 2200
    },
    {
      "epoch": 1.3855421686746987,
      "grad_norm": 12.384430885314941,
      "learning_rate": 4.653614457831326e-05,
      "loss": 2.221,
      "step": 2300
    },
    {
      "epoch": 1.4457831325301205,
      "grad_norm": 16.67611312866211,
      "learning_rate": 4.63855421686747e-05,
      "loss": 2.2907,
      "step": 2400
    },
    {
      "epoch": 1.5060240963855422,
      "grad_norm": 12.459834098815918,
      "learning_rate": 4.6234939759036145e-05,
      "loss": 2.171,
      "step": 2500
    },
    {
      "epoch": 1.5662650602409638,
      "grad_norm": 9.711917877197266,
      "learning_rate": 4.608433734939759e-05,
      "loss": 2.0442,
      "step": 2600
    },
    {
      "epoch": 1.6265060240963856,
      "grad_norm": 10.73808765411377,
      "learning_rate": 4.5933734939759037e-05,
      "loss": 2.0866,
      "step": 2700
    },
    {
      "epoch": 1.6867469879518073,
      "grad_norm": 13.721455574035645,
      "learning_rate": 4.578313253012048e-05,
      "loss": 2.0633,
      "step": 2800
    },
    {
      "epoch": 1.7469879518072289,
      "grad_norm": 10.37167739868164,
      "learning_rate": 4.563253012048193e-05,
      "loss": 1.9898,
      "step": 2900
    },
    {
      "epoch": 1.8072289156626506,
      "grad_norm": 13.848250389099121,
      "learning_rate": 4.5481927710843374e-05,
      "loss": 2.107,
      "step": 3000
    },
    {
      "epoch": 1.8674698795180724,
      "grad_norm": 8.292500495910645,
      "learning_rate": 4.533132530120482e-05,
      "loss": 2.2667,
      "step": 3100
    },
    {
      "epoch": 1.927710843373494,
      "grad_norm": 12.307710647583008,
      "learning_rate": 4.5180722891566266e-05,
      "loss": 2.1968,
      "step": 3200
    },
    {
      "epoch": 1.9879518072289155,
      "grad_norm": 8.281749725341797,
      "learning_rate": 4.503012048192771e-05,
      "loss": 2.0554,
      "step": 3300
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.902692437171936,
      "eval_runtime": 8.5374,
      "eval_samples_per_second": 205.214,
      "eval_steps_per_second": 25.652,
      "step": 3320
    },
    {
      "epoch": 2.0481927710843375,
      "grad_norm": 7.4634857177734375,
      "learning_rate": 4.487951807228916e-05,
      "loss": 1.8934,
      "step": 3400
    },
    {
      "epoch": 2.108433734939759,
      "grad_norm": 17.01007843017578,
      "learning_rate": 4.4728915662650604e-05,
      "loss": 2.1515,
      "step": 3500
    },
    {
      "epoch": 2.1686746987951806,
      "grad_norm": 12.736906051635742,
      "learning_rate": 4.457831325301205e-05,
      "loss": 1.9397,
      "step": 3600
    },
    {
      "epoch": 2.2289156626506026,
      "grad_norm": 9.124382972717285,
      "learning_rate": 4.4427710843373496e-05,
      "loss": 2.0933,
      "step": 3700
    },
    {
      "epoch": 2.289156626506024,
      "grad_norm": 15.95936393737793,
      "learning_rate": 4.427710843373494e-05,
      "loss": 2.3012,
      "step": 3800
    },
    {
      "epoch": 2.3493975903614457,
      "grad_norm": 13.074172973632812,
      "learning_rate": 4.412650602409639e-05,
      "loss": 2.1617,
      "step": 3900
    },
    {
      "epoch": 2.4096385542168672,
      "grad_norm": 13.909567832946777,
      "learning_rate": 4.3975903614457834e-05,
      "loss": 1.9455,
      "step": 4000
    },
    {
      "epoch": 2.4698795180722892,
      "grad_norm": 10.044440269470215,
      "learning_rate": 4.382530120481928e-05,
      "loss": 1.9896,
      "step": 4100
    },
    {
      "epoch": 2.5301204819277108,
      "grad_norm": 13.745163917541504,
      "learning_rate": 4.3674698795180726e-05,
      "loss": 1.9056,
      "step": 4200
    },
    {
      "epoch": 2.5903614457831328,
      "grad_norm": 11.497550010681152,
      "learning_rate": 4.352409638554217e-05,
      "loss": 2.0589,
      "step": 4300
    },
    {
      "epoch": 2.6506024096385543,
      "grad_norm": 9.286958694458008,
      "learning_rate": 4.337349397590362e-05,
      "loss": 1.9975,
      "step": 4400
    },
    {
      "epoch": 2.710843373493976,
      "grad_norm": 9.938584327697754,
      "learning_rate": 4.3222891566265064e-05,
      "loss": 1.7182,
      "step": 4500
    },
    {
      "epoch": 2.7710843373493974,
      "grad_norm": 14.194840431213379,
      "learning_rate": 4.307228915662651e-05,
      "loss": 2.0943,
      "step": 4600
    },
    {
      "epoch": 2.8313253012048194,
      "grad_norm": 12.120221138000488,
      "learning_rate": 4.2921686746987956e-05,
      "loss": 2.0429,
      "step": 4700
    },
    {
      "epoch": 2.891566265060241,
      "grad_norm": 11.69151782989502,
      "learning_rate": 4.27710843373494e-05,
      "loss": 1.9944,
      "step": 4800
    },
    {
      "epoch": 2.9518072289156625,
      "grad_norm": 10.848370552062988,
      "learning_rate": 4.262048192771085e-05,
      "loss": 1.947,
      "step": 4900
    },
    {
      "epoch": 3.0,
      "eval_loss": 1.7737756967544556,
      "eval_runtime": 8.4821,
      "eval_samples_per_second": 206.553,
      "eval_steps_per_second": 25.819,
      "step": 4980
    },
    {
      "epoch": 3.0120481927710845,
      "grad_norm": 11.623087882995605,
      "learning_rate": 4.2469879518072294e-05,
      "loss": 1.9272,
      "step": 5000
    },
    {
      "epoch": 3.072289156626506,
      "grad_norm": 14.08516788482666,
      "learning_rate": 4.231927710843373e-05,
      "loss": 1.8888,
      "step": 5100
    },
    {
      "epoch": 3.1325301204819276,
      "grad_norm": 10.391402244567871,
      "learning_rate": 4.2168674698795186e-05,
      "loss": 1.9774,
      "step": 5200
    },
    {
      "epoch": 3.1927710843373496,
      "grad_norm": 19.628530502319336,
      "learning_rate": 4.201807228915663e-05,
      "loss": 1.677,
      "step": 5300
    },
    {
      "epoch": 3.253012048192771,
      "grad_norm": 16.28702163696289,
      "learning_rate": 4.186746987951807e-05,
      "loss": 1.7643,
      "step": 5400
    },
    {
      "epoch": 3.3132530120481927,
      "grad_norm": 16.72796058654785,
      "learning_rate": 4.1716867469879523e-05,
      "loss": 1.9171,
      "step": 5500
    },
    {
      "epoch": 3.3734939759036147,
      "grad_norm": 12.801233291625977,
      "learning_rate": 4.156626506024097e-05,
      "loss": 1.8952,
      "step": 5600
    },
    {
      "epoch": 3.433734939759036,
      "grad_norm": 11.82905387878418,
      "learning_rate": 4.141566265060241e-05,
      "loss": 1.6668,
      "step": 5700
    },
    {
      "epoch": 3.4939759036144578,
      "grad_norm": 13.222818374633789,
      "learning_rate": 4.126506024096386e-05,
      "loss": 1.773,
      "step": 5800
    },
    {
      "epoch": 3.5542168674698793,
      "grad_norm": 25.662778854370117,
      "learning_rate": 4.11144578313253e-05,
      "loss": 1.9458,
      "step": 5900
    },
    {
      "epoch": 3.6144578313253013,
      "grad_norm": 9.396990776062012,
      "learning_rate": 4.0963855421686746e-05,
      "loss": 1.784,
      "step": 6000
    },
    {
      "epoch": 3.674698795180723,
      "grad_norm": 13.409318923950195,
      "learning_rate": 4.08132530120482e-05,
      "loss": 1.786,
      "step": 6100
    },
    {
      "epoch": 3.734939759036145,
      "grad_norm": 9.793889999389648,
      "learning_rate": 4.066265060240964e-05,
      "loss": 1.7571,
      "step": 6200
    },
    {
      "epoch": 3.7951807228915664,
      "grad_norm": 14.745963096618652,
      "learning_rate": 4.0512048192771084e-05,
      "loss": 1.7438,
      "step": 6300
    },
    {
      "epoch": 3.855421686746988,
      "grad_norm": 11.658580780029297,
      "learning_rate": 4.036144578313254e-05,
      "loss": 1.7892,
      "step": 6400
    },
    {
      "epoch": 3.9156626506024095,
      "grad_norm": 15.691290855407715,
      "learning_rate": 4.0210843373493976e-05,
      "loss": 1.6514,
      "step": 6500
    },
    {
      "epoch": 3.9759036144578315,
      "grad_norm": 15.40991497039795,
      "learning_rate": 4.006024096385542e-05,
      "loss": 1.8537,
      "step": 6600
    },
    {
      "epoch": 4.0,
      "eval_loss": 1.6837983131408691,
      "eval_runtime": 8.4521,
      "eval_samples_per_second": 207.287,
      "eval_steps_per_second": 25.911,
      "step": 6640
    },
    {
      "epoch": 4.036144578313253,
      "grad_norm": 10.19992446899414,
      "learning_rate": 3.9909638554216875e-05,
      "loss": 1.8017,
      "step": 6700
    },
    {
      "epoch": 4.096385542168675,
      "grad_norm": 12.588963508605957,
      "learning_rate": 3.9759036144578314e-05,
      "loss": 1.6926,
      "step": 6800
    },
    {
      "epoch": 4.156626506024097,
      "grad_norm": 15.568678855895996,
      "learning_rate": 3.960843373493976e-05,
      "loss": 1.8079,
      "step": 6900
    },
    {
      "epoch": 4.216867469879518,
      "grad_norm": 9.025261878967285,
      "learning_rate": 3.9457831325301206e-05,
      "loss": 1.6822,
      "step": 7000
    },
    {
      "epoch": 4.27710843373494,
      "grad_norm": 20.94942283630371,
      "learning_rate": 3.930722891566265e-05,
      "loss": 1.6442,
      "step": 7100
    },
    {
      "epoch": 4.337349397590361,
      "grad_norm": 9.009073257446289,
      "learning_rate": 3.91566265060241e-05,
      "loss": 1.6277,
      "step": 7200
    },
    {
      "epoch": 4.397590361445783,
      "grad_norm": 10.238191604614258,
      "learning_rate": 3.9006024096385544e-05,
      "loss": 1.6246,
      "step": 7300
    },
    {
      "epoch": 4.457831325301205,
      "grad_norm": 16.291149139404297,
      "learning_rate": 3.885542168674699e-05,
      "loss": 1.6173,
      "step": 7400
    },
    {
      "epoch": 4.518072289156627,
      "grad_norm": 11.416258811950684,
      "learning_rate": 3.8704819277108436e-05,
      "loss": 1.7259,
      "step": 7500
    },
    {
      "epoch": 4.578313253012048,
      "grad_norm": 9.042825698852539,
      "learning_rate": 3.855421686746988e-05,
      "loss": 1.722,
      "step": 7600
    },
    {
      "epoch": 4.63855421686747,
      "grad_norm": 15.897923469543457,
      "learning_rate": 3.840361445783133e-05,
      "loss": 1.7504,
      "step": 7700
    },
    {
      "epoch": 4.698795180722891,
      "grad_norm": 16.72411346435547,
      "learning_rate": 3.8253012048192774e-05,
      "loss": 1.5143,
      "step": 7800
    },
    {
      "epoch": 4.759036144578313,
      "grad_norm": 17.40875244140625,
      "learning_rate": 3.810240963855422e-05,
      "loss": 1.5645,
      "step": 7900
    },
    {
      "epoch": 4.8192771084337345,
      "grad_norm": 10.55685806274414,
      "learning_rate": 3.7951807228915666e-05,
      "loss": 1.6143,
      "step": 8000
    },
    {
      "epoch": 4.879518072289157,
      "grad_norm": 14.999202728271484,
      "learning_rate": 3.780120481927711e-05,
      "loss": 1.7388,
      "step": 8100
    },
    {
      "epoch": 4.9397590361445785,
      "grad_norm": 9.998218536376953,
      "learning_rate": 3.765060240963856e-05,
      "loss": 1.684,
      "step": 8200
    },
    {
      "epoch": 5.0,
      "grad_norm": 6.459669589996338,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 1.7148,
      "step": 8300
    },
    {
      "epoch": 5.0,
      "eval_loss": 1.5983117818832397,
      "eval_runtime": 8.4861,
      "eval_samples_per_second": 206.454,
      "eval_steps_per_second": 25.807,
      "step": 8300
    },
    {
      "epoch": 5.0602409638554215,
      "grad_norm": 17.479534149169922,
      "learning_rate": 3.734939759036144e-05,
      "loss": 1.4778,
      "step": 8400
    },
    {
      "epoch": 5.120481927710843,
      "grad_norm": 16.71990966796875,
      "learning_rate": 3.7198795180722895e-05,
      "loss": 1.5257,
      "step": 8500
    },
    {
      "epoch": 5.180722891566265,
      "grad_norm": 8.803360939025879,
      "learning_rate": 3.704819277108434e-05,
      "loss": 1.4269,
      "step": 8600
    },
    {
      "epoch": 5.240963855421687,
      "grad_norm": 13.963760375976562,
      "learning_rate": 3.689759036144578e-05,
      "loss": 1.6498,
      "step": 8700
    },
    {
      "epoch": 5.301204819277109,
      "grad_norm": 10.371642112731934,
      "learning_rate": 3.674698795180723e-05,
      "loss": 1.5067,
      "step": 8800
    },
    {
      "epoch": 5.36144578313253,
      "grad_norm": 17.72900390625,
      "learning_rate": 3.659638554216868e-05,
      "loss": 1.615,
      "step": 8900
    },
    {
      "epoch": 5.421686746987952,
      "grad_norm": 9.372553825378418,
      "learning_rate": 3.644578313253012e-05,
      "loss": 1.7155,
      "step": 9000
    },
    {
      "epoch": 5.481927710843373,
      "grad_norm": 14.844257354736328,
      "learning_rate": 3.629518072289157e-05,
      "loss": 1.5418,
      "step": 9100
    },
    {
      "epoch": 5.542168674698795,
      "grad_norm": 14.332006454467773,
      "learning_rate": 3.614457831325301e-05,
      "loss": 1.653,
      "step": 9200
    },
    {
      "epoch": 5.602409638554217,
      "grad_norm": 8.636214256286621,
      "learning_rate": 3.5993975903614456e-05,
      "loss": 1.4585,
      "step": 9300
    },
    {
      "epoch": 5.662650602409639,
      "grad_norm": 14.653029441833496,
      "learning_rate": 3.584337349397591e-05,
      "loss": 1.5835,
      "step": 9400
    },
    {
      "epoch": 5.72289156626506,
      "grad_norm": 14.31801986694336,
      "learning_rate": 3.569277108433735e-05,
      "loss": 1.4798,
      "step": 9500
    },
    {
      "epoch": 5.783132530120482,
      "grad_norm": 9.215559959411621,
      "learning_rate": 3.5542168674698794e-05,
      "loss": 1.4082,
      "step": 9600
    },
    {
      "epoch": 5.843373493975903,
      "grad_norm": 9.37511920928955,
      "learning_rate": 3.539156626506025e-05,
      "loss": 1.5953,
      "step": 9700
    },
    {
      "epoch": 5.903614457831325,
      "grad_norm": 13.185746192932129,
      "learning_rate": 3.5240963855421686e-05,
      "loss": 1.5698,
      "step": 9800
    },
    {
      "epoch": 5.9638554216867465,
      "grad_norm": 9.095678329467773,
      "learning_rate": 3.509036144578313e-05,
      "loss": 1.6305,
      "step": 9900
    },
    {
      "epoch": 6.0,
      "eval_loss": 1.5030840635299683,
      "eval_runtime": 8.5072,
      "eval_samples_per_second": 205.942,
      "eval_steps_per_second": 25.743,
      "step": 9960
    },
    {
      "epoch": 6.024096385542169,
      "grad_norm": 18.20340347290039,
      "learning_rate": 3.4939759036144585e-05,
      "loss": 1.3844,
      "step": 10000
    },
    {
      "epoch": 6.0843373493975905,
      "grad_norm": 16.24325180053711,
      "learning_rate": 3.4789156626506024e-05,
      "loss": 1.6099,
      "step": 10100
    },
    {
      "epoch": 6.144578313253012,
      "grad_norm": 20.892778396606445,
      "learning_rate": 3.463855421686747e-05,
      "loss": 1.5274,
      "step": 10200
    },
    {
      "epoch": 6.204819277108434,
      "grad_norm": 17.505252838134766,
      "learning_rate": 3.4487951807228916e-05,
      "loss": 1.5021,
      "step": 10300
    },
    {
      "epoch": 6.265060240963855,
      "grad_norm": 26.021373748779297,
      "learning_rate": 3.433734939759036e-05,
      "loss": 1.4367,
      "step": 10400
    },
    {
      "epoch": 6.325301204819277,
      "grad_norm": 19.15913963317871,
      "learning_rate": 3.418674698795181e-05,
      "loss": 1.5588,
      "step": 10500
    },
    {
      "epoch": 6.385542168674699,
      "grad_norm": 11.0361328125,
      "learning_rate": 3.4036144578313254e-05,
      "loss": 1.4634,
      "step": 10600
    },
    {
      "epoch": 6.445783132530121,
      "grad_norm": 125.325927734375,
      "learning_rate": 3.38855421686747e-05,
      "loss": 1.5993,
      "step": 10700
    },
    {
      "epoch": 6.506024096385542,
      "grad_norm": 12.458343505859375,
      "learning_rate": 3.3734939759036146e-05,
      "loss": 1.3403,
      "step": 10800
    },
    {
      "epoch": 6.566265060240964,
      "grad_norm": 11.156590461730957,
      "learning_rate": 3.358433734939759e-05,
      "loss": 1.4496,
      "step": 10900
    },
    {
      "epoch": 6.626506024096385,
      "grad_norm": 12.206133842468262,
      "learning_rate": 3.343373493975904e-05,
      "loss": 1.4747,
      "step": 11000
    },
    {
      "epoch": 6.686746987951807,
      "grad_norm": 9.530065536499023,
      "learning_rate": 3.3283132530120484e-05,
      "loss": 1.4525,
      "step": 11100
    },
    {
      "epoch": 6.746987951807229,
      "grad_norm": 12.407471656799316,
      "learning_rate": 3.313253012048193e-05,
      "loss": 1.4382,
      "step": 11200
    },
    {
      "epoch": 6.807228915662651,
      "grad_norm": 11.956735610961914,
      "learning_rate": 3.2981927710843376e-05,
      "loss": 1.3461,
      "step": 11300
    },
    {
      "epoch": 6.867469879518072,
      "grad_norm": 10.823210716247559,
      "learning_rate": 3.283132530120482e-05,
      "loss": 1.483,
      "step": 11400
    },
    {
      "epoch": 6.927710843373494,
      "grad_norm": 11.555419921875,
      "learning_rate": 3.268072289156627e-05,
      "loss": 1.3966,
      "step": 11500
    },
    {
      "epoch": 6.9879518072289155,
      "grad_norm": 14.415181159973145,
      "learning_rate": 3.253012048192771e-05,
      "loss": 1.3349,
      "step": 11600
    },
    {
      "epoch": 7.0,
      "eval_loss": 1.474212408065796,
      "eval_runtime": 8.4576,
      "eval_samples_per_second": 207.15,
      "eval_steps_per_second": 25.894,
      "step": 11620
    },
    {
      "epoch": 7.048192771084337,
      "grad_norm": 11.44060230255127,
      "learning_rate": 3.237951807228915e-05,
      "loss": 1.4697,
      "step": 11700
    },
    {
      "epoch": 7.108433734939759,
      "grad_norm": 8.577436447143555,
      "learning_rate": 3.2228915662650605e-05,
      "loss": 1.2376,
      "step": 11800
    },
    {
      "epoch": 7.168674698795181,
      "grad_norm": 8.598109245300293,
      "learning_rate": 3.207831325301205e-05,
      "loss": 1.3786,
      "step": 11900
    },
    {
      "epoch": 7.228915662650603,
      "grad_norm": 9.897390365600586,
      "learning_rate": 3.192771084337349e-05,
      "loss": 1.4552,
      "step": 12000
    },
    {
      "epoch": 7.289156626506024,
      "grad_norm": 11.309157371520996,
      "learning_rate": 3.177710843373494e-05,
      "loss": 1.3301,
      "step": 12100
    },
    {
      "epoch": 7.349397590361446,
      "grad_norm": 13.588397026062012,
      "learning_rate": 3.162650602409639e-05,
      "loss": 1.3033,
      "step": 12200
    },
    {
      "epoch": 7.409638554216867,
      "grad_norm": 17.72380256652832,
      "learning_rate": 3.147590361445783e-05,
      "loss": 1.3324,
      "step": 12300
    },
    {
      "epoch": 7.469879518072289,
      "grad_norm": 15.230622291564941,
      "learning_rate": 3.132530120481928e-05,
      "loss": 1.4346,
      "step": 12400
    },
    {
      "epoch": 7.530120481927711,
      "grad_norm": 8.711919784545898,
      "learning_rate": 3.117469879518072e-05,
      "loss": 1.4463,
      "step": 12500
    },
    {
      "epoch": 7.590361445783133,
      "grad_norm": 12.884709358215332,
      "learning_rate": 3.102409638554217e-05,
      "loss": 1.3979,
      "step": 12600
    },
    {
      "epoch": 7.650602409638554,
      "grad_norm": 7.819171905517578,
      "learning_rate": 3.087349397590362e-05,
      "loss": 1.3613,
      "step": 12700
    },
    {
      "epoch": 7.710843373493976,
      "grad_norm": 14.494720458984375,
      "learning_rate": 3.072289156626506e-05,
      "loss": 1.3645,
      "step": 12800
    },
    {
      "epoch": 7.771084337349397,
      "grad_norm": 8.557037353515625,
      "learning_rate": 3.057228915662651e-05,
      "loss": 1.4867,
      "step": 12900
    },
    {
      "epoch": 7.831325301204819,
      "grad_norm": 15.817266464233398,
      "learning_rate": 3.0421686746987953e-05,
      "loss": 1.4111,
      "step": 13000
    },
    {
      "epoch": 7.891566265060241,
      "grad_norm": 12.41916561126709,
      "learning_rate": 3.02710843373494e-05,
      "loss": 1.4571,
      "step": 13100
    },
    {
      "epoch": 7.951807228915663,
      "grad_norm": 8.58102798461914,
      "learning_rate": 3.012048192771085e-05,
      "loss": 1.3267,
      "step": 13200
    },
    {
      "epoch": 8.0,
      "eval_loss": 1.4187946319580078,
      "eval_runtime": 8.4379,
      "eval_samples_per_second": 207.634,
      "eval_steps_per_second": 25.954,
      "step": 13280
    },
    {
      "epoch": 8.012048192771084,
      "grad_norm": 21.266969680786133,
      "learning_rate": 2.996987951807229e-05,
      "loss": 1.3955,
      "step": 13300
    },
    {
      "epoch": 8.072289156626505,
      "grad_norm": 9.731679916381836,
      "learning_rate": 2.9819277108433734e-05,
      "loss": 1.2321,
      "step": 13400
    },
    {
      "epoch": 8.132530120481928,
      "grad_norm": 7.559240341186523,
      "learning_rate": 2.9668674698795183e-05,
      "loss": 1.3973,
      "step": 13500
    },
    {
      "epoch": 8.19277108433735,
      "grad_norm": 9.56966781616211,
      "learning_rate": 2.951807228915663e-05,
      "loss": 1.3156,
      "step": 13600
    },
    {
      "epoch": 8.25301204819277,
      "grad_norm": 11.325318336486816,
      "learning_rate": 2.9367469879518072e-05,
      "loss": 1.4034,
      "step": 13700
    },
    {
      "epoch": 8.313253012048193,
      "grad_norm": 10.262989044189453,
      "learning_rate": 2.921686746987952e-05,
      "loss": 1.5031,
      "step": 13800
    },
    {
      "epoch": 8.373493975903614,
      "grad_norm": 9.531769752502441,
      "learning_rate": 2.9066265060240967e-05,
      "loss": 1.2656,
      "step": 13900
    },
    {
      "epoch": 8.433734939759036,
      "grad_norm": 20.0947322845459,
      "learning_rate": 2.891566265060241e-05,
      "loss": 1.2132,
      "step": 14000
    },
    {
      "epoch": 8.493975903614459,
      "grad_norm": 11.005398750305176,
      "learning_rate": 2.876506024096386e-05,
      "loss": 1.3455,
      "step": 14100
    },
    {
      "epoch": 8.55421686746988,
      "grad_norm": 11.374781608581543,
      "learning_rate": 2.86144578313253e-05,
      "loss": 1.3642,
      "step": 14200
    },
    {
      "epoch": 8.614457831325302,
      "grad_norm": 9.24024772644043,
      "learning_rate": 2.8463855421686748e-05,
      "loss": 1.2632,
      "step": 14300
    },
    {
      "epoch": 8.674698795180722,
      "grad_norm": 12.463871955871582,
      "learning_rate": 2.8313253012048197e-05,
      "loss": 1.3373,
      "step": 14400
    },
    {
      "epoch": 8.734939759036145,
      "grad_norm": 10.841032028198242,
      "learning_rate": 2.816265060240964e-05,
      "loss": 1.3373,
      "step": 14500
    },
    {
      "epoch": 8.795180722891565,
      "grad_norm": 7.673752784729004,
      "learning_rate": 2.8012048192771085e-05,
      "loss": 1.2625,
      "step": 14600
    },
    {
      "epoch": 8.855421686746988,
      "grad_norm": 12.157937049865723,
      "learning_rate": 2.7861445783132535e-05,
      "loss": 1.3851,
      "step": 14700
    },
    {
      "epoch": 8.91566265060241,
      "grad_norm": 9.655732154846191,
      "learning_rate": 2.7710843373493977e-05,
      "loss": 1.2454,
      "step": 14800
    },
    {
      "epoch": 8.975903614457831,
      "grad_norm": 13.2116060256958,
      "learning_rate": 2.756024096385542e-05,
      "loss": 1.2879,
      "step": 14900
    },
    {
      "epoch": 9.0,
      "eval_loss": 1.4407944679260254,
      "eval_runtime": 8.4497,
      "eval_samples_per_second": 207.345,
      "eval_steps_per_second": 25.918,
      "step": 14940
    },
    {
      "epoch": 9.036144578313253,
      "grad_norm": 11.194474220275879,
      "learning_rate": 2.7409638554216873e-05,
      "loss": 1.3637,
      "step": 15000
    },
    {
      "epoch": 9.096385542168674,
      "grad_norm": 7.781562805175781,
      "learning_rate": 2.7259036144578315e-05,
      "loss": 1.365,
      "step": 15100
    },
    {
      "epoch": 9.156626506024097,
      "grad_norm": 14.338587760925293,
      "learning_rate": 2.7108433734939758e-05,
      "loss": 1.2662,
      "step": 15200
    },
    {
      "epoch": 9.216867469879517,
      "grad_norm": 7.145852088928223,
      "learning_rate": 2.6957831325301207e-05,
      "loss": 1.3001,
      "step": 15300
    },
    {
      "epoch": 9.27710843373494,
      "grad_norm": 10.030890464782715,
      "learning_rate": 2.6807228915662653e-05,
      "loss": 1.2302,
      "step": 15400
    },
    {
      "epoch": 9.337349397590362,
      "grad_norm": 12.817167282104492,
      "learning_rate": 2.6656626506024096e-05,
      "loss": 1.2519,
      "step": 15500
    },
    {
      "epoch": 9.397590361445783,
      "grad_norm": 12.929186820983887,
      "learning_rate": 2.6506024096385545e-05,
      "loss": 1.3826,
      "step": 15600
    },
    {
      "epoch": 9.457831325301205,
      "grad_norm": 20.108184814453125,
      "learning_rate": 2.635542168674699e-05,
      "loss": 1.1651,
      "step": 15700
    },
    {
      "epoch": 9.518072289156626,
      "grad_norm": 14.570110321044922,
      "learning_rate": 2.6204819277108434e-05,
      "loss": 1.1807,
      "step": 15800
    },
    {
      "epoch": 9.578313253012048,
      "grad_norm": 14.84096908569336,
      "learning_rate": 2.6054216867469883e-05,
      "loss": 1.1847,
      "step": 15900
    },
    {
      "epoch": 9.638554216867469,
      "grad_norm": 16.828916549682617,
      "learning_rate": 2.5903614457831325e-05,
      "loss": 1.2316,
      "step": 16000
    },
    {
      "epoch": 9.698795180722891,
      "grad_norm": 13.866145133972168,
      "learning_rate": 2.575301204819277e-05,
      "loss": 1.2761,
      "step": 16100
    },
    {
      "epoch": 9.759036144578314,
      "grad_norm": 12.728357315063477,
      "learning_rate": 2.560240963855422e-05,
      "loss": 1.1666,
      "step": 16200
    },
    {
      "epoch": 9.819277108433734,
      "grad_norm": 14.573493957519531,
      "learning_rate": 2.5451807228915663e-05,
      "loss": 1.2245,
      "step": 16300
    },
    {
      "epoch": 9.879518072289157,
      "grad_norm": 9.48653793334961,
      "learning_rate": 2.530120481927711e-05,
      "loss": 1.2604,
      "step": 16400
    },
    {
      "epoch": 9.939759036144578,
      "grad_norm": 14.554814338684082,
      "learning_rate": 2.515060240963856e-05,
      "loss": 1.2884,
      "step": 16500
    },
    {
      "epoch": 10.0,
      "grad_norm": 8.633191108703613,
      "learning_rate": 2.5e-05,
      "loss": 1.2472,
      "step": 16600
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.4141443967819214,
      "eval_runtime": 8.4478,
      "eval_samples_per_second": 207.391,
      "eval_steps_per_second": 25.924,
      "step": 16600
    },
    {
      "epoch": 10.060240963855422,
      "grad_norm": 14.70865249633789,
      "learning_rate": 2.4849397590361447e-05,
      "loss": 1.2126,
      "step": 16700
    },
    {
      "epoch": 10.120481927710843,
      "grad_norm": 11.351482391357422,
      "learning_rate": 2.4698795180722893e-05,
      "loss": 1.1999,
      "step": 16800
    },
    {
      "epoch": 10.180722891566266,
      "grad_norm": 10.683926582336426,
      "learning_rate": 2.454819277108434e-05,
      "loss": 1.2603,
      "step": 16900
    },
    {
      "epoch": 10.240963855421686,
      "grad_norm": 16.68342399597168,
      "learning_rate": 2.4397590361445785e-05,
      "loss": 1.2,
      "step": 17000
    },
    {
      "epoch": 10.301204819277109,
      "grad_norm": 12.485552787780762,
      "learning_rate": 2.4246987951807228e-05,
      "loss": 1.1345,
      "step": 17100
    },
    {
      "epoch": 10.36144578313253,
      "grad_norm": 11.696818351745605,
      "learning_rate": 2.4096385542168677e-05,
      "loss": 1.1992,
      "step": 17200
    },
    {
      "epoch": 10.421686746987952,
      "grad_norm": 9.428984642028809,
      "learning_rate": 2.3945783132530123e-05,
      "loss": 1.2282,
      "step": 17300
    },
    {
      "epoch": 10.481927710843374,
      "grad_norm": 17.98362922668457,
      "learning_rate": 2.3795180722891565e-05,
      "loss": 1.1767,
      "step": 17400
    },
    {
      "epoch": 10.542168674698795,
      "grad_norm": 13.109770774841309,
      "learning_rate": 2.364457831325301e-05,
      "loss": 1.2776,
      "step": 17500
    },
    {
      "epoch": 10.602409638554217,
      "grad_norm": 11.13005256652832,
      "learning_rate": 2.349397590361446e-05,
      "loss": 1.2868,
      "step": 17600
    },
    {
      "epoch": 10.662650602409638,
      "grad_norm": 8.208806037902832,
      "learning_rate": 2.3343373493975903e-05,
      "loss": 1.2273,
      "step": 17700
    },
    {
      "epoch": 10.72289156626506,
      "grad_norm": 9.784603118896484,
      "learning_rate": 2.319277108433735e-05,
      "loss": 1.2728,
      "step": 17800
    },
    {
      "epoch": 10.783132530120483,
      "grad_norm": 12.974748611450195,
      "learning_rate": 2.3042168674698795e-05,
      "loss": 1.2484,
      "step": 17900
    },
    {
      "epoch": 10.843373493975903,
      "grad_norm": 9.347596168518066,
      "learning_rate": 2.289156626506024e-05,
      "loss": 1.1735,
      "step": 18000
    },
    {
      "epoch": 10.903614457831326,
      "grad_norm": 14.119878768920898,
      "learning_rate": 2.2740963855421687e-05,
      "loss": 1.2504,
      "step": 18100
    },
    {
      "epoch": 10.963855421686747,
      "grad_norm": 12.281881332397461,
      "learning_rate": 2.2590361445783133e-05,
      "loss": 1.2511,
      "step": 18200
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.3803859949111938,
      "eval_runtime": 8.4214,
      "eval_samples_per_second": 208.041,
      "eval_steps_per_second": 26.005,
      "step": 18260
    },
    {
      "epoch": 11.024096385542169,
      "grad_norm": 9.06450080871582,
      "learning_rate": 2.243975903614458e-05,
      "loss": 1.1816,
      "step": 18300
    },
    {
      "epoch": 11.08433734939759,
      "grad_norm": 12.375842094421387,
      "learning_rate": 2.2289156626506025e-05,
      "loss": 1.2413,
      "step": 18400
    },
    {
      "epoch": 11.144578313253012,
      "grad_norm": 13.779211044311523,
      "learning_rate": 2.213855421686747e-05,
      "loss": 1.1274,
      "step": 18500
    },
    {
      "epoch": 11.204819277108435,
      "grad_norm": 9.220314979553223,
      "learning_rate": 2.1987951807228917e-05,
      "loss": 1.1375,
      "step": 18600
    },
    {
      "epoch": 11.265060240963855,
      "grad_norm": 9.0660400390625,
      "learning_rate": 2.1837349397590363e-05,
      "loss": 1.117,
      "step": 18700
    },
    {
      "epoch": 11.325301204819278,
      "grad_norm": 15.071297645568848,
      "learning_rate": 2.168674698795181e-05,
      "loss": 1.1248,
      "step": 18800
    },
    {
      "epoch": 11.385542168674698,
      "grad_norm": 14.379444122314453,
      "learning_rate": 2.1536144578313255e-05,
      "loss": 1.2947,
      "step": 18900
    },
    {
      "epoch": 11.44578313253012,
      "grad_norm": 11.867013931274414,
      "learning_rate": 2.13855421686747e-05,
      "loss": 1.2347,
      "step": 19000
    },
    {
      "epoch": 11.506024096385541,
      "grad_norm": 14.20995044708252,
      "learning_rate": 2.1234939759036147e-05,
      "loss": 1.068,
      "step": 19100
    },
    {
      "epoch": 11.566265060240964,
      "grad_norm": 11.97098445892334,
      "learning_rate": 2.1084337349397593e-05,
      "loss": 1.1846,
      "step": 19200
    },
    {
      "epoch": 11.626506024096386,
      "grad_norm": 8.735626220703125,
      "learning_rate": 2.0933734939759035e-05,
      "loss": 1.1602,
      "step": 19300
    },
    {
      "epoch": 11.686746987951807,
      "grad_norm": 9.540921211242676,
      "learning_rate": 2.0783132530120485e-05,
      "loss": 1.1541,
      "step": 19400
    },
    {
      "epoch": 11.74698795180723,
      "grad_norm": 10.996045112609863,
      "learning_rate": 2.063253012048193e-05,
      "loss": 1.1554,
      "step": 19500
    },
    {
      "epoch": 11.80722891566265,
      "grad_norm": 15.636848449707031,
      "learning_rate": 2.0481927710843373e-05,
      "loss": 1.1686,
      "step": 19600
    },
    {
      "epoch": 11.867469879518072,
      "grad_norm": 9.59758472442627,
      "learning_rate": 2.033132530120482e-05,
      "loss": 1.0948,
      "step": 19700
    },
    {
      "epoch": 11.927710843373493,
      "grad_norm": 10.483545303344727,
      "learning_rate": 2.018072289156627e-05,
      "loss": 1.1764,
      "step": 19800
    },
    {
      "epoch": 11.987951807228916,
      "grad_norm": 7.268214702606201,
      "learning_rate": 2.003012048192771e-05,
      "loss": 1.1346,
      "step": 19900
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.3848719596862793,
      "eval_runtime": 8.4673,
      "eval_samples_per_second": 206.914,
      "eval_steps_per_second": 25.864,
      "step": 19920
    },
    {
      "epoch": 12.048192771084338,
      "grad_norm": 22.33002471923828,
      "learning_rate": 1.9879518072289157e-05,
      "loss": 0.9949,
      "step": 20000
    },
    {
      "epoch": 12.108433734939759,
      "grad_norm": 11.478543281555176,
      "learning_rate": 1.9728915662650603e-05,
      "loss": 1.1223,
      "step": 20100
    },
    {
      "epoch": 12.168674698795181,
      "grad_norm": 17.601564407348633,
      "learning_rate": 1.957831325301205e-05,
      "loss": 1.0794,
      "step": 20200
    },
    {
      "epoch": 12.228915662650602,
      "grad_norm": 8.753165245056152,
      "learning_rate": 1.9427710843373495e-05,
      "loss": 1.118,
      "step": 20300
    },
    {
      "epoch": 12.289156626506024,
      "grad_norm": 12.090471267700195,
      "learning_rate": 1.927710843373494e-05,
      "loss": 1.1203,
      "step": 20400
    },
    {
      "epoch": 12.349397590361447,
      "grad_norm": 21.380651473999023,
      "learning_rate": 1.9126506024096387e-05,
      "loss": 1.0988,
      "step": 20500
    },
    {
      "epoch": 12.409638554216867,
      "grad_norm": 11.32951545715332,
      "learning_rate": 1.8975903614457833e-05,
      "loss": 1.083,
      "step": 20600
    },
    {
      "epoch": 12.46987951807229,
      "grad_norm": 10.460762023925781,
      "learning_rate": 1.882530120481928e-05,
      "loss": 1.1243,
      "step": 20700
    },
    {
      "epoch": 12.53012048192771,
      "grad_norm": 17.672910690307617,
      "learning_rate": 1.867469879518072e-05,
      "loss": 1.1581,
      "step": 20800
    },
    {
      "epoch": 12.590361445783133,
      "grad_norm": 9.003058433532715,
      "learning_rate": 1.852409638554217e-05,
      "loss": 1.1349,
      "step": 20900
    },
    {
      "epoch": 12.650602409638553,
      "grad_norm": 11.904072761535645,
      "learning_rate": 1.8373493975903617e-05,
      "loss": 1.2251,
      "step": 21000
    },
    {
      "epoch": 12.710843373493976,
      "grad_norm": 9.026190757751465,
      "learning_rate": 1.822289156626506e-05,
      "loss": 1.0606,
      "step": 21100
    },
    {
      "epoch": 12.771084337349398,
      "grad_norm": 11.000221252441406,
      "learning_rate": 1.8072289156626505e-05,
      "loss": 1.1287,
      "step": 21200
    },
    {
      "epoch": 12.831325301204819,
      "grad_norm": 9.88868236541748,
      "learning_rate": 1.7921686746987955e-05,
      "loss": 1.0914,
      "step": 21300
    },
    {
      "epoch": 12.891566265060241,
      "grad_norm": 15.056862831115723,
      "learning_rate": 1.7771084337349397e-05,
      "loss": 1.0548,
      "step": 21400
    },
    {
      "epoch": 12.951807228915662,
      "grad_norm": 16.476198196411133,
      "learning_rate": 1.7620481927710843e-05,
      "loss": 1.1792,
      "step": 21500
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.3383212089538574,
      "eval_runtime": 8.4176,
      "eval_samples_per_second": 208.136,
      "eval_steps_per_second": 26.017,
      "step": 21580
    },
    {
      "epoch": 13.012048192771084,
      "grad_norm": 12.24599552154541,
      "learning_rate": 1.7469879518072292e-05,
      "loss": 1.0501,
      "step": 21600
    },
    {
      "epoch": 13.072289156626505,
      "grad_norm": 10.244356155395508,
      "learning_rate": 1.7319277108433735e-05,
      "loss": 1.0984,
      "step": 21700
    },
    {
      "epoch": 13.132530120481928,
      "grad_norm": 9.918916702270508,
      "learning_rate": 1.716867469879518e-05,
      "loss": 1.1481,
      "step": 21800
    },
    {
      "epoch": 13.19277108433735,
      "grad_norm": 9.42471694946289,
      "learning_rate": 1.7018072289156627e-05,
      "loss": 1.0978,
      "step": 21900
    },
    {
      "epoch": 13.25301204819277,
      "grad_norm": 12.263805389404297,
      "learning_rate": 1.6867469879518073e-05,
      "loss": 1.0388,
      "step": 22000
    },
    {
      "epoch": 13.313253012048193,
      "grad_norm": 13.835927963256836,
      "learning_rate": 1.671686746987952e-05,
      "loss": 1.1141,
      "step": 22100
    },
    {
      "epoch": 13.373493975903614,
      "grad_norm": 8.785560607910156,
      "learning_rate": 1.6566265060240965e-05,
      "loss": 1.1353,
      "step": 22200
    },
    {
      "epoch": 13.433734939759036,
      "grad_norm": 24.04002571105957,
      "learning_rate": 1.641566265060241e-05,
      "loss": 1.0955,
      "step": 22300
    },
    {
      "epoch": 13.493975903614459,
      "grad_norm": 14.388693809509277,
      "learning_rate": 1.6265060240963857e-05,
      "loss": 1.0403,
      "step": 22400
    },
    {
      "epoch": 13.55421686746988,
      "grad_norm": 16.404537200927734,
      "learning_rate": 1.6114457831325303e-05,
      "loss": 1.0838,
      "step": 22500
    },
    {
      "epoch": 13.614457831325302,
      "grad_norm": 12.617488861083984,
      "learning_rate": 1.5963855421686745e-05,
      "loss": 1.0681,
      "step": 22600
    },
    {
      "epoch": 13.674698795180722,
      "grad_norm": 7.60827112197876,
      "learning_rate": 1.5813253012048195e-05,
      "loss": 1.0099,
      "step": 22700
    },
    {
      "epoch": 13.734939759036145,
      "grad_norm": 7.849460601806641,
      "learning_rate": 1.566265060240964e-05,
      "loss": 1.0765,
      "step": 22800
    },
    {
      "epoch": 13.795180722891565,
      "grad_norm": 10.586328506469727,
      "learning_rate": 1.5512048192771086e-05,
      "loss": 1.0332,
      "step": 22900
    },
    {
      "epoch": 13.855421686746988,
      "grad_norm": 13.660238265991211,
      "learning_rate": 1.536144578313253e-05,
      "loss": 0.9912,
      "step": 23000
    },
    {
      "epoch": 13.91566265060241,
      "grad_norm": 10.457139015197754,
      "learning_rate": 1.5210843373493977e-05,
      "loss": 1.076,
      "step": 23100
    },
    {
      "epoch": 13.975903614457831,
      "grad_norm": 7.776251316070557,
      "learning_rate": 1.5060240963855424e-05,
      "loss": 1.1674,
      "step": 23200
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.2865040302276611,
      "eval_runtime": 8.4093,
      "eval_samples_per_second": 208.34,
      "eval_steps_per_second": 26.043,
      "step": 23240
    },
    {
      "epoch": 14.036144578313253,
      "grad_norm": 14.207160949707031,
      "learning_rate": 1.4909638554216867e-05,
      "loss": 1.0935,
      "step": 23300
    },
    {
      "epoch": 14.096385542168674,
      "grad_norm": 11.101436614990234,
      "learning_rate": 1.4759036144578315e-05,
      "loss": 1.0731,
      "step": 23400
    },
    {
      "epoch": 14.156626506024097,
      "grad_norm": 12.155705451965332,
      "learning_rate": 1.460843373493976e-05,
      "loss": 1.0143,
      "step": 23500
    },
    {
      "epoch": 14.216867469879517,
      "grad_norm": 11.928243637084961,
      "learning_rate": 1.4457831325301205e-05,
      "loss": 1.0435,
      "step": 23600
    },
    {
      "epoch": 14.27710843373494,
      "grad_norm": 16.649595260620117,
      "learning_rate": 1.430722891566265e-05,
      "loss": 1.0797,
      "step": 23700
    },
    {
      "epoch": 14.337349397590362,
      "grad_norm": 14.165909767150879,
      "learning_rate": 1.4156626506024098e-05,
      "loss": 1.0042,
      "step": 23800
    },
    {
      "epoch": 14.397590361445783,
      "grad_norm": 9.470396041870117,
      "learning_rate": 1.4006024096385543e-05,
      "loss": 1.0454,
      "step": 23900
    },
    {
      "epoch": 14.457831325301205,
      "grad_norm": 10.385326385498047,
      "learning_rate": 1.3855421686746989e-05,
      "loss": 1.026,
      "step": 24000
    },
    {
      "epoch": 14.518072289156626,
      "grad_norm": 11.568987846374512,
      "learning_rate": 1.3704819277108436e-05,
      "loss": 1.054,
      "step": 24100
    },
    {
      "epoch": 14.578313253012048,
      "grad_norm": 6.361479759216309,
      "learning_rate": 1.3554216867469879e-05,
      "loss": 0.9941,
      "step": 24200
    },
    {
      "epoch": 14.638554216867469,
      "grad_norm": 10.675122261047363,
      "learning_rate": 1.3403614457831327e-05,
      "loss": 1.0922,
      "step": 24300
    },
    {
      "epoch": 14.698795180722891,
      "grad_norm": 12.425195693969727,
      "learning_rate": 1.3253012048192772e-05,
      "loss": 1.0155,
      "step": 24400
    },
    {
      "epoch": 14.759036144578314,
      "grad_norm": 8.762821197509766,
      "learning_rate": 1.3102409638554217e-05,
      "loss": 1.099,
      "step": 24500
    },
    {
      "epoch": 14.819277108433734,
      "grad_norm": 9.43861198425293,
      "learning_rate": 1.2951807228915663e-05,
      "loss": 1.0455,
      "step": 24600
    },
    {
      "epoch": 14.879518072289157,
      "grad_norm": 7.305152416229248,
      "learning_rate": 1.280120481927711e-05,
      "loss": 0.9818,
      "step": 24700
    },
    {
      "epoch": 14.939759036144578,
      "grad_norm": 11.864030838012695,
      "learning_rate": 1.2650602409638555e-05,
      "loss": 1.0755,
      "step": 24800
    },
    {
      "epoch": 15.0,
      "grad_norm": 20.639589309692383,
      "learning_rate": 1.25e-05,
      "loss": 1.0155,
      "step": 24900
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.2957841157913208,
      "eval_runtime": 8.4169,
      "eval_samples_per_second": 208.153,
      "eval_steps_per_second": 26.019,
      "step": 24900
    },
    {
      "epoch": 15.060240963855422,
      "grad_norm": 12.876925468444824,
      "learning_rate": 1.2349397590361447e-05,
      "loss": 1.0446,
      "step": 25000
    },
    {
      "epoch": 15.120481927710843,
      "grad_norm": 11.58169174194336,
      "learning_rate": 1.2198795180722893e-05,
      "loss": 1.0505,
      "step": 25100
    },
    {
      "epoch": 15.180722891566266,
      "grad_norm": 13.829821586608887,
      "learning_rate": 1.2048192771084338e-05,
      "loss": 1.05,
      "step": 25200
    },
    {
      "epoch": 15.240963855421686,
      "grad_norm": 12.285703659057617,
      "learning_rate": 1.1897590361445783e-05,
      "loss": 1.0261,
      "step": 25300
    },
    {
      "epoch": 15.301204819277109,
      "grad_norm": 19.40553855895996,
      "learning_rate": 1.174698795180723e-05,
      "loss": 1.1363,
      "step": 25400
    },
    {
      "epoch": 15.36144578313253,
      "grad_norm": 5.610882759094238,
      "learning_rate": 1.1596385542168675e-05,
      "loss": 0.9964,
      "step": 25500
    },
    {
      "epoch": 15.421686746987952,
      "grad_norm": 17.157751083374023,
      "learning_rate": 1.144578313253012e-05,
      "loss": 1.0757,
      "step": 25600
    },
    {
      "epoch": 15.481927710843374,
      "grad_norm": 9.994653701782227,
      "learning_rate": 1.1295180722891567e-05,
      "loss": 0.9395,
      "step": 25700
    },
    {
      "epoch": 15.542168674698795,
      "grad_norm": 14.44615650177002,
      "learning_rate": 1.1144578313253013e-05,
      "loss": 1.0167,
      "step": 25800
    },
    {
      "epoch": 15.602409638554217,
      "grad_norm": 15.736973762512207,
      "learning_rate": 1.0993975903614459e-05,
      "loss": 0.9858,
      "step": 25900
    },
    {
      "epoch": 15.662650602409638,
      "grad_norm": 10.010150909423828,
      "learning_rate": 1.0843373493975904e-05,
      "loss": 1.034,
      "step": 26000
    },
    {
      "epoch": 15.72289156626506,
      "grad_norm": 8.199516296386719,
      "learning_rate": 1.069277108433735e-05,
      "loss": 1.0671,
      "step": 26100
    },
    {
      "epoch": 15.783132530120483,
      "grad_norm": 12.757546424865723,
      "learning_rate": 1.0542168674698796e-05,
      "loss": 1.0027,
      "step": 26200
    },
    {
      "epoch": 15.843373493975903,
      "grad_norm": 17.03363800048828,
      "learning_rate": 1.0391566265060242e-05,
      "loss": 0.9403,
      "step": 26300
    },
    {
      "epoch": 15.903614457831326,
      "grad_norm": 5.884312152862549,
      "learning_rate": 1.0240963855421687e-05,
      "loss": 0.9827,
      "step": 26400
    },
    {
      "epoch": 15.963855421686747,
      "grad_norm": 7.1812262535095215,
      "learning_rate": 1.0090361445783134e-05,
      "loss": 0.9498,
      "step": 26500
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.3341459035873413,
      "eval_runtime": 8.5069,
      "eval_samples_per_second": 205.951,
      "eval_steps_per_second": 25.744,
      "step": 26560
    },
    {
      "epoch": 16.02409638554217,
      "grad_norm": 8.268779754638672,
      "learning_rate": 9.939759036144579e-06,
      "loss": 1.0043,
      "step": 26600
    },
    {
      "epoch": 16.08433734939759,
      "grad_norm": 10.612716674804688,
      "learning_rate": 9.789156626506024e-06,
      "loss": 1.0334,
      "step": 26700
    },
    {
      "epoch": 16.14457831325301,
      "grad_norm": 12.72981071472168,
      "learning_rate": 9.63855421686747e-06,
      "loss": 0.9137,
      "step": 26800
    },
    {
      "epoch": 16.204819277108435,
      "grad_norm": 10.871726036071777,
      "learning_rate": 9.487951807228916e-06,
      "loss": 0.9301,
      "step": 26900
    },
    {
      "epoch": 16.265060240963855,
      "grad_norm": 8.503959655761719,
      "learning_rate": 9.33734939759036e-06,
      "loss": 1.0299,
      "step": 27000
    },
    {
      "epoch": 16.325301204819276,
      "grad_norm": 12.711082458496094,
      "learning_rate": 9.186746987951808e-06,
      "loss": 0.9416,
      "step": 27100
    },
    {
      "epoch": 16.3855421686747,
      "grad_norm": 12.0672025680542,
      "learning_rate": 9.036144578313253e-06,
      "loss": 0.9009,
      "step": 27200
    },
    {
      "epoch": 16.44578313253012,
      "grad_norm": 13.318561553955078,
      "learning_rate": 8.885542168674699e-06,
      "loss": 0.9529,
      "step": 27300
    },
    {
      "epoch": 16.50602409638554,
      "grad_norm": 8.603972434997559,
      "learning_rate": 8.734939759036146e-06,
      "loss": 0.9395,
      "step": 27400
    },
    {
      "epoch": 16.566265060240966,
      "grad_norm": 11.582938194274902,
      "learning_rate": 8.58433734939759e-06,
      "loss": 1.0428,
      "step": 27500
    },
    {
      "epoch": 16.626506024096386,
      "grad_norm": 10.063677787780762,
      "learning_rate": 8.433734939759036e-06,
      "loss": 1.036,
      "step": 27600
    },
    {
      "epoch": 16.686746987951807,
      "grad_norm": 12.762730598449707,
      "learning_rate": 8.283132530120482e-06,
      "loss": 1.0074,
      "step": 27700
    },
    {
      "epoch": 16.746987951807228,
      "grad_norm": 20.628021240234375,
      "learning_rate": 8.132530120481928e-06,
      "loss": 1.0153,
      "step": 27800
    },
    {
      "epoch": 16.80722891566265,
      "grad_norm": 15.744721412658691,
      "learning_rate": 7.981927710843373e-06,
      "loss": 0.9966,
      "step": 27900
    },
    {
      "epoch": 16.867469879518072,
      "grad_norm": 15.654077529907227,
      "learning_rate": 7.83132530120482e-06,
      "loss": 1.0018,
      "step": 28000
    },
    {
      "epoch": 16.927710843373493,
      "grad_norm": 12.640127182006836,
      "learning_rate": 7.680722891566265e-06,
      "loss": 0.9599,
      "step": 28100
    },
    {
      "epoch": 16.987951807228917,
      "grad_norm": 8.126995086669922,
      "learning_rate": 7.530120481927712e-06,
      "loss": 1.054,
      "step": 28200
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.2604329586029053,
      "eval_runtime": 8.4493,
      "eval_samples_per_second": 207.354,
      "eval_steps_per_second": 25.919,
      "step": 28220
    },
    {
      "epoch": 17.048192771084338,
      "grad_norm": 13.839334487915039,
      "learning_rate": 7.379518072289157e-06,
      "loss": 0.9672,
      "step": 28300
    },
    {
      "epoch": 17.10843373493976,
      "grad_norm": 10.549060821533203,
      "learning_rate": 7.228915662650602e-06,
      "loss": 0.9585,
      "step": 28400
    },
    {
      "epoch": 17.16867469879518,
      "grad_norm": 9.006048202514648,
      "learning_rate": 7.078313253012049e-06,
      "loss": 0.9509,
      "step": 28500
    },
    {
      "epoch": 17.228915662650603,
      "grad_norm": 18.572532653808594,
      "learning_rate": 6.927710843373494e-06,
      "loss": 0.9251,
      "step": 28600
    },
    {
      "epoch": 17.289156626506024,
      "grad_norm": 16.158830642700195,
      "learning_rate": 6.7771084337349394e-06,
      "loss": 0.9869,
      "step": 28700
    },
    {
      "epoch": 17.349397590361445,
      "grad_norm": 11.101762771606445,
      "learning_rate": 6.626506024096386e-06,
      "loss": 1.0658,
      "step": 28800
    },
    {
      "epoch": 17.40963855421687,
      "grad_norm": 10.889130592346191,
      "learning_rate": 6.475903614457831e-06,
      "loss": 0.9522,
      "step": 28900
    },
    {
      "epoch": 17.46987951807229,
      "grad_norm": 9.147285461425781,
      "learning_rate": 6.325301204819277e-06,
      "loss": 0.9871,
      "step": 29000
    },
    {
      "epoch": 17.53012048192771,
      "grad_norm": 10.38326358795166,
      "learning_rate": 6.174698795180723e-06,
      "loss": 1.045,
      "step": 29100
    },
    {
      "epoch": 17.59036144578313,
      "grad_norm": 14.029674530029297,
      "learning_rate": 6.024096385542169e-06,
      "loss": 0.9271,
      "step": 29200
    },
    {
      "epoch": 17.650602409638555,
      "grad_norm": 13.182923316955566,
      "learning_rate": 5.873493975903615e-06,
      "loss": 0.9397,
      "step": 29300
    },
    {
      "epoch": 17.710843373493976,
      "grad_norm": 15.336541175842285,
      "learning_rate": 5.72289156626506e-06,
      "loss": 0.9144,
      "step": 29400
    },
    {
      "epoch": 17.771084337349397,
      "grad_norm": 12.90995979309082,
      "learning_rate": 5.572289156626506e-06,
      "loss": 0.9035,
      "step": 29500
    },
    {
      "epoch": 17.83132530120482,
      "grad_norm": 9.395803451538086,
      "learning_rate": 5.421686746987952e-06,
      "loss": 0.9485,
      "step": 29600
    },
    {
      "epoch": 17.89156626506024,
      "grad_norm": 11.728340148925781,
      "learning_rate": 5.271084337349398e-06,
      "loss": 0.955,
      "step": 29700
    },
    {
      "epoch": 17.951807228915662,
      "grad_norm": 10.588069915771484,
      "learning_rate": 5.120481927710843e-06,
      "loss": 0.976,
      "step": 29800
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.2173517942428589,
      "eval_runtime": 8.4629,
      "eval_samples_per_second": 207.02,
      "eval_steps_per_second": 25.878,
      "step": 29880
    }
  ],
  "logging_steps": 100,
  "max_steps": 33200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.288300492101222e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
