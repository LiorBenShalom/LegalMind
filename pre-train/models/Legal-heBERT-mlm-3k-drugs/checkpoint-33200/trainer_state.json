{
  "best_metric": 1.7690237760543823,
  "best_model_checkpoint": "./Legal-heBERT-mlm-3k-drugs/checkpoint-31540",
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 33200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.060240963855421686,
      "grad_norm": 16.135725021362305,
      "learning_rate": 4.984939759036145e-05,
      "loss": 3.766,
      "step": 100
    },
    {
      "epoch": 0.12048192771084337,
      "grad_norm": 16.62333869934082,
      "learning_rate": 4.9698795180722894e-05,
      "loss": 3.5011,
      "step": 200
    },
    {
      "epoch": 0.18072289156626506,
      "grad_norm": 21.045743942260742,
      "learning_rate": 4.954819277108434e-05,
      "loss": 3.5568,
      "step": 300
    },
    {
      "epoch": 0.24096385542168675,
      "grad_norm": 9.225144386291504,
      "learning_rate": 4.9397590361445786e-05,
      "loss": 3.3933,
      "step": 400
    },
    {
      "epoch": 0.30120481927710846,
      "grad_norm": 11.479402542114258,
      "learning_rate": 4.924698795180723e-05,
      "loss": 3.3947,
      "step": 500
    },
    {
      "epoch": 0.3614457831325301,
      "grad_norm": 15.899075508117676,
      "learning_rate": 4.909638554216868e-05,
      "loss": 3.3268,
      "step": 600
    },
    {
      "epoch": 0.42168674698795183,
      "grad_norm": 9.399020195007324,
      "learning_rate": 4.8945783132530124e-05,
      "loss": 2.9963,
      "step": 700
    },
    {
      "epoch": 0.4819277108433735,
      "grad_norm": 12.34286880493164,
      "learning_rate": 4.879518072289157e-05,
      "loss": 3.1298,
      "step": 800
    },
    {
      "epoch": 0.5421686746987951,
      "grad_norm": 15.879395484924316,
      "learning_rate": 4.8644578313253016e-05,
      "loss": 3.2453,
      "step": 900
    },
    {
      "epoch": 0.6024096385542169,
      "grad_norm": 21.589269638061523,
      "learning_rate": 4.8493975903614455e-05,
      "loss": 3.1465,
      "step": 1000
    },
    {
      "epoch": 0.6626506024096386,
      "grad_norm": 17.817720413208008,
      "learning_rate": 4.834337349397591e-05,
      "loss": 3.1584,
      "step": 1100
    },
    {
      "epoch": 0.7228915662650602,
      "grad_norm": 12.898751258850098,
      "learning_rate": 4.8192771084337354e-05,
      "loss": 3.2097,
      "step": 1200
    },
    {
      "epoch": 0.7831325301204819,
      "grad_norm": 13.271458625793457,
      "learning_rate": 4.804216867469879e-05,
      "loss": 3.1481,
      "step": 1300
    },
    {
      "epoch": 0.8433734939759037,
      "grad_norm": 17.586658477783203,
      "learning_rate": 4.7891566265060246e-05,
      "loss": 3.095,
      "step": 1400
    },
    {
      "epoch": 0.9036144578313253,
      "grad_norm": 10.947731018066406,
      "learning_rate": 4.774096385542169e-05,
      "loss": 2.9548,
      "step": 1500
    },
    {
      "epoch": 0.963855421686747,
      "grad_norm": 16.41858673095703,
      "learning_rate": 4.759036144578313e-05,
      "loss": 3.0696,
      "step": 1600
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.7412402629852295,
      "eval_runtime": 12.3312,
      "eval_samples_per_second": 142.078,
      "eval_steps_per_second": 17.76,
      "step": 1660
    },
    {
      "epoch": 1.0240963855421688,
      "grad_norm": 11.844685554504395,
      "learning_rate": 4.7439759036144584e-05,
      "loss": 3.0629,
      "step": 1700
    },
    {
      "epoch": 1.0843373493975903,
      "grad_norm": 14.129671096801758,
      "learning_rate": 4.728915662650602e-05,
      "loss": 2.9888,
      "step": 1800
    },
    {
      "epoch": 1.144578313253012,
      "grad_norm": 11.560163497924805,
      "learning_rate": 4.713855421686747e-05,
      "loss": 2.9785,
      "step": 1900
    },
    {
      "epoch": 1.2048192771084336,
      "grad_norm": 13.789216041564941,
      "learning_rate": 4.698795180722892e-05,
      "loss": 2.8533,
      "step": 2000
    },
    {
      "epoch": 1.2650602409638554,
      "grad_norm": 16.439605712890625,
      "learning_rate": 4.683734939759036e-05,
      "loss": 2.8082,
      "step": 2100
    },
    {
      "epoch": 1.3253012048192772,
      "grad_norm": 10.815258979797363,
      "learning_rate": 4.668674698795181e-05,
      "loss": 2.837,
      "step": 2200
    },
    {
      "epoch": 1.3855421686746987,
      "grad_norm": 14.258964538574219,
      "learning_rate": 4.653614457831326e-05,
      "loss": 2.8298,
      "step": 2300
    },
    {
      "epoch": 1.4457831325301205,
      "grad_norm": 13.712150573730469,
      "learning_rate": 4.63855421686747e-05,
      "loss": 3.0018,
      "step": 2400
    },
    {
      "epoch": 1.5060240963855422,
      "grad_norm": 19.50926399230957,
      "learning_rate": 4.6234939759036145e-05,
      "loss": 2.6945,
      "step": 2500
    },
    {
      "epoch": 1.5662650602409638,
      "grad_norm": 11.538487434387207,
      "learning_rate": 4.608433734939759e-05,
      "loss": 2.8347,
      "step": 2600
    },
    {
      "epoch": 1.6265060240963856,
      "grad_norm": 12.963778495788574,
      "learning_rate": 4.5933734939759037e-05,
      "loss": 3.0488,
      "step": 2700
    },
    {
      "epoch": 1.6867469879518073,
      "grad_norm": 17.513486862182617,
      "learning_rate": 4.578313253012048e-05,
      "loss": 2.8764,
      "step": 2800
    },
    {
      "epoch": 1.7469879518072289,
      "grad_norm": 14.504745483398438,
      "learning_rate": 4.563253012048193e-05,
      "loss": 2.6345,
      "step": 2900
    },
    {
      "epoch": 1.8072289156626506,
      "grad_norm": 13.438325881958008,
      "learning_rate": 4.5481927710843374e-05,
      "loss": 3.0163,
      "step": 3000
    },
    {
      "epoch": 1.8674698795180724,
      "grad_norm": 23.806535720825195,
      "learning_rate": 4.533132530120482e-05,
      "loss": 2.6164,
      "step": 3100
    },
    {
      "epoch": 1.927710843373494,
      "grad_norm": 13.528517723083496,
      "learning_rate": 4.5180722891566266e-05,
      "loss": 2.7457,
      "step": 3200
    },
    {
      "epoch": 1.9879518072289155,
      "grad_norm": 15.657308578491211,
      "learning_rate": 4.503012048192771e-05,
      "loss": 2.7172,
      "step": 3300
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.612240791320801,
      "eval_runtime": 12.4933,
      "eval_samples_per_second": 140.235,
      "eval_steps_per_second": 17.529,
      "step": 3320
    },
    {
      "epoch": 2.0481927710843375,
      "grad_norm": 14.847732543945312,
      "learning_rate": 4.487951807228916e-05,
      "loss": 2.835,
      "step": 3400
    },
    {
      "epoch": 2.108433734939759,
      "grad_norm": 9.549299240112305,
      "learning_rate": 4.4728915662650604e-05,
      "loss": 2.5722,
      "step": 3500
    },
    {
      "epoch": 2.1686746987951806,
      "grad_norm": 14.888090133666992,
      "learning_rate": 4.457831325301205e-05,
      "loss": 2.6111,
      "step": 3600
    },
    {
      "epoch": 2.2289156626506026,
      "grad_norm": 9.781625747680664,
      "learning_rate": 4.4427710843373496e-05,
      "loss": 2.6383,
      "step": 3700
    },
    {
      "epoch": 2.289156626506024,
      "grad_norm": 15.257969856262207,
      "learning_rate": 4.427710843373494e-05,
      "loss": 2.5709,
      "step": 3800
    },
    {
      "epoch": 2.3493975903614457,
      "grad_norm": 16.45595359802246,
      "learning_rate": 4.412650602409639e-05,
      "loss": 2.7836,
      "step": 3900
    },
    {
      "epoch": 2.4096385542168672,
      "grad_norm": 12.211313247680664,
      "learning_rate": 4.3975903614457834e-05,
      "loss": 2.5043,
      "step": 4000
    },
    {
      "epoch": 2.4698795180722892,
      "grad_norm": 13.743754386901855,
      "learning_rate": 4.382530120481928e-05,
      "loss": 2.8011,
      "step": 4100
    },
    {
      "epoch": 2.5301204819277108,
      "grad_norm": 12.155820846557617,
      "learning_rate": 4.3674698795180726e-05,
      "loss": 2.7119,
      "step": 4200
    },
    {
      "epoch": 2.5903614457831328,
      "grad_norm": 10.66183853149414,
      "learning_rate": 4.352409638554217e-05,
      "loss": 2.7093,
      "step": 4300
    },
    {
      "epoch": 2.6506024096385543,
      "grad_norm": 8.72943115234375,
      "learning_rate": 4.337349397590362e-05,
      "loss": 2.5828,
      "step": 4400
    },
    {
      "epoch": 2.710843373493976,
      "grad_norm": 10.503609657287598,
      "learning_rate": 4.3222891566265064e-05,
      "loss": 2.6884,
      "step": 4500
    },
    {
      "epoch": 2.7710843373493974,
      "grad_norm": 12.126480102539062,
      "learning_rate": 4.307228915662651e-05,
      "loss": 2.7325,
      "step": 4600
    },
    {
      "epoch": 2.8313253012048194,
      "grad_norm": 11.669336318969727,
      "learning_rate": 4.2921686746987956e-05,
      "loss": 2.6089,
      "step": 4700
    },
    {
      "epoch": 2.891566265060241,
      "grad_norm": 23.052133560180664,
      "learning_rate": 4.27710843373494e-05,
      "loss": 2.6466,
      "step": 4800
    },
    {
      "epoch": 2.9518072289156625,
      "grad_norm": 15.905881881713867,
      "learning_rate": 4.262048192771085e-05,
      "loss": 2.6313,
      "step": 4900
    },
    {
      "epoch": 3.0,
      "eval_loss": 2.4831721782684326,
      "eval_runtime": 12.5592,
      "eval_samples_per_second": 139.5,
      "eval_steps_per_second": 17.437,
      "step": 4980
    },
    {
      "epoch": 3.0120481927710845,
      "grad_norm": 14.9847412109375,
      "learning_rate": 4.2469879518072294e-05,
      "loss": 2.6353,
      "step": 5000
    },
    {
      "epoch": 3.072289156626506,
      "grad_norm": 14.022622108459473,
      "learning_rate": 4.231927710843373e-05,
      "loss": 2.4706,
      "step": 5100
    },
    {
      "epoch": 3.1325301204819276,
      "grad_norm": 15.306047439575195,
      "learning_rate": 4.2168674698795186e-05,
      "loss": 2.5218,
      "step": 5200
    },
    {
      "epoch": 3.1927710843373496,
      "grad_norm": 19.889949798583984,
      "learning_rate": 4.201807228915663e-05,
      "loss": 2.5357,
      "step": 5300
    },
    {
      "epoch": 3.253012048192771,
      "grad_norm": 10.397090911865234,
      "learning_rate": 4.186746987951807e-05,
      "loss": 2.3554,
      "step": 5400
    },
    {
      "epoch": 3.3132530120481927,
      "grad_norm": 8.099478721618652,
      "learning_rate": 4.1716867469879523e-05,
      "loss": 2.6377,
      "step": 5500
    },
    {
      "epoch": 3.3734939759036147,
      "grad_norm": 12.290262222290039,
      "learning_rate": 4.156626506024097e-05,
      "loss": 2.2616,
      "step": 5600
    },
    {
      "epoch": 3.433734939759036,
      "grad_norm": 12.227418899536133,
      "learning_rate": 4.141566265060241e-05,
      "loss": 2.5855,
      "step": 5700
    },
    {
      "epoch": 3.4939759036144578,
      "grad_norm": 16.37422752380371,
      "learning_rate": 4.126506024096386e-05,
      "loss": 2.3449,
      "step": 5800
    },
    {
      "epoch": 3.5542168674698793,
      "grad_norm": 7.601187229156494,
      "learning_rate": 4.11144578313253e-05,
      "loss": 2.3006,
      "step": 5900
    },
    {
      "epoch": 3.6144578313253013,
      "grad_norm": 12.303210258483887,
      "learning_rate": 4.0963855421686746e-05,
      "loss": 2.4424,
      "step": 6000
    },
    {
      "epoch": 3.674698795180723,
      "grad_norm": 9.58899974822998,
      "learning_rate": 4.08132530120482e-05,
      "loss": 2.4322,
      "step": 6100
    },
    {
      "epoch": 3.734939759036145,
      "grad_norm": 12.921128273010254,
      "learning_rate": 4.066265060240964e-05,
      "loss": 2.5831,
      "step": 6200
    },
    {
      "epoch": 3.7951807228915664,
      "grad_norm": 9.89083194732666,
      "learning_rate": 4.0512048192771084e-05,
      "loss": 2.4261,
      "step": 6300
    },
    {
      "epoch": 3.855421686746988,
      "grad_norm": 11.02531623840332,
      "learning_rate": 4.036144578313254e-05,
      "loss": 2.5356,
      "step": 6400
    },
    {
      "epoch": 3.9156626506024095,
      "grad_norm": 9.055903434753418,
      "learning_rate": 4.0210843373493976e-05,
      "loss": 2.4135,
      "step": 6500
    },
    {
      "epoch": 3.9759036144578315,
      "grad_norm": 11.178831100463867,
      "learning_rate": 4.006024096385542e-05,
      "loss": 2.4598,
      "step": 6600
    },
    {
      "epoch": 4.0,
      "eval_loss": 2.3156204223632812,
      "eval_runtime": 12.4996,
      "eval_samples_per_second": 140.165,
      "eval_steps_per_second": 17.521,
      "step": 6640
    },
    {
      "epoch": 4.036144578313253,
      "grad_norm": 11.692680358886719,
      "learning_rate": 3.9909638554216875e-05,
      "loss": 2.3511,
      "step": 6700
    },
    {
      "epoch": 4.096385542168675,
      "grad_norm": 11.245603561401367,
      "learning_rate": 3.9759036144578314e-05,
      "loss": 2.3697,
      "step": 6800
    },
    {
      "epoch": 4.156626506024097,
      "grad_norm": 23.24864387512207,
      "learning_rate": 3.960843373493976e-05,
      "loss": 2.3355,
      "step": 6900
    },
    {
      "epoch": 4.216867469879518,
      "grad_norm": 9.021707534790039,
      "learning_rate": 3.9457831325301206e-05,
      "loss": 2.325,
      "step": 7000
    },
    {
      "epoch": 4.27710843373494,
      "grad_norm": 13.519294738769531,
      "learning_rate": 3.930722891566265e-05,
      "loss": 2.3804,
      "step": 7100
    },
    {
      "epoch": 4.337349397590361,
      "grad_norm": 13.011618614196777,
      "learning_rate": 3.91566265060241e-05,
      "loss": 2.3749,
      "step": 7200
    },
    {
      "epoch": 4.397590361445783,
      "grad_norm": 13.188322067260742,
      "learning_rate": 3.9006024096385544e-05,
      "loss": 2.3156,
      "step": 7300
    },
    {
      "epoch": 4.457831325301205,
      "grad_norm": 10.452245712280273,
      "learning_rate": 3.885542168674699e-05,
      "loss": 2.2414,
      "step": 7400
    },
    {
      "epoch": 4.518072289156627,
      "grad_norm": 25.892513275146484,
      "learning_rate": 3.8704819277108436e-05,
      "loss": 2.2614,
      "step": 7500
    },
    {
      "epoch": 4.578313253012048,
      "grad_norm": 9.58487606048584,
      "learning_rate": 3.855421686746988e-05,
      "loss": 2.3338,
      "step": 7600
    },
    {
      "epoch": 4.63855421686747,
      "grad_norm": 17.85867691040039,
      "learning_rate": 3.840361445783133e-05,
      "loss": 2.3858,
      "step": 7700
    },
    {
      "epoch": 4.698795180722891,
      "grad_norm": 18.60890769958496,
      "learning_rate": 3.8253012048192774e-05,
      "loss": 2.3263,
      "step": 7800
    },
    {
      "epoch": 4.759036144578313,
      "grad_norm": 8.710838317871094,
      "learning_rate": 3.810240963855422e-05,
      "loss": 2.4064,
      "step": 7900
    },
    {
      "epoch": 4.8192771084337345,
      "grad_norm": 16.3905029296875,
      "learning_rate": 3.7951807228915666e-05,
      "loss": 2.4555,
      "step": 8000
    },
    {
      "epoch": 4.879518072289157,
      "grad_norm": 13.018982887268066,
      "learning_rate": 3.780120481927711e-05,
      "loss": 2.2437,
      "step": 8100
    },
    {
      "epoch": 4.9397590361445785,
      "grad_norm": 16.88832664489746,
      "learning_rate": 3.765060240963856e-05,
      "loss": 2.0207,
      "step": 8200
    },
    {
      "epoch": 5.0,
      "grad_norm": 12.871626853942871,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 2.2192,
      "step": 8300
    },
    {
      "epoch": 5.0,
      "eval_loss": 2.248164415359497,
      "eval_runtime": 12.5223,
      "eval_samples_per_second": 139.91,
      "eval_steps_per_second": 17.489,
      "step": 8300
    },
    {
      "epoch": 5.0602409638554215,
      "grad_norm": 15.71114730834961,
      "learning_rate": 3.734939759036144e-05,
      "loss": 2.3004,
      "step": 8400
    },
    {
      "epoch": 5.120481927710843,
      "grad_norm": 11.985574722290039,
      "learning_rate": 3.7198795180722895e-05,
      "loss": 2.1011,
      "step": 8500
    },
    {
      "epoch": 5.180722891566265,
      "grad_norm": 9.105306625366211,
      "learning_rate": 3.704819277108434e-05,
      "loss": 2.2348,
      "step": 8600
    },
    {
      "epoch": 5.240963855421687,
      "grad_norm": 8.642243385314941,
      "learning_rate": 3.689759036144578e-05,
      "loss": 2.12,
      "step": 8700
    },
    {
      "epoch": 5.301204819277109,
      "grad_norm": 10.878092765808105,
      "learning_rate": 3.674698795180723e-05,
      "loss": 2.2096,
      "step": 8800
    },
    {
      "epoch": 5.36144578313253,
      "grad_norm": 16.21038055419922,
      "learning_rate": 3.659638554216868e-05,
      "loss": 2.0835,
      "step": 8900
    },
    {
      "epoch": 5.421686746987952,
      "grad_norm": 10.80283260345459,
      "learning_rate": 3.644578313253012e-05,
      "loss": 2.1953,
      "step": 9000
    },
    {
      "epoch": 5.481927710843373,
      "grad_norm": 11.265413284301758,
      "learning_rate": 3.629518072289157e-05,
      "loss": 2.1843,
      "step": 9100
    },
    {
      "epoch": 5.542168674698795,
      "grad_norm": 12.532456398010254,
      "learning_rate": 3.614457831325301e-05,
      "loss": 2.0665,
      "step": 9200
    },
    {
      "epoch": 5.602409638554217,
      "grad_norm": 13.23777961730957,
      "learning_rate": 3.5993975903614456e-05,
      "loss": 2.2326,
      "step": 9300
    },
    {
      "epoch": 5.662650602409639,
      "grad_norm": 16.001705169677734,
      "learning_rate": 3.584337349397591e-05,
      "loss": 2.1166,
      "step": 9400
    },
    {
      "epoch": 5.72289156626506,
      "grad_norm": 13.634965896606445,
      "learning_rate": 3.569277108433735e-05,
      "loss": 2.0896,
      "step": 9500
    },
    {
      "epoch": 5.783132530120482,
      "grad_norm": 15.843360900878906,
      "learning_rate": 3.5542168674698794e-05,
      "loss": 2.2055,
      "step": 9600
    },
    {
      "epoch": 5.843373493975903,
      "grad_norm": 14.089707374572754,
      "learning_rate": 3.539156626506025e-05,
      "loss": 2.1481,
      "step": 9700
    },
    {
      "epoch": 5.903614457831325,
      "grad_norm": 15.05759334564209,
      "learning_rate": 3.5240963855421686e-05,
      "loss": 2.2198,
      "step": 9800
    },
    {
      "epoch": 5.9638554216867465,
      "grad_norm": 11.333760261535645,
      "learning_rate": 3.509036144578313e-05,
      "loss": 2.1772,
      "step": 9900
    },
    {
      "epoch": 6.0,
      "eval_loss": 2.192056655883789,
      "eval_runtime": 12.3407,
      "eval_samples_per_second": 141.97,
      "eval_steps_per_second": 17.746,
      "step": 9960
    },
    {
      "epoch": 6.024096385542169,
      "grad_norm": 8.949481010437012,
      "learning_rate": 3.4939759036144585e-05,
      "loss": 2.0248,
      "step": 10000
    },
    {
      "epoch": 6.0843373493975905,
      "grad_norm": 12.401188850402832,
      "learning_rate": 3.4789156626506024e-05,
      "loss": 2.1164,
      "step": 10100
    },
    {
      "epoch": 6.144578313253012,
      "grad_norm": 8.865478515625,
      "learning_rate": 3.463855421686747e-05,
      "loss": 2.0765,
      "step": 10200
    },
    {
      "epoch": 6.204819277108434,
      "grad_norm": 10.043485641479492,
      "learning_rate": 3.4487951807228916e-05,
      "loss": 2.0372,
      "step": 10300
    },
    {
      "epoch": 6.265060240963855,
      "grad_norm": 10.920104026794434,
      "learning_rate": 3.433734939759036e-05,
      "loss": 2.0739,
      "step": 10400
    },
    {
      "epoch": 6.325301204819277,
      "grad_norm": 8.516533851623535,
      "learning_rate": 3.418674698795181e-05,
      "loss": 2.0548,
      "step": 10500
    },
    {
      "epoch": 6.385542168674699,
      "grad_norm": 17.928680419921875,
      "learning_rate": 3.4036144578313254e-05,
      "loss": 2.0648,
      "step": 10600
    },
    {
      "epoch": 6.445783132530121,
      "grad_norm": 17.310914993286133,
      "learning_rate": 3.38855421686747e-05,
      "loss": 2.0316,
      "step": 10700
    },
    {
      "epoch": 6.506024096385542,
      "grad_norm": 15.88424301147461,
      "learning_rate": 3.3734939759036146e-05,
      "loss": 2.0267,
      "step": 10800
    },
    {
      "epoch": 6.566265060240964,
      "grad_norm": 10.214157104492188,
      "learning_rate": 3.358433734939759e-05,
      "loss": 2.1621,
      "step": 10900
    },
    {
      "epoch": 6.626506024096385,
      "grad_norm": 10.783038139343262,
      "learning_rate": 3.343373493975904e-05,
      "loss": 2.072,
      "step": 11000
    },
    {
      "epoch": 6.686746987951807,
      "grad_norm": 12.150382995605469,
      "learning_rate": 3.3283132530120484e-05,
      "loss": 2.0089,
      "step": 11100
    },
    {
      "epoch": 6.746987951807229,
      "grad_norm": 12.104662895202637,
      "learning_rate": 3.313253012048193e-05,
      "loss": 1.9926,
      "step": 11200
    },
    {
      "epoch": 6.807228915662651,
      "grad_norm": 15.701196670532227,
      "learning_rate": 3.2981927710843376e-05,
      "loss": 2.081,
      "step": 11300
    },
    {
      "epoch": 6.867469879518072,
      "grad_norm": 9.563748359680176,
      "learning_rate": 3.283132530120482e-05,
      "loss": 1.9227,
      "step": 11400
    },
    {
      "epoch": 6.927710843373494,
      "grad_norm": 13.486066818237305,
      "learning_rate": 3.268072289156627e-05,
      "loss": 2.1317,
      "step": 11500
    },
    {
      "epoch": 6.9879518072289155,
      "grad_norm": 12.759196281433105,
      "learning_rate": 3.253012048192771e-05,
      "loss": 2.1244,
      "step": 11600
    },
    {
      "epoch": 7.0,
      "eval_loss": 2.121952772140503,
      "eval_runtime": 12.3505,
      "eval_samples_per_second": 141.856,
      "eval_steps_per_second": 17.732,
      "step": 11620
    },
    {
      "epoch": 7.048192771084337,
      "grad_norm": 9.444178581237793,
      "learning_rate": 3.237951807228915e-05,
      "loss": 2.0399,
      "step": 11700
    },
    {
      "epoch": 7.108433734939759,
      "grad_norm": 12.324136734008789,
      "learning_rate": 3.2228915662650605e-05,
      "loss": 2.1069,
      "step": 11800
    },
    {
      "epoch": 7.168674698795181,
      "grad_norm": 11.247248649597168,
      "learning_rate": 3.207831325301205e-05,
      "loss": 1.9329,
      "step": 11900
    },
    {
      "epoch": 7.228915662650603,
      "grad_norm": 11.537498474121094,
      "learning_rate": 3.192771084337349e-05,
      "loss": 1.9376,
      "step": 12000
    },
    {
      "epoch": 7.289156626506024,
      "grad_norm": 14.937167167663574,
      "learning_rate": 3.177710843373494e-05,
      "loss": 2.0038,
      "step": 12100
    },
    {
      "epoch": 7.349397590361446,
      "grad_norm": 12.750118255615234,
      "learning_rate": 3.162650602409639e-05,
      "loss": 1.8888,
      "step": 12200
    },
    {
      "epoch": 7.409638554216867,
      "grad_norm": 8.334891319274902,
      "learning_rate": 3.147590361445783e-05,
      "loss": 1.9846,
      "step": 12300
    },
    {
      "epoch": 7.469879518072289,
      "grad_norm": 13.974340438842773,
      "learning_rate": 3.132530120481928e-05,
      "loss": 1.7657,
      "step": 12400
    },
    {
      "epoch": 7.530120481927711,
      "grad_norm": 12.133346557617188,
      "learning_rate": 3.117469879518072e-05,
      "loss": 1.9246,
      "step": 12500
    },
    {
      "epoch": 7.590361445783133,
      "grad_norm": 10.133448600769043,
      "learning_rate": 3.102409638554217e-05,
      "loss": 1.9363,
      "step": 12600
    },
    {
      "epoch": 7.650602409638554,
      "grad_norm": 11.542295455932617,
      "learning_rate": 3.087349397590362e-05,
      "loss": 2.0513,
      "step": 12700
    },
    {
      "epoch": 7.710843373493976,
      "grad_norm": 10.825553894042969,
      "learning_rate": 3.072289156626506e-05,
      "loss": 1.9051,
      "step": 12800
    },
    {
      "epoch": 7.771084337349397,
      "grad_norm": 20.114017486572266,
      "learning_rate": 3.057228915662651e-05,
      "loss": 1.938,
      "step": 12900
    },
    {
      "epoch": 7.831325301204819,
      "grad_norm": 18.674257278442383,
      "learning_rate": 3.0421686746987953e-05,
      "loss": 1.9325,
      "step": 13000
    },
    {
      "epoch": 7.891566265060241,
      "grad_norm": 12.202211380004883,
      "learning_rate": 3.02710843373494e-05,
      "loss": 1.9228,
      "step": 13100
    },
    {
      "epoch": 7.951807228915663,
      "grad_norm": 12.448355674743652,
      "learning_rate": 3.012048192771085e-05,
      "loss": 1.9814,
      "step": 13200
    },
    {
      "epoch": 8.0,
      "eval_loss": 2.0597710609436035,
      "eval_runtime": 12.3521,
      "eval_samples_per_second": 141.838,
      "eval_steps_per_second": 17.73,
      "step": 13280
    },
    {
      "epoch": 8.012048192771084,
      "grad_norm": 13.64299488067627,
      "learning_rate": 2.996987951807229e-05,
      "loss": 1.9796,
      "step": 13300
    },
    {
      "epoch": 8.072289156626505,
      "grad_norm": 10.312067031860352,
      "learning_rate": 2.9819277108433734e-05,
      "loss": 1.9292,
      "step": 13400
    },
    {
      "epoch": 8.132530120481928,
      "grad_norm": 11.542633056640625,
      "learning_rate": 2.9668674698795183e-05,
      "loss": 1.9009,
      "step": 13500
    },
    {
      "epoch": 8.19277108433735,
      "grad_norm": 12.151927947998047,
      "learning_rate": 2.951807228915663e-05,
      "loss": 1.7341,
      "step": 13600
    },
    {
      "epoch": 8.25301204819277,
      "grad_norm": 19.574878692626953,
      "learning_rate": 2.9367469879518072e-05,
      "loss": 1.8672,
      "step": 13700
    },
    {
      "epoch": 8.313253012048193,
      "grad_norm": 11.912753105163574,
      "learning_rate": 2.921686746987952e-05,
      "loss": 1.9732,
      "step": 13800
    },
    {
      "epoch": 8.373493975903614,
      "grad_norm": 10.394018173217773,
      "learning_rate": 2.9066265060240967e-05,
      "loss": 1.9024,
      "step": 13900
    },
    {
      "epoch": 8.433734939759036,
      "grad_norm": 10.62492561340332,
      "learning_rate": 2.891566265060241e-05,
      "loss": 1.8207,
      "step": 14000
    },
    {
      "epoch": 8.493975903614459,
      "grad_norm": 8.461356163024902,
      "learning_rate": 2.876506024096386e-05,
      "loss": 1.8702,
      "step": 14100
    },
    {
      "epoch": 8.55421686746988,
      "grad_norm": 16.770950317382812,
      "learning_rate": 2.86144578313253e-05,
      "loss": 1.8242,
      "step": 14200
    },
    {
      "epoch": 8.614457831325302,
      "grad_norm": 9.27564811706543,
      "learning_rate": 2.8463855421686748e-05,
      "loss": 1.9064,
      "step": 14300
    },
    {
      "epoch": 8.674698795180722,
      "grad_norm": 13.329981803894043,
      "learning_rate": 2.8313253012048197e-05,
      "loss": 1.8633,
      "step": 14400
    },
    {
      "epoch": 8.734939759036145,
      "grad_norm": 14.572525978088379,
      "learning_rate": 2.816265060240964e-05,
      "loss": 1.708,
      "step": 14500
    },
    {
      "epoch": 8.795180722891565,
      "grad_norm": 12.554889678955078,
      "learning_rate": 2.8012048192771085e-05,
      "loss": 1.855,
      "step": 14600
    },
    {
      "epoch": 8.855421686746988,
      "grad_norm": 9.166707038879395,
      "learning_rate": 2.7861445783132535e-05,
      "loss": 1.8835,
      "step": 14700
    },
    {
      "epoch": 8.91566265060241,
      "grad_norm": 13.764123916625977,
      "learning_rate": 2.7710843373493977e-05,
      "loss": 1.7695,
      "step": 14800
    },
    {
      "epoch": 8.975903614457831,
      "grad_norm": 12.972764015197754,
      "learning_rate": 2.756024096385542e-05,
      "loss": 2.0522,
      "step": 14900
    },
    {
      "epoch": 9.0,
      "eval_loss": 2.026339292526245,
      "eval_runtime": 12.3853,
      "eval_samples_per_second": 141.458,
      "eval_steps_per_second": 17.682,
      "step": 14940
    },
    {
      "epoch": 9.036144578313253,
      "grad_norm": 11.927181243896484,
      "learning_rate": 2.7409638554216873e-05,
      "loss": 1.9101,
      "step": 15000
    },
    {
      "epoch": 9.096385542168674,
      "grad_norm": 13.031174659729004,
      "learning_rate": 2.7259036144578315e-05,
      "loss": 1.6719,
      "step": 15100
    },
    {
      "epoch": 9.156626506024097,
      "grad_norm": 15.246146202087402,
      "learning_rate": 2.7108433734939758e-05,
      "loss": 1.9457,
      "step": 15200
    },
    {
      "epoch": 9.216867469879517,
      "grad_norm": 13.403050422668457,
      "learning_rate": 2.6957831325301207e-05,
      "loss": 1.7448,
      "step": 15300
    },
    {
      "epoch": 9.27710843373494,
      "grad_norm": 18.276582717895508,
      "learning_rate": 2.6807228915662653e-05,
      "loss": 1.8177,
      "step": 15400
    },
    {
      "epoch": 9.337349397590362,
      "grad_norm": 9.58532428741455,
      "learning_rate": 2.6656626506024096e-05,
      "loss": 1.8767,
      "step": 15500
    },
    {
      "epoch": 9.397590361445783,
      "grad_norm": 8.514535903930664,
      "learning_rate": 2.6506024096385545e-05,
      "loss": 1.781,
      "step": 15600
    },
    {
      "epoch": 9.457831325301205,
      "grad_norm": 12.047292709350586,
      "learning_rate": 2.635542168674699e-05,
      "loss": 1.8107,
      "step": 15700
    },
    {
      "epoch": 9.518072289156626,
      "grad_norm": 11.158848762512207,
      "learning_rate": 2.6204819277108434e-05,
      "loss": 1.7497,
      "step": 15800
    },
    {
      "epoch": 9.578313253012048,
      "grad_norm": 20.920249938964844,
      "learning_rate": 2.6054216867469883e-05,
      "loss": 1.921,
      "step": 15900
    },
    {
      "epoch": 9.638554216867469,
      "grad_norm": 18.277212142944336,
      "learning_rate": 2.5903614457831325e-05,
      "loss": 1.7447,
      "step": 16000
    },
    {
      "epoch": 9.698795180722891,
      "grad_norm": 15.001018524169922,
      "learning_rate": 2.575301204819277e-05,
      "loss": 1.7921,
      "step": 16100
    },
    {
      "epoch": 9.759036144578314,
      "grad_norm": 11.160061836242676,
      "learning_rate": 2.560240963855422e-05,
      "loss": 1.7339,
      "step": 16200
    },
    {
      "epoch": 9.819277108433734,
      "grad_norm": 11.523841857910156,
      "learning_rate": 2.5451807228915663e-05,
      "loss": 1.751,
      "step": 16300
    },
    {
      "epoch": 9.879518072289157,
      "grad_norm": 12.04468059539795,
      "learning_rate": 2.530120481927711e-05,
      "loss": 1.8052,
      "step": 16400
    },
    {
      "epoch": 9.939759036144578,
      "grad_norm": 8.470062255859375,
      "learning_rate": 2.515060240963856e-05,
      "loss": 1.8028,
      "step": 16500
    },
    {
      "epoch": 10.0,
      "grad_norm": 13.260085105895996,
      "learning_rate": 2.5e-05,
      "loss": 1.8492,
      "step": 16600
    },
    {
      "epoch": 10.0,
      "eval_loss": 1.9350709915161133,
      "eval_runtime": 12.4017,
      "eval_samples_per_second": 141.271,
      "eval_steps_per_second": 17.659,
      "step": 16600
    },
    {
      "epoch": 10.060240963855422,
      "grad_norm": 11.222063064575195,
      "learning_rate": 2.4849397590361447e-05,
      "loss": 1.7817,
      "step": 16700
    },
    {
      "epoch": 10.120481927710843,
      "grad_norm": 12.356430053710938,
      "learning_rate": 2.4698795180722893e-05,
      "loss": 1.6835,
      "step": 16800
    },
    {
      "epoch": 10.180722891566266,
      "grad_norm": 13.327044486999512,
      "learning_rate": 2.454819277108434e-05,
      "loss": 1.7518,
      "step": 16900
    },
    {
      "epoch": 10.240963855421686,
      "grad_norm": 12.512236595153809,
      "learning_rate": 2.4397590361445785e-05,
      "loss": 1.6915,
      "step": 17000
    },
    {
      "epoch": 10.301204819277109,
      "grad_norm": 14.809521675109863,
      "learning_rate": 2.4246987951807228e-05,
      "loss": 1.8053,
      "step": 17100
    },
    {
      "epoch": 10.36144578313253,
      "grad_norm": 11.028542518615723,
      "learning_rate": 2.4096385542168677e-05,
      "loss": 1.6797,
      "step": 17200
    },
    {
      "epoch": 10.421686746987952,
      "grad_norm": 14.476499557495117,
      "learning_rate": 2.3945783132530123e-05,
      "loss": 1.6569,
      "step": 17300
    },
    {
      "epoch": 10.481927710843374,
      "grad_norm": 11.640693664550781,
      "learning_rate": 2.3795180722891565e-05,
      "loss": 1.6738,
      "step": 17400
    },
    {
      "epoch": 10.542168674698795,
      "grad_norm": 16.399431228637695,
      "learning_rate": 2.364457831325301e-05,
      "loss": 1.7806,
      "step": 17500
    },
    {
      "epoch": 10.602409638554217,
      "grad_norm": 14.172414779663086,
      "learning_rate": 2.349397590361446e-05,
      "loss": 1.5845,
      "step": 17600
    },
    {
      "epoch": 10.662650602409638,
      "grad_norm": 12.316054344177246,
      "learning_rate": 2.3343373493975903e-05,
      "loss": 1.7719,
      "step": 17700
    },
    {
      "epoch": 10.72289156626506,
      "grad_norm": 12.225162506103516,
      "learning_rate": 2.319277108433735e-05,
      "loss": 1.7275,
      "step": 17800
    },
    {
      "epoch": 10.783132530120483,
      "grad_norm": 8.592450141906738,
      "learning_rate": 2.3042168674698795e-05,
      "loss": 1.6979,
      "step": 17900
    },
    {
      "epoch": 10.843373493975903,
      "grad_norm": 10.186752319335938,
      "learning_rate": 2.289156626506024e-05,
      "loss": 1.8165,
      "step": 18000
    },
    {
      "epoch": 10.903614457831326,
      "grad_norm": 20.98256492614746,
      "learning_rate": 2.2740963855421687e-05,
      "loss": 1.7856,
      "step": 18100
    },
    {
      "epoch": 10.963855421686747,
      "grad_norm": 12.937653541564941,
      "learning_rate": 2.2590361445783133e-05,
      "loss": 1.6682,
      "step": 18200
    },
    {
      "epoch": 11.0,
      "eval_loss": 1.9513624906539917,
      "eval_runtime": 12.3647,
      "eval_samples_per_second": 141.694,
      "eval_steps_per_second": 17.712,
      "step": 18260
    },
    {
      "epoch": 11.024096385542169,
      "grad_norm": 11.98076343536377,
      "learning_rate": 2.243975903614458e-05,
      "loss": 1.6582,
      "step": 18300
    },
    {
      "epoch": 11.08433734939759,
      "grad_norm": 12.206952095031738,
      "learning_rate": 2.2289156626506025e-05,
      "loss": 1.7072,
      "step": 18400
    },
    {
      "epoch": 11.144578313253012,
      "grad_norm": 21.244415283203125,
      "learning_rate": 2.213855421686747e-05,
      "loss": 1.6419,
      "step": 18500
    },
    {
      "epoch": 11.204819277108435,
      "grad_norm": 11.19680118560791,
      "learning_rate": 2.1987951807228917e-05,
      "loss": 1.722,
      "step": 18600
    },
    {
      "epoch": 11.265060240963855,
      "grad_norm": 12.699275970458984,
      "learning_rate": 2.1837349397590363e-05,
      "loss": 1.6773,
      "step": 18700
    },
    {
      "epoch": 11.325301204819278,
      "grad_norm": 12.362631797790527,
      "learning_rate": 2.168674698795181e-05,
      "loss": 1.7017,
      "step": 18800
    },
    {
      "epoch": 11.385542168674698,
      "grad_norm": 18.398616790771484,
      "learning_rate": 2.1536144578313255e-05,
      "loss": 1.6307,
      "step": 18900
    },
    {
      "epoch": 11.44578313253012,
      "grad_norm": 10.23253345489502,
      "learning_rate": 2.13855421686747e-05,
      "loss": 1.4934,
      "step": 19000
    },
    {
      "epoch": 11.506024096385541,
      "grad_norm": 10.210517883300781,
      "learning_rate": 2.1234939759036147e-05,
      "loss": 1.6401,
      "step": 19100
    },
    {
      "epoch": 11.566265060240964,
      "grad_norm": 16.464374542236328,
      "learning_rate": 2.1084337349397593e-05,
      "loss": 1.7158,
      "step": 19200
    },
    {
      "epoch": 11.626506024096386,
      "grad_norm": 10.506407737731934,
      "learning_rate": 2.0933734939759035e-05,
      "loss": 1.5853,
      "step": 19300
    },
    {
      "epoch": 11.686746987951807,
      "grad_norm": 14.96902847290039,
      "learning_rate": 2.0783132530120485e-05,
      "loss": 1.7126,
      "step": 19400
    },
    {
      "epoch": 11.74698795180723,
      "grad_norm": 13.937207221984863,
      "learning_rate": 2.063253012048193e-05,
      "loss": 1.6998,
      "step": 19500
    },
    {
      "epoch": 11.80722891566265,
      "grad_norm": 9.932404518127441,
      "learning_rate": 2.0481927710843373e-05,
      "loss": 1.7306,
      "step": 19600
    },
    {
      "epoch": 11.867469879518072,
      "grad_norm": 11.962567329406738,
      "learning_rate": 2.033132530120482e-05,
      "loss": 1.6763,
      "step": 19700
    },
    {
      "epoch": 11.927710843373493,
      "grad_norm": 12.72746753692627,
      "learning_rate": 2.018072289156627e-05,
      "loss": 1.6609,
      "step": 19800
    },
    {
      "epoch": 11.987951807228916,
      "grad_norm": 14.095961570739746,
      "learning_rate": 2.003012048192771e-05,
      "loss": 1.6016,
      "step": 19900
    },
    {
      "epoch": 12.0,
      "eval_loss": 1.8460464477539062,
      "eval_runtime": 12.3788,
      "eval_samples_per_second": 141.532,
      "eval_steps_per_second": 17.691,
      "step": 19920
    },
    {
      "epoch": 12.048192771084338,
      "grad_norm": 13.146195411682129,
      "learning_rate": 1.9879518072289157e-05,
      "loss": 1.515,
      "step": 20000
    },
    {
      "epoch": 12.108433734939759,
      "grad_norm": 10.781865119934082,
      "learning_rate": 1.9728915662650603e-05,
      "loss": 1.4383,
      "step": 20100
    },
    {
      "epoch": 12.168674698795181,
      "grad_norm": 14.071392059326172,
      "learning_rate": 1.957831325301205e-05,
      "loss": 1.5824,
      "step": 20200
    },
    {
      "epoch": 12.228915662650602,
      "grad_norm": 19.525991439819336,
      "learning_rate": 1.9427710843373495e-05,
      "loss": 1.5886,
      "step": 20300
    },
    {
      "epoch": 12.289156626506024,
      "grad_norm": 13.51597785949707,
      "learning_rate": 1.927710843373494e-05,
      "loss": 1.6814,
      "step": 20400
    },
    {
      "epoch": 12.349397590361447,
      "grad_norm": 19.645639419555664,
      "learning_rate": 1.9126506024096387e-05,
      "loss": 1.7043,
      "step": 20500
    },
    {
      "epoch": 12.409638554216867,
      "grad_norm": 9.62149429321289,
      "learning_rate": 1.8975903614457833e-05,
      "loss": 1.6537,
      "step": 20600
    },
    {
      "epoch": 12.46987951807229,
      "grad_norm": 12.747624397277832,
      "learning_rate": 1.882530120481928e-05,
      "loss": 1.567,
      "step": 20700
    },
    {
      "epoch": 12.53012048192771,
      "grad_norm": 9.130168914794922,
      "learning_rate": 1.867469879518072e-05,
      "loss": 1.6397,
      "step": 20800
    },
    {
      "epoch": 12.590361445783133,
      "grad_norm": 10.942102432250977,
      "learning_rate": 1.852409638554217e-05,
      "loss": 1.5215,
      "step": 20900
    },
    {
      "epoch": 12.650602409638553,
      "grad_norm": 12.342782020568848,
      "learning_rate": 1.8373493975903617e-05,
      "loss": 1.7015,
      "step": 21000
    },
    {
      "epoch": 12.710843373493976,
      "grad_norm": 10.522941589355469,
      "learning_rate": 1.822289156626506e-05,
      "loss": 1.5565,
      "step": 21100
    },
    {
      "epoch": 12.771084337349398,
      "grad_norm": 14.217508316040039,
      "learning_rate": 1.8072289156626505e-05,
      "loss": 1.5933,
      "step": 21200
    },
    {
      "epoch": 12.831325301204819,
      "grad_norm": 11.75989055633545,
      "learning_rate": 1.7921686746987955e-05,
      "loss": 1.5831,
      "step": 21300
    },
    {
      "epoch": 12.891566265060241,
      "grad_norm": 11.248298645019531,
      "learning_rate": 1.7771084337349397e-05,
      "loss": 1.5986,
      "step": 21400
    },
    {
      "epoch": 12.951807228915662,
      "grad_norm": 11.334368705749512,
      "learning_rate": 1.7620481927710843e-05,
      "loss": 1.5315,
      "step": 21500
    },
    {
      "epoch": 13.0,
      "eval_loss": 1.896715521812439,
      "eval_runtime": 12.3667,
      "eval_samples_per_second": 141.671,
      "eval_steps_per_second": 17.709,
      "step": 21580
    },
    {
      "epoch": 13.012048192771084,
      "grad_norm": 15.318700790405273,
      "learning_rate": 1.7469879518072292e-05,
      "loss": 1.5115,
      "step": 21600
    },
    {
      "epoch": 13.072289156626505,
      "grad_norm": 11.2378511428833,
      "learning_rate": 1.7319277108433735e-05,
      "loss": 1.595,
      "step": 21700
    },
    {
      "epoch": 13.132530120481928,
      "grad_norm": 14.825092315673828,
      "learning_rate": 1.716867469879518e-05,
      "loss": 1.5246,
      "step": 21800
    },
    {
      "epoch": 13.19277108433735,
      "grad_norm": 11.29450511932373,
      "learning_rate": 1.7018072289156627e-05,
      "loss": 1.547,
      "step": 21900
    },
    {
      "epoch": 13.25301204819277,
      "grad_norm": 8.651263236999512,
      "learning_rate": 1.6867469879518073e-05,
      "loss": 1.4871,
      "step": 22000
    },
    {
      "epoch": 13.313253012048193,
      "grad_norm": 9.290862083435059,
      "learning_rate": 1.671686746987952e-05,
      "loss": 1.5729,
      "step": 22100
    },
    {
      "epoch": 13.373493975903614,
      "grad_norm": 13.281071662902832,
      "learning_rate": 1.6566265060240965e-05,
      "loss": 1.4951,
      "step": 22200
    },
    {
      "epoch": 13.433734939759036,
      "grad_norm": 11.908756256103516,
      "learning_rate": 1.641566265060241e-05,
      "loss": 1.5375,
      "step": 22300
    },
    {
      "epoch": 13.493975903614459,
      "grad_norm": 16.90333366394043,
      "learning_rate": 1.6265060240963857e-05,
      "loss": 1.6465,
      "step": 22400
    },
    {
      "epoch": 13.55421686746988,
      "grad_norm": 9.90452766418457,
      "learning_rate": 1.6114457831325303e-05,
      "loss": 1.6304,
      "step": 22500
    },
    {
      "epoch": 13.614457831325302,
      "grad_norm": 9.488128662109375,
      "learning_rate": 1.5963855421686745e-05,
      "loss": 1.5986,
      "step": 22600
    },
    {
      "epoch": 13.674698795180722,
      "grad_norm": 14.505155563354492,
      "learning_rate": 1.5813253012048195e-05,
      "loss": 1.5138,
      "step": 22700
    },
    {
      "epoch": 13.734939759036145,
      "grad_norm": 14.16018009185791,
      "learning_rate": 1.566265060240964e-05,
      "loss": 1.578,
      "step": 22800
    },
    {
      "epoch": 13.795180722891565,
      "grad_norm": 9.730978012084961,
      "learning_rate": 1.5512048192771086e-05,
      "loss": 1.5528,
      "step": 22900
    },
    {
      "epoch": 13.855421686746988,
      "grad_norm": 10.642419815063477,
      "learning_rate": 1.536144578313253e-05,
      "loss": 1.5985,
      "step": 23000
    },
    {
      "epoch": 13.91566265060241,
      "grad_norm": 12.29862117767334,
      "learning_rate": 1.5210843373493977e-05,
      "loss": 1.5698,
      "step": 23100
    },
    {
      "epoch": 13.975903614457831,
      "grad_norm": 12.474818229675293,
      "learning_rate": 1.5060240963855424e-05,
      "loss": 1.6386,
      "step": 23200
    },
    {
      "epoch": 14.0,
      "eval_loss": 1.8422646522521973,
      "eval_runtime": 12.3848,
      "eval_samples_per_second": 141.464,
      "eval_steps_per_second": 17.683,
      "step": 23240
    },
    {
      "epoch": 14.036144578313253,
      "grad_norm": 7.948886394500732,
      "learning_rate": 1.4909638554216867e-05,
      "loss": 1.4904,
      "step": 23300
    },
    {
      "epoch": 14.096385542168674,
      "grad_norm": 12.453771591186523,
      "learning_rate": 1.4759036144578315e-05,
      "loss": 1.431,
      "step": 23400
    },
    {
      "epoch": 14.156626506024097,
      "grad_norm": 10.12651252746582,
      "learning_rate": 1.460843373493976e-05,
      "loss": 1.5895,
      "step": 23500
    },
    {
      "epoch": 14.216867469879517,
      "grad_norm": 14.64474868774414,
      "learning_rate": 1.4457831325301205e-05,
      "loss": 1.4854,
      "step": 23600
    },
    {
      "epoch": 14.27710843373494,
      "grad_norm": 10.723387718200684,
      "learning_rate": 1.430722891566265e-05,
      "loss": 1.5369,
      "step": 23700
    },
    {
      "epoch": 14.337349397590362,
      "grad_norm": 12.255675315856934,
      "learning_rate": 1.4156626506024098e-05,
      "loss": 1.5431,
      "step": 23800
    },
    {
      "epoch": 14.397590361445783,
      "grad_norm": 7.283170700073242,
      "learning_rate": 1.4006024096385543e-05,
      "loss": 1.586,
      "step": 23900
    },
    {
      "epoch": 14.457831325301205,
      "grad_norm": 9.280786514282227,
      "learning_rate": 1.3855421686746989e-05,
      "loss": 1.6476,
      "step": 24000
    },
    {
      "epoch": 14.518072289156626,
      "grad_norm": 21.26820945739746,
      "learning_rate": 1.3704819277108436e-05,
      "loss": 1.555,
      "step": 24100
    },
    {
      "epoch": 14.578313253012048,
      "grad_norm": 9.044486999511719,
      "learning_rate": 1.3554216867469879e-05,
      "loss": 1.547,
      "step": 24200
    },
    {
      "epoch": 14.638554216867469,
      "grad_norm": 18.692480087280273,
      "learning_rate": 1.3403614457831327e-05,
      "loss": 1.529,
      "step": 24300
    },
    {
      "epoch": 14.698795180722891,
      "grad_norm": 14.349543571472168,
      "learning_rate": 1.3253012048192772e-05,
      "loss": 1.5104,
      "step": 24400
    },
    {
      "epoch": 14.759036144578314,
      "grad_norm": 12.220719337463379,
      "learning_rate": 1.3102409638554217e-05,
      "loss": 1.5085,
      "step": 24500
    },
    {
      "epoch": 14.819277108433734,
      "grad_norm": 19.3514461517334,
      "learning_rate": 1.2951807228915663e-05,
      "loss": 1.5199,
      "step": 24600
    },
    {
      "epoch": 14.879518072289157,
      "grad_norm": 10.115775108337402,
      "learning_rate": 1.280120481927711e-05,
      "loss": 1.503,
      "step": 24700
    },
    {
      "epoch": 14.939759036144578,
      "grad_norm": 12.711344718933105,
      "learning_rate": 1.2650602409638555e-05,
      "loss": 1.5414,
      "step": 24800
    },
    {
      "epoch": 15.0,
      "grad_norm": 16.97374725341797,
      "learning_rate": 1.25e-05,
      "loss": 1.4311,
      "step": 24900
    },
    {
      "epoch": 15.0,
      "eval_loss": 1.8467463254928589,
      "eval_runtime": 12.3612,
      "eval_samples_per_second": 141.734,
      "eval_steps_per_second": 17.717,
      "step": 24900
    },
    {
      "epoch": 15.060240963855422,
      "grad_norm": 8.340834617614746,
      "learning_rate": 1.2349397590361447e-05,
      "loss": 1.5152,
      "step": 25000
    },
    {
      "epoch": 15.120481927710843,
      "grad_norm": 11.624658584594727,
      "learning_rate": 1.2198795180722893e-05,
      "loss": 1.4253,
      "step": 25100
    },
    {
      "epoch": 15.180722891566266,
      "grad_norm": 11.562843322753906,
      "learning_rate": 1.2048192771084338e-05,
      "loss": 1.4395,
      "step": 25200
    },
    {
      "epoch": 15.240963855421686,
      "grad_norm": 7.9004807472229,
      "learning_rate": 1.1897590361445783e-05,
      "loss": 1.4094,
      "step": 25300
    },
    {
      "epoch": 15.301204819277109,
      "grad_norm": 9.239548683166504,
      "learning_rate": 1.174698795180723e-05,
      "loss": 1.4749,
      "step": 25400
    },
    {
      "epoch": 15.36144578313253,
      "grad_norm": 12.552289009094238,
      "learning_rate": 1.1596385542168675e-05,
      "loss": 1.5358,
      "step": 25500
    },
    {
      "epoch": 15.421686746987952,
      "grad_norm": 15.652541160583496,
      "learning_rate": 1.144578313253012e-05,
      "loss": 1.4449,
      "step": 25600
    },
    {
      "epoch": 15.481927710843374,
      "grad_norm": 11.581581115722656,
      "learning_rate": 1.1295180722891567e-05,
      "loss": 1.4324,
      "step": 25700
    },
    {
      "epoch": 15.542168674698795,
      "grad_norm": 14.644842147827148,
      "learning_rate": 1.1144578313253013e-05,
      "loss": 1.5125,
      "step": 25800
    },
    {
      "epoch": 15.602409638554217,
      "grad_norm": 17.910030364990234,
      "learning_rate": 1.0993975903614459e-05,
      "loss": 1.5242,
      "step": 25900
    },
    {
      "epoch": 15.662650602409638,
      "grad_norm": 9.636841773986816,
      "learning_rate": 1.0843373493975904e-05,
      "loss": 1.5076,
      "step": 26000
    },
    {
      "epoch": 15.72289156626506,
      "grad_norm": 11.997894287109375,
      "learning_rate": 1.069277108433735e-05,
      "loss": 1.4879,
      "step": 26100
    },
    {
      "epoch": 15.783132530120483,
      "grad_norm": 15.01752758026123,
      "learning_rate": 1.0542168674698796e-05,
      "loss": 1.49,
      "step": 26200
    },
    {
      "epoch": 15.843373493975903,
      "grad_norm": 16.168004989624023,
      "learning_rate": 1.0391566265060242e-05,
      "loss": 1.4791,
      "step": 26300
    },
    {
      "epoch": 15.903614457831326,
      "grad_norm": 16.133838653564453,
      "learning_rate": 1.0240963855421687e-05,
      "loss": 1.5227,
      "step": 26400
    },
    {
      "epoch": 15.963855421686747,
      "grad_norm": 12.600210189819336,
      "learning_rate": 1.0090361445783134e-05,
      "loss": 1.6638,
      "step": 26500
    },
    {
      "epoch": 16.0,
      "eval_loss": 1.8501644134521484,
      "eval_runtime": 12.3815,
      "eval_samples_per_second": 141.501,
      "eval_steps_per_second": 17.688,
      "step": 26560
    },
    {
      "epoch": 16.02409638554217,
      "grad_norm": 13.982257843017578,
      "learning_rate": 9.939759036144579e-06,
      "loss": 1.4764,
      "step": 26600
    },
    {
      "epoch": 16.08433734939759,
      "grad_norm": 11.986162185668945,
      "learning_rate": 9.789156626506024e-06,
      "loss": 1.5474,
      "step": 26700
    },
    {
      "epoch": 16.14457831325301,
      "grad_norm": 10.987225532531738,
      "learning_rate": 9.63855421686747e-06,
      "loss": 1.5144,
      "step": 26800
    },
    {
      "epoch": 16.204819277108435,
      "grad_norm": 11.195724487304688,
      "learning_rate": 9.487951807228916e-06,
      "loss": 1.4025,
      "step": 26900
    },
    {
      "epoch": 16.265060240963855,
      "grad_norm": 8.109643936157227,
      "learning_rate": 9.33734939759036e-06,
      "loss": 1.4313,
      "step": 27000
    },
    {
      "epoch": 16.325301204819276,
      "grad_norm": 12.654388427734375,
      "learning_rate": 9.186746987951808e-06,
      "loss": 1.4595,
      "step": 27100
    },
    {
      "epoch": 16.3855421686747,
      "grad_norm": 7.853513240814209,
      "learning_rate": 9.036144578313253e-06,
      "loss": 1.4332,
      "step": 27200
    },
    {
      "epoch": 16.44578313253012,
      "grad_norm": 12.279748916625977,
      "learning_rate": 8.885542168674699e-06,
      "loss": 1.4396,
      "step": 27300
    },
    {
      "epoch": 16.50602409638554,
      "grad_norm": 12.506528854370117,
      "learning_rate": 8.734939759036146e-06,
      "loss": 1.4459,
      "step": 27400
    },
    {
      "epoch": 16.566265060240966,
      "grad_norm": 9.528404235839844,
      "learning_rate": 8.58433734939759e-06,
      "loss": 1.3219,
      "step": 27500
    },
    {
      "epoch": 16.626506024096386,
      "grad_norm": 9.780545234680176,
      "learning_rate": 8.433734939759036e-06,
      "loss": 1.4556,
      "step": 27600
    },
    {
      "epoch": 16.686746987951807,
      "grad_norm": 10.703930854797363,
      "learning_rate": 8.283132530120482e-06,
      "loss": 1.4211,
      "step": 27700
    },
    {
      "epoch": 16.746987951807228,
      "grad_norm": 12.614486694335938,
      "learning_rate": 8.132530120481928e-06,
      "loss": 1.2906,
      "step": 27800
    },
    {
      "epoch": 16.80722891566265,
      "grad_norm": 10.806793212890625,
      "learning_rate": 7.981927710843373e-06,
      "loss": 1.3637,
      "step": 27900
    },
    {
      "epoch": 16.867469879518072,
      "grad_norm": 10.380541801452637,
      "learning_rate": 7.83132530120482e-06,
      "loss": 1.4585,
      "step": 28000
    },
    {
      "epoch": 16.927710843373493,
      "grad_norm": 21.02495765686035,
      "learning_rate": 7.680722891566265e-06,
      "loss": 1.4892,
      "step": 28100
    },
    {
      "epoch": 16.987951807228917,
      "grad_norm": 10.736624717712402,
      "learning_rate": 7.530120481927712e-06,
      "loss": 1.5462,
      "step": 28200
    },
    {
      "epoch": 17.0,
      "eval_loss": 1.7860124111175537,
      "eval_runtime": 12.3961,
      "eval_samples_per_second": 141.334,
      "eval_steps_per_second": 17.667,
      "step": 28220
    },
    {
      "epoch": 17.048192771084338,
      "grad_norm": 13.390864372253418,
      "learning_rate": 7.379518072289157e-06,
      "loss": 1.4036,
      "step": 28300
    },
    {
      "epoch": 17.10843373493976,
      "grad_norm": 14.977014541625977,
      "learning_rate": 7.228915662650602e-06,
      "loss": 1.366,
      "step": 28400
    },
    {
      "epoch": 17.16867469879518,
      "grad_norm": 9.684245109558105,
      "learning_rate": 7.078313253012049e-06,
      "loss": 1.4666,
      "step": 28500
    },
    {
      "epoch": 17.228915662650603,
      "grad_norm": 14.282641410827637,
      "learning_rate": 6.927710843373494e-06,
      "loss": 1.41,
      "step": 28600
    },
    {
      "epoch": 17.289156626506024,
      "grad_norm": 15.831113815307617,
      "learning_rate": 6.7771084337349394e-06,
      "loss": 1.4865,
      "step": 28700
    },
    {
      "epoch": 17.349397590361445,
      "grad_norm": 10.421630859375,
      "learning_rate": 6.626506024096386e-06,
      "loss": 1.433,
      "step": 28800
    },
    {
      "epoch": 17.40963855421687,
      "grad_norm": 13.693915367126465,
      "learning_rate": 6.475903614457831e-06,
      "loss": 1.4752,
      "step": 28900
    },
    {
      "epoch": 17.46987951807229,
      "grad_norm": 12.399365425109863,
      "learning_rate": 6.325301204819277e-06,
      "loss": 1.4468,
      "step": 29000
    },
    {
      "epoch": 17.53012048192771,
      "grad_norm": 10.92682933807373,
      "learning_rate": 6.174698795180723e-06,
      "loss": 1.4122,
      "step": 29100
    },
    {
      "epoch": 17.59036144578313,
      "grad_norm": 12.92502498626709,
      "learning_rate": 6.024096385542169e-06,
      "loss": 1.3651,
      "step": 29200
    },
    {
      "epoch": 17.650602409638555,
      "grad_norm": 12.173294067382812,
      "learning_rate": 5.873493975903615e-06,
      "loss": 1.4523,
      "step": 29300
    },
    {
      "epoch": 17.710843373493976,
      "grad_norm": 10.297429084777832,
      "learning_rate": 5.72289156626506e-06,
      "loss": 1.3848,
      "step": 29400
    },
    {
      "epoch": 17.771084337349397,
      "grad_norm": 13.285170555114746,
      "learning_rate": 5.572289156626506e-06,
      "loss": 1.4038,
      "step": 29500
    },
    {
      "epoch": 17.83132530120482,
      "grad_norm": 15.664533615112305,
      "learning_rate": 5.421686746987952e-06,
      "loss": 1.4023,
      "step": 29600
    },
    {
      "epoch": 17.89156626506024,
      "grad_norm": 13.18876838684082,
      "learning_rate": 5.271084337349398e-06,
      "loss": 1.4222,
      "step": 29700
    },
    {
      "epoch": 17.951807228915662,
      "grad_norm": 8.151145935058594,
      "learning_rate": 5.120481927710843e-06,
      "loss": 1.4733,
      "step": 29800
    },
    {
      "epoch": 18.0,
      "eval_loss": 1.8213077783584595,
      "eval_runtime": 12.3798,
      "eval_samples_per_second": 141.521,
      "eval_steps_per_second": 17.69,
      "step": 29880
    },
    {
      "epoch": 18.012048192771083,
      "grad_norm": 10.481730461120605,
      "learning_rate": 4.969879518072289e-06,
      "loss": 1.4142,
      "step": 29900
    },
    {
      "epoch": 18.072289156626507,
      "grad_norm": 8.398188591003418,
      "learning_rate": 4.819277108433735e-06,
      "loss": 1.3061,
      "step": 30000
    },
    {
      "epoch": 18.132530120481928,
      "grad_norm": 18.44734001159668,
      "learning_rate": 4.66867469879518e-06,
      "loss": 1.5151,
      "step": 30100
    },
    {
      "epoch": 18.19277108433735,
      "grad_norm": 11.113758087158203,
      "learning_rate": 4.518072289156626e-06,
      "loss": 1.4407,
      "step": 30200
    },
    {
      "epoch": 18.253012048192772,
      "grad_norm": 8.635100364685059,
      "learning_rate": 4.367469879518073e-06,
      "loss": 1.4102,
      "step": 30300
    },
    {
      "epoch": 18.313253012048193,
      "grad_norm": 10.252155303955078,
      "learning_rate": 4.216867469879518e-06,
      "loss": 1.4074,
      "step": 30400
    },
    {
      "epoch": 18.373493975903614,
      "grad_norm": 12.21285629272461,
      "learning_rate": 4.066265060240964e-06,
      "loss": 1.3566,
      "step": 30500
    },
    {
      "epoch": 18.433734939759034,
      "grad_norm": 10.116174697875977,
      "learning_rate": 3.91566265060241e-06,
      "loss": 1.4451,
      "step": 30600
    },
    {
      "epoch": 18.49397590361446,
      "grad_norm": 10.70619010925293,
      "learning_rate": 3.765060240963856e-06,
      "loss": 1.3714,
      "step": 30700
    },
    {
      "epoch": 18.55421686746988,
      "grad_norm": 11.94266414642334,
      "learning_rate": 3.614457831325301e-06,
      "loss": 1.3603,
      "step": 30800
    },
    {
      "epoch": 18.6144578313253,
      "grad_norm": 10.716464042663574,
      "learning_rate": 3.463855421686747e-06,
      "loss": 1.2899,
      "step": 30900
    },
    {
      "epoch": 18.674698795180724,
      "grad_norm": 13.77443790435791,
      "learning_rate": 3.313253012048193e-06,
      "loss": 1.406,
      "step": 31000
    },
    {
      "epoch": 18.734939759036145,
      "grad_norm": 13.727127075195312,
      "learning_rate": 3.1626506024096387e-06,
      "loss": 1.4357,
      "step": 31100
    },
    {
      "epoch": 18.795180722891565,
      "grad_norm": 13.167829513549805,
      "learning_rate": 3.0120481927710846e-06,
      "loss": 1.4716,
      "step": 31200
    },
    {
      "epoch": 18.855421686746986,
      "grad_norm": 4.4979729652404785,
      "learning_rate": 2.86144578313253e-06,
      "loss": 1.4265,
      "step": 31300
    },
    {
      "epoch": 18.91566265060241,
      "grad_norm": 13.056204795837402,
      "learning_rate": 2.710843373493976e-06,
      "loss": 1.4232,
      "step": 31400
    },
    {
      "epoch": 18.97590361445783,
      "grad_norm": 9.219202041625977,
      "learning_rate": 2.5602409638554217e-06,
      "loss": 1.3037,
      "step": 31500
    },
    {
      "epoch": 19.0,
      "eval_loss": 1.7690237760543823,
      "eval_runtime": 12.3819,
      "eval_samples_per_second": 141.497,
      "eval_steps_per_second": 17.687,
      "step": 31540
    },
    {
      "epoch": 19.03614457831325,
      "grad_norm": 15.635387420654297,
      "learning_rate": 2.4096385542168676e-06,
      "loss": 1.4612,
      "step": 31600
    },
    {
      "epoch": 19.096385542168676,
      "grad_norm": 14.809669494628906,
      "learning_rate": 2.259036144578313e-06,
      "loss": 1.2777,
      "step": 31700
    },
    {
      "epoch": 19.156626506024097,
      "grad_norm": 8.544891357421875,
      "learning_rate": 2.108433734939759e-06,
      "loss": 1.3248,
      "step": 31800
    },
    {
      "epoch": 19.216867469879517,
      "grad_norm": 11.488088607788086,
      "learning_rate": 1.957831325301205e-06,
      "loss": 1.407,
      "step": 31900
    },
    {
      "epoch": 19.27710843373494,
      "grad_norm": 8.986291885375977,
      "learning_rate": 1.8072289156626506e-06,
      "loss": 1.3696,
      "step": 32000
    },
    {
      "epoch": 19.337349397590362,
      "grad_norm": 7.773539066314697,
      "learning_rate": 1.6566265060240966e-06,
      "loss": 1.378,
      "step": 32100
    },
    {
      "epoch": 19.397590361445783,
      "grad_norm": 14.139798164367676,
      "learning_rate": 1.5060240963855423e-06,
      "loss": 1.479,
      "step": 32200
    },
    {
      "epoch": 19.457831325301203,
      "grad_norm": 16.524311065673828,
      "learning_rate": 1.355421686746988e-06,
      "loss": 1.3865,
      "step": 32300
    },
    {
      "epoch": 19.518072289156628,
      "grad_norm": 12.240504264831543,
      "learning_rate": 1.2048192771084338e-06,
      "loss": 1.275,
      "step": 32400
    },
    {
      "epoch": 19.57831325301205,
      "grad_norm": 16.85777473449707,
      "learning_rate": 1.0542168674698796e-06,
      "loss": 1.3816,
      "step": 32500
    },
    {
      "epoch": 19.63855421686747,
      "grad_norm": 17.97746467590332,
      "learning_rate": 9.036144578313253e-07,
      "loss": 1.3799,
      "step": 32600
    },
    {
      "epoch": 19.698795180722893,
      "grad_norm": 11.067954063415527,
      "learning_rate": 7.530120481927712e-07,
      "loss": 1.3685,
      "step": 32700
    },
    {
      "epoch": 19.759036144578314,
      "grad_norm": 18.09210205078125,
      "learning_rate": 6.024096385542169e-07,
      "loss": 1.4164,
      "step": 32800
    },
    {
      "epoch": 19.819277108433734,
      "grad_norm": 14.046141624450684,
      "learning_rate": 4.5180722891566265e-07,
      "loss": 1.4431,
      "step": 32900
    },
    {
      "epoch": 19.879518072289155,
      "grad_norm": 15.9526948928833,
      "learning_rate": 3.0120481927710845e-07,
      "loss": 1.2842,
      "step": 33000
    },
    {
      "epoch": 19.93975903614458,
      "grad_norm": 12.220635414123535,
      "learning_rate": 1.5060240963855423e-07,
      "loss": 1.3548,
      "step": 33100
    },
    {
      "epoch": 20.0,
      "grad_norm": 15.810035705566406,
      "learning_rate": 0.0,
      "loss": 1.4388,
      "step": 33200
    },
    {
      "epoch": 20.0,
      "eval_loss": 1.8410133123397827,
      "eval_runtime": 12.2828,
      "eval_samples_per_second": 142.638,
      "eval_steps_per_second": 17.83,
      "step": 33200
    }
  ],
  "logging_steps": 100,
  "max_steps": 33200,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.991790712496128e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
