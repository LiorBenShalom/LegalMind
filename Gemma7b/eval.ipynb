{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:00<00:00, 2275.05 examples/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.11it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Evaluating Base Model:   0%|          | 0/299 [00:00<?, ?it/s]You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Evaluating Base Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [03:14<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Base Model â€” Loss: 5.5136 | Perplexity: 248.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:05<00:00,  1.32s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Evaluating Fine-Tuned Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [03:04<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Fine-Tuned Model â€” Loss: 5.2122 | Perplexity: 183.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.212235548424482, 183.50383169063318)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, math\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Constants ---\n",
    "BASE_MODEL = \"google/gemma-7b\"\n",
    "ADAPTER_PATH = \"/home/liorkob/M.Sc/thesis/gemma_output/clm_lora_gpt_facts/my_gpt_dataset\"\n",
    "DATASET_PATH = \"/home/liorkob/M.Sc/thesis/data/hf_datasets/my_gpt_dataset\"\n",
    "\n",
    "# --- Load dataset ---\n",
    "dataset = load_from_disk(DATASET_PATH)\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token  # just in case\n",
    "\n",
    "# --- Tokenization ---\n",
    "def tokenize(example):\n",
    "    enc = tokenizer(example[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "test_tokenized = test_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# --- DataLoader ---\n",
    "def collate_fn(batch):\n",
    "    return tokenizer.pad(batch, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "loader = DataLoader(test_tokenized, batch_size=1, collate_fn=collate_fn)\n",
    "\n",
    "# --- Eval function ---\n",
    "def evaluate_model(model, label):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "\n",
    "    for batch in tqdm(loader, desc=f\"Evaluating {label}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            losses.append(outputs.loss.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f\"\\nðŸ“Š {label} â€” Loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f}\")\n",
    "    return avg_loss, perplexity\n",
    "\n",
    "# --- Load base model ---\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "evaluate_model(base_model, \"Base Model\")\n",
    "\n",
    "# --- Load fine-tuned model ---\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "ft_model = PeftModel.from_pretrained(ft_model, ADAPTER_PATH)\n",
    "evaluate_model(ft_model, \"Fine-Tuned Model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4gemma_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
