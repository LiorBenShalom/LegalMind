{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation no API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "\n",
    "required_parts = [\n",
    "    \"××ª×—××™ ×¢× ×™×©×”\", \"××—×™×“×•×ª ×‘×¢× ×™×©×”\", \"××ª×—× ×”×¢× ×™×©×”\", \"××ª×—× ×¢× ×™×©×”\", \"×“×™×•×Ÿ\",\n",
    "    \"×¢× ×™×©×” × ×”×•×’×”\", \"×”×¢× ×™×©×” ×”× ×•×”×’×ª\", \"×¢× ×™×©×” × ×•×”×’×ª\", \"××ª×—× ×”×¢×•× ×©\", \"××ª×—× ×¢×•× ×©\",\n",
    "    \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\", \"×•×”×›×¨×¢×”\", \"×”×”×¨×©×¢×”\", \"××“×™× ×™×•×ª ×”×¢× ×™×©×” ×”× ×”×•×’×”\"\n",
    "]\n",
    "citation_patterns = {\n",
    "    '×¢\"×¤': r'×¢\"×¤ (\\d+/\\d+)',\n",
    "    '×¢×¤\"×’': r'×¢×¤\"×’ (\\d+/\\d+)',\n",
    "    '×ª×´×¤': r'×ª×´×¤ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    '×¢×¤×´×’': r'×¢×¤×´×’ (\\d+/\\d+)',\n",
    "    '×¨×¢×´×¤': r'×¨×¢×´×¤ (\\d+/\\d+)',\n",
    "    '×ª×¤\"×—': r'×ª×¤\"×—\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "}\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/home/liorkob/best_model.pt\"  # Path to your saved model\n",
    "tokenizer = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def extract_citations(para_text):\n",
    "    \"\"\"\n",
    "    Extracts all citations and their full references from the text.\n",
    "    Returns a list of tuples: (citation_type, full_citation).\n",
    "    \"\"\"\n",
    "    citations = []\n",
    "\n",
    "    for citation_type, pattern in citation_patterns.items():\n",
    "        matches = re.findall(pattern, para_text)  # Find all matches for the pattern\n",
    "        for match in matches:\n",
    "            full_citation = f\"{citation_type} {match}\"  # Construct full citation\n",
    "            citations.append((citation_type, full_citation))\n",
    "\n",
    "    return citations  # List of (citation_type, full_citation)\n",
    "\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"Extracts the first occurrence of a required part in the CSV and all subsequent rows.\"\"\"\n",
    "    start_index = None\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "    return csv_data.iloc[start_index:] if start_index is not None else pd.DataFrame(columns=csv_data.columns)\n",
    "import re\n",
    "\n",
    "def process_and_tag(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"Process a .docx document and its corresponding CSV to check citations and tag with predictions.\"\"\"\n",
    "    try:\n",
    "        # Load the document and CSV\n",
    "        doc = docx.Document(docx_path)\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Iterate through paragraphs\n",
    "        for i, paragraph in enumerate(doc.paragraphs):\n",
    "            para_text = paragraph.text.strip()\n",
    "            if not para_text:\n",
    "                continue  # Skip empty paragraphs\n",
    "\n",
    "            found_citations = extract_citations(para_text)\n",
    "\n",
    "            if not found_citations:\n",
    "                continue  # No citations found, move to the next paragraph\n",
    "\n",
    "            for found_citation, full_citation in found_citations:\n",
    "\n",
    "                is_relevant = False\n",
    "                matching_part = None\n",
    "\n",
    "                # Check if the citation is in relevant parts\n",
    "                for _, row in csv_data.iterrows():\n",
    "                    part_text = row.get(\"text\", \"\")\n",
    "                    if any(req_part in row.get(\"part\", \"\") for req_part in required_parts) and part_text in para_text:\n",
    "                        is_relevant = True\n",
    "                        matching_part = row[\"part\"]\n",
    "                        break  # Stop searching once a match is found\n",
    "\n",
    "                if is_relevant:\n",
    "                    # Tag the paragraph using the model\n",
    "                    encoding = tokenizer(para_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "                    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "                    with torch.no_grad():\n",
    "                        output = model(**encoding)\n",
    "                        prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "                    # Append only when is_relevant = True\n",
    "                    results.append({\n",
    "                        'paragraph_number': i,\n",
    "                        'paragraph_text': para_text,\n",
    "                        'citation': full_citation,\n",
    "                        'part': matching_part,\n",
    "                        'predicted_label': prediction, \n",
    "                    })\n",
    "\n",
    "                    print(f\"Tagged citation: Paragraph {i}, Part: {matching_part}, Prediction: {prediction}\")\n",
    "                    print(f\"Text: {para_text}\\n\")\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "        # Save results\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Tagged citations saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {docx_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        docx_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_{year}')\n",
    "        csv_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_csv_{year}')\n",
    "        output_directory = Path(f'/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}')\n",
    "\n",
    "        output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file_path in docx_directory.glob(\"*.docx\"):\n",
    "            try:\n",
    "                new_file_path = file_path.stem\n",
    "                print(f\"Processing {new_file_path}\")\n",
    "\n",
    "                csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "                if file_path.exists() and csv_file.exists():\n",
    "                    output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "                    process_and_tag(str(file_path), str(csv_file), str(output_file))\n",
    "\n",
    "                else:\n",
    "                    if not file_path.exists():\n",
    "                        print(f\"Document file not found: {file_path}\")\n",
    "                    if not csv_file.exists():\n",
    "                        print(f\"CSV file not found for: {csv_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all results to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import docx\n",
    "\n",
    "def merge_results(csv_directory: str, output_csv: str):\n",
    "    csv_directory = Path(csv_directory)\n",
    "    all_data = []\n",
    "    \n",
    "    # Iterate over CSV files\n",
    "    for file_path in csv_directory.glob(\"*.csv\"):\n",
    "        try:\n",
    "            if file_path.stat().st_size == 0:  # Check if file is empty\n",
    "                print(f\"Skipping empty file: {file_path.name}\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty:  # Check if the file is empty even after reading\n",
    "                print(f\"Skipping empty DataFrame: {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            df[\"source_file\"] = file_path.name  # Add filename column\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path.name}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        merged_df = pd.concat(all_data, ignore_index=True)\n",
    "        merged_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Merged CSV saved to: {output_csv}\")\n",
    "    else:\n",
    "        print(\"No valid CSV files found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        csv_directory = f\"/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}\"\n",
    "        output_csv = f\"{csv_directory}/merged_results_{year}.csv\"\n",
    "        \n",
    "        merge_results(csv_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation WITH API -v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import docx\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "print([m.id for m in models])\n",
    "model_info = client.models.retrieve(\"gpt-4o\")\n",
    "print(model_info)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define required sections and citation patterns\n",
    "required_parts = [\n",
    "    \"××ª×—××™ ×¢× ×™×©×”\", \"××—×™×“×•×ª ×‘×¢× ×™×©×”\", \"××ª×—× ×”×¢× ×™×©×”\", \"××ª×—× ×¢× ×™×©×”\", \"×“×™×•×Ÿ\",\n",
    "    \"×¢× ×™×©×” × ×”×•×’×”\", \"×”×¢× ×™×©×” ×”× ×•×”×’×ª\", \"×¢× ×™×©×” × ×•×”×’×ª\", \"××ª×—× ×”×¢×•× ×©\", \"××ª×—× ×¢×•× ×©\",\n",
    "    \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\", \"×•×”×›×¨×¢×”\", \"×”×”×¨×©×¢×”\", \"××“×™× ×™×•×ª ×”×¢× ×™×©×” ×”× ×”×•×’×”\"\n",
    "]\n",
    "citation_patterns = ['×¢\"×¤', '×ª\"×¤', '×¢×¤\"×’', '×¢×´×¤', '×ª×´×¤', '×¢×¤×´×’']\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model_path = \"/home/liorkob/best_model.pt\"  # Path to your saved model\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model_bert.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "def split_preserving_structure(text):\n",
    "    paragraphs = re.split(r'(?<=\\d\\.)\\s', text)  # Split after numbers followed by a period\n",
    "    return [para.strip() for para in paragraphs if para.strip()]\n",
    "\n",
    "def query_gpt(text):\n",
    "    \"\"\"\n",
    "    Queries GPT-4o to extract and segment legal citations.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"×”×˜×§×¡×˜ ×”×‘× ××›×™×œ ××¡×¤×¨ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™×, ×©×”× ×”×¤× ×™×•×ª ×œ×”×—×œ×˜×•×ª ×©×œ ×‘×ª×™ ××©×¤×˜ ×•×”× × ×›×ª×‘×™× ×‘×¤×•×¨××˜ ×”×‘×: \"\n",
    "        \"×¡×•×’ ×”×”×œ×™×š (×¢\\\"×¤, ×¢×¤\\\"×’, ×ª\\\"×¤ ×•×›×•'), ××¡×¤×¨ ×”×ª×™×§, ×©××•×ª ×”×¦×“×“×™×, ×•×ª××¨×™×š ×”×”×—×œ×˜×” ×‘×¡×•×’×¨×™×™×. \"\n",
    "        \"×œ×“×•×’××”: ×¢\\\"×¤ 4173/07 ×¤×œ×•× ×™ × ' ××“×™× ×ª ×™×©×¨××œ (2007).\\n\\n\"\n",
    "\n",
    "        \" **×”× ×—×™×•×ª ×§×¨×™×˜×™×•×ª:**\\n\"\n",
    "        \" **××™×Ÿ ×œ×¢×¨×•×š, ×œ×©× ×•×ª ××• ×œ×”×©××™×˜ ×©×•× ×—×œ×§ ××”×˜×§×¡×˜ ×”××§×•×¨×™** â€“ ×›×œ ×”×ª×•×›×Ÿ ×—×™×™×‘ ×œ×”×•×¤×™×¢ ×›×¤×™ ×©×”×•×.\\n\"\n",
    "        \" **×™×© ×œ×¤×¦×œ ×œ×¤×¡×§××•×ª ×œ×¤×™ ×”×¦×™×˜×•×˜×™× ×”××©×¤×˜×™×™×**, ×›×š ×©×›×œ ×¤×¡×§×” ×ª×›×™×œ ×¦×™×˜×•×˜ ×¢× ×”×”×§×©×¨ ×”××ª××™×.\\n\"\n",
    "        \" **×× ××¡×¤×¨ ×¦×™×˜×•×˜×™× ××ª×™×™×—×¡×™× ×œ××•×ª×• ××§×¨×” ×™×© ×œ×”×©××™×¨× ×™×—×“ ×‘××•×ª×” ×¤×¡×§×”**.\\n\"\n",
    "        \" **×”×¤×¡×§××•×ª ×—×™×™×‘×•×ª ×œ×”×•×¤×™×¢ ×‘×¡×“×¨ ×”××§×•×¨×™ ×©×œ×”×Ÿ** â€“ ××™×Ÿ ×œ×¢×¨×‘×‘ ××• ×œ×”×–×™×– ×—×œ×§×™× ×‘×˜×§×¡×˜.\\n\"\n",
    "        \" **××™×Ÿ ×œ×™×¦×•×¨ ×¤×¡×§××•×ª ×©××™×Ÿ ×‘×”×Ÿ ×¦×™×˜×•×˜ ××©×¤×˜×™**.\\n\\n\"\n",
    "\n",
    "        \"### ğŸ” ×“×•×’×××•×ª ×œ×¤×™×¦×•×œ × ×›×•×Ÿ ×•×©×’×•×™:\\n\\n\"\n",
    "\n",
    "        \"âŒ **×¤×™×¦×•×œ ×©×’×•×™ (×œ× × ×›×•×Ÿ):**\\n\"\n",
    "        \"1. ×¢\\\"×¤ 1234/20 ××“×™× ×ª ×™×©×¨××œ × ' ×›×”×Ÿ (2020)\\n\"\n",
    "        \"2. ×˜×§×¡×˜ ×›×œ×œ×™ ×‘×œ×™ ×¦×™×˜×•×˜ â€“ ××™×Ÿ ×œ××¤×©×¨ ×–××ª.\\n\\n\"\n",
    "\n",
    "        \"âœ… **×¤×™×¦×•×œ × ×›×•×Ÿ (×›×Ÿ × ×›×•×Ÿ):**\\n\"\n",
    "        \"1. ×¢\\\"×¤ 5678/15 ×œ×•×™ × ' ××“×™× ×ª ×™×©×¨××œ (2015) - ×‘××§×¨×” ×–×” × ×§×‘×¢ ×›×™...\\n\"\n",
    "        \"2. ×¢\\\"×¤ 9876/18 ×›×”×Ÿ × ' ××“×™× ×ª ×™×©×¨××œ (2018) - ×‘× ×¡×™×‘×•×ª ×“×•××•×ª, ×”×•×—×œ×˜ ×›×™...\\n\\n\"\n",
    "\n",
    "        \"âœ… **×›××©×¨ ×›×œ ×”×¦×™×˜×•×˜×™× ×©×™×™×›×™× ×œ××•×ª×• ×”×§×©×¨ - ×™×© ×œ×”×©××™×¨× ×™×—×“:**\\n\"\n",
    "        \"1. \\\"×¢×œ ×“×¨×š ×”×›×œ×œ, ×‘×™×ª ××©×¤×˜ ×–×” × ×“×¨×© ×œ×¦×¢×¨×™ ×œ× ××—×ª ×œ××™×¨×•×¢×™× ××¢×™×Ÿ ××œ×” ×©×œ ×¤×ª×¨×•×Ÿ ×¡×›×¡×•×›×™× ×‘×“×¨×›×™ ××œ×™××•×ª, \"\n",
    "        \"×•×œ× ×ª×ª×›×Ÿ ××—×œ×•×§×ª ×›×™ ×™×© ×œ×”×˜×™×œ ×¢×•× ×©×™× ××©××¢×•×ª×™×™×, ×¢×œ ×¤×™ ×¨×•×‘ ×××—×•×¨×™ ×¡×•×¨×’ ×•×‘×¨×™×—, ×›×“×™ ×œ×¢×§×•×¨ ×ª×•×¤×¢×•×ª ××œ×” ××”×©×•×¨×©. \"\n",
    "        \"××™×Ÿ ××§×•× ×œ×¡×•×‘×œ× ×•×ª ×›×œ×¤×™ ×™×“ ×§×œ×” ×¢×œ ×”×”×“×§ ××• ×§×ª ×¡×›×™×Ÿ ××• ×‘××§×œ ×—×•×‘×œ×™×. ×•×¢×œ ×›×š × ×××¨ ×™×© ××§×•× ×©×’× ×‘×™×ª ×”××©×¤×˜ ×™×ª×¨×•× ××ª ×—×œ×§×• \"\n",
    "        \"×œ××œ×—××” × ×’×“ ×”××œ×™××•×ª. ×œ×¢×™×ª×™× ×™×© ×ª×—×•×©×” ×©×›×œ ×××¨×” ×œ× × ×›×•× ×” ××• ×”×ª× ×”×’×•×ª ×©×‘×¢×™× ×™ ××—×¨ ×¡×•×˜×” ××Ÿ ×”×©×•×¨×”, ×•×œ×• ×‘××§×¦×ª, \"\n",
    "        \"××”×•×•×” ×”×¦×“×§×” ×¢×‘×•×¨ ×”×¤×•×’×¢ ×•×¡×‘×™×‘×ª×• ×œ×¤×’×•×¢ ×‘×××¦×¢×•×ª × ×©×§ ×§×¨ ×‘×™×—×™×“ ×•×‘×¡×‘×™×‘×ª×•\\\" \"\n",
    "        \"(×¢×•×“ ×œ×¢× ×™×™×Ÿ ×–×” ×¨××• ×¢\\\"×¤ 4173/07 ×¤×œ×•× ×™ × ' ××“×™× ×ª ×™×©×¨××œ (2007); ×¢\\\"×¤ 8991/10 ××›×‘×™ × ' ××“×™× ×ª ×™×©×¨××œ (2011); \"\n",
    "        \"×¢\\\"×¤ 7360/13 ×˜××”× × ' ××“×™× ×ª ×™×©×¨××œ (2014).)\\n\\n\"\n",
    "\n",
    "        \"âš ï¸ **×—×©×•×‘ ×××•×“:**\\n\"\n",
    "        \"âœ” **×›×œ ×”×˜×§×¡×˜ ×—×™×™×‘ ×œ×”×•×¤×™×¢ ×›×¤×™ ×©×”×•×, ×œ×œ× ×©×™× ×•×™, ×¢×¨×™×›×” ××• ×”×©××˜×”.**\\n\"\n",
    "        \"âœ” **×›×œ ×¤×¡×§×” ×—×™×™×‘×ª ×œ×”×›×™×œ ×¦×™×˜×•×˜ ××©×¤×˜×™ ×•×œ×©××•×¨ ×¢×œ ×”×”×§×©×¨ ×”××§×•×¨×™ ×©×œ×”.**\\n\"\n",
    "        \"âœ” **×× ×™×© ×¦×™×˜×•×˜×™× ×”×§×©×•×¨×™× ×–×” ×œ×–×”, ×™×© ×œ×”×©××™×¨× ×™×—×“.**\\n\"\n",
    "        \"âœ” **××™×Ÿ ×œ×©× ×•×ª ××ª ×¡×“×¨ ×”×¤×¡×§××•×ª ×•××™×Ÿ ×œ×™×¦×•×¨ ×˜×§×¡×˜×™× ×—×“×©×™×.**\\n\\n\"\n",
    "\n",
    "        f\"Text: {text}\\n\"\n",
    "        \"Processed Segments:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Change from \"gpt-4o-2024-11-20\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to extract and structure legal citations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        processed_text = response.choices[0].message.content\n",
    "\n",
    "        # Debugging: Print raw GPT response\n",
    "        print(\"RAW GPT OUTPUT:\\n\", processed_text)\n",
    "\n",
    "        # Improved paragraph splitting while preserving structure\n",
    "        split_paragraphs = split_preserving_structure(processed_text)\n",
    "\n",
    "        # Remove duplicates\n",
    "        split_paragraphs = list(dict.fromkeys(split_paragraphs))\n",
    "\n",
    "        # Debugging: Print processed paragraphs\n",
    "        print(\"PROCESSED PARAGRAPHS:\\n\", split_paragraphs)\n",
    "\n",
    "        # Return processed text\n",
    "        return split_paragraphs if split_paragraphs else [text]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ GPT API error: {e}\")\n",
    "        return [text]  # Return original text in case of failure\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a required part in the CSV and all subsequent rows.\n",
    "    \"\"\"\n",
    "    start_index = None\n",
    "\n",
    "    # Find the first row containing a required part\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    # If a match is found, return only relevant rows\n",
    "    if start_index is not None:\n",
    "        return csv_data.iloc[start_index:]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=csv_data.columns)  # Return an empty DataFrame if no matches found\n",
    "\n",
    "def enforce_citation_splitting(split_paragraphs):\n",
    "    \"\"\"\n",
    "    Ensures each citation is properly separated, even if GPT fails.\n",
    "    \"\"\"\n",
    "    refined = []\n",
    "    citation_pattern = re.compile(r'(×¢\"?×¤|×¢×¤\"×’|×ª\"?×¤) \\d+[-/]?\\d{2,5} .*?\\[\\d{1,2}\\.\\d{2,4}\\]')\n",
    "\n",
    "    for para in split_paragraphs:\n",
    "        matches = citation_pattern.findall(para)\n",
    "\n",
    "        if len(matches) > 1:\n",
    "            segments = citation_pattern.split(para)\n",
    "            for i in range(1, len(segments), 2):  \n",
    "                citation = segments[i].strip()\n",
    "                context = segments[i + 1].strip() if i + 1 < len(segments) else \"\"\n",
    "                refined.append(f\"{citation} {context}\")\n",
    "        else:\n",
    "            refined.append(para.strip())\n",
    "\n",
    "    return refined\n",
    "\n",
    "def process_and_tag_with_split(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a .docx document and its corresponding CSV, split paragraphs only if they contain \n",
    "    multiple citations, ensure they are within relevant parts, and tag with predictions.\n",
    "    \"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    filtered_csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "    results = []\n",
    "    for i, paragraph in enumerate(doc.paragraphs):\n",
    "        para_text = paragraph.text.strip()\n",
    "        if not para_text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        # Count the number of citations in the paragraph\n",
    "        citation_count = sum(para_text.count(pattern) for pattern in citation_patterns)\n",
    "\n",
    "        is_relevant = False\n",
    "        matching_part = None\n",
    "\n",
    "        for _, row in filtered_csv_data.iterrows():\n",
    "            part_text = row.get(\"text\", \"\")\n",
    "            if any(req_part in row.get(\"part\", \"\") for req_part in required_parts) and part_text in para_text:\n",
    "                is_relevant = True\n",
    "                matching_part = row[\"part\"]\n",
    "                break  # Stop searching once a match is found\n",
    "\n",
    "        if is_relevant:\n",
    "            # Tag the paragraph using the model\n",
    "            encoding = tokenizer_bert(para_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "            encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "            with torch.no_grad():\n",
    "                output = model_bert(**encoding)\n",
    "                prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "            # SPLIT ONLY IF TAG IS 1\n",
    "            if prediction == 1:\n",
    "                if citation_count > 1:\n",
    "                    split_paragraphs = query_gpt(para_text)\n",
    "                    split_paragraphs = enforce_citation_splitting(split_paragraphs)\n",
    "                    original_paragraph = para_text  # Store the original text\n",
    "                else:\n",
    "                    split_paragraphs = [para_text]\n",
    "                    original_paragraph = None  # Not split, so no original needed\n",
    "            else:\n",
    "                split_paragraphs = [para_text]  # Keep as is\n",
    "                original_paragraph = None  # No need to store original\n",
    "\n",
    "            for split_text in split_paragraphs:\n",
    "                # Check if the split paragraph contains a citation\n",
    "                if not any(pattern in split_text for pattern in citation_patterns):\n",
    "                    continue  # Skip non-citation paragraphs\n",
    "\n",
    "                # Save results\n",
    "                result = {\n",
    "                    'paragraph_number': i,\n",
    "                    'original_paragraph': original_paragraph if citation_count > 1 else split_text,  # Store original if split\n",
    "                    'paragraph_text': split_text,\n",
    "                    'part': matching_part,\n",
    "                    'predicted_label': prediction\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Tagged citations saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docx_directory = Path('/home/liorkob/thesis/lcp/data/docx_2019')\n",
    "    csv_directory = Path('/home/liorkob/thesis/lcp/data/docx_csv_2019')\n",
    "    output_directory = Path('/home/liorkob/thesis/lcp/data/tag_citations_csv_2019')\n",
    "\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in docx_directory.glob(\"*.docx\"):\n",
    "        new_file_path = file_path.stem\n",
    "        print(f\"Processing {new_file_path}\")\n",
    "\n",
    "        csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "        \n",
    "        if file_path.exists() and csv_file.exists():\n",
    "            output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "            process_and_tag_with_split(str(file_path), str(csv_file), str(output_file))\n",
    "        else:\n",
    "            if not file_path.exists():\n",
    "                print(f\"Document file not found: {file_path}\")\n",
    "            if not csv_file.exists():\n",
    "                print(f\"CSV file not found for: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation WITH API -v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: ×‘×¢×¤\"×’ (×‘\"×©) 31067-11-11\n",
      "match.group(): ×‘×ª×´×¤ 18402-08-13\n",
      "match.group(): ×ª×¤×´× 12345-01-22\n",
      "match.group(): ×ª×´×¤ 12345-01-22\n",
      "match.group(): ×¢.×¤. 567/22\n",
      "match.group(): ×¢×´×¤ 567/22\n",
      "match.group(): ×‘×ª.×¤. 56255-02-12\n",
      "match.group(): ×¢×¤\"×’ 6074/93\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# List of legal acronyms (same as yours)\n",
    "acronyms = [\n",
    "    \"××‘\", \"××‘×¢\", \"××™××•×¦\", \"×××¦\", \"××¤\", \"××¤×—\", \"××ª\", \"××ª×¤\", \"×‘××¤\", \"×‘××©\", \"×‘×‘× \", \"×‘×’×¦\", \"×‘×“×\", \"×‘×“×\",\n",
    "    \"×‘×“××©\", \"×‘×”× \", \"×‘×”×¢\", \"×‘×”×©\", \"×‘×™×“×\", \"×‘×™×“×¢\", \"×‘×œ\", \"×‘×œ×\", \"×‘×\", \"×‘×¢×\", \"×‘×¢×—\", \"×‘×¢×\", \"×‘×¢×§\", \"×‘×¤\",\n",
    "    \"×‘×¤×\", \"×‘×¤×ª\", \"×‘×¦×\", \"×‘×¦×”×\", \"×‘×§\", \"×‘×§×\", \"×‘×§×©×”\", \"×‘×¨×\", \"×‘×¨×¢\", \"×‘×¨×¢×¤\", \"×‘×¨×©\", \"×‘×©\", \"×‘×©×\",\n",
    "    \"×‘×©×’×¦\", \"×‘×©×”×ª\", \"×‘×©×–\", \"×‘×©×\", \"×‘×©×¢\", \"×‘×©×¤\", \"×‘×ª×ª\", \"×’×–×–\", \"×’××¨\", \"×’×¤\", \"×“×‘×¢\", \"×“×—\", \"×“×˜\", \"×“×™×•× \",\n",
    "    \"×“×\", \"×“××¨\", \"×“××©\", \"×“× \", \"×“× ×\", \"×“× ×’×¦\", \"×“× ×\", \"×“× ×¤\", \"×”×“\", \"×”×“×¤\", \"×”×•×¦×œ×¤\", \"×”×˜\", \"×”×›\", \"×”×\",\n",
    "    \"×”××“\", \"×”××\", \"×”××¢\", \"×”××©\", \"×”× \", \"×”×¡×ª\", \"×”×¢\", \"×”×¢×–\", \"×”×¤\", \"×”×¤×‘\", \"×”×¤×\", \"×”×¦×\", \"×”×©\", \"×”×©×\",\n",
    "    \"×”×©×’×¦\", \"×”×©×¤\", \"×”×©×¨\", \"×”×ª\", \"×•×—×§\", \"×•×¢\", \"×•×©×\", \"×•×©×§\", \"×•×©×¨\", \"×–×™\", \"×—×\", \"×—×‘×¨\", \"×—×“\", \"×—×“×\",\n",
    "    \"×—×“×œ×¤\", \"×—×“×œ×ª\", \"×—×“×\", \"×—×“×¤\", \"×—×”×¢\", \"×—×™\", \"×—× \", \"×—×¡×\", \"×—×¢×\", \"×—×¢×§\", \"×—×©\", \"×™×•×©\", \"×™×™×ª×\", \"×™××\",\n",
    "    \"×™×¡\", \"×›×¦\", \"×\", \"××\", \"××‘×›\", \"××‘×¡\", \"××•× ×•×¤×•×œ×™× \", \"××–×’\", \"××—\", \"××—×•×–\", \"××—×¢\", \"××˜\", \"××˜×›×œ\", \"××™\",\n",
    "    \"××™×‘\", \"××›\", \"××\", \"××¡\", \"××¡×˜\", \"××¢×™\", \"××¢×ª\", \"××§×\", \"××¨×›×–\", \"××ª\", \"× \", \"× ×‘\", \"× ×‘×\", \"× ×\", \"× ××‘\",\n",
    "    \"× ×¢×“\", \"× ×¢×¨\", \"×¡×‘×\", \"×¡×¢\", \"×¡×¢×©\", \"×¡×§\", \"×¡×§×›\", \"×¢\", \"×¢×\", \"×¢××—\", \"×¢××¤\", \"×¢×‘\", \"×¢×‘××¤\", \"×¢×‘×–\", \"×¢×‘×—\",\n",
    "    \"×¢×‘×™\", \"×¢×‘×œ\", \"×¢×‘××¦\", \"×¢×‘×¢×—\", \"×¢×‘×¤\", \"×¢×‘×¨\", \"×¢×‘×©×”×ª\", \"×¢×’×¨\", \"×¢×“×™\", \"×¢×“×\", \"×¢×”×’\", \"×¢×”×¡\", \"×¢×”×¤\",\n",
    "    \"×¢×•\", \"×¢×•×¨×¤\", \"×¢×–\", \"×¢×—\", \"×¢×—×\", \"×¢×—×“×œ×¤\", \"×¢×—×“×¤\", \"×¢×—×“×ª\", \"×¢×—×”×¡\", \"×¢×—×¢\", \"×¢×—×§\", \"×¢×—×¨\", \"×¢×›×‘\",\n",
    "    \"×¢×œ\", \"×¢×œ×\", \"×¢×œ×‘×©\", \"×¢×œ×—\", \"×¢×œ×¢\", \"×¢×\", \"×¢××\", \"×¢××”\", \"×¢××–\", \"×¢××—\", \"×¢××™\", \"×¢××œ×¢\", \"×¢××\", \"×¢×× \",\n",
    "    \"×¢××¤\", \"×¢××¦\", \"×¢××§\", \"×¢××¨×\", \"×¢××©\", \"×¢××©×\", \"×¢××ª\", \"×¢× \", \"×¢× ×\", \"×¢× ×\", \"×¢× ××\", \"×¢× ××©\", \"×¢× ×¤\",\n",
    "    \"×¢×¡×\", \"×¢×¡×§\", \"×¢×¢\", \"×¢×¢×\", \"×¢×¢×\", \"×¢×¢×¨\", \"×¢×¢×ª×\", \"×¢×¤\", \"×¢×¤×\", \"×¢×¤×’\", \"×¢×¤×”×’\", \"×¢×¤×\", \"×¢×¤××§\",\n",
    "    \"×¢×¤× \", \"×¢×¤×¡\", \"×¢×¤×¡×¤\", \"×¢×¤×¢\", \"×¢×¤×¨\", \"×¢×¤×ª\", \"×¢×¦×\", \"×¢×§\", \"×¢×§×’\", \"×¢×§×\", \"×¢×§× \", \"×¢×§×¤\", \"×¢×¨\", \"×¢×¨×\",\n",
    "    \"×¢×¨×’×¦\", \"×¢×¨×\", \"×¢×¨×¢×•×¨\", \"×¢×¨×¤\", \"×¢×¨×¨\", \"×¢×©\", \"×¢×©×\", \"×¢×©×\", \"×¢×©×¨\", \"×¢×©×ª\", \"×¢×©×ª×©\", \"×¢×ª\", \"×¢×ª×\",\n",
    "    \"×¢×ª×\", \"×¢×ª×¤×‘\", \"×¢×ª×¦\", \"×¤×\", \"×¤×”\", \"×¤×œ\", \"×¤×œ×\", \"×¤×\", \"×¤××¨\", \"×¤×¢×\", \"×¤×§×—\", \"×¤×¨\", \"×¤×¨×§\", \"×¤×©×–\",\n",
    "    \"×¤×©×¨\", \"×¤×ª\", \"×¦×\", \"×¦×‘× \", \"×¦×”\", \"×¦×•\", \"×¦×—\", \"×¦×\", \"×§×’\", \"×§×¤\", \"×¨×—×“×¤\", \"×¨××©\", \"×¨×¢\", \"×¨×¢×\", \"×¨×¢×‘\",\n",
    "    \"×¨×¢×‘×¡\", \"×¨×¢×•\", \"×¨×¢×\", \"×¨×¢×¡\", \"×¨×¢×¤\", \"×¨×¢×¤×\", \"×¨×¢×¦\", \"×¨×¢×¨\", \"×¨×¢×¨×¦\", \"×¨×¢×©\", \"×¨×¢×ª×\", \"×¨×¦×¤\", \"×¨×ª×§\",\n",
    "    \"×©\", \"×©×‘×“\", \"×©×\", \"×©××™\", \"×©× ×\", \"×©×¢\", \"×©×¢×\", \"×©×§\", \"×©×©\", \"×ª×\", \"×ª××“×\", \"×ª××—\", \"×ª××\", \"×ª××§\", \"×ª×‘\",\n",
    "    \"×ª×‘×›\", \"×ª×‘×¢\", \"×ª×’\", \"×ª×’×\", \"×ª×“\", \"×ª×“×\", \"×ª×”×’\", \"×ª×”× \", \"×ª×”×¡\", \"×ª×•×‘\", \"×ª×•×—\", \"×ª×—\", \"×ª×—×¤\", \"×ª×—×ª\",\n",
    "    \"×ª×˜\", \"×ª×™\", \"×ª×›\", \"×ª×œ×\", \"×ª×œ×‘\", \"×ª×œ×”×\", \"×ª×œ×¤\", \"×ª×œ×ª×\", \"×ª×\", \"×ª××”×—\", \"×ª××\", \"×ª××§\", \"×ª××¨\",\n",
    "    \"×ª××©\", \"×ª× ×’\", \"×ª× ×–\", \"×ª×¢\", \"×ª×¢×\", \"×ª×¢×–\", \"×ª×¤\", \"×ª×¤×‘\", \"×ª×¤×—\", \"×ª×¤×—×¢\", \"×ª×¤×›\", \"×ª×¤×\", \"×ª×¤×¢\",\n",
    "    \"×ª×¤×§\", \"×ª×¦\", \"×ª×§\", \"×ª×§×—\", \"×ª×§×\", \"×ª×¨×\", \"×ª×ª\", \"×ª×ª×—\", \"×ª×ª×¢\", \"×ª×ª×¢×\", \"×ª×ª×§\"\n",
    "]\n",
    "\n",
    "def create_acronym_variants(acronyms):\n",
    "    acronym_variants = []\n",
    "    for a in acronyms:\n",
    "        if len(a) > 1:\n",
    "            # Case 1: Original acronym with quotes/dots before last letter\n",
    "            base_acronym = a\n",
    "            if a.startswith('×‘') or a.startswith('×•') or a.startswith('×”'):\n",
    "                # Also add variant without the prefix letter\n",
    "                base_acronym = a[1:]\n",
    "            \n",
    "            # For each acronym (both with and without prefix)\n",
    "            for acr in [a, base_acronym]:\n",
    "                if len(acr) > 1:\n",
    "                    # Standard quote/dot before last letter\n",
    "                    quoted = rf\"{acr[:-1]}[\\\"'×´]{acr[-1]}\"\n",
    "                    with_dot = rf\"{acr[:-1]}\\.{acr[-1]}\"\n",
    "                    acronym_variants.append(f\"(?:{quoted}|{with_dot})\")\n",
    "                    \n",
    "                    # Add dot-separated variant\n",
    "                    dots_between = '\\.'.join(list(acr))\n",
    "                    acronym_variants.append(dots_between)\n",
    "    \n",
    "    return '|'.join(acronym_variants)\n",
    "        \n",
    "acronym_pattern = create_acronym_variants(acronyms)\n",
    "\n",
    "# Ensure the numbers follow the correct format\n",
    "number_pattern = r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "'''\n",
    "citation_pattern = fr'''\n",
    "    (?<!\\w)                      # Ensure no letter before\n",
    "    ([×-×ª]?)                     # Optional single Hebrew prefix letter (but no isolated matches)\n",
    "    ({acronym_pattern})           # Captures acronym (short & long)\n",
    "    \\.?                          # Optional dot after acronym\n",
    "    \\s*                          # Optional spaces\n",
    "    (\\((.*?)\\))?                  # Optional court location in parentheses\n",
    "    \\s*[-/]?\\s*                  # Required space or separator before case number\n",
    "    ({number_pattern})            # Captures case number formats\n",
    "    (?!\\w)                       # Ensure no letter after\n",
    "'''.strip()\n",
    "\n",
    "# Compile regex with verbose flag for readability\n",
    "citation_regex = re.compile(citation_pattern, re.VERBOSE)\n",
    "\n",
    "# Test the regex with example text\n",
    "test_text = '×‘×¢×¤\"×’ (×‘\"×©) 31067-11-11'\n",
    "match = citation_regex.search(test_text)\n",
    "if match:\n",
    "    print(f\"Matched: {match.group()}\")\n",
    "else:\n",
    "    print(\"No match\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"×‘×ª×´×¤ 18402-08-13\",\n",
    "    \"×ª×¤×´× 12345-01-22\",\n",
    "    \"×ª×´×¤ 12345-01-22\",\n",
    "    \"×¢.×¤. 567/22\",\n",
    "\"×¢×´×¤ 567/22\", \n",
    "'×‘×ª.×¤. 56255-02-12',\n",
    "'×¢×¤\"×’ 6074/93'\n",
    "]\n",
    "# Extract matches\n",
    "for text in test_cases:\n",
    "    match = re.search(citation_regex, text)\n",
    "\n",
    "    if match:\n",
    "        print(f\"match.group(): {match.group()}\")\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "def extract_citations_from_csv(csv_data):\n",
    "    citations = []\n",
    "    text_column = csv_data[\"text\"].astype(str)  # Convert to string to avoid NaN issues\n",
    "    pd.set_option(\"display.max_colwidth\", None)  # Ensure full text is displayed\n",
    "    # print(\"\\n\".join(text_column))  # Print each row as a full text\n",
    "    for i, text in enumerate(text_column, 1):\n",
    "        print(f\"{i}. {text}\")\n",
    "\n",
    "    matches = text_column.str.extractall(citation_regex)  # Extract structured matches\n",
    "    print(\"Extracted Matches:\")\n",
    "    print(matches)\n",
    "\n",
    "    print(\"Extracted DataFrame:\", matches)  # Debugging step\n",
    "    \n",
    "    for _, row in matches.iterrows():\n",
    "        # Build the citation string, joining all valid elements\n",
    "        citation = \" \".join(map(str, filter(pd.notna, row))).strip()\n",
    "\n",
    "        # Clean up extra spaces\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "\n",
    "        # Optionally remove unwanted prefixes like \"×‘\", \"×•\", \"×¨\"\n",
    "        citation = re.sub(r\"^\\b[×‘×•×•×¨]\\b\\s*\", \"\", citation)\n",
    "\n",
    "        # Remove invalid extra words (e.g., \"×¢×œ 12\")\n",
    "        if re.match(r\"^×¢×œ \\d+$\", citation):  \n",
    "            continue  # Skip invalid cases like \"×¢×œ 12\"\n",
    "\n",
    "        # Fix duplicated court locations, e.g., \"(××—×•×–×™ ××¨×›×–) ××—×•×–×™ ××¨×›×–\" â†’ \"(××—×•×–×™ ××¨×›×–)\"\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "\n",
    "        # Add the cleaned citation to the list\n",
    "        citations.append(citation)\n",
    "    \n",
    "    # Return citations as a list, even if some are empty or missing optional groups\n",
    "    return citations if citations else []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 11:06:04.086039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744099564.937118 2889771 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744099565.205763 2889771 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744099569.116398 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744099569.116480 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744099569.116485 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744099569.116490 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-08 11:06:09.538452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2889771/2638286754.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_bert.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ×ª\"×¤ 08âˆ•207\n",
      "Processing ×ª\"×¤ 34607-03-17\n",
      "Processing ×ª×¤\"×§ 19437-12-16\n",
      "Processing ×ª\"×¤ 16759-07-17\n",
      "Processing ×ª\"×¤ 58269-02-19\n",
      "Processing ×ª\"×¤ 50607-03-19\n",
      "Processing ×ª\"×¤ 41247-05-19\n",
      "Processing ×ª\"×¤ 45102-06-19\n",
      "Processing ×ª\"×¤ 18218-11-19\n",
      "Processing ×ª\"×¤ 6047-11-19\n",
      "Processing ×ª\"×¤ 65763-12-19\n",
      "Processing ×ª\"×¤ 48152-02-20\n",
      "Processing ×ª\"×¤ 28746-05-20\n",
      "Processing ×ª\"×¤ 57459-06-20\n",
      "Processing ×ª\"×¤ 12380-06-20\n",
      "Processing ×ª\"×¤ 8440-07-20\n",
      "Processing ×ª\"×¤ 24302-08-20\n",
      "Processing ×ª\"×¤ 43084-09-20\n",
      "Processing ×ª×¤\"×§ 29591-10-20\n",
      "Processing ×ª\"×¤ 39690-02-21\n",
      "Processing ×ª\"×¤ 40908-02-21\n",
      "Processing ×ª\"×¤ 10393-03-21\n",
      "Processing ×ª\"×¤ 56850-03-21\n",
      "Processing ×ª×¤\"×§ 7171-03-21\n",
      "Processing ×ª\"×¤ 3526-04-21\n",
      "Processing ×ª\"×¤ 986-06-21\n",
      "Processing ×ª\"×¤ 36546-08-21\n",
      "Processing ×ª\"×¤ 42879-09-21\n",
      "Processing ×ª\"×¤ 30884-10-21\n",
      "Processing ×ª\"×¤ 47780-11-21\n",
      "Processing ×ª\"×¤ 9851-11-21\n",
      "Processing ×ª\"×¤ 16014-01-22\n",
      "Processing ×ª\"×¤ 54272-09-14\n",
      "Processing ×ª\"×¤ 18588-11-10\n",
      "Processing ×ª\"×¤ 2892-07-10\n",
      "Processing ×ª\"×¤ 56235-06-13\n",
      "Processing ×ª\"×¤ 44226-07-14\n",
      "Processing ×ª\"×¤ 9343-07-14\n",
      "Processing ×ª\"×¤ 23142-09-14\n",
      "Processing ×ª\"×¤ 737-10-13\n",
      "Processing ×ª\"×¤ 16958-01-14\n",
      "Processing ×ª\"×¤ 33508-01-15\n",
      "Processing ×ª\"×¤ 17778-01-14\n",
      "Processing ×ª\"×¤ 18510-10-14\n",
      "Processing ×ª\"×¤ 55487-03-15\n",
      "Processing ×ª\"×¤ 37551-01-16\n",
      "Processing ×ª\"×¤ 9109-02-16\n",
      "Processing ×ª\"×¤ 52031-06-16\n",
      "Processing ×ª\"×¤ 24514-09-15\n",
      "Processing ×ª\"×¤ 31122-11-15\n",
      "Processing ×ª\"×¤ 46896-11-15\n",
      "Processing ×ª\"×¤ 6788-06-16\n",
      "Processing ×ª\"×¤ 11608-07-16\n",
      "Processing ×ª\"×¤ 22679-09-16\n",
      "Processing ×ª\"×¤ 63255-11-16\n",
      "Processing ×ª\"×¤ 11720-07-16\n",
      "Processing ×ª\"×¤ 57384-09-16\n",
      "Processing ×ª\"×¤ 22609-10-16\n",
      "Processing ×ª\"×¤ 58033-11-16\n",
      "Processing ×ª\"×¤ 19902-12-16\n",
      "Processing ×ª\"×¤ 701-12-16\n",
      "Processing ×ª\"×¤ 26620-12-16\n",
      "Processing ×ª\"×¤ 59971-01-17\n",
      "Processing ×ª\"×¤ 620-01-17\n",
      "Processing ×ª\"×¤ 51451-02-17\n",
      "Processing ×ª\"×¤ 30876-03-17\n",
      "Processing ×ª\"×¤ 40239-09-17\n",
      "Processing ×ª\"×¤ 37176-11-17\n",
      "Processing ×ª\"×¤ 17677-04-17\n",
      "Processing ×ª\"×¤ 7629-11-17\n",
      "Processing ×ª\"×¤ 73652-01-18\n",
      "Processing ×ª\"×¤ 5680-02-18\n",
      "Processing ×ª\"×¤ 5775-02-18\n",
      "Processing ×ª\"×¤ 58507-02-18\n",
      "Processing ×ª\"×¤ 69278-05-18\n",
      "Processing ×ª\"×¤ 16826-04-18\n",
      "Processing ×ª\"×¤ 46786-04-18\n",
      "Processing ×ª\"×¤ 39201-05-18\n",
      "Processing ×ª\"×¤ 6997-06-18\n",
      "Processing ×ª\"×¤ 67342-07-18\n",
      "Processing ×ª\"×¤ 39573-10-18\n",
      "Processing ×ª\"×¤ 58591-08-18\n",
      "Processing ×ª\"×¤ 8864-08-18\n",
      "Processing ×ª\"×¤ 809-09-18\n",
      "Processing ×ª×¤\"×— 42681-11-18\n",
      "Processing ×ª\"×¤ 68942-01-19\n",
      "Processing ×ª\"×¤ 68958-01-19\n",
      "Processing ×ª\"×¤ 45480-02-19\n",
      "Processing ×ª\"×¤ 70401-02-19\n",
      "Processing ×ª\"×¤ 14488-04-19\n",
      "Processing ×ª\"×¤ 14578-04-19\n",
      "Processing ×ª\"×¤ 4435-07-19\n",
      "Processing ×ª×¤\"×— 31343-04-19\n",
      "Processing ×ª\"×¤ 33338-02-20\n",
      "Processing ×ª\"×¤ 66437-07-20\n",
      "Processing ×ª\"×¤ 5778-09-20\n",
      "Processing ×ª\"×¤ 1338-12-20\n",
      "Processing ×ª\"×¤ 6678-12-20\n",
      "Processing ×ª\"×¤ 10139-03-21\n",
      "Processing ×ª\"×¤ 35998-11-21\n",
      "Processing ×ª\"×¤ 29863-03-22\n",
      "Processing ×ª\"×¤ 60484-03-22\n",
      "Processing ×ª\"×¤ 47042-01-22\n",
      "Processing ×ª\"×¤ 45199-01-22\n",
      "Processing ×ª\"×¤ 42162-12-21\n",
      "Processing ×ª\"×¤ 54208-06-22\n",
      "Processing ×ª\"×¤ 60934-03-22\n",
      "Processing ×ª\"×¤ 32175-07-22\n",
      "Processing ×ª\"×¤ 7867-04-22\n",
      "Processing ×ª\"×¤ 48194-05-22\n",
      "Processing ×ª\"×¤ 53625-07-22\n",
      "Processing ×ª\"×¤ 15747-08-22\n",
      "Processing ×ª\"×¤ 47987-12-22\n",
      "Processing ×ª\"×¤ 15884-08-22\n",
      "Processing ×ª\"×¤ 36801-10-22\n",
      "Processing ×ª\"×¤ 1990-11-22\n",
      "Processing ×ª\"×¤ 46786-01-23\n",
      "Processing ×ª\"×¤ 13598-05-23\n",
      "Processing ×ª\"×¤ 49875-05-23\n",
      "Processing ×ª\"×¤ 8444-04-23\n",
      "Processing ×ª\"×¤ 65592-01-23\n",
      "Processing ×ª\"×¤ 21137-11-23\n",
      "Processing ×ª\"×¤ 53640-01-23\n",
      "Processing ×ª\"×¤ 52567-05-23\n",
      "Processing ×ª\"×¤ 14553-12-23\n",
      "Processing ×ª\"×¤ 15727-06-23\n",
      "Processing ×ª\"×¤ 35088-07-24\n",
      "Processing ×ª\"×¤ 41833-08-23\n",
      "Processing ×ª\"×¤ 19985-04-14\n",
      "Processing ×ª\"×¤ 29504-09-13\n",
      "Processing ×ª\"×¤ 1276-05-15\n",
      "Processing ×ª\"×¤ 13336-05-15\n",
      "Processing ×ª\"×¤ 64418-09-16\n",
      "Processing ×ª\"×¤ 12320-10-17\n",
      "Processing ×ª\"×¤ 59578-12-21\n",
      "Processing ×ª\"×¤ 21142-01-22\n",
      "Processing ×ª\"×¤ 33898-01-22\n",
      "Processing ×ª\"×¤ 40650-03-22\n",
      "Processing ×ª\"×¤ 62924-03-22\n",
      "Processing ×ª\"×¤ 66877-03-22\n",
      "Processing ×ª\"×¤ 66922-03-22\n",
      "Processing ×ª\"×¤ 36033-05-22\n",
      "Processing ×ª\"×¤ 39259-05-22\n",
      "Processing ×ª\"×¤ 1062-08-22\n",
      "Processing ×ª\"×¤ 21724-08-22\n",
      "Processing ×ª\"×¤ 6749-09-22\n",
      "Processing ×ª\"×¤ 37606-11-22\n",
      "Processing ×ª\"×¤ 64232-11-22\n",
      "Processing ×ª\"×¤ 20664-12-22\n",
      "Processing ×ª\"×¤ 18076-01-23\n",
      "Processing ×ª\"×¤ 30353-01-23\n",
      "Processing ×ª\"×¤ 21629-02-23\n",
      "Processing ×ª\"×¤ 41988-02-23\n",
      "Processing ×ª\"×¤ 4621-02-23\n",
      "Processing ×ª\"×¤ 57179-02-23\n",
      "Processing ×ª\"×¤ 67009-02-23\n",
      "Processing ×ª\"×¤ 20257-03-23\n",
      "Processing ×ª\"×¤ 7086-04-23\n",
      "Processing ×ª\"×¤ 9457-04-23\n",
      "Processing ×ª\"×¤ 72468-05-23\n",
      "Processing ×ª\"×¤ 61816-07-23\n",
      "Processing ×ª\"×¤ 28122-06-23\n",
      "Processing ×ª\"×¤ 30009-08-23\n",
      "Processing ×ª\"×¤ 35299-09-23\n",
      "Processing ×ª\"×¤ 13373-10-23\n",
      "Processing ×ª\"×¤ 49510-11-23\n",
      "Processing ×ª\"×¤ 8306-11-23\n",
      "Processing ×ª\"×¤ 13704-12-23\n",
      "Processing ×ª\"×¤ 26004-12-23\n",
      "Processing ×ª\"×¤ 16800-01-24\n",
      "Processing ×ª\"×¤ 26596-01-24\n",
      "Processing ×ª\"×¤ 3818-01-24\n",
      "Processing ×ª\"×¤ 9804-01-24\n",
      "Processing ×ª\"×¤ 28565-05-24\n",
      "Processing ×ª\"×¤ 46424-05-24\n",
      "Processing ×ª.×¤ 1231âˆ•02\n",
      "Processing ×ª\"×¤ 30506-01-22\n",
      "1. ×”×¢×‘×™×¨×•×ª ×©×¢×‘×¨ ×”× ××©× ×—××•×¨×•×ª.\n",
      "2. ×¢×‘×™×¨×•×ª ×‘×ª×—×•× ×”×¡××™× ×”××¡×•×›× ×™× × ×•×©××•×ª ×‘×—×•×‘×Ÿ ×¤×•×˜× ×¦×™××œ ×œ×¤×’×™×¢×” ××©××¢×•×ª×™×ª ×‘×—×‘×¨×”.\n",
      "3. ××“×•×‘×¨ ×‘× ×’×¢ ×”××ª×¤×©×˜ ×‘××”×™×¨×•×ª ×•××©×—×™×ª ×—×™×™×”× ×©×œ ×× ×©×™×, ×‘×™×Ÿ ×× ×–×” ×¦×™×‘×•×¨ ×”××©×ª××©×™× ×‘×¡××™× ×•×‘×™×Ÿ ×× ×¦×™×‘×•×¨ ×”× ×¤×’×¢×™× ××¢×‘×™×¨×•×ª ×”××‘×•×¦×¢×•×ª ×¢\"×™ ××œ×• ×”××©×ª××©×™× ×‘×¡××™×.\n",
      "4. ×”××©×ª××©×™× ×‘×¡××™× ×”×•×¤×›×™× â€“ ×××–×¨×—×™× ×¢××œ× ×™×™×, ×”×× ×”×œ×™× ×—×™×™× ××¡×•×“×¨×™×, ×œ××™ ×©×¢×•××“×™× ×‘×©×•×œ×™ ×”×—×‘×¨×”, ××™× × ×ª×•×¨××™× ×œ××—×¨×™× ×•×œ× ×–×• ××œ×, ×©×”×˜×™×¤×•×œ ×‘×”× ×•×¤×¨× ×¡×ª× ××•×˜×œ×™× ×¢×œ ××—×¨×™×.\n",
      "5. ×‘× ×•×¡×£, ×›×ª×•×¦××” ××¢×‘×™×¨×•×ª ××œ×”, ××•×¢×‘×¨×™× ××™×“ ×œ×™×“ ×›×¡×¤×™× ×‘×œ×ª×™ ××“×•×•×—×™× ×‘×”×™×§×£ ×¢×¦×•×, ×—×œ×§× ××•×¦××™× ×“×¨×›× ×œ××™××•×Ÿ ×¤×¢×™×œ×•×ª ×¢×‘×¨×™×™× ×™×ª ×‘×ª×—×•××™× ×©×•× ×™×.\n",
      "6. ×›×œ ×–××ª, ×‘×©×œ ×¢×‘×¨×™×™× ×™× ×”××—×–×™×§×™× ×•××¤×™×¦×™× ×¡××™× ××¡×•×›× ×™×, ×‘×“×•××” ×œ× ××©× ×“× ×Ÿ.\n",
      "7. ×¢×œ ××£ ×¨×•×—×•×ª ×©×•× ×•×ª ×”× ×•×©×‘×•×ª ×‘×ª×—×•× ×–×”, ×’× ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡, ×”×•× ×—×•××¨ ×××›×¨, ×©×©×™××•×© ××ª××©×š ×‘×• ×¢×œ×•×œ ×œ×’×¨×•× × ×–×§×™× ×œ× ×§×œ×™× ×‘×ª×—×•× ×”×ª×•×‘× ×”, ×ª×¤×™×¡×ª ×”××¦×™××•×ª ×•×‘×™×™×—×•×“ ×‘×ª×—×•× ×”×ª×¤×§×•×“×™ ×•×”×ª×¢×¡×•×§×ª×™, ×•××›××Ÿ ×’× â€“ × ×–×§×™× ×—×‘×¨×ª×™×™×.\n",
      "8. ×¢×œ ×—×•××¨×ª×Ÿ ×©×œ ×¢×‘×™×¨×•×ª ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ, ×¨××• ×¢\"×¤ 2596/18 ×–× ×–×•×¨×™ × ' ××“×™× ×ª ×™×©×¨××œ (×¤×•×¨×¡× ×‘×××’×¨×™×):\n",
      "9. ×¦×¨×›× ×™× ×•××©×ª××©×™× ×•×× ×©×™× × ×•×¨××˜×™×‘×™×™×, ×©×‘×¢×‘×¨ ×œ× ×”×™×• × ×›×•× ×™× ×œ×™×˜×•×œ ×¢×œ ×¢×¦×× ×¡×™×›×•×Ÿ ×œ×”×¡×ª×‘×š ×‘×¢×•×œ× ×”×¤×œ×™×œ×™, × ×›×•× ×™× ×›×™×•× ×œ×™×œ×š ×¦×¢×“ × ×•×¡×£ ×•×œ×”×¤×•×š ×œ××’×“×œ×™× ×•×œ×¡×•×—×¨×™× ×‘×¡×. ×–××ª, ××ª×•×š ×ª×¤×™×¡×” ×©×’×•×™×” ×›×™ ××“×•×‘×¨ ×‘\"×¡××™× ×§×œ×™×\", ×•×‘×”×™× ×ª×Ÿ ×”×˜×›× ×•×œ×•×’×™×” ×”×××¤×©×¨×ª ××›×™×¨×” ×•×”×¤×¦×” ×§×œ×” ×•\"×¡×˜×¨×™×œ×™×ª\" ×©×œ ×¡××™×. ×‘×¨×, ×¡×—×¨ ×‘×¡××™× ×”×•× ×¡×—×¨ ×‘×¡××™×. ×™×“×¢ ×›×œ ××™ ×©××”×¨×”×¨ ×‘×“×¨×›×™× ×œ×¢×©×™×™×ª ×›×¡×£ ×§×œ, ×›×™ ××“×™× ×™×•×ª ×”×¢× ×™×©×” ×œ× ×”×©×ª× ×ª×” ×•×‘×™×ª ×”××©×¤×˜ ×¨×•××” ×‘×—×•××¨×” ×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×•×”×¤×¦×” ×©×œ ×¡××™× ××¡×•×›× ×™×, ×’× ×¡××™× \"×§×œ×™×\", ×ª×•×š ×”×˜×œ×ª ×¢× ×™×©×” ××©××¢×•×ª×™×ª ×•××¨×ª×™×¢×”. ×¦×¨×›× ×™× ×•××©×ª××©×™× â€“ ×¨××• ×”×•×–×”×¨×ª× (×”×”×“×’×©×” ××™× ×” ×‘××§×•×¨).\n",
      "10. ×¢×•×“ ×¨××• ×¨×¢\"×¤ 8695/19 ×¤×¡×• × ' ××“×™× ×ª ×™×©×¨××œ (×¤×•×¨×¡× ×‘×××’×¨×™×):\n",
      "11. ×‘×™×ª ××©×¤×˜ ×–×” ×©×‘ ×•×”×“×’×™×© ×›×™ ×¢×œ ××“×™× ×™×•×ª ×”×¢× ×™×©×” ×‘×’×™×Ÿ ×¡×—×¨ ×•×”×¤×¦×” ×©×œ ×¡××™× ××¡×•×›× ×™×, ×•×‘×›×œ×œ×Ÿ ×¡××™× ×”× ×—×©×‘×™× ×›\"×¡××™× ×§×œ×™×\", ×œ×”×™×•×ª ××¨×ª×™×¢×” ×•××©××¢×•×ª×™×ª, ×•×›×™ ×™×© ×œ×¨××•×ª ×‘×—×•××¨×” × ×™×¡×™×•× ×•×ª ×œ×¢×©×™×™×ª ×¨×•×•×— ×›×¡×¤×™ ×‘×××¦×¢×•×ª ×¡×—×¨ ×‘×¡××™× ××œ×”.\n",
      "12. (×”×”×“×’×©×•×ª - ××™× ×Ÿ ×‘××§×•×¨).\n",
      "13. ×‘× ×•×’×¢ ×œ×¢×‘×™×¨×” ×©×œ ×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª, ×¨××• ×“×‘×¨×™ ×‘×™×ª ×”××©×¤×˜ ×”×¢×œ×™×•×Ÿ ×‘××¡×’×¨×ª ×¢\"×¤ 1345/08 ××™×¡×˜×—×¨×•×‘ × ' ××“×™× ×ª ×™×©×¨××œ (×¤×•×¨×¡× ×‘×××’×¨×™×):\n",
      "14. ××™×Ÿ ×× ×•×¡ ××”×›×‘×“×ª ×”×™×“ ×¢×œ ×”××—×–×™×§×™× ×¡××™× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª, ×©×›×œ ×‘×¨ ×“×¢×ª ××‘×™×Ÿ ×›×™ × ×•×¢×“×• ×œ×¦×¨×™×›×ª ×”×–×•×œ×ª, ×§×¨×™, ×œ×”×•×¡×¤×ª ×©××Ÿ ×¢×œ ××“×•×¨×ª ×”×¡××™× ××©×¨ ×œ×”×‘×•×ª×™×” ××•×¤×¤×•×ª ×¨×‘×™× ×•×˜×•×‘×™×, ××• ×¨×‘×™× ×©×”×™×• ×˜×•×‘×™×. ×¢×‘×™×¨×” ×–×• ×”×™× ×ª××•××ª×” ×”×¡×˜×˜×•×˜×•×¨×™×ª ×©×œ ×¢×‘×™×¨×ª ×”×¡×—×¨ ×‘×¡××™×, ××œ× ×©×œ× × ×™×ª×Ÿ ×œ×”×•×›×™×— ×œ×’×‘×™×” ××ª ×”×¡×—×¨ ×¢×¦××•, ×•× ×§×‘×¢ ×œ×©×ª×™×”×Ÿ ×¢×•× ×©×” ×–×”×”, ×¢×•× ×© ××™×¨×‘×™ ×©×œ ×¢×©×¨×™× ×©× ×•×ª ×××¡×¨ ×•×§× ×¡ ×¤×™ ×¢×©×¨×™× ×•×—××™×©×” ××–×” ×”×§×‘×•×¢ ×‘×¡×¢×™×£ 61(×)(4) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×”×¢×•××“ ×›×™×•× ×¢×œ 202,000 â‚ª...\n",
      "15. (×”×”×“×’×©×•×ª - ××™× ×Ÿ ×‘××§×•×¨).\n",
      "16. ×¢×œ ×‘×™×ª ×”××©×¤×˜ ×œ×”×¨×ª×™×¢ ×”×™×—×™×“ ×•×”×¨×‘×™× ××¤× ×™ ×¢×‘×™×¨×•×ª ×”×¡××™×, ×•×œ×”×™×œ×—× ×‘×ª×•×¤×¢×” ×”×§×©×” ×©×œ × ×’×¢ ×”×¡××™×, ×”×¦×•×‘×¨×ª ×ª××•×¦×” ×‘×©× ×™× ×”××—×¨×•× ×•×ª.\n",
      "17. ×¢×¤\"×’ 30646-10-23 ×—××××“×” × ' ××“×™× ×ª ×™×©×¨××œ â€“ ×”××¢×¨×¢×¨ ×”×•×¨×©×¢, ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘×©×ª×™ ×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ××‘×™×¡ ×•×¢×‘×™×¨×” ××—×ª ×©×œ ×”×—×–×§×ª ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ××‘×™×¡ ×©×œ× ×œ×¦×¨×™×›×ª×• ×”×¢×¦××™×ª. ×‘×”×ª×× ×œ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×”××ª×•×§×Ÿ, ×¡×—×¨ ×”× ××©× ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ××‘×™×¡ ×‘××©×§×œ ×©×œ 13.02 ×’×¨× ×ª××•×¨×ª 400 â‚ª; ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ××‘×™×¡ ×‘××©×§×œ 83.97 ×’×¨× ×ª××•×¨×ª 2,400 â‚ª ×•×›×Ÿ ×”×—×–×™×§ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ××‘×™×¡ ×‘××©×§×œ 42.49 ×’×¨× ×©×œ× ×œ×¦×¨×™×›×ª×• ×”×¢×¦××™×ª. ××•×ª×‘ ×–×” ×§×‘×¢ ××ª×—× ×¢× ×™×©×” ×”× ×¢ ×‘×™×Ÿ 16 ×¢×“ 30 ×—×•×“×©×™ ×××¡×¨ ×‘×¤×•×¢×œ. ×œ××•×¨ × ×¡×™×‘×•×ª ××™×©×™×•×ª ×—×¨×™×’×•×ª, ××¦× ×‘×™×ª ×”××©×¤×˜ ×œ×—×¨×•×’ ×××ª×—× ×”×¢× ×™×©×” ×•×’×–×¨ ×¢×œ ×”× ××©× 12 ×—×•×“×©×™ ×××¡×¨ ×‘×¤×•×¢×œ; ×××¡×¨×™× ×¢×œ ×ª× ××™; ×§× ×¡; ×¤×¡×™×œ×” ×‘×¤×•×¢×œ ×•×¢×œ ×ª× ××™ ×©×œ ×¨×™×©×™×•×Ÿ ×”× ×”×™×’×”. ×¢×¨×¢×•×¨×• ×¢×œ ×—×•××¨×ª ×”×¢×•× ×© ×”×ª×§×‘×œ×” ×‘× ×•×’×¢ ×œ×¨×›×™×‘ ×¨×™×©×™×•×Ÿ ×”× ×”×™×’×” ×‘×œ×‘×“, ×•×–××ª ×‘×©×™× ×œ×‘ ×œ× ×¡×™×‘×•×ª ××™×©×™×•×ª.\n",
      "18. ×¢×¤\"×’ 4842-05-23 ×‘×Ÿ × ×— × ' ××“×™× ×ª ×™×©×¨××œ â€“ ×”××¢×¨×¢×¨ ×”×•×¨×©×¢, ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘×¢×‘×™×¨×•×ª ×©×œ ×¡×™×•×¢ ×œ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×•×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª. ×‘×”×ª×× ×œ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×”××ª×•×§×Ÿ, ×¡×™×™×¢ ×”× ××©× ×œ××—×¨ ×œ×¡×—×•×¨ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 9.94 ×’×¨× ×ª××•×¨×ª 500 â‚ª, ×•×‘××•×ª×• ××¢××“, ×”×—×–×™×§ ×‘×¨×›×‘×• ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡, ×‘××©×§×œ 530 ×’×¨×, ×©×œ× ×œ×¦×¨×™×›×ª×• ×”×¢×¦××™×ª. ××•×ª×‘ ×–×” ×§×‘×¢ ××ª×—× ×¢× ×™×©×” ×”× ×¢ ×‘×™×Ÿ 12 ×•×¢×“ 24 ×—×•×“×©×™ ×××¡×¨ ×‘×¤×•×¢×œ, ×•×’×–×¨ ×¢×œ ×”× ××©× 15 ×—×•×“×©×™ ×××¡×¨, ×œ×¦×“ ×¢× ×™×©×” × ×œ×•×•×™×ª ×•×—×™×œ×•×˜ ×”×¨×›×‘ ×‘×• ×”×¡×ª×™×™×¢. ×‘×©×œ × ×¡×™×‘×•×ª ××™×•×—×“×•×ª ×©×œ × ××©× ×–×”, ×”×¡×›×™××” ×”××©×™×‘×” ×œ×‘×§×©×ª ×”×”×’× ×” ×œ×”×¤×—×™×ª ××ª ×¢×•× ×© ×”×××¡×¨ ×œ-11 ×—×•×“×©×™ ×××¡×¨ ×•×‘×™×˜×•×œ ×¤×¡×™×œ×ª ×¨×™×©×™×•×Ÿ ×”× ×”×™×’×” ×‘×¤×•×¢×œ, ×›××©×¨ ×œ× ×”×™×” ×›×œ ×©×™× ×•×™ ×‘×™×ª×¨ ×¨×›×™×‘×™ ×’×–×¨ ×”×“×™×Ÿ. ×‘×™×ª ×”××©×¤×˜ ×”××—×•×–×™ (×›×‘' ×”×©×•×¤×˜ ×' ×‘×™×ª×Ÿ) ×¦×™×™×Ÿ, ×›×™ ×”×¢×•× ×© ×©×”×•×˜×œ ×¢×œ ×”× ××©× ×‘×‘×™×ª ×”××©×¤×˜ ×§×× ×”×™× ×• ××ª××™× ×œ××¢×©×” ×”×¢×‘×™×¨×”.\n",
      "19. ×ª\"×¤ 64772-05-22 ××“×™× ×ª ×™×©×¨××œ × ' ××‘×• ×‘×œ××œ â€“ ×”× ××©× ×”×•×¨×©×¢, ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘×¢×‘×™×¨×” ×©×œ × ×™×¡×™×•×Ÿ ×œ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 850 ×’×¨×, ×ª××•×¨×ª 7,500 â‚ª. ××•×ª×‘ ×–×” ××™××¥ ××ª ××ª×—× ×”×¢× ×™×©×” ××œ×™×• ×¢×ª×¨×” ×”×ª×‘×™×¢×” ×•×”×¢××™×“×• ×›×š ×©×™× ×•×¢ ×‘×™×Ÿ 10 ×•×¢×“ 20 ×—×•×“×©×™ ×××¡×¨, ×ª×•×š ×©×¦×™×™×Ÿ ×‘×’×–×¨ ×”×“×™×Ÿ, ×›×™ ×¨××•×™ ×”×™×” ×œ×¢×ª×•×¨ ×œ××ª×—× ×¢× ×™×©×” ×’×‘×•×” ×™×•×ª×¨.\n",
      "20. ×ª\"×¤ 5072-11-21 ××“×™× ×ª ×™×©×¨××œ × ' ××‘×Ÿ ×¢×ª××™ â€“ ×”× ××©× ×”×•×¨×©×¢, ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘×¢×‘×™×¨×•×ª ×©×œ ×§×©×™×¨×ª ×§×©×¨ ×œ×¢×©×•×ª ×¤×©×¢, ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×•×”×—×–×§×ª ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª. ×‘×¢×¡×§×” ×”×¨××©×•× ×”, ××›×¨ ×”× ××©× ×œ×¡×•×›×Ÿ ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 24 ×’×¨× × ×˜×•, ×ª××•×¨×ª 400 â‚ª; ×‘×¢×¡×§×” ×”×©× ×™×”, ××›×¨ ×”× ××©× ×œ×¡×•×›×Ÿ ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 14.85 ×’×¨× × ×˜×• ×ª××•×¨×ª 300 â‚ª; ×‘×¢×¡×§×” ×”×©×œ×™×©×™×ª, ××›×¨ ×”× ××©× ×œ××—×¨ ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 7.08 ×’×¨× ×ª××•×¨×ª 300 â‚ª ×•×‘××•×ª×• ××¢××“, ×”×—×–×™×§ ×‘×¨×›×‘×• ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 77.06 ×’×¨×, ×©×œ× ×œ×¦×¨×™×›×ª×• ×”×¢×¦××™×ª. ××•×ª×‘ ×–×” ×§×‘×¢ ××ª×—× ×¢× ×™×©×” ×”× ×¢ ×‘×™×Ÿ 10 ×•×¢×“ 20 ×—×•×“×©×™ ×××¡×¨.\n",
      "21. ×ª\"×¤ 54272-11-23 ××“×™× ×ª ×™×©×¨××œ × ' ×§×•×¨×–'×‘×™×¥' â€“ ×”× ××©× ×”×•×¨×©×¢, ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘××¡×’×¨×ª ×”×¡×“×¨, ×‘×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡××™×; ×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª; ×”×—×–×§×ª ×¡× ×œ×¦×¨×™×›×” ×¢×¦××™×ª. ×‘×¢×¡×§×” ×”×¨××©×•× ×”, ×¡×—×¨ ×”× ××©× ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 20 ×’×¨× ×ª××•×¨×ª 1,000 â‚ª ×œ×¡×•×›×Ÿ ××©×˜×¨×ª×™. ×‘××•×ª×• ××¢××“, ×”×—×–×™×§, ×‘× ×•×¡×£, ×‘-10 ×’×¨× ×§× ×‘×™×¡ ×•-5 ×™×—×™×“×•×ª ×¡× ××¡×•×›×Ÿ ××¡×•×’ ADB-BUTINACA; ×‘×¢×¡×§×” ×”×©× ×™×”, ×¡×—×¨ ×”× ××©× ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 10 ×’×¨× ×ª××•×¨×ª 400 â‚ª; ×‘×¢×¡×§×” ×”×©×œ×™×©×™×ª, ×¡×—×¨ ×”× ××©× ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 20 ×’×¨× ×ª××•×¨×ª 800 â‚ª; ×”× ××©× ×”×—×–×™×§ ×‘×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§× ×‘×™×¡ ×‘××©×§×œ 10 ×’×¨×. ××•×ª×‘ ×–×” ×§×‘×¢ ××ª×—× ×¢× ×™×©×” ×”× ×¢ ×‘×™×Ÿ 18 ×•×¢×“ 36 ×—×•×“×©×™ ×××¡×¨.\n",
      "22. ×‘××§×¨×” ×“× ×Ÿ, ×§×™×™××•×ª ××¡×¤×¨ × ×¡×™×‘×•×ª ×œ×—×•××¨×”, ×©×™×© ×‘×”×Ÿ ×›×“×™ ×œ×”×¢×™×“ ×¢×œ ×¤×¢×™×œ×•×ª ×¢×§×‘×™×ª ×•×××•×¨×’× ×ª ×‘×ª×—×•× ×”×¡×—×¨ ×‘×¡××™× ××¡×•×›× ×™×, ×•×›×Ÿ ×¢×œ ×”×™×•×ª ×”× ××©× ×‘×“×¨×’ ××©××¢×•×ª×™ ×‘×”×™×¨×¨×›×™×” ×”×¢×‘×¨×™×™× ×™×ª: ×”× ××©× ×§×™×‘×œ ××ª ×¤×¨×˜×™ ×”\"×œ×§×•×—×•×ª\" ×××“× ××—×¨, ××©×¨ ××œ×™×• ×”×™×• ×¤×•× ×™× ×× ×©×™× ×‘×××¦×¢×•×ª ×”×˜×œ×’×¨× ×œ×¦×•×¨×š ×¨×›×™×©×” ×©×œ ×¡××™× ××¡×•×›× ×™×; ×”× ××©× ×™×¦×¨ ×§×©×¨ ×¢× ×”×¤×•× ×™×;  ×¢×¨×š ×¢×¡×§××•×ª ×¢× ×œ×§×•×—×•×ª ×©× ×—×–×• ×œ×”×™×•×ª ××–×“×× ×™×; ×”× ××©× ×¡×™×¤×§ ×”×¡××™× ××™×“×™×ª ×œ××—×¨ ×”×¡×™×›×•×, ×•××›××Ÿ ×©×”×™×• × ×’×™×©×™× ×‘×¢×‘×•×¨×• ×¡××™× ×‘××•×¤×Ÿ ×–××™×Ÿ; ×”×¢×¡×§××•×ª ×œ× × ×¢×¨×›×• ×‘×›××•×™×•×ª ×§×˜× ×•×ª, ××œ× ×‘×›××•×™×•×ª ×‘×™× ×•× ×™×•×ª; ×‘×—×œ×§ ××”××§×¨×™×, ×”×¡×™×¢ ×”× ××©× ××ª ×”××—×¨ ×œ××§×•× ×”×¢×¡×§××•×ª; ×‘× ×•×¡×£, × ×”×’ ×”× ××©× ×‘×¨×›×‘×•, ×›××©×¨ ×”×•× × ×•×”×’ ×ª×—×ª ×”×©×¤×¢×ª ×¡××™× ××¡×•×›× ×™×, ×•×”×—×–×™×§ ×‘×¡××™× ××¡×•×›× ×™× ×‘×¨×›×‘×•.\n",
      "23. ×œ××•×¨ ×¨×™×‘×•×™ ×”×¢×¡×§××•×ª; ×˜×™×‘ ×”×¡×; ×›××•×ª ×”×¡× ×‘×” ×¡×—×¨ ×”× ××©×; ×¡×›×•××™ ×”×›×¡×£ ×©×§×™×‘×œ ×œ×™×“×™×• ×‘××¡×’×¨×ª ×”×¢×¡×§××•×ª; ×›××•×ª ×”×¡× ×©×”×—×–×™×§ ×”× ××©× ×©×œ× ×œ×¦×¨×™×›×ª×• ×”×¢×¦××™×ª ×‘×¨×›×‘×•, ×ª×•×š ×©×”×•× × ×•×”×’ ×œ×œ× ×¨×™×©×™×•×Ÿ × ×”×™×’×” ×‘×ª×•×§×£ ×•×©×¢×” ×©×”×•× ×ª×—×ª ×”×©×¤×¢×ª ×¡××™× ××¡×•×›× ×™× â€“ ××•×¦× ×‘×™×ª ×”××©×¤×˜ ×œ×××¥ ××ª ××ª×—× ×”×¢× ×™×©×” ××œ×™×• ×¢×ª×¨×” ×”×ª×‘×™×¢×” ×•×œ×”×¢××™×“×• ×›×š ×©×™× ×•×¢ ×‘×™×Ÿ 30 ×•×¢×“ 50 ×—×•×“×©×™ ×××¡×¨ ×‘×¤×•×¢×œ.\n",
      "24. ×œ×—×•×‘×ª ×”× ××©×, ×”×¨×©×¢×•×ª ×§×•×“××•×ª ×‘×¢×‘×™×¨×•×ª ×¡××™×; ×¨×›×•×©; ×•××œ×™××•×ª.\n",
      "25. ×”× ××©× ×¢×‘×¨ ××ª ×”×¢×‘×™×¨×•×ª ×“× ×Ÿ, ×©×¢×” ×©×œ×—×•×‘×ª×• ×©× ×™ ×××¡×¨×™× ××•×ª× ×™× ×‘×¨×™ ×”×¤×¢×œ×”, ××©×¨ ×œ× ×”×™×•×• ×’×•×¨× ××¨×ª×™×¢ ×¢×‘×•×¨×•.\n",
      "26. ×¢×•×“ ×œ×—×•×‘×ª×•, ×”×¨×©×¢×•×ª ×‘×¢×‘×™×¨×•×ª ×ª×¢×‘×•×¨×”.\n",
      "27. ××©×•×¨×ª ×”×“×™×Ÿ, ×¨××•×™ ×”×™×” ×œ×’×–×•×¨ ×¢×•× ×©×• ×©×œ ×”× ××©× ×‘×—×œ×§×• ×”×‘×™× ×•× ×™-×’×‘×•×” ×©×œ ××ª×—× ×”×¢× ×™×©×”.\n",
      "28. ××œ×, ×©×”× ××©× ×¤× ×” ×œ×¢×–×¨×” ×¢×•×“ ×‘×©×œ×‘ ××¢×¦×¨×•, ×•×”×©×ª×œ×‘ ×‘×§×”×™×œ×” ×˜×™×¤×•×œ×™×ª, ×©× ×¢×‘×¨ ×˜×™×¤×•×œ ××™× ×˜× ×¡×™×‘×™, ×•×¢×‘×¨ ×‘×”×¦×œ×—×” ××ª ×›×œ ×©×œ×‘×™ ×”×§×”×™×œ×” ×”×˜×™×¤×•×œ×™×ª. ×œ××—×¨ ××›×Ÿ, ×”×©×ª×œ×‘ ×”× ××©× ×‘×”×•×¡×˜×œ, ×•×¡×™×™× ×©×œ×‘ ×˜×™×¤×•×œ×™ ×–×” ×‘×”×¦×œ×—×” ×’× ×›×Ÿ.\n",
      "29. ×¢× ×©×—×¨×•×¨×• ××”×”×•×¡×˜×œ, ×”×©×ª×œ×‘ ×”× ××©× ×‘××¢×’×œ ×”×ª×¢×¡×•×§×”, ×ª×•×š ×©×”×—×œ ×œ×”×ª×¤×¨× ×¡ ××¢×‘×•×“×” ×™×¦×™×‘×”. ×‘××§×‘×™×œ, ×©×•×œ×‘ ×”× ××©× ×‘×”×œ×™×š ×˜×™×¤×•×œ×™ × ×•×¡×£ ×‘××¡×’×¨×ª \"×‘×™×ª ×—×•×¡×Ÿ\", ×©× ×××©×™×š ×‘×©×™×—×•×ª ×¤×¨×˜× ×™×•×ª.\n",
      "30. ×”× ××©× ×’×“×œ ×‘× ×¡×™×‘×•×ª ×—×™×™× ×§×©×•×ª, ×•×‘×¡×•×¤×• ×©×œ ×“×‘×¨, × ×“×—×§ ×œ×©×•×œ×™ ×”×—×‘×¨×”, ×”×ª×¢×¨×” ×¢× ×’×•×¨××™× ×¢×‘×¨×™×™× ×™×™× ×•×”×ª××›×¨ ×œ×¡××™×. ×—×¨×£ ×”×××•×¨, ×”×¦×œ×™×— ×”× ××©× ×œ×’×™×™×¡ ×›×•×—×•×ª×™×•, ×”×ª××™×“ ×‘×”×œ×™×š ×”×˜×™×¤×•×œ×™, × ×’××œ ××¡××™× ×•×‘×“×™×§×•×ª ×©×¢×‘×¨ ×œ××™×ª×•×¨ ×©×¨×™×“×™ ×¡× â€“ ×™×¦××• ×©×œ×™×œ×™×•×ª.\n",
      "31. ××ª×¡×§×™×¨×™ ×©×™×¨×•×ª ×”××‘×—×Ÿ ×¢×•×œ×”, ×›×™ ×”× ××©× ×¢×‘×¨ ×›×‘×¨×ª ×“×¨×š ××©××¢×•×ª×™×ª, ×¤× ×™×• ×œ×©×™×§×•×, ×•×”×•× ××‘×˜× ×©××™×¤×•×ª ×œ×©××¨ ××ª ××•×¨×— ×”×—×™×™× ×”× ×•×¨××˜×™×‘×™ ×•×©×•××¨ ×”×—×•×§ ×©×”×—×œ ×‘×•.\n",
      "32. ×‘×¡×•×¤×• ×©×œ ×™×•× â€“ ×—×–×¨ ×‘×• ×©×™×¨×•×ª ×”××‘×—×Ÿ ××”××œ×¦×ª×• ×¢×œ ×¢× ×™×©×” ×‘×“××•×ª ×××¡×¨ ×‘×¢×‘×•×“×•×ª ×©×™×¨×•×ª ×•×”××œ×™×¥ ×¢×œ ×¢× ×™×©×” ×©×™×§×•××™×ª ×‘×“××•×ª ×¦×• ×©×œ\"×¦ ×œ×¦×“ ×¦×• ××‘×—×Ÿ, ×‘××¡×’×¨×ª×• ×™××©×™×š ×”× ××©× ×‘×”×œ×™×š ×”×˜×™×¤×•×œ×™.\n",
      "33. ×‘×™×ª ×”××©×¤×˜ ×”×ª×¨×©×, ×›×™ ×”× ××©× × ×•×˜×œ ××—×¨×™×•×ª ×¢×œ ××¢×©×™×•, ××‘×™×¢ ×—×¨×˜×” ×¢××•×§×” ×¢×œ×™×”×, ×•×× ×¡×” ×‘×›×œ ×××•×“×• ×œ× ×”×œ ×—×™×™× ××–×¨×—×™×™× ×¢××œ× ×™×™× ×ª×•×š ×¤×¨× ×¡×ª ×‘× ×™ ××©×¤×—×ª×•.\n",
      "34. ×‘××›×œ×•×œ ×”× ×¡×™×‘×•×ª, ×œ××•×¨ ×›×‘×¨×ª ×”×“×¨×š ×”×˜×™×¤×•×œ×™×ª ×”××©××¢×•×ª×™×ª ×©×¢×‘×¨ ×”× ××©×, ××©×¨ ×™×© ×‘×” ×›×“×™ ×œ×”×¦×‘×™×¢ ×¢×œ ×–× ×™×—×ª ×“×¨×›×• ×”×¢×‘×¨×™×™× ×™×ª â€“ ××‘×œ×™ ×©×™×”×™×” ×‘×›×š ×‘×‘×—×™× ×ª ×ª×§×“×™× ×œ××§×¨×™× ××—×¨×™×, × ×•×˜×” ×”×›×£ ×œ×—×¨×™×’×” ×××ª×—× ×”×¢× ×™×©×” ××˜×¢××™ ×©×™×§×•× ×”× ××©×, ×•××ª×•×£ ×¡××›×•×ª×• ×©×œ ×‘×™×ª ×”××©×¤×˜ ×‘×”×ª×× ×œ×”×•×¨××•×ª ×¡×¢×™×£ 40×“' ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×– â€“ 1977, ××•×¦× ×‘×™×ª ×”××©×¤×˜ ×œ×××¥ ×”××œ×¦×•×ª ×©×™×¨×•×ª ×”××‘×—×Ÿ ×‘×¢× ×™× ×•, ×•×œ×”×¡×ª×¤×§ ×‘×¢×•× ×© ×—×™× ×•×›×™ ×‘××¡×’×¨×ª ×¦×• ×©×œ\"×¦. ×–××ª, ×›××•×‘×Ÿ, ×‘×ª×•×¡×¤×ª ×××¡×¨ ××•×ª× ×” ××¨×ª×™×¢, ×œ×‘×œ ×™×”×™×Ÿ ×”× ××©× ×œ×¢×‘×•×¨ ×©×•×‘ ×¢×‘×™×¨×•×ª ×“×•××•×ª.\n",
      "35. ××©×¨ ×œ×××¡×¨×™× ×”××•×ª× ×™× ××’×–×¨×™ ×”×“×™×Ÿ ×ª/2 ×•-×ª/3, ××ª×•×§×£ ×¡××›×•×ª×• ×©×œ ×‘×™×ª ×”××©×¤×˜ ×‘×”×ª×× ×œ×”×•×¨××•×ª ×¡×¢×™×£ 85 ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×– â€“ 1977, ×œ××—×¨ ×©×”× ××©× ×¢×‘×¨, ×›×××•×¨, ×”×œ×™×š ×˜×™×¤×•×œ×™ ××©××¢×•×ª×™ ×‘×ª×—×•× ×”×¡××™× ×”××¡×•×›× ×™×, ××•×¦× ×‘×™×ª ×”××©×¤×˜ ×œ×”××¨×™×š ××ª ×©× ×™ ×”×××¡×¨×™× ×œ×ª×§×•×¤×” × ×•×¡×¤×ª, ××©×¨ ×™×”×™×” ×‘×” ×›×“×™ ×œ×”×•×•×ª ×’×•×¨× ××¨×ª×™×¢ ×œ× ××©×.\n",
      "36. ×‘×©×œ ×”×× ×™×¢ ×”×›×œ×›×œ×™ ×”×¢×•××“ ×‘×¨×§×¢ ×œ×¢×‘×™×¨×•×ª ×¡××™×, ××•×¦× ×‘×™×ª ×”××©×¤×˜ ×œ×”×˜×™×œ ×¢×™×¦×•× ×›×¡×¤×™ ××¡×•×’ ×§× ×¡, ××•×œ×, ×œ××•×¨ ××¦×‘×• ×”×›×œ×›×œ×™ ×©×œ ×”× ××©× ×•×¢×œ ×× ×ª ×©×œ× ×œ×¤×’×•×¢ ×‘×¡×™×›×•×™×™ ×©×™×§×•××•, ×œ× ×™×•×¢××“ ×¢×œ ×”×¦×“ ×”×’×‘×•×”.\n",
      "37. ×¢×•×“ ××•×¦× ×‘×™×ª ×”××©×¤×˜, ×›×™ ×™×© ×œ×”×˜×™×œ ×¤×¡×™×œ×” ×‘×¤×•×¢×œ ×•×¢×œ ×ª× ××™ ×©×œ ×¨×™×©×™×•×Ÿ ×”× ×”×™×’×” ×•×–××ª ××ª×•×§×£ ×¡××›×•×ª×• ×‘×”×ª×× ×œ×”×•×¨××•×ª ×¡×¢×™×£ 37×(×) ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×ª×©×œ\"×’ â€“ 1973 ×•×›×Ÿ ×¡×¢×™×£ 43 ×œ×¤×§×•×“×ª ×”×ª×¢×‘×•×¨×” [× ×•×¡×— ×—×“×©], ×ª×©×›\"× - 1961. ×”× ××©× × ×”×’ ×‘×¨×›×‘×•, ×©×¢×” ×©×ª×•×§×£ ×¨×™×©×™×•×Ÿ ×”× ×”×™×’×” ×©×œ×• ×¤×§×¢ ××¢×œ ×©× ×”; ×‘×–××Ÿ ×©×”×•× ××—×–×™×§ ×‘×¨×›×‘×• ×¡××™× ××¡×•×›× ×™×; ×•×‘×–××Ÿ ×©×”×•× × ×•×”×’ ×ª×—×ª ×”×©×¤×¢×ª ×¡××™× ××¡×•×›× ×™×. ××•×¤×Ÿ × ×”×™×’×ª×• ×©×œ ×”× ××©× ×”×§×™× ×¡×™×›×•×Ÿ ×©×œ ×××© ×œ×©×œ×•×× ×•×œ×—×™×™×”× ×©×œ ×”××©×ª××©×™× ×‘×“×¨×š ×•×—×¨×£ ×”×”×œ×™×š ×”×©×™×§×•××™ ×©×¢×‘×¨, ××—×™×™×‘ ×”×¨×—×§×ª×• ××”×›×‘×™×© ×œ×ª×§×•×¤×” ××©××¢×•×ª×™×ª.\n",
      "38. ×œ××—×¨ ×©×‘×™×ª ×”××©×¤×˜ ×¢×™×™×Ÿ ×‘×˜×™×¢×•× ×™ ×”×¦×“×“×™× ×‘×›×ª×‘; ×©××¢ ×˜×™×¢×•× ×™ ×”×¦×“×“×™× ×¢×œ ×¤×”; ×¢×™×™×Ÿ ×‘×¨××™×•×ª ×œ×¢×•× ×©; ×¢×™×™×Ÿ ×‘×ª×¡×§×™×¨×™ ×©×™×¨×•×ª ×”××‘×—×Ÿ ×œ××‘×•×’×¨×™×; ×¢×™×™×Ÿ ×‘×¤×¡×™×§×”; ×•×œ××—×¨ ×©×©××¢ ×“×‘×¨×• ×”××—×¨×•×Ÿ ×©×œ ×”× ××©×; ×’×•×–×¨ ×¢×œ ×”× ××©× ××ª ×”×¢×•× ×©×™× ×›×“×œ×§××Ÿ:\n",
      "39. ×”××¨×›×ª ×ª×§×•×¤×ª ×”×××¡×¨ ×”××•×ª× ×” ×‘×Ÿ 8 ×—×•×“×©×™× ××’×–×¨ ×”×“×™×Ÿ ×ª/2 ×œ××©×š ×©× ×ª×™×™× × ×•×¡×¤×•×ª ××”×™×•×;\n",
      "40. ×”××¨×›×ª ×ª×§×•×¤×ª ×”×××¡×¨ ×”××•×ª× ×” ×‘×Ÿ 4 ×—×•×“×©×™× ××’×–×¨ ×”×“×™×Ÿ ×ª/3 ×œ××©×š ×©× ×ª×™×™× × ×•×¡×¤×•×ª ××”×™×•×;\n",
      "41. ×§× ×¡ ×‘×¡×š 7,500 â‚ª ××• 60 ×™××™ ×××¡×¨ ×ª××•×¨×ª×•. ×”×§× ×¡ ×™×©×•×œ× ×‘ - 7 ×ª×©×œ×•××™× ×©×•×•×™× ×”×—×œ ××™×•× 15.09.24 ×•×‘×›×œ 15 ×œ×—×•×“×© ×”×¢×•×§×‘. ×œ× ×™×¢××•×“ ×”× ××©× ×‘××—×“ ×”×ª×©×œ×•××™× ×‘××•×¢×“ â€“ ×ª×¢××•×“ ×”×™×ª×¨×” ×œ×¤×™×¨×¢×•×Ÿ ××™×“×™;\n",
      "42. ×”× ××©× ×™×¦×”×™×¨ ×¢×œ ×”×ª×—×™×™×‘×•×ª ×‘×¡×š 5,000 â‚ª ×œ×”×™×× ×¢, ×‘×ª×•×š ×©×œ×•×© ×©× ×™× ××”×™×•×, ××›×œ ×¢×‘×™×¨×” ×”××¤×¢×™×œ×” ××ª ××—×“ ×”×××¡×¨×™× ×”××•×ª× ×™×. ×œ× ×™×¦×”×™×¨ ×”× ××©× ×¢×œ ×”×”×ª×—×™×™×‘×•×ª ×”×™×•× â€“ ×™×™××¡×¨ ×œ××©×š 21 ×™×•×;\n",
      "43. ×”× ××©× ×™×¨×¦×” ×¢×‘×•×“×•×ª ×©×œ\"×¦ ×‘×”×™×§×£ ×©×œ 300 ×©×¢×•×ª, ×‘×”×ª×× ×œ×ª×›× ×™×ª ×”×©×œ\"×¦ ×©×ª×’×•×‘×© ×¢\"×™ ×©×™×¨×•×ª ×”××‘×—×Ÿ ×•×ª×•×’×© ×œ×‘×™×ª ×”××©×¤×˜ ×‘×ª×•×š 30 ×™×•× ××”×™×•×. ×¢×œ ×”× ××©× ×œ×”×ª×™×™×¦×‘, ×‘×ª×•×š 7 ×™××™× ××”×™×•×, ×‘×©×™×¨×•×ª ×”××‘×—×Ÿ, ×œ×§×‘×œ×ª ×”×•×¨××•×ª ×‘× ×•×’×¢ ×œ×¨×™×¦×•×™ ×¢×‘×•×“×•×ª ×”×©×œ\"×¦. ×”× ××©× ××•×–×”×¨, ×›×™ ××™ ×”×ª×™×™×¦×‘×•×ª ×œ×¨×™×¦×•×™ ×¢×‘×•×“×•×ª ×”×©×œ\"×¦, ××• ××™ ×©×™×ª×•×£ ×¤×¢×•×œ×” ×‘× ×•×’×¢ ×œ×¨×™×¦×•×™×Ÿ â€“ ×¢×œ×•×œ ×œ×”×‘×™× ×œ×”×¤×§×¢×ª ×¦×• ×”×©×œ\"×¦ ×•×œ×“×™×•×Ÿ ××—×“×© ×‘×©××œ×ª ×”×¢×•× ×© ×‘×ª×™×§ ×–×”, ×¢×œ ×›×œ ×”××©×ª××¢ ××›×š;\n",
      "44. ×”× ××©× ×™×¢××•×“ ×‘××‘×—×Ÿ ×œ××©×š ×©× ×” ×•××—×¦×” ××”×™×•×. ×‘××¡×’×¨×ª ×¦×• ×”××‘×—×Ÿ, ×™×”×™×” ×¢×œ×™×• ×œ×”×©×ª×ª×£ ×‘×›×œ ×”×œ×™×š ×˜×™×¤×•×œ×™ ××• ××¢×§×‘×™ ×›×¤×™ ×©×™×•××œ×¥ ×¢\"×™ ×©×™×¨×•×ª ×”××‘×—×Ÿ. ×¢×œ ×”× ××©× ×œ×”×ª×™×™×¦×‘, ×‘×ª×•×š 7 ×™××™× ××”×™×•×, ×‘×©×™×¨×•×ª ×”××‘×—×Ÿ ×œ×§×‘×œ×ª ×”×•×¨××•×ª ××ª××™××•×ª. ×”× ××©× ××•×–×”×¨, ×›×™ ××™ ×©×™×ª×•×£ ×¤×¢×•×œ×” ×‘××¡×’×¨×ª ×¦×• ×”××‘×—×Ÿ ×¢×œ×•×œ ×œ×”×‘×™× ×œ×”×¤×§×¢×ª×• ×•×œ×“×™×•×Ÿ ××—×“×© ×‘×©××œ×ª ×”×¢×•× ×© ×‘×ª×™×§ ×–×”, ×¢×œ ×›×œ ×”××©×ª××¢ ××›×š;\n",
      "45. ×¤×¡×™×œ×” ×‘×¤×•×¢×œ, ××§×‘×œ ××• ××”×—×–×™×§ ×¨×©×™×•×Ÿ × ×”×™×’×” ×œ×¨×›×‘ ×× ×•×¢×™, ×œ××©×š ×©× ×ª×™×™× ×”×—×œ ××”×™×•×. ×¢×œ ×”× ××©× ×œ×”×¤×§×™×“ ×¨×©×™×•× ×•, ××• ×ª×¦×”×™×¨ ××ª××™×, ×‘××–×›×™×¨×•×ª ×‘×™×ª ××©×¤×˜ ×”×©×œ×•× ×‘×‘××¨ ×©×‘×¢ ×¢×“ ×™×•× ×”×¢×‘×•×“×” ×”×‘×, ×§×¨×™, 17.07.24, ×©×¢×” 12:00. ××•×‘×”×¨ ×œ× ××©×, ×›×™ ×›×œ ×¢×•×“ ×œ× ×”×•×¤×§×“ ×”×¨×©×™×•×Ÿ ××• ×”×ª×¦×”×™×¨ â€“ ×™×”×™×” ×¤×¡×•×œ ××œ× ×”×•×’, ××š ×”×¤×¡×™×œ×” ×œ× ×ª×™×× ×”;\n",
      "46. ×¤×¡×™×œ×” ××§×‘×œ ×•××”×—×–×™×§ ×¨×©×™×•×Ÿ × ×”×™×’×” ×œ×¨×›×‘ ×× ×•×¢×™, ×‘×ª  6 ×—×“×©×™×, ×¢×œ ×ª× ××™, ×ª×§×•×¤×ª ×”×ª× ××™ -×œ××©×š 3 ×©× ×™× ××¡×™×•× ×”×¤×¡×™×œ×” ×‘×¤×•×¢×œ;\n",
      "47. ×”×©××“×ª ×”×¡××™× - ×‘×—×œ×•×£ ×ª×§×•×¤×ª ×”×¢×¨×¢×•×¨.\n",
      "48. ×¢×•×ª×§ ×’×–×¨ ×”×“×™×Ÿ ×™×•×¢×‘×¨ ×œ×©×™×¨×•×ª ×”××‘×—×Ÿ ×œ××‘×•×’×¨×™× ×•×œ×××•× ×” ×¢×œ ×¢×‘×•×“×•×ª ×”×©×™×¨×•×ª ×‘×©×‘\"×¡.\n",
      "49. 51293715129371×¦×• ××‘×—×Ÿ ×™×•×’×© ×œ×—×ª×™××” ×‘×ª×•×š 30 ×™×•× ××”×™×•×. \n",
      "50. 5467831354678313×”×•×“×¢×” ×–×›×•×ª ×”×¢×¨×¢×•×¨.\n",
      "51. × ×™×ª× ×” ×”×™×•×, ×™' ×ª××•×– ×ª×©×¤\"×“, 16 ×™×•×œ×™ 2024, ×‘××¢××“ ×”×¦×“×“×™×.\n",
      "52. ×‘×¢× ×™×™×Ÿ ×¢×¨×™×›×” ×•×©×™× ×•×™×™× ×‘××¡××›×™ ×¤×¡×™×§×”, ×—×§×™×§×” ×•×¢×•×“ ×‘××ª×¨ × ×‘×• â€“ ×”×§×© ×›××Ÿ\n",
      "Extracted Matches:\n",
      "             0     1    2    3            4\n",
      "    match                                  \n",
      "138 0      NaN   ×¢\"×¤  NaN  NaN      2596/18\n",
      "140 0        ×¨   ×¢\"×¤  NaN  NaN      8695/19\n",
      "143 0      NaN   ×¢\"×¤  NaN  NaN      1345/08\n",
      "147 0      NaN  ×¢×¤\"×’  NaN  NaN  30646-10-23\n",
      "148 0      NaN  ×¢×¤\"×’  NaN  NaN   4842-05-23\n",
      "149 0      NaN   ×ª\"×¤  NaN  NaN  64772-05-22\n",
      "150 0      NaN   ×ª\"×¤  NaN  NaN   5072-11-21\n",
      "151 0      NaN   ×ª\"×¤  NaN  NaN  54272-11-23\n",
      "Extracted DataFrame:              0     1    2    3            4\n",
      "    match                                  \n",
      "138 0      NaN   ×¢\"×¤  NaN  NaN      2596/18\n",
      "140 0        ×¨   ×¢\"×¤  NaN  NaN      8695/19\n",
      "143 0      NaN   ×¢\"×¤  NaN  NaN      1345/08\n",
      "147 0      NaN  ×¢×¤\"×’  NaN  NaN  30646-10-23\n",
      "148 0      NaN  ×¢×¤\"×’  NaN  NaN   4842-05-23\n",
      "149 0      NaN   ×ª\"×¤  NaN  NaN  64772-05-22\n",
      "150 0      NaN   ×ª\"×¤  NaN  NaN   5072-11-21\n",
      "151 0      NaN   ×ª\"×¤  NaN  NaN  54272-11-23\n",
      "×¢\"×¤ 2596/18\n",
      "×¢\"×¤ 8695/19\n",
      "×¢\"×¤ 1345/08\n",
      "×¢×¤\"×’ 30646-10-23\n",
      "×¢×¤\"×’ 4842-05-23\n",
      "×ª\"×¤ 64772-05-22\n",
      "×ª\"×¤ 5072-11-21\n",
      "×ª\"×¤ 54272-11-23\n",
      "Processed document saved to: /home/liorkob/M.Sc/thesis/data/drugs/tag_citations/×ª\"×¤ 30506-01-22.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import docx\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Define required sections and citation patterns\n",
    "required_parts = [\n",
    "    \"××ª×—××™ ×¢× ×™×©×”\", \"××—×™×“×•×ª ×‘×¢× ×™×©×”\", \"××ª×—× ×”×¢× ×™×©×”\", \"××ª×—× ×¢× ×™×©×”\", \"×“×™×•×Ÿ\",\n",
    "    \"×¢× ×™×©×” × ×”×•×’×”\", \"×”×¢× ×™×©×” ×”× ×•×”×’×ª\", \"×¢× ×™×©×” × ×•×”×’×ª\", \"××ª×—× ×”×¢×•× ×©\", \"××ª×—× ×¢×•× ×©\",\n",
    "    \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\", \"×•×”×›×¨×¢×”\", \"×”×”×¨×©×¢×”\", \"××“×™× ×™×•×ª ×”×¢× ×™×©×” ×”× ×”×•×’×”\"\n",
    "]\n",
    "\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model_path = \"/home/liorkob/classifier_relvant_citation_model.pt\" \n",
    "tokenizer_bert = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model_bert.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "def split_preserving_structure(text):\n",
    "    paragraphs = re.split(r'(?<=\\d\\.)\\s', text)  # Split after numbers followed by a period\n",
    "    return [para.strip() for para in paragraphs if para.strip()]\n",
    "\n",
    "def query_gpt(text,citation):\n",
    "    \"\"\"\n",
    "    Queries GPT-4o to extract and segment legal citations.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following legal text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Your task is to extract **only** the part of the text that directly relates to the citation \"{citation}\".\n",
    "    \n",
    "    **Extraction Rules:**\n",
    "    - **Do not modify any wording.** Keep the original phrasing exactly as it appears in the provided document.\n",
    "    - **Do not summarize or rephrase.**\n",
    "    - **Return only the relevant portion**, not the full text.\n",
    "    - **Handle grouped citations carefully:**\n",
    "        - If the citation appears in a list following \"×¨××• ×œ××©×œ ...\" or similar, include the preceding explanation that applies to all citations.\n",
    "        - Do not include other citations from the listâ€”return only the text relevant to \"{citation}\".\n",
    "    - **Handle case explanations properly:**\n",
    "        - If the citation is explained in a specific section (e.g., \"×‘×¢\"×¤ 9373/10 ×•×ª×“ × ' ××“×™× ×ª ×™×©×¨××œ...\"), extract the **entire explanation** of the case.\n",
    "        - Do not remove any important context about the court ruling.\n",
    "    - Do **not** extract only \"(×¨×¢\"×¤ 2718/04)\" without the legal principle it supports.\n",
    "\n",
    "\n",
    "    Only return the extracted text. Do not include unrelated content or formatting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to extract and structure legal citations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        processed_text = response.choices[0].message.content\n",
    "\n",
    "\n",
    "            # Debug: Print the extracted response from GPT\n",
    "        # print(\"\\n===== DEBUG: GPT RESPONSE =====\")\n",
    "        # print(f\"Citation: {citation}\")\n",
    "        # print(\"Prompt Sent to GPT:\")\n",
    "        # print(prompt)\n",
    "        # print(\"Extracted Text:\")\n",
    "        # print(processed_text)\n",
    "        # print(\"==============================\\n\")\n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ GPT API error: {e}\")\n",
    "        return [text]  # Return original text in case of failure\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a required part in the CSV and all subsequent rows.\n",
    "    \"\"\"\n",
    "    start_index = None\n",
    "\n",
    "    # Find the first row containing a required part\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    # If a match is found, return only relevant rows\n",
    "    if start_index is not None:\n",
    "        return csv_data.iloc[start_index:]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=csv_data.columns)  # Return an empty DataFrame if no matches found\n",
    "\n",
    "\n",
    "\n",
    "# Function to find all occurrences of a citation in the document\n",
    "def find_all_occurrences(doc, citation):\n",
    "    indices = []\n",
    "    for i, paragraph in enumerate(doc.paragraphs):\n",
    "        if citation in paragraph.text:\n",
    "            indices.append(i)  # Store all occurrences of the citation\n",
    "    return indices\n",
    "\n",
    "# Function to get relevant context for each occurrence of the citation\n",
    "def get_context_paragraphs(doc, index, citation):\n",
    "    context_text = []\n",
    "\n",
    "    # Search for the closest non-empty previous paragraph\n",
    "    prev_index = index - 1\n",
    "    while prev_index >= 0 and not doc.paragraphs[prev_index].text.strip():\n",
    "        prev_index -= 1  # Move backwards until finding text\n",
    "\n",
    "    if prev_index >= 0:\n",
    "        context_text.append(doc.paragraphs[prev_index].text.strip())\n",
    "\n",
    "    # Get the current paragraph (must exist, but check if empty)\n",
    "    curr_text = doc.paragraphs[index].text.strip()\n",
    "    if curr_text:\n",
    "        context_text.append(curr_text)\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: Empty paragraph for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None  # Skip this occurrence if the current paragraph is empty\n",
    "\n",
    "    # Search for the closest non-empty next paragraph\n",
    "    next_index = index + 1\n",
    "    while next_index < len(doc.paragraphs) and not doc.paragraphs[next_index].text.strip():\n",
    "        next_index += 1  # Move forward until finding text\n",
    "\n",
    "    if next_index < len(doc.paragraphs):\n",
    "        context_text.append(doc.paragraphs[next_index].text.strip())\n",
    "\n",
    "    # Ensure we have at least one non-empty paragraph\n",
    "    if not context_text:\n",
    "        print(f\"âš ï¸ Warning: No valid text found for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None\n",
    "\n",
    "    return \"\\n\".join(context_text).strip()\n",
    "\n",
    "\n",
    "# Function to process and tag document paragraphs\n",
    "def process_and_tag_with_split(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a .docx document and its corresponding CSV, find relevant paragraphs with context, \n",
    "    extract relevant text using GPT, tag with BERT, and store results.\n",
    "    \"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    filtered_csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "    citations = extract_citations_from_csv(filtered_csv_data)\n",
    "    results = []\n",
    "\n",
    "    for citation in citations:\n",
    "        citation_indices = find_all_occurrences(doc, citation)  # Find all occurrences\n",
    "\n",
    "        # Collect all contexts where the citation appears\n",
    "        merged_contexts = []\n",
    "        for index in citation_indices:\n",
    "            full_context = get_context_paragraphs(doc, index, citation)\n",
    "            if full_context:\n",
    "                merged_contexts.append(full_context)\n",
    "\n",
    "        # If no valid contexts found, skip this citation\n",
    "        if not merged_contexts:\n",
    "            continue  \n",
    "\n",
    "        # Merge all valid contexts into one, ensuring uniqueness\n",
    "        final_context = \"\\n\".join(set(merged_contexts)).strip()  # Remove duplicates\n",
    "        print(citation)\n",
    "        # print(final_context)\n",
    "\n",
    "        # Ask GPT to extract the relevant part\n",
    "        extracted_text = query_gpt(final_context, citation)\n",
    "\n",
    "        # Tag the extracted text with BERT\n",
    "        encoding = tokenizer_bert(extracted_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "        encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model_bert(**encoding)\n",
    "            prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "        # Store only one result per citation\n",
    "        result = {\n",
    "            'citation': citation,\n",
    "            'context_text': final_context,\n",
    "            'extracted_text': extracted_text,\n",
    "            'predicted_label': prediction\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed document saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docx_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs/docx')\n",
    "    csv_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs/docx_csv')\n",
    "    output_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs/tag_citations')\n",
    "\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in docx_directory.glob(\"*.docx\"):\n",
    "        new_file_path = file_path.stem\n",
    "        print(f\"Processing {new_file_path}\")\n",
    "\n",
    "        csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "        \n",
    "        if file_path.exists() and csv_file.exists():\n",
    "            output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "            if output_file.exists():\n",
    "                continue\n",
    "            process_and_tag_with_split(str(file_path), str(csv_file), str(output_file))\n",
    "        else:\n",
    "            if not file_path.exists():\n",
    "                print(f\"Document file not found: {file_path}\")\n",
    "            if not csv_file.exists():\n",
    "                print(f\"CSV file not found for: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get URLS from docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx.oxml.ns import qn\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('âˆ•', '/')).strip()\n",
    "\n",
    "\n",
    "def normalize_citation(citation):\n",
    "    \"\"\"Normalize citation by removing prefixes and standardizing format.\"\"\"\n",
    "    if not citation:\n",
    "        return None\n",
    "    # Standardize quotes\n",
    "    citation = citation.replace('×´', '\"').replace('×´', '\"').replace('×´', '\"')\n",
    "    # Remove extra spaces\n",
    "    citation = re.sub(r'\\s+', ' ', citation).strip()\n",
    "    # Remove common prefixes, including ×¨×¢\"×¤\n",
    "    citation = re.sub(r'^(×¢\"?×¤|×ª\"?×¤|×¢×¤\"?×’|×¨×¢\"?×¤)\\s+', '', citation)\n",
    "    return citation\n",
    "\n",
    "\n",
    "# citation_patterns = {\n",
    "#     '×¢\"×¤': r'×¢\"×¤ (\\d+/\\d+)',\n",
    "#     '×¢×¤\"×’': r'×¢×¤\"×’ (\\d+/\\d+)',\n",
    "#     '×ª×´×¤': r'×ª×´×¤ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "#     '×¢×¤×´×’': r'×¢×¤×´×’ (\\d+/\\d+)',\n",
    "#     '×¨×¢×´×¤': r'×¨×¢×´×¤ (\\d+/\\d+)',\n",
    "#     '×ª×¤\"×—': r'×ª×¤\"×—\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "# }\n",
    "\n",
    "# def extract_citations(text):\n",
    "#     \"\"\"Extracts citations from the paragraph_text column based on predefined patterns.\"\"\"\n",
    "#     matches = []\n",
    "#     for label, pattern in citation_patterns.items():\n",
    "#         found = re.findall(pattern, text)\n",
    "#         matches.extend([f\"{label} {m}\" for m in found])\n",
    "#     return matches[0] if matches else None\n",
    "\n",
    "def extract_citations(text):\n",
    "    \"\"\"Extracts legal citations from a single text string.\"\"\"\n",
    "    matches = citation_regex.findall(text)\n",
    "    citations = []\n",
    "    for match in matches:\n",
    "        citation = \" \".join(filter(None, match)).strip()\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "        citation = re.sub(r\"^\\b[×‘×•×•×¨]\\b\\s*\", \"\", citation)\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        if not re.match(r\"^×¢×œ \\d+$\", citation):\n",
    "            citations.append(citation)\n",
    "    return citations[0] if citations else None\n",
    "\n",
    "\n",
    "def getLinkedText(soup):\n",
    "    links = []\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            parts = tag.text.split('\"')\n",
    "            if len(parts) > 1:  # Ensure the URL exists before accessing index 1\n",
    "                url = parts[1]\n",
    "            else:\n",
    "                print(f\"âš ï¸ Warning: No valid URL found in HYPERLINK tag: {tag.text}\")\n",
    "                url = None  # Assign None if URL is missing\n",
    "\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None and maybe_text.text.strip() != \"\":\n",
    "                    text += maybe_text.text.strip()\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None and maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                    break\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "    return links\n",
    "def getURLs(soup, links):\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            for rel in soup.find_all(\"Relationship\"):\n",
    "                if rel[\"Id\"] == link[\"id\"]:\n",
    "                    link[\"href\"] = rel[\"Target\"]\n",
    "    return links\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_hyperlinks(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from a .docx file and returns a dictionary \n",
    "    where the linked text is mapped to its corresponding URL.\n",
    "    \"\"\"\n",
    "    # Open the .docx file as a zip archive\n",
    "    try:\n",
    "        archive = zipfile.ZipFile(docx_path, \"r\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"âŒ Error: Cannot open {docx_path} (Bad ZIP format)\")\n",
    "        return {}\n",
    "\n",
    "    # Extract main document XML\n",
    "    try:\n",
    "        file_data = archive.read(\"word/document.xml\")\n",
    "        doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "        linked_text = getLinkedText(doc_soup)\n",
    "    except KeyError:\n",
    "        print(f\"âš ï¸ Warning: No document.xml found in {docx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Extract hyperlink relationships from _rels/document.xml.rels\n",
    "    try:\n",
    "        url_data = archive.read(\"word/_rels/document.xml.rels\")\n",
    "        url_soup = BeautifulSoup(url_data, \"xml\")\n",
    "        links_with_urls = getURLs(url_soup, linked_text)\n",
    "    except KeyError:\n",
    "        print(f\"âš ï¸ Warning: No _rels/document.xml.rels found in {docx_path}\")\n",
    "        links_with_urls = linked_text\n",
    "\n",
    "    # Extract footnotes (if available)\n",
    "    try:\n",
    "        footnote_data = archive.read(\"word/footnotes.xml\")\n",
    "        footnote_soup = BeautifulSoup(footnote_data, \"xml\")\n",
    "        footnote_links = getLinkedText(footnote_soup)\n",
    "\n",
    "        footnote_url_data = archive.read(\"word/_rels/footnotes.xml.rels\")\n",
    "        footnote_url_soup = BeautifulSoup(footnote_url_data, \"xml\")\n",
    "        footnote_links_with_urls = getURLs(footnote_url_soup, footnote_links)\n",
    "\n",
    "        # Merge footnote links\n",
    "        links_with_urls += footnote_links_with_urls\n",
    "    except KeyError:\n",
    "        pass  # No footnotes found, continue\n",
    "\n",
    "    # Convert extracted links to a dictionary: {linked_text: URL}\n",
    "    return {link[\"text\"]: link.get(\"href\", None) for link in links_with_urls}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def update_csv_with_links(csv_path, doc_path):\n",
    "    csv_path = Path(csv_path)  # Convert to Path object if not already\n",
    "    \n",
    "    # **Check if CSV is empty before reading**\n",
    "    if not csv_path.exists() or csv_path.stat().st_size == 0:  \n",
    "        print(f\"Skipping empty or missing file: {csv_path.name}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # **Check if the DataFrame is empty after loading**\n",
    "        if df.empty:\n",
    "            print(f\"Skipping empty DataFrame: {csv_path.name}\")\n",
    "            return\n",
    "        \n",
    "        # Normalize extracted citations\n",
    "        df[\"extracted_citation\"] = df[\"paragraph_text\"].apply(\n",
    "            lambda text: normalize_citation(extract_citations(text)) if pd.notna(text) else None\n",
    "        )\n",
    "        \n",
    "        # Normalize citation_links keys\n",
    "        citation_links = extract_hyperlinks(doc_path)\n",
    "        normalized_citation_links = {normalize_citation(k): v for k, v in citation_links.items()}\n",
    "        \n",
    "        # Assign URLs to citations\n",
    "        df[\"link\"] = df[\"extracted_citation\"].apply(\n",
    "            lambda text: normalized_citation_links.get(text, None) if pd.notna(text) else None\n",
    "        )\n",
    "        \n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated CSV saved to: {csv_path}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Skipping {csv_path.name}: CSV file is empty or unreadable.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "def find_matching_docx(csv_name, docx_directory):\n",
    "    normalized_csv_name = normalize_case_name(csv_name.replace('.csv', '.docx'))\n",
    "    for root, _, files in os.walk(docx_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".docx\") and normalize_case_name(file) == normalized_csv_name:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def process_all_csvs(csv_directory, docx_directory):\n",
    "    for root, _, files in os.walk(csv_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(root, file)\n",
    "                docx_path = find_matching_docx(file, docx_directory)\n",
    "                # if file != '×ª\"×¤ 49772-11-16.csv':\n",
    "                #     continue\n",
    "                if docx_path:\n",
    "                    update_csv_with_links(csv_path, docx_path)\n",
    "                else:\n",
    "                    print(f\"No matching DOCX found for: {file}\")\n",
    "\n",
    "docx_csv_dir = f\"/home/liorkob/M.Sc/thesis/data/drugs/docx_csv\"\n",
    "citations_dir = f\"/home/liorkob/M.Sc/thesis/data/drugs/tag_citations\"\n",
    "process_all_csvs(citations_dir, docx_csv_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories and paths\n",
    "UPDATED_CSV_DIR = \"/home/liorkob/M.Sc/thesis/data/drugs/tag_citations\"\n",
    "\n",
    "def verify_updated_files_tag_1():\n",
    "    \"\"\"Iterates over updated CSV files and prints missing links for rows where tag is 1.\"\"\"\n",
    "\n",
    "    for root, _, files in os.walk(UPDATED_CSV_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                # Filter rows with tag = 1 and missing links\n",
    "                missing_links = df[(df[\"predicted_label\"] == 1) & (df[\"extracted_citation\"].notna()) & (df[\"link\"].isna())]\n",
    "                \n",
    "                if not missing_links.empty:\n",
    "                    print(f\"\\nğŸ” Missing links in file: {file}\")\n",
    "                    for _, row in missing_links.iterrows():\n",
    "                        print(f\"- Citation: {row['extracted_citation']}\")\n",
    "                        print(f\"  Paragraph: {row['paragraph_text'][:200]}...\")  # Show first 200 chars\n",
    "                        print(\"-\" * 50)\n",
    "\n",
    "# Run verification for tag = 1\n",
    "verify_updated_files_tag_1()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
