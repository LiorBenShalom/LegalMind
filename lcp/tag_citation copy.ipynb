{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation no API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "\n",
    "required_parts = [\n",
    "    \"מתחמי ענישה\", \"אחידות בענישה\", \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\",\n",
    "    \"ענישה נהוגה\", \"הענישה הנוהגת\", \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\",\n",
    "    \"מדיניות הענישה\", \"והכרעה\", \"ההרשעה\", \"מדיניות הענישה הנהוגה\"\n",
    "]\n",
    "citation_patterns = {\n",
    "    'ע\"פ': r'ע\"פ (\\d+/\\d+)',\n",
    "    'עפ\"ג': r'עפ\"ג (\\d+/\\d+)',\n",
    "    'ת״פ': r'ת״פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ״ג': r'עפ״ג (\\d+/\\d+)',\n",
    "    'רע״פ': r'רע״פ (\\d+/\\d+)',\n",
    "    'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "}\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/home/liorkob/best_model.pt\"  # Path to your saved model\n",
    "tokenizer = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def extract_citations(para_text):\n",
    "    \"\"\"\n",
    "    Extracts all citations and their full references from the text.\n",
    "    Returns a list of tuples: (citation_type, full_citation).\n",
    "    \"\"\"\n",
    "    citations = []\n",
    "\n",
    "    for citation_type, pattern in citation_patterns.items():\n",
    "        matches = re.findall(pattern, para_text)  # Find all matches for the pattern\n",
    "        for match in matches:\n",
    "            full_citation = f\"{citation_type} {match}\"  # Construct full citation\n",
    "            citations.append((citation_type, full_citation))\n",
    "\n",
    "    return citations  # List of (citation_type, full_citation)\n",
    "\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"Extracts the first occurrence of a required part in the CSV and all subsequent rows.\"\"\"\n",
    "    start_index = None\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "    return csv_data.iloc[start_index:] if start_index is not None else pd.DataFrame(columns=csv_data.columns)\n",
    "import re\n",
    "\n",
    "def process_and_tag(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"Process a .docx document and its corresponding CSV to check citations and tag with predictions.\"\"\"\n",
    "    try:\n",
    "        # Load the document and CSV\n",
    "        doc = docx.Document(docx_path)\n",
    "        csv_data = pd.read_csv(csv_path)\n",
    "        csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # Iterate through paragraphs\n",
    "        for i, paragraph in enumerate(doc.paragraphs):\n",
    "            para_text = paragraph.text.strip()\n",
    "            if not para_text:\n",
    "                continue  # Skip empty paragraphs\n",
    "\n",
    "            found_citations = extract_citations(para_text)\n",
    "\n",
    "            if not found_citations:\n",
    "                continue  # No citations found, move to the next paragraph\n",
    "\n",
    "            for found_citation, full_citation in found_citations:\n",
    "\n",
    "                is_relevant = False\n",
    "                matching_part = None\n",
    "\n",
    "                # Check if the citation is in relevant parts\n",
    "                for _, row in csv_data.iterrows():\n",
    "                    part_text = row.get(\"text\", \"\")\n",
    "                    if any(req_part in row.get(\"part\", \"\") for req_part in required_parts) and part_text in para_text:\n",
    "                        is_relevant = True\n",
    "                        matching_part = row[\"part\"]\n",
    "                        break  # Stop searching once a match is found\n",
    "\n",
    "                if is_relevant:\n",
    "                    # Tag the paragraph using the model\n",
    "                    encoding = tokenizer(para_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "                    encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "                    with torch.no_grad():\n",
    "                        output = model(**encoding)\n",
    "                        prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "                    # Append only when is_relevant = True\n",
    "                    results.append({\n",
    "                        'paragraph_number': i,\n",
    "                        'paragraph_text': para_text,\n",
    "                        'citation': full_citation,\n",
    "                        'part': matching_part,\n",
    "                        'predicted_label': prediction, \n",
    "                    })\n",
    "\n",
    "                    print(f\"Tagged citation: Paragraph {i}, Part: {matching_part}, Prediction: {prediction}\")\n",
    "                    print(f\"Text: {para_text}\\n\")\n",
    "\n",
    "        # Convert results to a DataFrame\n",
    "        results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "        # Save results\n",
    "        results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Tagged citations saved to: {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {docx_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        docx_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_{year}')\n",
    "        csv_directory = Path(f'/home/liorkob/thesis/lcp/data/docx_csv_{year}')\n",
    "        output_directory = Path(f'/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}')\n",
    "\n",
    "        output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for file_path in docx_directory.glob(\"*.docx\"):\n",
    "            try:\n",
    "                new_file_path = file_path.stem\n",
    "                print(f\"Processing {new_file_path}\")\n",
    "\n",
    "                csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "                if file_path.exists() and csv_file.exists():\n",
    "                    output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "                    process_and_tag(str(file_path), str(csv_file), str(output_file))\n",
    "\n",
    "                else:\n",
    "                    if not file_path.exists():\n",
    "                        print(f\"Document file not found: {file_path}\")\n",
    "                    if not csv_file.exists():\n",
    "                        print(f\"CSV file not found for: {csv_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path.name}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all results to one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import docx\n",
    "\n",
    "def merge_results(csv_directory: str, output_csv: str):\n",
    "    csv_directory = Path(csv_directory)\n",
    "    all_data = []\n",
    "    \n",
    "    # Iterate over CSV files\n",
    "    for file_path in csv_directory.glob(\"*.csv\"):\n",
    "        try:\n",
    "            if file_path.stat().st_size == 0:  # Check if file is empty\n",
    "                print(f\"Skipping empty file: {file_path.name}\")\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(file_path)\n",
    "            if df.empty:  # Check if the file is empty even after reading\n",
    "                print(f\"Skipping empty DataFrame: {file_path.name}\")\n",
    "                continue\n",
    "\n",
    "            df[\"source_file\"] = file_path.name  # Add filename column\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path.name}: {e}\")\n",
    "    \n",
    "    if all_data:\n",
    "        merged_df = pd.concat(all_data, ignore_index=True)\n",
    "        merged_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Merged CSV saved to: {output_csv}\")\n",
    "    else:\n",
    "        print(\"No valid CSV files found.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for year in [2018,2019,2020]:\n",
    "        csv_directory = f\"/home/liorkob/thesis/lcp/data/tag_citations_csv_{year}\"\n",
    "        output_csv = f\"{csv_directory}/merged_results_{year}.csv\"\n",
    "        \n",
    "        merge_results(csv_directory, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation WITH API -v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import docx\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "print([m.id for m in models])\n",
    "model_info = client.models.retrieve(\"gpt-4o\")\n",
    "print(model_info)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Define required sections and citation patterns\n",
    "required_parts = [\n",
    "    \"מתחמי ענישה\", \"אחידות בענישה\", \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\",\n",
    "    \"ענישה נהוגה\", \"הענישה הנוהגת\", \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\",\n",
    "    \"מדיניות הענישה\", \"והכרעה\", \"ההרשעה\", \"מדיניות הענישה הנהוגה\"\n",
    "]\n",
    "citation_patterns = ['ע\"פ', 'ת\"פ', 'עפ\"ג', 'ע״פ', 'ת״פ', 'עפ״ג']\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model_path = \"/home/liorkob/best_model.pt\"  # Path to your saved model\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model_bert.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "def split_preserving_structure(text):\n",
    "    paragraphs = re.split(r'(?<=\\d\\.)\\s', text)  # Split after numbers followed by a period\n",
    "    return [para.strip() for para in paragraphs if para.strip()]\n",
    "\n",
    "def query_gpt(text):\n",
    "    \"\"\"\n",
    "    Queries GPT-4o to extract and segment legal citations.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"הטקסט הבא מכיל מספר ציטוטים משפטיים, שהם הפניות להחלטות של בתי משפט והם נכתבים בפורמט הבא: \"\n",
    "        \"סוג ההליך (ע\\\"פ, עפ\\\"ג, ת\\\"פ וכו'), מספר התיק, שמות הצדדים, ותאריך ההחלטה בסוגריים. \"\n",
    "        \"לדוגמה: ע\\\"פ 4173/07 פלוני נ' מדינת ישראל (2007).\\n\\n\"\n",
    "\n",
    "        \" **הנחיות קריטיות:**\\n\"\n",
    "        \" **אין לערוך, לשנות או להשמיט שום חלק מהטקסט המקורי** – כל התוכן חייב להופיע כפי שהוא.\\n\"\n",
    "        \" **יש לפצל לפסקאות לפי הציטוטים המשפטיים**, כך שכל פסקה תכיל ציטוט עם ההקשר המתאים.\\n\"\n",
    "        \" **אם מספר ציטוטים מתייחסים לאותו מקרה יש להשאירם יחד באותה פסקה**.\\n\"\n",
    "        \" **הפסקאות חייבות להופיע בסדר המקורי שלהן** – אין לערבב או להזיז חלקים בטקסט.\\n\"\n",
    "        \" **אין ליצור פסקאות שאין בהן ציטוט משפטי**.\\n\\n\"\n",
    "\n",
    "        \"### 🔍 דוגמאות לפיצול נכון ושגוי:\\n\\n\"\n",
    "\n",
    "        \"❌ **פיצול שגוי (לא נכון):**\\n\"\n",
    "        \"1. ע\\\"פ 1234/20 מדינת ישראל נ' כהן (2020)\\n\"\n",
    "        \"2. טקסט כללי בלי ציטוט – אין לאפשר זאת.\\n\\n\"\n",
    "\n",
    "        \"✅ **פיצול נכון (כן נכון):**\\n\"\n",
    "        \"1. ע\\\"פ 5678/15 לוי נ' מדינת ישראל (2015) - במקרה זה נקבע כי...\\n\"\n",
    "        \"2. ע\\\"פ 9876/18 כהן נ' מדינת ישראל (2018) - בנסיבות דומות, הוחלט כי...\\n\\n\"\n",
    "\n",
    "        \"✅ **כאשר כל הציטוטים שייכים לאותו הקשר - יש להשאירם יחד:**\\n\"\n",
    "        \"1. \\\"על דרך הכלל, בית משפט זה נדרש לצערי לא אחת לאירועים מעין אלה של פתרון סכסוכים בדרכי אלימות, \"\n",
    "        \"ולא תתכן מחלוקת כי יש להטיל עונשים משמעותיים, על פי רוב מאחורי סורג ובריח, כדי לעקור תופעות אלה מהשורש. \"\n",
    "        \"אין מקום לסובלנות כלפי יד קלה על ההדק או קת סכין או במקל חובלים. ועל כך נאמר יש מקום שגם בית המשפט יתרום את חלקו \"\n",
    "        \"למלחמה נגד האלימות. לעיתים יש תחושה שכל אמרה לא נכונה או התנהגות שבעיני אחר סוטה מן השורה, ולו במקצת, \"\n",
    "        \"מהווה הצדקה עבור הפוגע וסביבתו לפגוע באמצעות נשק קר ביחיד ובסביבתו\\\" \"\n",
    "        \"(עוד לעניין זה ראו ע\\\"פ 4173/07 פלוני נ' מדינת ישראל (2007); ע\\\"פ 8991/10 מכבי נ' מדינת ישראל (2011); \"\n",
    "        \"ע\\\"פ 7360/13 טאהא נ' מדינת ישראל (2014).)\\n\\n\"\n",
    "\n",
    "        \"⚠️ **חשוב מאוד:**\\n\"\n",
    "        \"✔ **כל הטקסט חייב להופיע כפי שהוא, ללא שינוי, עריכה או השמטה.**\\n\"\n",
    "        \"✔ **כל פסקה חייבת להכיל ציטוט משפטי ולשמור על ההקשר המקורי שלה.**\\n\"\n",
    "        \"✔ **אם יש ציטוטים הקשורים זה לזה, יש להשאירם יחד.**\\n\"\n",
    "        \"✔ **אין לשנות את סדר הפסקאות ואין ליצור טקסטים חדשים.**\\n\\n\"\n",
    "\n",
    "        f\"Text: {text}\\n\"\n",
    "        \"Processed Segments:\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Change from \"gpt-4o-2024-11-20\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to extract and structure legal citations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        processed_text = response.choices[0].message.content\n",
    "\n",
    "        # Debugging: Print raw GPT response\n",
    "        print(\"RAW GPT OUTPUT:\\n\", processed_text)\n",
    "\n",
    "        # Improved paragraph splitting while preserving structure\n",
    "        split_paragraphs = split_preserving_structure(processed_text)\n",
    "\n",
    "        # Remove duplicates\n",
    "        split_paragraphs = list(dict.fromkeys(split_paragraphs))\n",
    "\n",
    "        # Debugging: Print processed paragraphs\n",
    "        print(\"PROCESSED PARAGRAPHS:\\n\", split_paragraphs)\n",
    "\n",
    "        # Return processed text\n",
    "        return split_paragraphs if split_paragraphs else [text]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 GPT API error: {e}\")\n",
    "        return [text]  # Return original text in case of failure\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a required part in the CSV and all subsequent rows.\n",
    "    \"\"\"\n",
    "    start_index = None\n",
    "\n",
    "    # Find the first row containing a required part\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    # If a match is found, return only relevant rows\n",
    "    if start_index is not None:\n",
    "        return csv_data.iloc[start_index:]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=csv_data.columns)  # Return an empty DataFrame if no matches found\n",
    "\n",
    "def enforce_citation_splitting(split_paragraphs):\n",
    "    \"\"\"\n",
    "    Ensures each citation is properly separated, even if GPT fails.\n",
    "    \"\"\"\n",
    "    refined = []\n",
    "    citation_pattern = re.compile(r'(ע\"?פ|עפ\"ג|ת\"?פ) \\d+[-/]?\\d{2,5} .*?\\[\\d{1,2}\\.\\d{2,4}\\]')\n",
    "\n",
    "    for para in split_paragraphs:\n",
    "        matches = citation_pattern.findall(para)\n",
    "\n",
    "        if len(matches) > 1:\n",
    "            segments = citation_pattern.split(para)\n",
    "            for i in range(1, len(segments), 2):  \n",
    "                citation = segments[i].strip()\n",
    "                context = segments[i + 1].strip() if i + 1 < len(segments) else \"\"\n",
    "                refined.append(f\"{citation} {context}\")\n",
    "        else:\n",
    "            refined.append(para.strip())\n",
    "\n",
    "    return refined\n",
    "\n",
    "def process_and_tag_with_split(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a .docx document and its corresponding CSV, split paragraphs only if they contain \n",
    "    multiple citations, ensure they are within relevant parts, and tag with predictions.\n",
    "    \"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    filtered_csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "    results = []\n",
    "    for i, paragraph in enumerate(doc.paragraphs):\n",
    "        para_text = paragraph.text.strip()\n",
    "        if not para_text:\n",
    "            continue  # Skip empty paragraphs\n",
    "\n",
    "        # Count the number of citations in the paragraph\n",
    "        citation_count = sum(para_text.count(pattern) for pattern in citation_patterns)\n",
    "\n",
    "        is_relevant = False\n",
    "        matching_part = None\n",
    "\n",
    "        for _, row in filtered_csv_data.iterrows():\n",
    "            part_text = row.get(\"text\", \"\")\n",
    "            if any(req_part in row.get(\"part\", \"\") for req_part in required_parts) and part_text in para_text:\n",
    "                is_relevant = True\n",
    "                matching_part = row[\"part\"]\n",
    "                break  # Stop searching once a match is found\n",
    "\n",
    "        if is_relevant:\n",
    "            # Tag the paragraph using the model\n",
    "            encoding = tokenizer_bert(para_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "            encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "            with torch.no_grad():\n",
    "                output = model_bert(**encoding)\n",
    "                prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "            # SPLIT ONLY IF TAG IS 1\n",
    "            if prediction == 1:\n",
    "                if citation_count > 1:\n",
    "                    split_paragraphs = query_gpt(para_text)\n",
    "                    split_paragraphs = enforce_citation_splitting(split_paragraphs)\n",
    "                    original_paragraph = para_text  # Store the original text\n",
    "                else:\n",
    "                    split_paragraphs = [para_text]\n",
    "                    original_paragraph = None  # Not split, so no original needed\n",
    "            else:\n",
    "                split_paragraphs = [para_text]  # Keep as is\n",
    "                original_paragraph = None  # No need to store original\n",
    "\n",
    "            for split_text in split_paragraphs:\n",
    "                # Check if the split paragraph contains a citation\n",
    "                if not any(pattern in split_text for pattern in citation_patterns):\n",
    "                    continue  # Skip non-citation paragraphs\n",
    "\n",
    "                # Save results\n",
    "                result = {\n",
    "                    'paragraph_number': i,\n",
    "                    'original_paragraph': original_paragraph if citation_count > 1 else split_text,  # Store original if split\n",
    "                    'paragraph_text': split_text,\n",
    "                    'part': matching_part,\n",
    "                    'predicted_label': prediction\n",
    "                }\n",
    "                results.append(result)\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Tagged citations saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docx_directory = Path('/home/liorkob/thesis/lcp/data/docx_2019')\n",
    "    csv_directory = Path('/home/liorkob/thesis/lcp/data/docx_csv_2019')\n",
    "    output_directory = Path('/home/liorkob/thesis/lcp/data/tag_citations_csv_2019')\n",
    "\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in docx_directory.glob(\"*.docx\"):\n",
    "        new_file_path = file_path.stem\n",
    "        print(f\"Processing {new_file_path}\")\n",
    "\n",
    "        csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "        \n",
    "        if file_path.exists() and csv_file.exists():\n",
    "            output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "            process_and_tag_with_split(str(file_path), str(csv_file), str(output_file))\n",
    "        else:\n",
    "            if not file_path.exists():\n",
    "                print(f\"Document file not found: {file_path}\")\n",
    "            if not csv_file.exists():\n",
    "                print(f\"CSV file not found for: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract citation WITH API -v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: בעפ\"ג (ב\"ש) 31067-11-11\n",
      "match.group(): בת״פ 18402-08-13\n",
      "match.group(): תפ״א 12345-01-22\n",
      "match.group(): ת״פ 12345-01-22\n",
      "match.group(): ע.פ. 567/22\n",
      "match.group(): ע״פ 567/22\n",
      "match.group(): בת.פ. 56255-02-12\n",
      "match.group(): עפ\"ג 6074/93\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# List of legal acronyms (same as yours)\n",
    "acronyms = [\n",
    "    \"אב\", \"אבע\", \"אימוצ\", \"אמצ\", \"אפ\", \"אפח\", \"את\", \"אתפ\", \"באפ\", \"באש\", \"בבנ\", \"בגצ\", \"בדא\", \"בדמ\",\n",
    "    \"בדמש\", \"בהנ\", \"בהע\", \"בהש\", \"בידמ\", \"בידע\", \"בל\", \"בלמ\", \"במ\", \"בעא\", \"בעח\", \"בעמ\", \"בעק\", \"בפ\",\n",
    "    \"בפמ\", \"בפת\", \"בצא\", \"בצהמ\", \"בק\", \"בקמ\", \"בקשה\", \"ברמ\", \"ברע\", \"ברעפ\", \"ברש\", \"בש\", \"בשא\",\n",
    "    \"בשגצ\", \"בשהת\", \"בשז\", \"בשמ\", \"בשע\", \"בשפ\", \"בתת\", \"גזז\", \"גמר\", \"גפ\", \"דבע\", \"דח\", \"דט\", \"דיונ\",\n",
    "    \"דמ\", \"דמר\", \"דמש\", \"דנ\", \"דנא\", \"דנגצ\", \"דנמ\", \"דנפ\", \"הד\", \"הדפ\", \"הוצלפ\", \"הט\", \"הכ\", \"המ\",\n",
    "    \"המד\", \"הממ\", \"המע\", \"המש\", \"הנ\", \"הסת\", \"הע\", \"העז\", \"הפ\", \"הפב\", \"הפמ\", \"הצמ\", \"הש\", \"השא\",\n",
    "    \"השגצ\", \"השפ\", \"השר\", \"הת\", \"וחק\", \"וע\", \"ושמ\", \"ושק\", \"ושר\", \"זי\", \"חא\", \"חבר\", \"חד\", \"חדא\",\n",
    "    \"חדלפ\", \"חדלת\", \"חדמ\", \"חדפ\", \"חהע\", \"חי\", \"חנ\", \"חסמ\", \"חעמ\", \"חעק\", \"חש\", \"יוש\", \"ייתא\", \"ימא\",\n",
    "    \"יס\", \"כצ\", \"מ\", \"מא\", \"מבכ\", \"מבס\", \"מונופולינ\", \"מזג\", \"מח\", \"מחוז\", \"מחע\", \"מט\", \"מטכל\", \"מי\",\n",
    "    \"מיב\", \"מכ\", \"ממ\", \"מס\", \"מסט\", \"מעי\", \"מעת\", \"מקמ\", \"מרכז\", \"מת\", \"נ\", \"נב\", \"נבא\", \"נמ\", \"נמב\",\n",
    "    \"נעד\", \"נער\", \"סבא\", \"סע\", \"סעש\", \"סק\", \"סקכ\", \"ע\", \"עא\", \"עאח\", \"עאפ\", \"עב\", \"עבאפ\", \"עבז\", \"עבח\",\n",
    "    \"עבי\", \"עבל\", \"עבמצ\", \"עבעח\", \"עבפ\", \"עבר\", \"עבשהת\", \"עגר\", \"עדי\", \"עדמ\", \"עהג\", \"עהס\", \"עהפ\",\n",
    "    \"עו\", \"עורפ\", \"עז\", \"עח\", \"עחא\", \"עחדלפ\", \"עחדפ\", \"עחדת\", \"עחהס\", \"עחע\", \"עחק\", \"עחר\", \"עכב\",\n",
    "    \"על\", \"עלא\", \"עלבש\", \"עלח\", \"עלע\", \"עמ\", \"עמא\", \"עמה\", \"עמז\", \"עמח\", \"עמי\", \"עמלע\", \"עממ\", \"עמנ\",\n",
    "    \"עמפ\", \"עמצ\", \"עמק\", \"עמרמ\", \"עמש\", \"עמשמ\", \"עמת\", \"ענ\", \"ענא\", \"ענמ\", \"ענמא\", \"ענמש\", \"ענפ\",\n",
    "    \"עסא\", \"עסק\", \"עע\", \"עעא\", \"עעמ\", \"עער\", \"עעתא\", \"עפ\", \"עפא\", \"עפג\", \"עפהג\", \"עפמ\", \"עפמק\",\n",
    "    \"עפנ\", \"עפס\", \"עפספ\", \"עפע\", \"עפר\", \"עפת\", \"עצמ\", \"עק\", \"עקג\", \"עקמ\", \"עקנ\", \"עקפ\", \"ער\", \"ערא\",\n",
    "    \"ערגצ\", \"ערמ\", \"ערעור\", \"ערפ\", \"ערר\", \"עש\", \"עשא\", \"עשמ\", \"עשר\", \"עשת\", \"עשתש\", \"עת\", \"עתא\",\n",
    "    \"עתמ\", \"עתפב\", \"עתצ\", \"פא\", \"פה\", \"פל\", \"פלא\", \"פמ\", \"פמר\", \"פעמ\", \"פקח\", \"פר\", \"פרק\", \"פשז\",\n",
    "    \"פשר\", \"פת\", \"צא\", \"צבנ\", \"צה\", \"צו\", \"צח\", \"צמ\", \"קג\", \"קפ\", \"רחדפ\", \"רמש\", \"רע\", \"רעא\", \"רעב\",\n",
    "    \"רעבס\", \"רעו\", \"רעמ\", \"רעס\", \"רעפ\", \"רעפא\", \"רעצ\", \"רער\", \"רערצ\", \"רעש\", \"רעתא\", \"רצפ\", \"רתק\",\n",
    "    \"ש\", \"שבד\", \"שמ\", \"שמי\", \"שנא\", \"שע\", \"שעמ\", \"שק\", \"שש\", \"תא\", \"תאדמ\", \"תאח\", \"תאמ\", \"תאק\", \"תב\",\n",
    "    \"תבכ\", \"תבע\", \"תג\", \"תגא\", \"תד\", \"תדא\", \"תהג\", \"תהנ\", \"תהס\", \"תוב\", \"תוח\", \"תח\", \"תחפ\", \"תחת\",\n",
    "    \"תט\", \"תי\", \"תכ\", \"תלא\", \"תלב\", \"תלהמ\", \"תלפ\", \"תלתמ\", \"תמ\", \"תמהח\", \"תממ\", \"תמק\", \"תמר\",\n",
    "    \"תמש\", \"תנג\", \"תנז\", \"תע\", \"תעא\", \"תעז\", \"תפ\", \"תפב\", \"תפח\", \"תפחע\", \"תפכ\", \"תפמ\", \"תפע\",\n",
    "    \"תפק\", \"תצ\", \"תק\", \"תקח\", \"תקמ\", \"תרמ\", \"תת\", \"תתח\", \"תתע\", \"תתעא\", \"תתק\"\n",
    "]\n",
    "\n",
    "def create_acronym_variants(acronyms):\n",
    "    acronym_variants = []\n",
    "    for a in acronyms:\n",
    "        if len(a) > 1:\n",
    "            # Case 1: Original acronym with quotes/dots before last letter\n",
    "            base_acronym = a\n",
    "            if a.startswith('ב') or a.startswith('ו') or a.startswith('ה'):\n",
    "                # Also add variant without the prefix letter\n",
    "                base_acronym = a[1:]\n",
    "            \n",
    "            # For each acronym (both with and without prefix)\n",
    "            for acr in [a, base_acronym]:\n",
    "                if len(acr) > 1:\n",
    "                    # Standard quote/dot before last letter\n",
    "                    quoted = rf\"{acr[:-1]}[\\\"'״]{acr[-1]}\"\n",
    "                    with_dot = rf\"{acr[:-1]}\\.{acr[-1]}\"\n",
    "                    acronym_variants.append(f\"(?:{quoted}|{with_dot})\")\n",
    "                    \n",
    "                    # Add dot-separated variant\n",
    "                    dots_between = '\\.'.join(list(acr))\n",
    "                    acronym_variants.append(dots_between)\n",
    "    \n",
    "    return '|'.join(acronym_variants)\n",
    "        \n",
    "acronym_pattern = create_acronym_variants(acronyms)\n",
    "\n",
    "# Ensure the numbers follow the correct format\n",
    "number_pattern = r'''\n",
    "    (?:\n",
    "        \\d{1,6}[-/]\\d{2}[-/]\\d{2}  # Format: 31067-11-11\n",
    "        | \\d{1,6}[-/]\\d{1,6}         # Format: 895/09\n",
    "        | \\d{1,6}-\\d{2}-\\d{2}        # Format: 31067-11-11 (hyphenated)\n",
    "    )\n",
    "'''\n",
    "citation_pattern = fr'''\n",
    "    (?<!\\w)                      # Ensure no letter before\n",
    "    ([א-ת]?)                     # Optional single Hebrew prefix letter (but no isolated matches)\n",
    "    ({acronym_pattern})           # Captures acronym (short & long)\n",
    "    \\.?                          # Optional dot after acronym\n",
    "    \\s*                          # Optional spaces\n",
    "    (\\((.*?)\\))?                  # Optional court location in parentheses\n",
    "    \\s*[-/]?\\s*                  # Required space or separator before case number\n",
    "    ({number_pattern})            # Captures case number formats\n",
    "    (?!\\w)                       # Ensure no letter after\n",
    "'''.strip()\n",
    "\n",
    "# Compile regex with verbose flag for readability\n",
    "citation_regex = re.compile(citation_pattern, re.VERBOSE)\n",
    "\n",
    "# Test the regex with example text\n",
    "test_text = 'בעפ\"ג (ב\"ש) 31067-11-11'\n",
    "match = citation_regex.search(test_text)\n",
    "if match:\n",
    "    print(f\"Matched: {match.group()}\")\n",
    "else:\n",
    "    print(\"No match\")\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"בת״פ 18402-08-13\",\n",
    "    \"תפ״א 12345-01-22\",\n",
    "    \"ת״פ 12345-01-22\",\n",
    "    \"ע.פ. 567/22\",\n",
    "\"ע״פ 567/22\", \n",
    "'בת.פ. 56255-02-12',\n",
    "'עפ\"ג 6074/93'\n",
    "]\n",
    "# Extract matches\n",
    "for text in test_cases:\n",
    "    match = re.search(citation_regex, text)\n",
    "\n",
    "    if match:\n",
    "        print(f\"match.group(): {match.group()}\")\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "def extract_citations_from_csv(csv_data):\n",
    "    citations = []\n",
    "    text_column = csv_data[\"text\"].astype(str)  # Convert to string to avoid NaN issues\n",
    "    pd.set_option(\"display.max_colwidth\", None)  # Ensure full text is displayed\n",
    "    # print(\"\\n\".join(text_column))  # Print each row as a full text\n",
    "    for i, text in enumerate(text_column, 1):\n",
    "        print(f\"{i}. {text}\")\n",
    "\n",
    "    matches = text_column.str.extractall(citation_regex)  # Extract structured matches\n",
    "    print(\"Extracted Matches:\")\n",
    "    print(matches)\n",
    "\n",
    "    print(\"Extracted DataFrame:\", matches)  # Debugging step\n",
    "    \n",
    "    for _, row in matches.iterrows():\n",
    "        # Build the citation string, joining all valid elements\n",
    "        citation = \" \".join(map(str, filter(pd.notna, row))).strip()\n",
    "\n",
    "        # Clean up extra spaces\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "\n",
    "        # Optionally remove unwanted prefixes like \"ב\", \"ו\", \"ר\"\n",
    "        citation = re.sub(r\"^\\b[בוור]\\b\\s*\", \"\", citation)\n",
    "\n",
    "        # Remove invalid extra words (e.g., \"על 12\")\n",
    "        if re.match(r\"^על \\d+$\", citation):  \n",
    "            continue  # Skip invalid cases like \"על 12\"\n",
    "\n",
    "        # Fix duplicated court locations, e.g., \"(מחוזי מרכז) מחוזי מרכז\" → \"(מחוזי מרכז)\"\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "\n",
    "        # Add the cleaned citation to the list\n",
    "        citations.append(citation)\n",
    "    \n",
    "    # Return citations as a list, even if some are empty or missing optional groups\n",
    "    return citations if citations else []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 11:06:04.086039: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744099564.937118 2889771 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744099565.205763 2889771 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744099569.116398 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744099569.116480 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744099569.116485 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744099569.116490 2889771 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-08 11:06:09.538452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2889771/2638286754.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_bert.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ת\"פ 08∕207\n",
      "Processing ת\"פ 34607-03-17\n",
      "Processing תפ\"ק 19437-12-16\n",
      "Processing ת\"פ 16759-07-17\n",
      "Processing ת\"פ 58269-02-19\n",
      "Processing ת\"פ 50607-03-19\n",
      "Processing ת\"פ 41247-05-19\n",
      "Processing ת\"פ 45102-06-19\n",
      "Processing ת\"פ 18218-11-19\n",
      "Processing ת\"פ 6047-11-19\n",
      "Processing ת\"פ 65763-12-19\n",
      "Processing ת\"פ 48152-02-20\n",
      "Processing ת\"פ 28746-05-20\n",
      "Processing ת\"פ 57459-06-20\n",
      "Processing ת\"פ 12380-06-20\n",
      "Processing ת\"פ 8440-07-20\n",
      "Processing ת\"פ 24302-08-20\n",
      "Processing ת\"פ 43084-09-20\n",
      "Processing תפ\"ק 29591-10-20\n",
      "Processing ת\"פ 39690-02-21\n",
      "Processing ת\"פ 40908-02-21\n",
      "Processing ת\"פ 10393-03-21\n",
      "Processing ת\"פ 56850-03-21\n",
      "Processing תפ\"ק 7171-03-21\n",
      "Processing ת\"פ 3526-04-21\n",
      "Processing ת\"פ 986-06-21\n",
      "Processing ת\"פ 36546-08-21\n",
      "Processing ת\"פ 42879-09-21\n",
      "Processing ת\"פ 30884-10-21\n",
      "Processing ת\"פ 47780-11-21\n",
      "Processing ת\"פ 9851-11-21\n",
      "Processing ת\"פ 16014-01-22\n",
      "Processing ת\"פ 54272-09-14\n",
      "Processing ת\"פ 18588-11-10\n",
      "Processing ת\"פ 2892-07-10\n",
      "Processing ת\"פ 56235-06-13\n",
      "Processing ת\"פ 44226-07-14\n",
      "Processing ת\"פ 9343-07-14\n",
      "Processing ת\"פ 23142-09-14\n",
      "Processing ת\"פ 737-10-13\n",
      "Processing ת\"פ 16958-01-14\n",
      "Processing ת\"פ 33508-01-15\n",
      "Processing ת\"פ 17778-01-14\n",
      "Processing ת\"פ 18510-10-14\n",
      "Processing ת\"פ 55487-03-15\n",
      "Processing ת\"פ 37551-01-16\n",
      "Processing ת\"פ 9109-02-16\n",
      "Processing ת\"פ 52031-06-16\n",
      "Processing ת\"פ 24514-09-15\n",
      "Processing ת\"פ 31122-11-15\n",
      "Processing ת\"פ 46896-11-15\n",
      "Processing ת\"פ 6788-06-16\n",
      "Processing ת\"פ 11608-07-16\n",
      "Processing ת\"פ 22679-09-16\n",
      "Processing ת\"פ 63255-11-16\n",
      "Processing ת\"פ 11720-07-16\n",
      "Processing ת\"פ 57384-09-16\n",
      "Processing ת\"פ 22609-10-16\n",
      "Processing ת\"פ 58033-11-16\n",
      "Processing ת\"פ 19902-12-16\n",
      "Processing ת\"פ 701-12-16\n",
      "Processing ת\"פ 26620-12-16\n",
      "Processing ת\"פ 59971-01-17\n",
      "Processing ת\"פ 620-01-17\n",
      "Processing ת\"פ 51451-02-17\n",
      "Processing ת\"פ 30876-03-17\n",
      "Processing ת\"פ 40239-09-17\n",
      "Processing ת\"פ 37176-11-17\n",
      "Processing ת\"פ 17677-04-17\n",
      "Processing ת\"פ 7629-11-17\n",
      "Processing ת\"פ 73652-01-18\n",
      "Processing ת\"פ 5680-02-18\n",
      "Processing ת\"פ 5775-02-18\n",
      "Processing ת\"פ 58507-02-18\n",
      "Processing ת\"פ 69278-05-18\n",
      "Processing ת\"פ 16826-04-18\n",
      "Processing ת\"פ 46786-04-18\n",
      "Processing ת\"פ 39201-05-18\n",
      "Processing ת\"פ 6997-06-18\n",
      "Processing ת\"פ 67342-07-18\n",
      "Processing ת\"פ 39573-10-18\n",
      "Processing ת\"פ 58591-08-18\n",
      "Processing ת\"פ 8864-08-18\n",
      "Processing ת\"פ 809-09-18\n",
      "Processing תפ\"ח 42681-11-18\n",
      "Processing ת\"פ 68942-01-19\n",
      "Processing ת\"פ 68958-01-19\n",
      "Processing ת\"פ 45480-02-19\n",
      "Processing ת\"פ 70401-02-19\n",
      "Processing ת\"פ 14488-04-19\n",
      "Processing ת\"פ 14578-04-19\n",
      "Processing ת\"פ 4435-07-19\n",
      "Processing תפ\"ח 31343-04-19\n",
      "Processing ת\"פ 33338-02-20\n",
      "Processing ת\"פ 66437-07-20\n",
      "Processing ת\"פ 5778-09-20\n",
      "Processing ת\"פ 1338-12-20\n",
      "Processing ת\"פ 6678-12-20\n",
      "Processing ת\"פ 10139-03-21\n",
      "Processing ת\"פ 35998-11-21\n",
      "Processing ת\"פ 29863-03-22\n",
      "Processing ת\"פ 60484-03-22\n",
      "Processing ת\"פ 47042-01-22\n",
      "Processing ת\"פ 45199-01-22\n",
      "Processing ת\"פ 42162-12-21\n",
      "Processing ת\"פ 54208-06-22\n",
      "Processing ת\"פ 60934-03-22\n",
      "Processing ת\"פ 32175-07-22\n",
      "Processing ת\"פ 7867-04-22\n",
      "Processing ת\"פ 48194-05-22\n",
      "Processing ת\"פ 53625-07-22\n",
      "Processing ת\"פ 15747-08-22\n",
      "Processing ת\"פ 47987-12-22\n",
      "Processing ת\"פ 15884-08-22\n",
      "Processing ת\"פ 36801-10-22\n",
      "Processing ת\"פ 1990-11-22\n",
      "Processing ת\"פ 46786-01-23\n",
      "Processing ת\"פ 13598-05-23\n",
      "Processing ת\"פ 49875-05-23\n",
      "Processing ת\"פ 8444-04-23\n",
      "Processing ת\"פ 65592-01-23\n",
      "Processing ת\"פ 21137-11-23\n",
      "Processing ת\"פ 53640-01-23\n",
      "Processing ת\"פ 52567-05-23\n",
      "Processing ת\"פ 14553-12-23\n",
      "Processing ת\"פ 15727-06-23\n",
      "Processing ת\"פ 35088-07-24\n",
      "Processing ת\"פ 41833-08-23\n",
      "Processing ת\"פ 19985-04-14\n",
      "Processing ת\"פ 29504-09-13\n",
      "Processing ת\"פ 1276-05-15\n",
      "Processing ת\"פ 13336-05-15\n",
      "Processing ת\"פ 64418-09-16\n",
      "Processing ת\"פ 12320-10-17\n",
      "Processing ת\"פ 59578-12-21\n",
      "Processing ת\"פ 21142-01-22\n",
      "Processing ת\"פ 33898-01-22\n",
      "Processing ת\"פ 40650-03-22\n",
      "Processing ת\"פ 62924-03-22\n",
      "Processing ת\"פ 66877-03-22\n",
      "Processing ת\"פ 66922-03-22\n",
      "Processing ת\"פ 36033-05-22\n",
      "Processing ת\"פ 39259-05-22\n",
      "Processing ת\"פ 1062-08-22\n",
      "Processing ת\"פ 21724-08-22\n",
      "Processing ת\"פ 6749-09-22\n",
      "Processing ת\"פ 37606-11-22\n",
      "Processing ת\"פ 64232-11-22\n",
      "Processing ת\"פ 20664-12-22\n",
      "Processing ת\"פ 18076-01-23\n",
      "Processing ת\"פ 30353-01-23\n",
      "Processing ת\"פ 21629-02-23\n",
      "Processing ת\"פ 41988-02-23\n",
      "Processing ת\"פ 4621-02-23\n",
      "Processing ת\"פ 57179-02-23\n",
      "Processing ת\"פ 67009-02-23\n",
      "Processing ת\"פ 20257-03-23\n",
      "Processing ת\"פ 7086-04-23\n",
      "Processing ת\"פ 9457-04-23\n",
      "Processing ת\"פ 72468-05-23\n",
      "Processing ת\"פ 61816-07-23\n",
      "Processing ת\"פ 28122-06-23\n",
      "Processing ת\"פ 30009-08-23\n",
      "Processing ת\"פ 35299-09-23\n",
      "Processing ת\"פ 13373-10-23\n",
      "Processing ת\"פ 49510-11-23\n",
      "Processing ת\"פ 8306-11-23\n",
      "Processing ת\"פ 13704-12-23\n",
      "Processing ת\"פ 26004-12-23\n",
      "Processing ת\"פ 16800-01-24\n",
      "Processing ת\"פ 26596-01-24\n",
      "Processing ת\"פ 3818-01-24\n",
      "Processing ת\"פ 9804-01-24\n",
      "Processing ת\"פ 28565-05-24\n",
      "Processing ת\"פ 46424-05-24\n",
      "Processing ת.פ 1231∕02\n",
      "Processing ת\"פ 30506-01-22\n",
      "1. העבירות שעבר הנאשם חמורות.\n",
      "2. עבירות בתחום הסמים המסוכנים נושאות בחובן פוטנציאל לפגיעה משמעותית בחברה.\n",
      "3. מדובר בנגע המתפשט במהירות ומשחית חייהם של אנשים, בין אם זה ציבור המשתמשים בסמים ובין אם ציבור הנפגעים מעבירות המבוצעות ע\"י אלו המשתמשים בסמים.\n",
      "4. המשתמשים בסמים הופכים – מאזרחים עמלניים, המנהלים חיים מסודרים, למי שעומדים בשולי החברה, אינם תורמים לאחרים ולא זו אלא, שהטיפול בהם ופרנסתם מוטלים על אחרים.\n",
      "5. בנוסף, כתוצאה מעבירות אלה, מועברים מיד ליד כספים בלתי מדווחים בהיקף עצום, חלקם מוצאים דרכם למימון פעילות עבריינית בתחומים שונים.\n",
      "6. כל זאת, בשל עבריינים המחזיקים ומפיצים סמים מסוכנים, בדומה לנאשם דנן.\n",
      "7. על אף רוחות שונות הנושבות בתחום זה, גם סם מסוכן מסוג קנביס, הוא חומר ממכר, ששימוש מתמשך בו עלול לגרום נזקים לא קלים בתחום התובנה, תפיסת המציאות ובייחוד בתחום התפקודי והתעסוקתי, ומכאן גם – נזקים חברתיים.\n",
      "8. על חומרתן של עבירות סחר בסם מסוכן, ראו ע\"פ 2596/18 זנזורי נ' מדינת ישראל (פורסם במאגרים):\n",
      "9. צרכנים ומשתמשים ואנשים נורמטיביים, שבעבר לא היו נכונים ליטול על עצמם סיכון להסתבך בעולם הפלילי, נכונים כיום לילך צעד נוסף ולהפוך למגדלים ולסוחרים בסם. זאת, מתוך תפיסה שגויה כי מדובר ב\"סמים קלים\", ובהינתן הטכנולוגיה המאפשרת מכירה והפצה קלה ו\"סטרילית\" של סמים. ברם, סחר בסמים הוא סחר בסמים. ידע כל מי שמהרהר בדרכים לעשיית כסף קל, כי מדיניות הענישה לא השתנתה ובית המשפט רואה בחומרה עבירות של סחר והפצה של סמים מסוכנים, גם סמים \"קלים\", תוך הטלת ענישה משמעותית ומרתיעה. צרכנים ומשתמשים – ראו הוזהרתם (ההדגשה אינה במקור).\n",
      "10. עוד ראו רע\"פ 8695/19 פסו נ' מדינת ישראל (פורסם במאגרים):\n",
      "11. בית משפט זה שב והדגיש כי על מדיניות הענישה בגין סחר והפצה של סמים מסוכנים, ובכללן סמים הנחשבים כ\"סמים קלים\", להיות מרתיעה ומשמעותית, וכי יש לראות בחומרה ניסיונות לעשיית רווח כספי באמצעות סחר בסמים אלה.\n",
      "12. (ההדגשות - אינן במקור).\n",
      "13. בנוגע לעבירה של החזקת סם שלא לצריכה עצמית, ראו דברי בית המשפט העליון במסגרת ע\"פ 1345/08 איסטחרוב נ' מדינת ישראל (פורסם במאגרים):\n",
      "14. אין מנוס מהכבדת היד על המחזיקים סמים שלא לצריכה עצמית, שכל בר דעת מבין כי נועדו לצריכת הזולת, קרי, להוספת שמן על מדורת הסמים אשר להבותיה אופפות רבים וטובים, או רבים שהיו טובים. עבירה זו היא תאומתה הסטטוטורית של עבירת הסחר בסמים, אלא שלא ניתן להוכיח לגביה את הסחר עצמו, ונקבע לשתיהן עונשה זהה, עונש מירבי של עשרים שנות מאסר וקנס פי עשרים וחמישה מזה הקבוע בסעיף 61(א)(4) לחוק העונשין, העומד כיום על 202,000 ₪...\n",
      "15. (ההדגשות - אינן במקור).\n",
      "16. על בית המשפט להרתיע היחיד והרבים מפני עבירות הסמים, ולהילחם בתופעה הקשה של נגע הסמים, הצוברת תאוצה בשנים האחרונות.\n",
      "17. עפ\"ג 30646-10-23 חמאמדה נ' מדינת ישראל – המערער הורשע, על פי הודאתו, בשתי עבירות של סחר בסם מסוכן מסוג קנאביס ועבירה אחת של החזקת סם מסוכן מסוג קנאביס שלא לצריכתו העצמית. בהתאם לעובדות כתב האישום המתוקן, סחר הנאשם בסם מסוכן מסוג קנאביס במשקל של 13.02 גרם תמורת 400 ₪; סחר בסם מסוכן מסוג קנאביס במשקל 83.97 גרם תמורת 2,400 ₪ וכן החזיק בסם מסוכן מסוג קנאביס במשקל 42.49 גרם שלא לצריכתו העצמית. מותב זה קבע מתחם ענישה הנע בין 16 עד 30 חודשי מאסר בפועל. לאור נסיבות אישיות חריגות, מצא בית המשפט לחרוג ממתחם הענישה וגזר על הנאשם 12 חודשי מאסר בפועל; מאסרים על תנאי; קנס; פסילה בפועל ועל תנאי של רישיון הנהיגה. ערעורו על חומרת העונש התקבלה בנוגע לרכיב רישיון הנהיגה בלבד, וזאת בשים לב לנסיבות אישיות.\n",
      "18. עפ\"ג 4842-05-23 בן נח נ' מדינת ישראל – המערער הורשע, על פי הודאתו, בעבירות של סיוע לסחר בסם מסוכן מסוג קנביס והחזקת סם שלא לצריכה עצמית. בהתאם לעובדות כתב האישום המתוקן, סייע הנאשם לאחר לסחור בסם מסוכן מסוג קנביס במשקל 9.94 גרם תמורת 500 ₪, ובאותו מעמד, החזיק ברכבו בסם מסוכן מסוג קנביס, במשקל 530 גרם, שלא לצריכתו העצמית. מותב זה קבע מתחם ענישה הנע בין 12 ועד 24 חודשי מאסר בפועל, וגזר על הנאשם 15 חודשי מאסר, לצד ענישה נלווית וחילוט הרכב בו הסתייע. בשל נסיבות מיוחדות של נאשם זה, הסכימה המשיבה לבקשת ההגנה להפחית את עונש המאסר ל-11 חודשי מאסר וביטול פסילת רישיון הנהיגה בפועל, כאשר לא היה כל שינוי ביתר רכיבי גזר הדין. בית המשפט המחוזי (כב' השופט א' ביתן) ציין, כי העונש שהוטל על הנאשם בבית המשפט קמא הינו מתאים למעשה העבירה.\n",
      "19. ת\"פ 64772-05-22 מדינת ישראל נ' אבו בלאל – הנאשם הורשע, על פי הודאתו, בעבירה של ניסיון לסחר בסם מסוכן מסוג קנביס במשקל 850 גרם, תמורת 7,500 ₪. מותב זה אימץ את מתחם הענישה אליו עתרה התביעה והעמידו כך שינוע בין 10 ועד 20 חודשי מאסר, תוך שציין בגזר הדין, כי ראוי היה לעתור למתחם ענישה גבוה יותר.\n",
      "20. ת\"פ 5072-11-21 מדינת ישראל נ' אבן עתמי – הנאשם הורשע, על פי הודאתו, בעבירות של קשירת קשר לעשות פשע, סחר בסם מסוכן מסוג קנביס והחזקת סם מסוכן מסוג קנביס שלא לצריכה עצמית. בעסקה הראשונה, מכר הנאשם לסוכן סם מסוכן מסוג קנביס במשקל 24 גרם נטו, תמורת 400 ₪; בעסקה השניה, מכר הנאשם לסוכן סם מסוכן מסוג קנביס במשקל 14.85 גרם נטו תמורת 300 ₪; בעסקה השלישית, מכר הנאשם לאחר סם מסוכן מסוג קנביס במשקל 7.08 גרם תמורת 300 ₪ ובאותו מעמד, החזיק ברכבו בסם מסוכן מסוג קנביס במשקל 77.06 גרם, שלא לצריכתו העצמית. מותב זה קבע מתחם ענישה הנע בין 10 ועד 20 חודשי מאסר.\n",
      "21. ת\"פ 54272-11-23 מדינת ישראל נ' קורז'ביץ' – הנאשם הורשע, על פי הודאתו במסגרת הסדר, בעבירות של סחר בסמים; החזקת סם שלא לצריכה עצמית; החזקת סם לצריכה עצמית. בעסקה הראשונה, סחר הנאשם בסם מסוכן מסוג קנביס במשקל 20 גרם תמורת 1,000 ₪ לסוכן משטרתי. באותו מעמד, החזיק, בנוסף, ב-10 גרם קנביס ו-5 יחידות סם מסוכן מסוג ADB-BUTINACA; בעסקה השניה, סחר הנאשם בסם מסוכן מסוג קנביס במשקל 10 גרם תמורת 400 ₪; בעסקה השלישית, סחר הנאשם בסם מסוכן מסוג קנביס במשקל 20 גרם תמורת 800 ₪; הנאשם החזיק בסם מסוכן מסוג קנביס במשקל 10 גרם. מותב זה קבע מתחם ענישה הנע בין 18 ועד 36 חודשי מאסר.\n",
      "22. במקרה דנן, קיימות מספר נסיבות לחומרה, שיש בהן כדי להעיד על פעילות עקבית ומאורגנת בתחום הסחר בסמים מסוכנים, וכן על היות הנאשם בדרג משמעותי בהיררכיה העבריינית: הנאשם קיבל את פרטי ה\"לקוחות\" מאדם אחר, אשר אליו היו פונים אנשים באמצעות הטלגרם לצורך רכישה של סמים מסוכנים; הנאשם יצר קשר עם הפונים;  ערך עסקאות עם לקוחות שנחזו להיות מזדמנים; הנאשם סיפק הסמים מידית לאחר הסיכום, ומכאן שהיו נגישים בעבורו סמים באופן זמין; העסקאות לא נערכו בכמויות קטנות, אלא בכמויות בינוניות; בחלק מהמקרים, הסיע הנאשם את האחר למקום העסקאות; בנוסף, נהג הנאשם ברכבו, כאשר הוא נוהג תחת השפעת סמים מסוכנים, והחזיק בסמים מסוכנים ברכבו.\n",
      "23. לאור ריבוי העסקאות; טיב הסם; כמות הסם בה סחר הנאשם; סכומי הכסף שקיבל לידיו במסגרת העסקאות; כמות הסם שהחזיק הנאשם שלא לצריכתו העצמית ברכבו, תוך שהוא נוהג ללא רישיון נהיגה בתוקף ושעה שהוא תחת השפעת סמים מסוכנים – מוצא בית המשפט לאמץ את מתחם הענישה אליו עתרה התביעה ולהעמידו כך שינוע בין 30 ועד 50 חודשי מאסר בפועל.\n",
      "24. לחובת הנאשם, הרשעות קודמות בעבירות סמים; רכוש; ואלימות.\n",
      "25. הנאשם עבר את העבירות דנן, שעה שלחובתו שני מאסרים מותנים ברי הפעלה, אשר לא היוו גורם מרתיע עבורו.\n",
      "26. עוד לחובתו, הרשעות בעבירות תעבורה.\n",
      "27. משורת הדין, ראוי היה לגזור עונשו של הנאשם בחלקו הבינוני-גבוה של מתחם הענישה.\n",
      "28. אלא, שהנאשם פנה לעזרה עוד בשלב מעצרו, והשתלב בקהילה טיפולית, שם עבר טיפול אינטנסיבי, ועבר בהצלחה את כל שלבי הקהילה הטיפולית. לאחר מכן, השתלב הנאשם בהוסטל, וסיים שלב טיפולי זה בהצלחה גם כן.\n",
      "29. עם שחרורו מההוסטל, השתלב הנאשם במעגל התעסוקה, תוך שהחל להתפרנס מעבודה יציבה. במקביל, שולב הנאשם בהליך טיפולי נוסף במסגרת \"בית חוסן\", שם ממשיך בשיחות פרטניות.\n",
      "30. הנאשם גדל בנסיבות חיים קשות, ובסופו של דבר, נדחק לשולי החברה, התערה עם גורמים עברייניים והתמכר לסמים. חרף האמור, הצליח הנאשם לגייס כוחותיו, התמיד בהליך הטיפולי, נגמל מסמים ובדיקות שעבר לאיתור שרידי סם – יצאו שליליות.\n",
      "31. מתסקירי שירות המבחן עולה, כי הנאשם עבר כברת דרך משמעותית, פניו לשיקום, והוא מבטא שאיפות לשמר את אורח החיים הנורמטיבי ושומר החוק שהחל בו.\n",
      "32. בסופו של יום – חזר בו שירות המבחן מהמלצתו על ענישה בדמות מאסר בעבודות שירות והמליץ על ענישה שיקומית בדמות צו של\"צ לצד צו מבחן, במסגרתו ימשיך הנאשם בהליך הטיפולי.\n",
      "33. בית המשפט התרשם, כי הנאשם נוטל אחריות על מעשיו, מביע חרטה עמוקה עליהם, ומנסה בכל מאודו לנהל חיים אזרחיים עמלניים תוך פרנסת בני משפחתו.\n",
      "34. במכלול הנסיבות, לאור כברת הדרך הטיפולית המשמעותית שעבר הנאשם, אשר יש בה כדי להצביע על זניחת דרכו העבריינית – מבלי שיהיה בכך בבחינת תקדים למקרים אחרים, נוטה הכף לחריגה ממתחם הענישה מטעמי שיקום הנאשם, ומתוף סמכותו של בית המשפט בהתאם להוראות סעיף 40ד' לחוק העונשין, תשל\"ז – 1977, מוצא בית המשפט לאמץ המלצות שירות המבחן בענינו, ולהסתפק בעונש חינוכי במסגרת צו של\"צ. זאת, כמובן, בתוספת מאסר מותנה מרתיע, לבל יהין הנאשם לעבור שוב עבירות דומות.\n",
      "35. אשר למאסרים המותנים מגזרי הדין ת/2 ו-ת/3, מתוקף סמכותו של בית המשפט בהתאם להוראות סעיף 85 לחוק העונשין, תשל\"ז – 1977, לאחר שהנאשם עבר, כאמור, הליך טיפולי משמעותי בתחום הסמים המסוכנים, מוצא בית המשפט להאריך את שני המאסרים לתקופה נוספת, אשר יהיה בה כדי להוות גורם מרתיע לנאשם.\n",
      "36. בשל המניע הכלכלי העומד ברקע לעבירות סמים, מוצא בית המשפט להטיל עיצום כספי מסוג קנס, אולם, לאור מצבו הכלכלי של הנאשם ועל מנת שלא לפגוע בסיכויי שיקומו, לא יועמד על הצד הגבוה.\n",
      "37. עוד מוצא בית המשפט, כי יש להטיל פסילה בפועל ועל תנאי של רישיון הנהיגה וזאת מתוקף סמכותו בהתאם להוראות סעיף 37א(א) לפקודת הסמים המסוכנים [נוסח חדש], תשל\"ג – 1973 וכן סעיף 43 לפקודת התעבורה [נוסח חדש], תשכ\"א - 1961. הנאשם נהג ברכבו, שעה שתוקף רישיון הנהיגה שלו פקע מעל שנה; בזמן שהוא מחזיק ברכבו סמים מסוכנים; ובזמן שהוא נוהג תחת השפעת סמים מסוכנים. אופן נהיגתו של הנאשם הקים סיכון של ממש לשלומם ולחייהם של המשתמשים בדרך וחרף ההליך השיקומי שעבר, מחייב הרחקתו מהכביש לתקופה משמעותית.\n",
      "38. לאחר שבית המשפט עיין בטיעוני הצדדים בכתב; שמע טיעוני הצדדים על פה; עיין בראיות לעונש; עיין בתסקירי שירות המבחן למבוגרים; עיין בפסיקה; ולאחר ששמע דברו האחרון של הנאשם; גוזר על הנאשם את העונשים כדלקמן:\n",
      "39. הארכת תקופת המאסר המותנה בן 8 חודשים מגזר הדין ת/2 למשך שנתיים נוספות מהיום;\n",
      "40. הארכת תקופת המאסר המותנה בן 4 חודשים מגזר הדין ת/3 למשך שנתיים נוספות מהיום;\n",
      "41. קנס בסך 7,500 ₪ או 60 ימי מאסר תמורתו. הקנס ישולם ב - 7 תשלומים שווים החל מיום 15.09.24 ובכל 15 לחודש העוקב. לא יעמוד הנאשם באחד התשלומים במועד – תעמוד היתרה לפירעון מידי;\n",
      "42. הנאשם יצהיר על התחייבות בסך 5,000 ₪ להימנע, בתוך שלוש שנים מהיום, מכל עבירה המפעילה את אחד המאסרים המותנים. לא יצהיר הנאשם על ההתחייבות היום – ייאסר למשך 21 יום;\n",
      "43. הנאשם ירצה עבודות של\"צ בהיקף של 300 שעות, בהתאם לתכנית השל\"צ שתגובש ע\"י שירות המבחן ותוגש לבית המשפט בתוך 30 יום מהיום. על הנאשם להתייצב, בתוך 7 ימים מהיום, בשירות המבחן, לקבלת הוראות בנוגע לריצוי עבודות השל\"צ. הנאשם מוזהר, כי אי התייצבות לריצוי עבודות השל\"צ, או אי שיתוף פעולה בנוגע לריצוין – עלול להביא להפקעת צו השל\"צ ולדיון מחדש בשאלת העונש בתיק זה, על כל המשתמע מכך;\n",
      "44. הנאשם יעמוד במבחן למשך שנה ומחצה מהיום. במסגרת צו המבחן, יהיה עליו להשתתף בכל הליך טיפולי או מעקבי כפי שיומלץ ע\"י שירות המבחן. על הנאשם להתייצב, בתוך 7 ימים מהיום, בשירות המבחן לקבלת הוראות מתאימות. הנאשם מוזהר, כי אי שיתוף פעולה במסגרת צו המבחן עלול להביא להפקעתו ולדיון מחדש בשאלת העונש בתיק זה, על כל המשתמע מכך;\n",
      "45. פסילה בפועל, מקבל או מהחזיק רשיון נהיגה לרכב מנועי, למשך שנתיים החל מהיום. על הנאשם להפקיד רשיונו, או תצהיר מתאים, במזכירות בית משפט השלום בבאר שבע עד יום העבודה הבא, קרי, 17.07.24, שעה 12:00. מובהר לנאשם, כי כל עוד לא הופקד הרשיון או התצהיר – יהיה פסול מלנהוג, אך הפסילה לא תימנה;\n",
      "46. פסילה מקבל ומהחזיק רשיון נהיגה לרכב מנועי, בת  6 חדשים, על תנאי, תקופת התנאי -למשך 3 שנים מסיום הפסילה בפועל;\n",
      "47. השמדת הסמים - בחלוף תקופת הערעור.\n",
      "48. עותק גזר הדין יועבר לשירות המבחן למבוגרים ולממונה על עבודות השירות בשב\"ס.\n",
      "49. 51293715129371צו מבחן יוגש לחתימה בתוך 30 יום מהיום. \n",
      "50. 5467831354678313הודעה זכות הערעור.\n",
      "51. ניתנה היום, י' תמוז תשפ\"ד, 16 יולי 2024, במעמד הצדדים.\n",
      "52. בעניין עריכה ושינויים במסמכי פסיקה, חקיקה ועוד באתר נבו – הקש כאן\n",
      "Extracted Matches:\n",
      "             0     1    2    3            4\n",
      "    match                                  \n",
      "138 0      NaN   ע\"פ  NaN  NaN      2596/18\n",
      "140 0        ר   ע\"פ  NaN  NaN      8695/19\n",
      "143 0      NaN   ע\"פ  NaN  NaN      1345/08\n",
      "147 0      NaN  עפ\"ג  NaN  NaN  30646-10-23\n",
      "148 0      NaN  עפ\"ג  NaN  NaN   4842-05-23\n",
      "149 0      NaN   ת\"פ  NaN  NaN  64772-05-22\n",
      "150 0      NaN   ת\"פ  NaN  NaN   5072-11-21\n",
      "151 0      NaN   ת\"פ  NaN  NaN  54272-11-23\n",
      "Extracted DataFrame:              0     1    2    3            4\n",
      "    match                                  \n",
      "138 0      NaN   ע\"פ  NaN  NaN      2596/18\n",
      "140 0        ר   ע\"פ  NaN  NaN      8695/19\n",
      "143 0      NaN   ע\"פ  NaN  NaN      1345/08\n",
      "147 0      NaN  עפ\"ג  NaN  NaN  30646-10-23\n",
      "148 0      NaN  עפ\"ג  NaN  NaN   4842-05-23\n",
      "149 0      NaN   ת\"פ  NaN  NaN  64772-05-22\n",
      "150 0      NaN   ת\"פ  NaN  NaN   5072-11-21\n",
      "151 0      NaN   ת\"פ  NaN  NaN  54272-11-23\n",
      "ע\"פ 2596/18\n",
      "ע\"פ 8695/19\n",
      "ע\"פ 1345/08\n",
      "עפ\"ג 30646-10-23\n",
      "עפ\"ג 4842-05-23\n",
      "ת\"פ 64772-05-22\n",
      "ת\"פ 5072-11-21\n",
      "ת\"פ 54272-11-23\n",
      "Processed document saved to: /home/liorkob/M.Sc/thesis/data/drugs/tag_citations/ת\"פ 30506-01-22.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import docx\n",
    "import re\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Define required sections and citation patterns\n",
    "required_parts = [\n",
    "    \"מתחמי ענישה\", \"אחידות בענישה\", \"מתחם הענישה\", \"מתחם ענישה\", \"דיון\",\n",
    "    \"ענישה נהוגה\", \"הענישה הנוהגת\", \"ענישה נוהגת\", \"מתחם העונש\", \"מתחם עונש\",\n",
    "    \"מדיניות הענישה\", \"והכרעה\", \"ההרשעה\", \"מדיניות הענישה הנהוגה\"\n",
    "]\n",
    "\n",
    "\n",
    "# Check for CUDA availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the trained BERT model and tokenizer\n",
    "model_path = \"/home/liorkob/classifier_relvant_citation_model.pt\" \n",
    "tokenizer_bert = BertTokenizer.from_pretrained('avichr/heBERT')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('avichr/heBERT', num_labels=2)\n",
    "model_bert.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "\n",
    "\n",
    "def split_preserving_structure(text):\n",
    "    paragraphs = re.split(r'(?<=\\d\\.)\\s', text)  # Split after numbers followed by a period\n",
    "    return [para.strip() for para in paragraphs if para.strip()]\n",
    "\n",
    "def query_gpt(text,citation):\n",
    "    \"\"\"\n",
    "    Queries GPT-4o to extract and segment legal citations.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Given the following legal text:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Your task is to extract **only** the part of the text that directly relates to the citation \"{citation}\".\n",
    "    \n",
    "    **Extraction Rules:**\n",
    "    - **Do not modify any wording.** Keep the original phrasing exactly as it appears in the provided document.\n",
    "    - **Do not summarize or rephrase.**\n",
    "    - **Return only the relevant portion**, not the full text.\n",
    "    - **Handle grouped citations carefully:**\n",
    "        - If the citation appears in a list following \"ראו למשל ...\" or similar, include the preceding explanation that applies to all citations.\n",
    "        - Do not include other citations from the list—return only the text relevant to \"{citation}\".\n",
    "    - **Handle case explanations properly:**\n",
    "        - If the citation is explained in a specific section (e.g., \"בע\"פ 9373/10 ותד נ' מדינת ישראל...\"), extract the **entire explanation** of the case.\n",
    "        - Do not remove any important context about the court ruling.\n",
    "    - Do **not** extract only \"(רע\"פ 2718/04)\" without the legal principle it supports.\n",
    "\n",
    "\n",
    "    Only return the extracted text. Do not include unrelated content or formatting.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to extract and structure legal citations.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        processed_text = response.choices[0].message.content\n",
    "\n",
    "\n",
    "            # Debug: Print the extracted response from GPT\n",
    "        # print(\"\\n===== DEBUG: GPT RESPONSE =====\")\n",
    "        # print(f\"Citation: {citation}\")\n",
    "        # print(\"Prompt Sent to GPT:\")\n",
    "        # print(prompt)\n",
    "        # print(\"Extracted Text:\")\n",
    "        # print(processed_text)\n",
    "        # print(\"==============================\\n\")\n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"🚨 GPT API error: {e}\")\n",
    "        return [text]  # Return original text in case of failure\n",
    "def filter_csv_relevant_parts(csv_data):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a required part in the CSV and all subsequent rows.\n",
    "    \"\"\"\n",
    "    start_index = None\n",
    "\n",
    "    # Find the first row containing a required part\n",
    "    for idx, row in csv_data.iterrows():\n",
    "        if any(req_part in str(row.get(\"part\", \"\")) for req_part in required_parts):\n",
    "            start_index = idx\n",
    "            break\n",
    "\n",
    "    # If a match is found, return only relevant rows\n",
    "    if start_index is not None:\n",
    "        return csv_data.iloc[start_index:]\n",
    "    else:\n",
    "        return pd.DataFrame(columns=csv_data.columns)  # Return an empty DataFrame if no matches found\n",
    "\n",
    "\n",
    "\n",
    "# Function to find all occurrences of a citation in the document\n",
    "def find_all_occurrences(doc, citation):\n",
    "    indices = []\n",
    "    for i, paragraph in enumerate(doc.paragraphs):\n",
    "        if citation in paragraph.text:\n",
    "            indices.append(i)  # Store all occurrences of the citation\n",
    "    return indices\n",
    "\n",
    "# Function to get relevant context for each occurrence of the citation\n",
    "def get_context_paragraphs(doc, index, citation):\n",
    "    context_text = []\n",
    "\n",
    "    # Search for the closest non-empty previous paragraph\n",
    "    prev_index = index - 1\n",
    "    while prev_index >= 0 and not doc.paragraphs[prev_index].text.strip():\n",
    "        prev_index -= 1  # Move backwards until finding text\n",
    "\n",
    "    if prev_index >= 0:\n",
    "        context_text.append(doc.paragraphs[prev_index].text.strip())\n",
    "\n",
    "    # Get the current paragraph (must exist, but check if empty)\n",
    "    curr_text = doc.paragraphs[index].text.strip()\n",
    "    if curr_text:\n",
    "        context_text.append(curr_text)\n",
    "    else:\n",
    "        print(f\"⚠️ Warning: Empty paragraph for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None  # Skip this occurrence if the current paragraph is empty\n",
    "\n",
    "    # Search for the closest non-empty next paragraph\n",
    "    next_index = index + 1\n",
    "    while next_index < len(doc.paragraphs) and not doc.paragraphs[next_index].text.strip():\n",
    "        next_index += 1  # Move forward until finding text\n",
    "\n",
    "    if next_index < len(doc.paragraphs):\n",
    "        context_text.append(doc.paragraphs[next_index].text.strip())\n",
    "\n",
    "    # Ensure we have at least one non-empty paragraph\n",
    "    if not context_text:\n",
    "        print(f\"⚠️ Warning: No valid text found for citation {citation} at index {index}. Skipping occurrence.\")\n",
    "        return None\n",
    "\n",
    "    return \"\\n\".join(context_text).strip()\n",
    "\n",
    "\n",
    "# Function to process and tag document paragraphs\n",
    "def process_and_tag_with_split(docx_path: str, csv_path: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Process a .docx document and its corresponding CSV, find relevant paragraphs with context, \n",
    "    extract relevant text using GPT, tag with BERT, and store results.\n",
    "    \"\"\"\n",
    "    doc = docx.Document(docx_path)\n",
    "    csv_data = pd.read_csv(csv_path)\n",
    "    filtered_csv_data = filter_csv_relevant_parts(csv_data)\n",
    "\n",
    "    citations = extract_citations_from_csv(filtered_csv_data)\n",
    "    results = []\n",
    "\n",
    "    for citation in citations:\n",
    "        citation_indices = find_all_occurrences(doc, citation)  # Find all occurrences\n",
    "\n",
    "        # Collect all contexts where the citation appears\n",
    "        merged_contexts = []\n",
    "        for index in citation_indices:\n",
    "            full_context = get_context_paragraphs(doc, index, citation)\n",
    "            if full_context:\n",
    "                merged_contexts.append(full_context)\n",
    "\n",
    "        # If no valid contexts found, skip this citation\n",
    "        if not merged_contexts:\n",
    "            continue  \n",
    "\n",
    "        # Merge all valid contexts into one, ensuring uniqueness\n",
    "        final_context = \"\\n\".join(set(merged_contexts)).strip()  # Remove duplicates\n",
    "        print(citation)\n",
    "        # print(final_context)\n",
    "\n",
    "        # Ask GPT to extract the relevant part\n",
    "        extracted_text = query_gpt(final_context, citation)\n",
    "\n",
    "        # Tag the extracted text with BERT\n",
    "        encoding = tokenizer_bert(extracted_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "        encoding = {key: val.to(device) for key, val in encoding.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model_bert(**encoding)\n",
    "            prediction = torch.argmax(output.logits, dim=-1).item()\n",
    "\n",
    "        # Store only one result per citation\n",
    "        result = {\n",
    "            'citation': citation,\n",
    "            'context_text': final_context,\n",
    "            'extracted_text': extracted_text,\n",
    "            'predicted_label': prediction\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    # Save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Processed document saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    docx_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs/docx')\n",
    "    csv_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs/docx_csv')\n",
    "    output_directory = Path('/home/liorkob/M.Sc/thesis/data/drugs/tag_citations')\n",
    "\n",
    "    output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for file_path in docx_directory.glob(\"*.docx\"):\n",
    "        new_file_path = file_path.stem\n",
    "        print(f\"Processing {new_file_path}\")\n",
    "\n",
    "        csv_file = csv_directory / f\"{new_file_path}.csv\"\n",
    "        \n",
    "        if file_path.exists() and csv_file.exists():\n",
    "            output_file = output_directory / f\"{file_path.stem}.csv\"\n",
    "            if output_file.exists():\n",
    "                continue\n",
    "            process_and_tag_with_split(str(file_path), str(csv_file), str(output_file))\n",
    "        else:\n",
    "            if not file_path.exists():\n",
    "                print(f\"Document file not found: {file_path}\")\n",
    "            if not csv_file.exists():\n",
    "                print(f\"CSV file not found for: {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get URLS from docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from docx.oxml.ns import qn\n",
    "from docx.opc.constants import RELATIONSHIP_TYPE as RT\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "\n",
    "def normalize_citation(citation):\n",
    "    \"\"\"Normalize citation by removing prefixes and standardizing format.\"\"\"\n",
    "    if not citation:\n",
    "        return None\n",
    "    # Standardize quotes\n",
    "    citation = citation.replace('״', '\"').replace('״', '\"').replace('״', '\"')\n",
    "    # Remove extra spaces\n",
    "    citation = re.sub(r'\\s+', ' ', citation).strip()\n",
    "    # Remove common prefixes, including רע\"פ\n",
    "    citation = re.sub(r'^(ע\"?פ|ת\"?פ|עפ\"?ג|רע\"?פ)\\s+', '', citation)\n",
    "    return citation\n",
    "\n",
    "\n",
    "# citation_patterns = {\n",
    "#     'ע\"פ': r'ע\"פ (\\d+/\\d+)',\n",
    "#     'עפ\"ג': r'עפ\"ג (\\d+/\\d+)',\n",
    "#     'ת״פ': r'ת״פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "#     'עפ״ג': r'עפ״ג (\\d+/\\d+)',\n",
    "#     'רע״פ': r'רע״פ (\\d+/\\d+)',\n",
    "#     'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "# }\n",
    "\n",
    "# def extract_citations(text):\n",
    "#     \"\"\"Extracts citations from the paragraph_text column based on predefined patterns.\"\"\"\n",
    "#     matches = []\n",
    "#     for label, pattern in citation_patterns.items():\n",
    "#         found = re.findall(pattern, text)\n",
    "#         matches.extend([f\"{label} {m}\" for m in found])\n",
    "#     return matches[0] if matches else None\n",
    "\n",
    "def extract_citations(text):\n",
    "    \"\"\"Extracts legal citations from a single text string.\"\"\"\n",
    "    matches = citation_regex.findall(text)\n",
    "    citations = []\n",
    "    for match in matches:\n",
    "        citation = \" \".join(filter(None, match)).strip()\n",
    "        citation = re.sub(r\"\\s{2,}\", \" \", citation)\n",
    "        citation = re.sub(r\"^\\b[בוור]\\b\\s*\", \"\", citation)\n",
    "        citation = re.sub(r\"\\((.*?)\\)\\s+\\1\", r\"(\\1)\", citation)\n",
    "        if not re.match(r\"^על \\d+$\", citation):\n",
    "            citations.append(citation)\n",
    "    return citations[0] if citations else None\n",
    "\n",
    "\n",
    "def getLinkedText(soup):\n",
    "    links = []\n",
    "    for tag in soup.find_all(\"hyperlink\"):\n",
    "        try:\n",
    "            links.append({\"id\": tag[\"r:id\"], \"text\": tag.text})\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    for tag in soup.find_all(\"instrText\"):\n",
    "        if \"HYPERLINK\" in tag.text:\n",
    "            parts = tag.text.split('\"')\n",
    "            if len(parts) > 1:  # Ensure the URL exists before accessing index 1\n",
    "                url = parts[1]\n",
    "            else:\n",
    "                print(f\"⚠️ Warning: No valid URL found in HYPERLINK tag: {tag.text}\")\n",
    "                url = None  # Assign None if URL is missing\n",
    "\n",
    "            temp = tag.parent.next_sibling\n",
    "            text = \"\"\n",
    "\n",
    "            while temp is not None:\n",
    "                maybe_text = temp.find(\"t\")\n",
    "                if maybe_text is not None and maybe_text.text.strip() != \"\":\n",
    "                    text += maybe_text.text.strip()\n",
    "                maybe_end = temp.find(\"fldChar[w:fldCharType]\")\n",
    "                if maybe_end is not None and maybe_end[\"w:fldCharType\"] == \"end\":\n",
    "                    break\n",
    "                temp = temp.next_sibling\n",
    "\n",
    "            links.append({\"id\": None, \"href\": url, \"text\": text})\n",
    "    return links\n",
    "def getURLs(soup, links):\n",
    "    for link in links:\n",
    "        if \"href\" not in link:\n",
    "            for rel in soup.find_all(\"Relationship\"):\n",
    "                if rel[\"Id\"] == link[\"id\"]:\n",
    "                    link[\"href\"] = rel[\"Target\"]\n",
    "    return links\n",
    "\n",
    "import zipfile\n",
    "\n",
    "def extract_hyperlinks(docx_path):\n",
    "    \"\"\"\n",
    "    Extracts hyperlinks from a .docx file and returns a dictionary \n",
    "    where the linked text is mapped to its corresponding URL.\n",
    "    \"\"\"\n",
    "    # Open the .docx file as a zip archive\n",
    "    try:\n",
    "        archive = zipfile.ZipFile(docx_path, \"r\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"❌ Error: Cannot open {docx_path} (Bad ZIP format)\")\n",
    "        return {}\n",
    "\n",
    "    # Extract main document XML\n",
    "    try:\n",
    "        file_data = archive.read(\"word/document.xml\")\n",
    "        doc_soup = BeautifulSoup(file_data, \"xml\")\n",
    "        linked_text = getLinkedText(doc_soup)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No document.xml found in {docx_path}\")\n",
    "        return {}\n",
    "\n",
    "    # Extract hyperlink relationships from _rels/document.xml.rels\n",
    "    try:\n",
    "        url_data = archive.read(\"word/_rels/document.xml.rels\")\n",
    "        url_soup = BeautifulSoup(url_data, \"xml\")\n",
    "        links_with_urls = getURLs(url_soup, linked_text)\n",
    "    except KeyError:\n",
    "        print(f\"⚠️ Warning: No _rels/document.xml.rels found in {docx_path}\")\n",
    "        links_with_urls = linked_text\n",
    "\n",
    "    # Extract footnotes (if available)\n",
    "    try:\n",
    "        footnote_data = archive.read(\"word/footnotes.xml\")\n",
    "        footnote_soup = BeautifulSoup(footnote_data, \"xml\")\n",
    "        footnote_links = getLinkedText(footnote_soup)\n",
    "\n",
    "        footnote_url_data = archive.read(\"word/_rels/footnotes.xml.rels\")\n",
    "        footnote_url_soup = BeautifulSoup(footnote_url_data, \"xml\")\n",
    "        footnote_links_with_urls = getURLs(footnote_url_soup, footnote_links)\n",
    "\n",
    "        # Merge footnote links\n",
    "        links_with_urls += footnote_links_with_urls\n",
    "    except KeyError:\n",
    "        pass  # No footnotes found, continue\n",
    "\n",
    "    # Convert extracted links to a dictionary: {linked_text: URL}\n",
    "    return {link[\"text\"]: link.get(\"href\", None) for link in links_with_urls}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def update_csv_with_links(csv_path, doc_path):\n",
    "    csv_path = Path(csv_path)  # Convert to Path object if not already\n",
    "    \n",
    "    # **Check if CSV is empty before reading**\n",
    "    if not csv_path.exists() or csv_path.stat().st_size == 0:  \n",
    "        print(f\"Skipping empty or missing file: {csv_path.name}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # **Check if the DataFrame is empty after loading**\n",
    "        if df.empty:\n",
    "            print(f\"Skipping empty DataFrame: {csv_path.name}\")\n",
    "            return\n",
    "        \n",
    "        # Normalize extracted citations\n",
    "        df[\"extracted_citation\"] = df[\"paragraph_text\"].apply(\n",
    "            lambda text: normalize_citation(extract_citations(text)) if pd.notna(text) else None\n",
    "        )\n",
    "        \n",
    "        # Normalize citation_links keys\n",
    "        citation_links = extract_hyperlinks(doc_path)\n",
    "        normalized_citation_links = {normalize_citation(k): v for k, v in citation_links.items()}\n",
    "        \n",
    "        # Assign URLs to citations\n",
    "        df[\"link\"] = df[\"extracted_citation\"].apply(\n",
    "            lambda text: normalized_citation_links.get(text, None) if pd.notna(text) else None\n",
    "        )\n",
    "        \n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Updated CSV saved to: {csv_path}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Skipping {csv_path.name}: CSV file is empty or unreadable.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "\n",
    "def find_matching_docx(csv_name, docx_directory):\n",
    "    normalized_csv_name = normalize_case_name(csv_name.replace('.csv', '.docx'))\n",
    "    for root, _, files in os.walk(docx_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".docx\") and normalize_case_name(file) == normalized_csv_name:\n",
    "                return os.path.join(root, file)\n",
    "    return None\n",
    "\n",
    "def process_all_csvs(csv_directory, docx_directory):\n",
    "    for root, _, files in os.walk(csv_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(root, file)\n",
    "                docx_path = find_matching_docx(file, docx_directory)\n",
    "                # if file != 'ת\"פ 49772-11-16.csv':\n",
    "                #     continue\n",
    "                if docx_path:\n",
    "                    update_csv_with_links(csv_path, docx_path)\n",
    "                else:\n",
    "                    print(f\"No matching DOCX found for: {file}\")\n",
    "\n",
    "docx_csv_dir = f\"/home/liorkob/M.Sc/thesis/data/drugs/docx_csv\"\n",
    "citations_dir = f\"/home/liorkob/M.Sc/thesis/data/drugs/tag_citations\"\n",
    "process_all_csvs(citations_dir, docx_csv_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define directories and paths\n",
    "UPDATED_CSV_DIR = \"/home/liorkob/M.Sc/thesis/data/drugs/tag_citations\"\n",
    "\n",
    "def verify_updated_files_tag_1():\n",
    "    \"\"\"Iterates over updated CSV files and prints missing links for rows where tag is 1.\"\"\"\n",
    "\n",
    "    for root, _, files in os.walk(UPDATED_CSV_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(csv_path)\n",
    "\n",
    "                # Filter rows with tag = 1 and missing links\n",
    "                missing_links = df[(df[\"predicted_label\"] == 1) & (df[\"extracted_citation\"].notna()) & (df[\"link\"].isna())]\n",
    "                \n",
    "                if not missing_links.empty:\n",
    "                    print(f\"\\n🔍 Missing links in file: {file}\")\n",
    "                    for _, row in missing_links.iterrows():\n",
    "                        print(f\"- Citation: {row['extracted_citation']}\")\n",
    "                        print(f\"  Paragraph: {row['paragraph_text'][:200]}...\")  # Show first 200 chars\n",
    "                        print(\"-\" * 50)\n",
    "\n",
    "# Run verification for tag = 1\n",
    "verify_updated_files_tag_1()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
