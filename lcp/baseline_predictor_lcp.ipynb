{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n",
    "\n",
    "docx_csv_dir = \"/home/liorkob/thesis/lcp/data/docx_csv_2018\"\n",
    "citations_dir = \"/home/liorkob/thesis/lcp/data/citations_csv_2018_with_tags\"\n",
    "\n",
    "\n",
    "# Citation patterns\n",
    "citation_patterns = {\n",
    "    'ע\"פ': r'ע\"פ (\\d+/\\d+)',\n",
    "    'ת\"פ': r'ת\"פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ\"ג': r'עפ\"ג (\\d+/\\d+)',\n",
    "    'ע״פ': r'ע״פ (\\d+/\\d+)',\n",
    "    'ת״פ': r'ת״פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ״ג': r'עפ״ג (\\d+/\\d+)',\n",
    "}\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Section search logic\n",
    "primary_sections = [\"הכרעת הדין\", \"אישום\", \"רקע\", \"כללי\", \"כתב אישום\", \"כתב האישום\"]\n",
    "secondary_sections = [\"תסקיר\", \"שירות המבחן\"]\n",
    "tertiary_sections = [\"גזר דין\", \"גזר הדין\", \"בעניינו של \", \"פסק דין\", \"פסק הדין\"]\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "# Step 1: Extract Relevant Text\n",
    "def extract_relevant_text_verdict(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    extracted_text = \"\"\n",
    "    found_primary = found_secondary = found_tertiary = False\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        part = row['part']\n",
    "        text = row['text']\n",
    "\n",
    "        if any(keyword in part for keyword in primary_sections):\n",
    "            extracted_text += text + \" \"\n",
    "            found_primary = True\n",
    "        elif not found_primary and any(keyword in part for keyword in secondary_sections):\n",
    "            extracted_text += text + \" \"\n",
    "            found_secondary = True\n",
    "        elif not found_primary and not found_secondary and any(keyword in part for keyword in tertiary_sections):\n",
    "            extracted_text += text + \" \"\n",
    "            found_tertiary = True\n",
    "\n",
    "    if not extracted_text.strip():\n",
    "        print(f\"CSV not found: {csv_path}\")\n",
    "        extracted_text = \" \".join(df['text'].astype(str).tolist())\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "# Step 2: Generate Embeddings with Mapping\n",
    "def generate_embeddings(docx_csv_dir):\n",
    "    embeddings = {}\n",
    "    for file in Path(docx_csv_dir).rglob(\"*.csv\"):  # Recursively find all CSV files\n",
    "        doc_name = os.path.splitext(file.name)[0]  # Extract the filename without extension\n",
    "        print(f\"Processing file: {file}, doc_name: {doc_name}\")\n",
    "        \n",
    "        # Ensure the full file path is used in extraction\n",
    "        csv_path = str(file)\n",
    "        print(f\"Looking for CSV at: {csv_path}\")\n",
    "        \n",
    "        # Extract relevant text using the full file path\n",
    "        relevant_text = extract_relevant_text_verdict(csv_path)\n",
    "        if relevant_text:\n",
    "            try:\n",
    "                embeddings[doc_name] = model.encode(relevant_text, convert_to_tensor=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding {doc_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Skipped embedding for {doc_name} due to lack of relevant text.\")\n",
    "    return embeddings\n",
    "\n",
    "# Step 3: Build Graph from Citation Data\n",
    "def build_graph(citations_dir):\n",
    "    G = nx.DiGraph()\n",
    "    node_sources = {}  # Dictionary to track the source of each node\n",
    "\n",
    "    for file in os.listdir(citations_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(citations_dir, file)\n",
    "            doc_name = normalize_case_name(os.path.splitext(file)[0])  # Normalize case name\n",
    "\n",
    "            # Read the CSV and handle empty files\n",
    "            try:\n",
    "                citations = pd.read_csv(file_path)\n",
    "                if citations.empty:\n",
    "                    print(f\"Skipping empty CSV: {file}\")\n",
    "                    continue\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Skipping empty CSV: {file}\")\n",
    "                continue\n",
    "\n",
    "            # Extract citations and build the graph\n",
    "            for _, row in citations.iterrows():\n",
    "                para_text = row['paragraph_text']\n",
    "                for key, pattern in citation_patterns.items():\n",
    "                    matches = re.finditer(pattern, para_text)\n",
    "                    for match in matches:\n",
    "                        cited_case = normalize_case_name(f'{key} {match.group(1)}')\n",
    "                        G.add_edge(doc_name, cited_case)  # Add edge between the verdict and cited case\n",
    "                        print(\"edge:\", doc_name, \",\", cited_case)\n",
    "                        node_sources[cited_case] = doc_name  # Store source of the cited case\n",
    "\n",
    "    return G, node_sources\n",
    "\n",
    "# Step 4: Compute Graph Similarity\n",
    "def compute_graph_similarity(G, verdict_a, verdict_b):\n",
    "    try:\n",
    "        shortest_path_length = nx.shortest_path_length(G, source=verdict_a, target=verdict_b)\n",
    "        return 1 / (1 + shortest_path_length)\n",
    "    except nx.NetworkXNoPath:\n",
    "        return 0\n",
    "\n",
    "# # Step 5: Combine Graph and Textual Similarity\n",
    "# def compute_combined_similarity(G, embeddings, verdict_a, verdict_b, alpha=0.5):\n",
    "#     # Check if nodes exist in the graph\n",
    "#     if verdict_a not in G.nodes:\n",
    "#         print(f\"Node {verdict_a} not found in the graph.\")\n",
    "#         return 0\n",
    "#     if verdict_b not in G.nodes:\n",
    "#         print(f\"Node {verdict_b} not found in the graph.\")\n",
    "#         return 0\n",
    "\n",
    "#     # Compute similarities\n",
    "#     graph_sim = compute_graph_similarity(G, verdict_a, verdict_b)\n",
    "#     text_sim = 0\n",
    "#     if verdict_a in embeddings and verdict_b in embeddings:\n",
    "#         text_sim = util.cos_sim(embeddings[verdict_a], embeddings[verdict_b]).item()\n",
    "\n",
    "#     # Combine graph and textual similarity\n",
    "#     return alpha * graph_sim + (1 - alpha) * text_sim\n",
    "# Step 5: Combine Graph and Textual Similarity\n",
    "def compute_combined_similarity(G, embeddings, verdict_a, verdict_b, alpha=0.5):\n",
    "    # Check if nodes exist in the graph\n",
    "    if verdict_a not in G.nodes:\n",
    "        print(f\"Node {verdict_a} not found in the graph.\")\n",
    "        return 0\n",
    "    if verdict_b not in G.nodes:\n",
    "        print(f\"Node {verdict_b} not found in the graph.\")\n",
    "        return 0\n",
    "\n",
    "    # Compute graph similarity\n",
    "    graph_sim = compute_graph_similarity(G, verdict_a, verdict_b)\n",
    "    print(f\"Graph similarity : {graph_sim}\")\n",
    "\n",
    "    # Compute textual similarity\n",
    "    text_sim = 0\n",
    "    if verdict_a in embeddings and verdict_b in embeddings:\n",
    "        text_sim = util.cos_sim(embeddings[verdict_a], embeddings[verdict_b]).item()\n",
    "    print(f\"Textual similarity : {text_sim}\")\n",
    "\n",
    "    # Combine graph and textual similarity\n",
    "    combined_similarity = alpha * graph_sim + (1 - alpha) * text_sim\n",
    "    print(f\"Combined similarity between {verdict_a} and {verdict_b}: {combined_similarity}\")\n",
    "    \n",
    "    return combined_similarity\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Build graph\n",
    "    graph, node_sources = build_graph(citations_dir)\n",
    "    print(f\"Graph loaded with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Graph nodes:\", graph.nodes)\n",
    "    \n",
    "    # Extract document names from docx_csv_dir\n",
    "    docx_cases = [normalize_case_name(os.path.splitext(file.name)[0]) for file in Path(docx_csv_dir).rglob(\"*.csv\")]\n",
    "    \n",
    "    # Find mismatches\n",
    "    only_in_graph = set(graph.nodes) - set(docx_cases)\n",
    "    only_in_docx = set(docx_cases) - set(graph.nodes)\n",
    "    print(only_in_graph)\n",
    "    print(only_in_docx)\n",
    "\n",
    "        # # Generate embeddings\n",
    "    # verdict_embeddings = generate_embeddings(docx_csv_dir)\n",
    "    # print(f\"Generated embeddings for {len(verdict_embeddings)} verdicts.\")\n",
    "    # print(\"Embedding keys:\", verdict_embeddings.keys())\n",
    "\n",
    "    # # Select verdicts that exist in both graph and embeddings\n",
    "    # embedding_keys = list(verdict_embeddings.keys())\n",
    "    \n",
    "    # # Loop through all pairs of verdicts that exist in both the graph and embeddings\n",
    "    # verdict_pairs = [(a, b) for a in embedding_keys for b in embedding_keys if a != b and a in graph.nodes and b in graph.nodes]\n",
    "\n",
    "    # if verdict_pairs:\n",
    "    #     for verdict_a, verdict_b in verdict_pairs:\n",
    "    #         print(f\"Computing similarity for {verdict_a} and {verdict_b}\")\n",
    "    #         # Compute combined similarity\n",
    "    #         similarity = compute_combined_similarity(graph, verdict_embeddings, verdict_a, verdict_b, alpha=0.5)\n",
    "    #         print(f\"Final combined similarity between {verdict_a} and {verdict_b}: {similarity}\")\n",
    "    # else:\n",
    "    #     print(\"No verdicts found that exist in both the graph and embeddings.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Directories\n",
    "docx_csv_dir = \"/home/liorkob/thesis/lcp/data/docx_csv_2018\"\n",
    "citations_dir = \"/home/liorkob/thesis/lcp/data/tag_citations_csv_2018\"\n",
    "output_csv = \"/home/liorkob/thesis/lcp/graph_data.csv\"\n",
    "\n",
    "# Citation patterns\n",
    "citation_patterns = {\n",
    "    'ע\"פ': r'ע\"פ (\\d+/\\d+)',\n",
    "    'ת\"פ': r'ת\"פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ\"ג': r'עפ\"ג (\\d+/\\d+)',\n",
    "    'ע״פ': r'ע״פ (\\d+/\\d+)',\n",
    "    'ת״פ': r'ת״פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "    'עפ״ג': r'עפ״ג (\\d+/\\d+)',\n",
    "}\n",
    "\n",
    "# Section search logic\n",
    "primary_sections = [\"הכרעת הדין\", \"אישום\", \"רקע\", \"כללי\", \"כתב אישום\", \"כתב האישום\"]\n",
    "secondary_sections = [\"תסקיר\", \"שירות המבחן\"]\n",
    "tertiary_sections = [\"גזר דין\", \"גזר הדין\", \"בעניינו של \", \"פסק דין\", \"פסק הדין\"]\n",
    "\n",
    "def normalize_case_name(case_name):\n",
    "    \"\"\"Normalize case names by removing extra spaces and fixing slashes.\"\"\"\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "# Extract relevant text from verdict CSV\n",
    "def extract_relevant_text(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    extracted_text = \"\"\n",
    "    found_primary = found_secondary = found_tertiary = False\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        part = row['part']\n",
    "        text = row['text']\n",
    "\n",
    "        if any(keyword in part for keyword in primary_sections):\n",
    "            extracted_text += text + \" \"\n",
    "            found_primary = True\n",
    "        elif not found_primary and any(keyword in part for keyword in secondary_sections):\n",
    "            extracted_text += text + \" \"\n",
    "            found_secondary = True\n",
    "        elif not found_primary and not found_secondary and any(keyword in part for keyword in tertiary_sections):\n",
    "            extracted_text += text + \" \"\n",
    "            found_tertiary = True\n",
    "\n",
    "    if not extracted_text.strip():\n",
    "        extracted_text = \" \".join(df['text'].astype(str).tolist())\n",
    "\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Build graph from citation data\n",
    "def build_graph(citations_dir):\n",
    "    G = nx.DiGraph()\n",
    "    rows = []\n",
    "    \n",
    "    for file in os.listdir(citations_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(citations_dir, file)\n",
    "            doc_name = normalize_case_name(os.path.splitext(file)[0])\n",
    "\n",
    "            try:\n",
    "                citations = pd.read_csv(file_path)\n",
    "                if citations.empty:\n",
    "                    continue\n",
    "            except pd.errors.EmptyDataError:\n",
    "                continue\n",
    "\n",
    "            for _, row in citations.iterrows():\n",
    "                para_text = row['paragraph_text']\n",
    "                for key, pattern in citation_patterns.items():\n",
    "                    matches = re.finditer(pattern, para_text)\n",
    "                    for match in matches:\n",
    "                        cited_case = normalize_case_name(f'{key} {match.group(1)}')\n",
    "                        G.add_edge(doc_name, cited_case)\n",
    "                        rows.append({\n",
    "                            \"source\": doc_name,\n",
    "                            \"target\": cited_case,\n",
    "                            \"source_citation\": para_text\n",
    "                        })\n",
    "    \n",
    "    return G, pd.DataFrame(rows)\n",
    "\n",
    "# Extract text for embeddings and update node data\n",
    "def update_node_texts(df, docx_csv_dir):\n",
    "    text_data = {}\n",
    "    \n",
    "    for file in Path(docx_csv_dir).rglob(\"*.csv\"):\n",
    "        doc_name = normalize_case_name(os.path.splitext(file.name)[0])\n",
    "        text_data[doc_name] = extract_relevant_text(str(file))\n",
    "    \n",
    "    df[\"source_text\"] = df[\"source\"].map(text_data)\n",
    "    df[\"target_text\"] = df[\"target\"].map(text_data)\n",
    "    return df\n",
    "\n",
    "# Save graph data to CSV\n",
    "def save_graph_to_csv(df, output_csv):\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"CSV saved to {output_csv}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Build the citation graph\n",
    "    graph, df_graph = build_graph(citations_dir)\n",
    "    print(f\"Graph loaded with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    \n",
    "    # Update nodes with text from docx CSVs\n",
    "    df_graph = update_node_texts(df_graph, docx_csv_dir)\n",
    "    \n",
    "    # Save graph structure with text details\n",
    "    save_graph_to_csv(df_graph, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import re\n",
    "# from pathlib import Path\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# import numpy as np\n",
    "\n",
    "# # Define base directory and years\n",
    "# base_dir = \"/home/liorkob/thesis/lcp/data\"\n",
    "# years = [\"2018\", \"2019\", \"2020\"]  # Adjust years as needed\n",
    "\n",
    "# # Define directory structure\n",
    "# dirs = {\n",
    "#     \"docx_csv\": [os.path.join(base_dir, f\"docx_csv_{year}\") for year in years],\n",
    "#     \"citations_csv\": [os.path.join(base_dir, f\"tag_citations_csv_{year}\") for year in years],\n",
    "# }\n",
    "\n",
    "# # Define embedding CSV files\n",
    "# embedding_files = {\n",
    "#     \"verdicts\": \"processed_verdicts_with_gpt.csv\",\n",
    "#     \"appeals\": \"processed_appeals_with_gpt.csv\"\n",
    "# }\n",
    "\n",
    "# # Load HeBERT model\n",
    "# model_name = \"avichr/heBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # Citation patterns\n",
    "# citation_patterns = {\n",
    "#     'ע\"פ': r'ע\"פ (\\d+/\\d+)',\n",
    "#     'עפ\"ג': r'עפ\"ג (\\d+/\\d+)',\n",
    "#     'ת\"פ': r'ת\"פ (\\d+[-/]\\d+[-/]\\d+)',\n",
    "#     'עפ״ג': r'עפ״ג (\\d+/\\d+)',\n",
    "#     'רע״פ': r'רע״פ (\\d+/\\d+)',\n",
    "#     'תפ\"ח': r'תפ\"ח\\s*(\\d+[-/]\\d+[-/]\\d+)',\n",
    "# }\n",
    "\n",
    "# # Normalize case names\n",
    "# def normalize_case_name(case_name):\n",
    "#     return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "# # Build graph from multiple citation directories\n",
    "# def build_graph(citations_dirs):\n",
    "#     G = nx.DiGraph()\n",
    "#     for citations_dir in citations_dirs:\n",
    "#         if not os.path.exists(citations_dir):\n",
    "#             print(f\"Warning: {citations_dir} not found. Skipping...\")\n",
    "#             continue\n",
    "        \n",
    "#         for file in os.listdir(citations_dir):\n",
    "#             if file.endswith(\".csv\"):\n",
    "#                 file_path = os.path.join(citations_dir, file)\n",
    "#                 doc_name = normalize_case_name(os.path.splitext(file)[0])\n",
    "\n",
    "#                 try:\n",
    "#                     citations = pd.read_csv(file_path)\n",
    "#                     if citations.empty:\n",
    "#                         continue\n",
    "#                 except pd.errors.EmptyDataError:\n",
    "#                     continue\n",
    "\n",
    "#                 for _, row in citations.iterrows():\n",
    "#                     if row[\"predicted_label\"]==1:\n",
    "#                     # para_text = row['paragraph_text']\n",
    "#                         cited_case = normalize_case_name(row['citation'])\n",
    "#                         G.add_edge(doc_name, cited_case)\n",
    "#                         print(\"edge between:\",doc_name, cited_case)\n",
    "#     return G\n",
    "\n",
    "# # Compute dataset statistics\n",
    "# def compute_dataset_statistics(graph):\n",
    "#     num_cases = graph.number_of_nodes()\n",
    "#     num_citations = graph.number_of_edges()\n",
    "#     degrees = [d for _, d in graph.degree()]\n",
    "#     avg_citations = sum(degrees) / num_cases if num_cases > 0 else 0\n",
    "    \n",
    "#     in_degrees = [d for _, d in graph.in_degree()]\n",
    "#     out_degrees = [d for _, d in graph.out_degree()]\n",
    "    \n",
    "#     stats_df = pd.DataFrame({\n",
    "#         \"Metric\": [\"Total Cases\", \"Total Citations\", \"Avg Citations per Case\", \"Max In-Degree\", \"Max Out-Degree\"],\n",
    "#         \"Value\": [num_cases, num_citations, avg_citations, max(in_degrees, default=0), max(out_degrees, default=0)]\n",
    "#     })\n",
    "#     return stats_df, degrees, in_degrees, out_degrees\n",
    "\n",
    "# # Generate plots\n",
    "# def generate_plots(degrees, in_degrees, out_degrees, graph):\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     sns.histplot(degrees, bins=20, kde=True)\n",
    "#     plt.xlabel(\"Number of Citations\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.title(\"Citation Distribution\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     sns.histplot(in_degrees, bins=20, kde=True)\n",
    "#     plt.xlabel(\"In-Degree (Citations Received)\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.title(\"In-Degree Distribution\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     sns.histplot(out_degrees, bins=20, kde=True)\n",
    "#     plt.xlabel(\"Out-Degree (Citations Given)\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.title(\"Out-Degree Distribution\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     nx.draw(graph, with_labels=False, node_size=10, alpha=0.5, edge_color=\"gray\")\n",
    "#     plt.title(\"Legal Citation Network\")\n",
    "#     plt.show()\n",
    "\n",
    "# # Run processing with all citation directories\n",
    "# all_citation_dirs = dirs[\"citations_csv\"]\n",
    "# graph = build_graph(all_citation_dirs)\n",
    "# dataset_stats, degrees, in_degrees, out_degrees = compute_dataset_statistics(graph)\n",
    "\n",
    "# # Display statistics\n",
    "# print(\"Dataset Statistics:\")\n",
    "# print(dataset_stats)\n",
    "\n",
    "# # Generate plots\n",
    "# generate_plots(degrees, in_degrees, out_degrees, graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "# Define base directory and years\n",
    "base_dir = \"/home/liorkob/thesis/lcp/data\"\n",
    "years = [\"2018\", \"2019\", \"2020\"]\n",
    "\n",
    "# Define directories\n",
    "dirs = {\n",
    "    \"docx_csv\": [os.path.join(base_dir, f\"docx_csv_{year}\") for year in years],\n",
    "    \"citations_csv\": [os.path.join(base_dir, f\"tag_citations_csv_{year}\") for year in years],\n",
    "}\n",
    "\n",
    "# Define embedding CSV files\n",
    "embedding_files = {\n",
    "    \"verdicts\": \"processed_verdicts_with_gpt.csv\",\n",
    "    \"appeals\": \"processed_appeals_with_gpt.csv\"\n",
    "}\n",
    "\n",
    "# Load HeBERT model\n",
    "model_name = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Normalize case names\n",
    "def normalize_case_name(case_name):\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "# Extract embeddings for case texts\n",
    "def get_case_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Load precomputed embeddings\n",
    "def load_precomputed_embeddings(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Warning: {csv_path} not found. Skipping...\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        verdict = normalize_case_name(row['verdict'])\n",
    "        try:\n",
    "            text = row['extracted_gpt_facts'].strip()\n",
    "            if text:\n",
    "                embeddings[verdict] = get_case_embedding(text)\n",
    "            else:\n",
    "                raise ValueError(\"Empty extracted_gpt_facts text\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping embedding for {verdict} due to error: {e}\")\n",
    "            continue  \n",
    "    return embeddings\n",
    "\n",
    "# Compute similarity scores\n",
    "def compute_similarity(embeddings):\n",
    "    valid_embeddings = {k: v for k, v in embeddings.items() if isinstance(v, np.ndarray) and v.ndim == 1 and v.size > 0}\n",
    "    \n",
    "    if len(valid_embeddings) < len(embeddings):\n",
    "        print(f\"Warning: {len(embeddings) - len(valid_embeddings)} embeddings were invalid and removed.\")\n",
    "    \n",
    "    case_names = list(valid_embeddings.keys())\n",
    "    emb_matrix = np.array([valid_embeddings[case] for case in case_names])\n",
    "    \n",
    "    if emb_matrix.shape[0] == 0:\n",
    "        raise ValueError(\"No valid embeddings available for similarity computation.\")\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(emb_matrix)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=case_names, columns=case_names)\n",
    "    \n",
    "    # Save the similarity matrix\n",
    "    similarity_df.to_csv(\"case_similarity_matrix.csv\")\n",
    "    \n",
    "    return similarity_df, case_names, emb_matrix\n",
    "\n",
    "# Extract citation pairs from the citation network\n",
    "def get_citation_pairs(graph, num_pairs=10):\n",
    "    edges = list(graph.edges())\n",
    "    if len(edges) < num_pairs:\n",
    "        print(f\"Warning: Only {len(edges)} citation pairs found in the graph!\")\n",
    "        num_pairs = len(edges)\n",
    "    return random.sample(edges, num_pairs)\n",
    "\n",
    "# Generate random pairs that are not citations\n",
    "def get_random_pairs(graph, num_pairs=10):\n",
    "    nodes = list(graph.nodes())\n",
    "    random_pairs = set()\n",
    "    while len(random_pairs) < num_pairs:\n",
    "        pair = tuple(random.sample(nodes, 2))\n",
    "        if pair not in graph.edges() and pair[::-1] not in graph.edges():  \n",
    "            random_pairs.add(pair)\n",
    "    return list(random_pairs)\n",
    "\n",
    "# Compute similarity for selected pairs\n",
    "def compute_pairwise_similarity(pairs, embeddings, pair_type, num_required=10):\n",
    "    valid_pairs = []\n",
    "    similarities = []\n",
    "    \n",
    "    for case1, case2 in pairs:\n",
    "        if case1 in embeddings and case2 in embeddings:\n",
    "            sim = cosine_similarity([embeddings[case1]], [embeddings[case2]])[0][0]\n",
    "            valid_pairs.append((case1, case2))\n",
    "            similarities.append(sim)\n",
    "        if len(valid_pairs) == num_required:\n",
    "            break  # Stop once we have enough valid pairs\n",
    "    \n",
    "    if len(valid_pairs) < num_required:\n",
    "        print(f\"Warning: Only {len(valid_pairs)} valid {pair_type} pairs found!\")\n",
    "    \n",
    "    return valid_pairs, similarities\n",
    "\n",
    "# Build the citation network\n",
    "def build_graph(citations_dirs):\n",
    "    G = nx.DiGraph()\n",
    "    for citations_dir in citations_dirs:\n",
    "        if not os.path.exists(citations_dir):\n",
    "            print(f\"Warning: {citations_dir} not found. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        for file in os.listdir(citations_dir):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(citations_dir, file)\n",
    "                doc_name = normalize_case_name(os.path.splitext(file)[0])\n",
    "\n",
    "                try:\n",
    "                    citations = pd.read_csv(file_path)\n",
    "                    if citations.empty:\n",
    "                        continue\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    continue\n",
    "\n",
    "                for _, row in citations.iterrows():\n",
    "                    if row[\"predicted_label\"] == 1:\n",
    "                        cited_case = normalize_case_name(row['citation'])\n",
    "                        G.add_edge(doc_name, cited_case)\n",
    "    return G\n",
    "\n",
    "# Load embeddings\n",
    "embeddings = {}\n",
    "for key, csv_file in embedding_files.items():\n",
    "    embeddings.update(load_precomputed_embeddings(csv_file))\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix, case_names, emb_matrix = compute_similarity(embeddings)\n",
    "\n",
    "# Build the citation graph\n",
    "citation_dirs = dirs[\"citations_csv\"]\n",
    "graph = build_graph(citation_dirs)\n",
    "# Select citation and random pairs\n",
    "citation_pairs = get_citation_pairs(graph, num_pairs=15)  # Get more to ensure 10 valid\n",
    "random_pairs = get_random_pairs(graph, num_pairs=15)\n",
    "\n",
    "# Compute similarity scores (ensuring 10 valid pairs)\n",
    "citation_pairs, citation_similarities = compute_pairwise_similarity(citation_pairs, embeddings, \"Citation\", 5)\n",
    "random_pairs, random_similarities = compute_pairwise_similarity(random_pairs, embeddings, \"Random\", 5)\n",
    "\n",
    "# Save all pairs to a CSV file\n",
    "pairs_data = {\n",
    "    \"Case Pairs\": [f\"{p1} ↔ {p2}\" for p1, p2 in citation_pairs + random_pairs],\n",
    "    \"Pair Type\": [\"Citation Pair\"] * len(citation_pairs) + [\"Random Pair\"] * len(random_pairs),\n",
    "    \"Similarity Score\": citation_similarities + random_similarities\n",
    "}\n",
    "pairs_df = pd.DataFrame(pairs_data)\n",
    "pairs_df.to_csv(\"pairs_similarity.csv\", index=False)\n",
    "\n",
    "# Heatmap plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "heatmap_pivot = pairs_df.pivot(index=\"Case Pairs\", columns=\"Pair Type\", values=\"Similarity Score\")\n",
    "sns.heatmap(heatmap_pivot, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar=True)\n",
    "plt.title(\"Citation Pairs vs. Random Pairs Similarity Heatmap\")\n",
    "plt.xlabel(\"Pair Type\")\n",
    "plt.ylabel(\"Case Pairs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define base directory and years\n",
    "base_dir = \"/home/liorkob/thesis/lcp/data\"\n",
    "years = [\"2018\", \"2019\", \"2020\"]\n",
    "\n",
    "# Define directory structure\n",
    "dirs = {\n",
    "    \"docx_csv\": [os.path.join(base_dir, f\"docx_csv_{year}\") for year in years],\n",
    "    \"citations_csv\": [os.path.join(base_dir, f\"tag_citations_csv_{year}\") for year in years],\n",
    "}\n",
    "\n",
    "# Define embedding CSV files\n",
    "embedding_files = {\n",
    "    \"verdicts\": \"processed_verdicts_with_gpt.csv\",\n",
    "    \"appeals\": \"processed_appeals_with_gpt.csv\"\n",
    "}\n",
    "\n",
    "# Load HeBERT model\n",
    "model_name = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Normalize case names\n",
    "def normalize_case_name(case_name):\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "# Extract embeddings for case texts\n",
    "def get_case_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Build graph from multiple citation directories\n",
    "def build_graph(citations_dirs):\n",
    "    G = nx.DiGraph()\n",
    "    for citations_dir in citations_dirs:\n",
    "        if not os.path.exists(citations_dir):\n",
    "            print(f\"Warning: {citations_dir} not found. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        for file in os.listdir(citations_dir):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(citations_dir, file)\n",
    "                doc_name = normalize_case_name(os.path.splitext(file)[0])\n",
    "\n",
    "                try:\n",
    "                    citations = pd.read_csv(file_path)\n",
    "                    if citations.empty:\n",
    "                        continue\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    continue\n",
    "\n",
    "                for _, row in citations.iterrows():\n",
    "                    if row[\"predicted_label\"] == 1:\n",
    "                        cited_case = normalize_case_name(row['citation'])\n",
    "                        G.add_edge(doc_name, cited_case)\n",
    "    return G\n",
    "\n",
    "# Compute graph-based similarity metrics\n",
    "def compute_graph_similarity(graph, case_pairs):\n",
    "    similarities = {}\n",
    "\n",
    "    for case1, case2 in case_pairs:\n",
    "        sim_scores = {}\n",
    "\n",
    "        if graph.has_node(case1) and graph.has_node(case2):\n",
    "            # Compute Jaccard similarity\n",
    "            neighbors1 = set(graph.neighbors(case1))\n",
    "            neighbors2 = set(graph.neighbors(case2))\n",
    "            intersection = len(neighbors1 & neighbors2)\n",
    "            union = len(neighbors1 | neighbors2)\n",
    "            sim_scores[\"jaccard\"] = intersection / union if union > 0 else 0\n",
    "\n",
    "            # Compute shortest path length\n",
    "            try:\n",
    "                sim_scores[\"shortest_path\"] = 1 / (1 + nx.shortest_path_length(graph, case1, case2))\n",
    "            except nx.NetworkXNoPath:\n",
    "                sim_scores[\"shortest_path\"] = 0\n",
    "\n",
    "        similarities[(case1, case2)] = sim_scores\n",
    "    return similarities\n",
    "\n",
    "# Compute embedding-based similarity\n",
    "def compute_embedding_similarity(embeddings):\n",
    "    case_names = list(embeddings.keys())\n",
    "    emb_matrix = np.array([embeddings[case] for case in case_names])\n",
    "    similarity_matrix = cosine_similarity(emb_matrix)\n",
    "    return pd.DataFrame(similarity_matrix, index=case_names, columns=case_names)\n",
    "\n",
    "# Combine graph and embedding similarities\n",
    "def compute_combined_similarity(graph, embeddings):\n",
    "    case_pairs = [(case1, case2) for case1 in embeddings for case2 in embeddings if case1 != case2]\n",
    "    graph_similarities = compute_graph_similarity(graph, case_pairs)\n",
    "    embedding_similarities = compute_embedding_similarity(embeddings)\n",
    "\n",
    "    combined_scores = {}\n",
    "    for (case1, case2) in case_pairs:\n",
    "        graph_score = graph_similarities.get((case1, case2), {}).get(\"jaccard\", 0)\n",
    "        embedding_score = embedding_similarities.at[case1, case2] if case1 in embedding_similarities.index and case2 in embedding_similarities.columns else 0\n",
    "        combined_scores[(case1, case2)] = (graph_score + embedding_score) / 2  # Simple average\n",
    "\n",
    "    return combined_scores\n",
    "\n",
    "# Load precomputed embeddings from CSV files\n",
    "def load_precomputed_embeddings(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Warning: {csv_path} not found. Skipping...\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        verdict = normalize_case_name(row['verdict'])\n",
    "        try:\n",
    "            text = row['extracted_gpt_facts'].strip()\n",
    "            if text:\n",
    "                embeddings[verdict] = get_case_embedding(text)\n",
    "            else:\n",
    "                raise ValueError(\"Empty extracted_gpt_facts text\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping embedding for {verdict} due to error: {e}\")\n",
    "            continue  \n",
    "    return embeddings\n",
    "\n",
    "# Load and merge precomputed embeddings\n",
    "embeddings = {}\n",
    "for key, csv_file in embedding_files.items():\n",
    "    embeddings.update(load_precomputed_embeddings(csv_file))\n",
    "\n",
    "# Build citation graph\n",
    "all_citation_dirs = dirs[\"citations_csv\"]\n",
    "graph = build_graph(all_citation_dirs)\n",
    "\n",
    "# Compute similarity metrics\n",
    "combined_similarities = compute_combined_similarity(graph, embeddings)\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "similarity_df = pd.DataFrame(combined_similarities.items(), columns=[\"Case Pair\", \"Combined Similarity\"])\n",
    "similarity_df = similarity_df.sort_values(by=\"Combined Similarity\", ascending=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"Top 10 Most Similar Case Pairs:\")\n",
    "print(similarity_df.head(10))\n",
    "\n",
    "# Generate a heatmap for visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pd.pivot_table(similarity_df, values=\"Combined Similarity\", index=\"Case Pair\", aggfunc=\"mean\"), cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Combined Case Similarity Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Define base directory and years\n",
    "base_dir = \"/home/liorkob/thesis/lcp/data\"\n",
    "years = [\"2018\", \"2019\", \"2020\"]\n",
    "\n",
    "# Define directory structure\n",
    "dirs = {\n",
    "    \"docx_csv\": [os.path.join(base_dir, f\"docx_csv_{year}\") for year in years],\n",
    "    \"citations_csv\": [os.path.join(base_dir, f\"tag_citations_csv_{year}\") for year in years],\n",
    "}\n",
    "\n",
    "# Define embedding CSV files\n",
    "embedding_files = {\n",
    "    \"verdicts\": \"processed_verdicts_with_gpt.csv\",\n",
    "    \"appeals\": \"processed_appeals_with_gpt.csv\"\n",
    "}\n",
    "\n",
    "# Load HeBERT model\n",
    "model_name = \"avichr/Legal-heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Normalize case names\n",
    "def normalize_case_name(case_name):\n",
    "    return re.sub(r'\\s+', ' ', case_name.replace('∕', '/')).strip()\n",
    "\n",
    "# Extract embeddings for case texts\n",
    "def get_case_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Load precomputed embeddings\n",
    "def load_precomputed_embeddings(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Warning: {csv_path} not found. Skipping...\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_csv(\"processed_verdicts_with_gpt.csv\")\n",
    "    print(df[['verdict', 'extracted_gpt_facts']].sample(5))\n",
    "    embeddings = {}\n",
    "    for _, row in df.iterrows():\n",
    "        verdict = normalize_case_name(row['verdict'])\n",
    "        try:\n",
    "            text = row['extracted_gpt_facts'].strip()\n",
    "            if text:\n",
    "                embeddings[verdict] = get_case_embedding(text)\n",
    "            else:\n",
    "                raise ValueError(\"Empty extracted_gpt_facts text\")\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping embedding for {verdict} due to error: {e}\")\n",
    "            continue  \n",
    "    return embeddings\n",
    "\n",
    "# Train-test split\n",
    "def train_test_split_cases(positive_pairs, all_cases, test_size=0.2):\n",
    "    positive_pairs = list(positive_pairs)  # ✅ Fix: Convert set to list\n",
    "    positive_train, positive_test = train_test_split(positive_pairs, test_size=test_size, random_state=42)\n",
    "\n",
    "    negative_pairs = set()\n",
    "    while len(negative_pairs) < len(positive_pairs):\n",
    "        case1, case2 = random.sample(all_cases, 2)\n",
    "        if (case1, case2) not in positive_pairs and (case2, case1) not in positive_pairs:\n",
    "            negative_pairs.add((case1, case2))\n",
    "\n",
    "    negative_pairs = list(negative_pairs)  # ✅ Convert negative_pairs to list\n",
    "    negative_train, negative_test = train_test_split(negative_pairs, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_data = positive_train + negative_train\n",
    "    test_data = positive_test + negative_test\n",
    "    \n",
    "    train_labels = [1] * len(positive_train) + [0] * len(negative_train)\n",
    "    test_labels = [1] * len(positive_test) + [0] * len(negative_test)\n",
    "\n",
    "    print(f\"📊 Train size: {len(train_data)}, Test size: {len(test_data)}\")\n",
    "    print(f\"✅ Sample Positive Train Pairs: {train_data[:3]}\")\n",
    "    print(f\"❌ Sample Negative Train Pairs: {negative_train[:3]}\")\n",
    "    \n",
    "    return train_data, test_data, train_labels, test_labels\n",
    "\n",
    "def compute_graph_similarity(graph, case_pairs):\n",
    "    similarities = {}\n",
    "\n",
    "    for case1, case2 in case_pairs:\n",
    "        sim_scores = {}\n",
    "\n",
    "        if not graph.has_node(case1) or not graph.has_node(case2):\n",
    "            print(f\"⚠️ Missing node(s) → {case1} or {case2} not in graph\")\n",
    "            continue  # Skip if a case is missing\n",
    "\n",
    "        # Compute Jaccard similarity\n",
    "        neighbors1 = set(graph.neighbors(case1))\n",
    "        neighbors2 = set(graph.neighbors(case2))\n",
    "        intersection = len(neighbors1 & neighbors2)\n",
    "        union = len(neighbors1 | neighbors2)\n",
    "        sim_scores[\"jaccard\"] = intersection / union if union > 0 else 0\n",
    "\n",
    "        # Compute shortest path length\n",
    "        try:\n",
    "            sim_scores[\"shortest_path\"] = 1 / (1 + nx.shortest_path_length(graph, case1, case2))\n",
    "        except nx.NetworkXNoPath:\n",
    "            sim_scores[\"shortest_path\"] = 0\n",
    "\n",
    "        similarities[(case1, case2)] = sim_scores\n",
    "\n",
    "        # 🔍 Debug: Print valid similarities\n",
    "        if sim_scores[\"jaccard\"] > 0 or sim_scores[\"shortest_path\"] > 0:\n",
    "            print(f\"📌 Graph Similarity ({case1}, {case2}) → Jaccard: {sim_scores['jaccard']:.3f}, Shortest Path: {sim_scores['shortest_path']:.3f}\")\n",
    "\n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Compute embedding-based similarity\n",
    "def compute_embedding_similarity(embeddings):\n",
    "    case_names = list(embeddings.keys())\n",
    "    emb_matrix = np.array([embeddings[case] for case in case_names])\n",
    "    similarity_matrix = cosine_similarity(emb_matrix)\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=case_names, columns=case_names)\n",
    "\n",
    "    # 🔍 Debug: Print high similarity pairs\n",
    "    print(\"📌 Top 5 Embedding Similarities:\")\n",
    "    top_similar_pairs = similarity_df.unstack().sort_values(ascending=False).drop_duplicates().head(5)\n",
    "    print(top_similar_pairs)\n",
    "\n",
    "    return similarity_df\n",
    "\n",
    "# 🚀 **Step 1: Load embeddings**\n",
    "embeddings = {}\n",
    "for key, csv_file in embedding_files.items():\n",
    "    embeddings.update(load_precomputed_embeddings(csv_file))\n",
    "\n",
    "# 🚀 **Step 2: Get all cases**\n",
    "all_cases = list(embeddings.keys())\n",
    "\n",
    "# 🚀 **Step 3: Load citation pairs BEFORE building the graph**\n",
    "positive_pairs = set()\n",
    "for citations_dir in dirs[\"citations_csv\"]:\n",
    "    if os.path.exists(citations_dir):\n",
    "        for file in os.listdir(citations_dir):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file_path = os.path.join(citations_dir, file)\n",
    "                doc_name = normalize_case_name(os.path.splitext(file)[0])\n",
    "\n",
    "                try:\n",
    "                    citations = pd.read_csv(file_path)\n",
    "                    for _, row in citations.iterrows():\n",
    "                        if row[\"predicted_label\"] == 1:\n",
    "                            cited_case = normalize_case_name(row['citation'])\n",
    "                            positive_pairs.add((doc_name, cited_case))\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    continue\n",
    "\n",
    "# 🚀 **Step 4: Split Train-Test Before Graph is Built**\n",
    "train_data, test_data, train_labels, test_labels = train_test_split_cases(positive_pairs, all_cases)\n",
    "\n",
    "# 🚀 **Step 5: Build Graph ONLY on Train Data**\n",
    "graph = nx.DiGraph()\n",
    "for case1, case2 in train_data:\n",
    "    graph.add_edge(case1, case2)\n",
    "\n",
    "# 🚀 **Step 6: Compute Similarities**\n",
    "graph_similarities = compute_graph_similarity(graph, train_data + test_data)\n",
    "embedding_similarities = compute_embedding_similarity(embeddings)\n",
    "\n",
    "# 🚀 **Step 7: Compute Final Similarity Scores**\n",
    "final_scores = []\n",
    "for case1, case2 in train_data + test_data:\n",
    "    graph_score = graph_similarities.get((case1, case2), {}).get(\"jaccard\", 0)\n",
    "    embedding_score = embedding_similarities.at[case1, case2] if case1 in embedding_similarities.index and case2 in embedding_similarities.columns else 0\n",
    "    if graph_score > 0 and embedding_score > 0:\n",
    "        combined_score = (graph_score + embedding_score) / 2\n",
    "    elif graph_score > 0:\n",
    "        combined_score = graph_score\n",
    "    elif embedding_score > 0:\n",
    "        combined_score = embedding_score\n",
    "    else:\n",
    "        combined_score = 0  # If both are 0\n",
    "    final_scores.append(combined_score)\n",
    "\n",
    "print(f\"🔍 Example of computed similarity scores: {final_scores[:5]}\")\n",
    "\n",
    "# 🚀 **Step 8: Evaluate**\n",
    "roc_auc = roc_auc_score(test_labels, final_scores[len(train_data):])\n",
    "precision, recall, _ = precision_recall_curve(test_labels, final_scores[len(train_data):])\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "print(f\"🎯 ROC AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"🎯 Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_true = test_labels\n",
    "y_pred = [1 if s > 0.5 else 0 for s in final_scores[len(train_data):]]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(final_scores[:len(train_data)], bins=20, color=\"blue\", label=\"Similar Cases\", alpha=0.6)\n",
    "sns.histplot(final_scores[len(train_data):], bins=20, color=\"red\", label=\"Random Pairs\", alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Similarity Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Similarity Score Distribution\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 🚀 **Step 9: Plot Precision-Recall Curve**\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/liorkob/thesis/lcp/data/docx_csv_2018'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m required_parts \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mחקיקה\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# required_parts = [\"אחידות בענישה\",\"מתחם הענישה\",\"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\", \"הענישה הנוהגת\",\"ענישה נוהגת\", \"מתחם העונש\" ,\"מתחם עונש\",\"מדיניות הענישה\" \"והכרעה\", \"ההרשעה\",\"מדיניות הענישה הנהוגה\"]\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# required_parts=[\"הכרעת הדין\", \"אישום\" ,\"רקע\" ,\"כללי\" ,\"כתב אישום\",\"כתב האישום\"]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Run the verification\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mverify_verdict_parts_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequired_parts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 21\u001b[0m, in \u001b[0;36mverify_verdict_parts_from_csv\u001b[0;34m(output_directory, required_parts)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mVerifies the presence of required parts in each CSV file generated from verdict processing.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m- Identifies and lists verdicts where none of the required parts are found.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m verdicts_with_no_parts \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Store verdicts where none of the parts exist\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/liorkob/thesis/lcp/data/docx_csv_2018'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def verify_verdict_parts_from_csv(output_directory, required_parts):\n",
    "    \"\"\"\n",
    "    Verifies the presence of required parts in each CSV file generated from verdict processing.\n",
    "\n",
    "    Parameters:\n",
    "    - output_directory (str): Directory containing the output CSV files.\n",
    "    - required_parts (list): List of strings representing the required parts.\n",
    "\n",
    "    Output:\n",
    "    - Prints the parts for each verdict.\n",
    "    - Identifies and lists verdicts where none of the required parts are found.\n",
    "    \"\"\"\n",
    "    verdicts_with_no_parts = []  # Store verdicts where none of the parts exist\n",
    "\n",
    "    for file in os.listdir(output_directory):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(output_directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        verdict_name = os.path.splitext(file)[0]\n",
    "        \n",
    "        print(f\"Verifying Verdict: {verdict_name}\")\n",
    "        \n",
    "        # Extract unique parts from the DataFrame\n",
    "        verdict_parts = df['part'].dropna().astype(str).unique()  # Ensure all parts are strings\n",
    "        \n",
    "        # Print all parts for the verdict\n",
    "        print(\"  Parts in the verdict:\")\n",
    "        for part in verdict_parts:\n",
    "            print(f\"    - {part}\")\n",
    "        \n",
    "        # Check if none of the required parts exist\n",
    "        if not any(req_part in part for req_part in required_parts for part in verdict_parts):\n",
    "            verdicts_with_no_parts.append((verdict_name,verdict_parts))  # Add to the list of problematic verdicts\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    # Print verdicts with no matching parts\n",
    "    if verdicts_with_no_parts:\n",
    "        print(\"Verdicts with no matching parts:\")\n",
    "        for verdict,parts in verdicts_with_no_parts:\n",
    "            print(f\"  - {verdict}\")\n",
    "            print(f\"parts: {parts}\")\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"All verdicts have at least one matching part.\")\n",
    "\n",
    "# Define the directory containing the output CSV files\n",
    "output_directory = \"/home/liorkob/thesis/lcp/data/docx_csv_2018\"\n",
    "\n",
    "# Define the required parts (partial matching supported)\n",
    "required_parts =[\"חקיקה\"]\n",
    "# required_parts = [\"אחידות בענישה\",\"מתחם הענישה\",\"מתחם ענישה\", \"דיון\", \"ענישה נהוגה\", \"הענישה הנוהגת\",\"ענישה נוהגת\", \"מתחם העונש\" ,\"מתחם עונש\",\"מדיניות הענישה\" \"והכרעה\", \"ההרשעה\",\"מדיניות הענישה הנהוגה\"]\n",
    "# required_parts=[\"הכרעת הדין\", \"אישום\" ,\"רקע\" ,\"כללי\" ,\"כתב אישום\",\"כתב האישום\"]\n",
    "# Run the verification\n",
    "verify_verdict_parts_from_csv(output_directory, required_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  extended_end_row = df[df.index > potential_end_idx][df[\"part\"] != df.loc[start_idx, \"part\"]]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Start and End have the same name for verdict 'ת\"פ 11786-06-16'. Searching for next distinct part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  extended_end_row = df[df.index > potential_end_idx][df[\"part\"] != df.loc[start_idx, \"part\"]]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:44: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  extended_end_row = df[df.index > potential_end_idx][df[\"part\"] != df.loc[start_idx, \"part\"]]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Start and End have the same name for verdict 'ת\"פ 58505-11-19'. Searching for next distinct part.\n",
      "⚠️ Warning: Start and End have the same name for verdict 'ת\"פ 48867-03-20'. Searching for next distinct part.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
      "/tmp/ipykernel_1033638/3683213275.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Statistics ===\n",
      "   Total CSV Files Processed  Successful Extractions  Failed Extractions\n",
      "0                        149                     149                   0\n",
      "\n",
      "=== Sample of Successful Extractions (Start & End Parts) ===\n",
      "           verdict  start_part            end_part\n",
      "0  ת\"פ 16420-10-16     גזר דין         עמדת המדינה\n",
      "1   ת\"פ 1995-03-17        כללי  תסקיר שירות המבחן \n",
      "2  ת\"פ 21139-04-17     גזר דין   תסקיר שירות המבחן\n",
      "3  ת\"פ 13632-08-17     גזר דין    תסקיר שרות מבחן:\n",
      "4  ת\"פ 17856-06-17  כתב-האישום   תסקיר שירות המבחן\n",
      "\n",
      "✅ Process complete. Results saved as 'processed_verdicts.csv' and 'failed_verdicts.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\n",
    "    \"טענות\", \"עמדת\", \"תסקיר\", \"שירות\", \"מבחן\", \"דיון\", \"התסקיר\",\n",
    "    \"טיעוני\", \"הצדדים\", \"צדדים\", \"והכרעה\", \"האישום השני\", \"ראיות\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "\n",
    "    # Find the first row where 'part' contains a start pattern (case-insensitive, partial match)\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    if start_row.empty:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "    start_idx = start_row.index.min()\n",
    "    start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Find the first row where 'part' contains an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Ensure end is after start and not identical in name\n",
    "    if not end_row.empty:\n",
    "        potential_end_idx = end_row.index.min()\n",
    "\n",
    "        # If the end part is the same as the start part, look further down\n",
    "        if df.loc[potential_end_idx, \"part\"] == df.loc[start_idx, \"part\"]:\n",
    "            print(f\"⚠️ Warning: Start and End have the same name for verdict '{df['verdict'].iloc[0]}'. Searching for next distinct part.\")\n",
    "\n",
    "            # Find the next part that is different from the start part\n",
    "            extended_end_row = df[df.index > potential_end_idx][df[\"part\"] != df.loc[start_idx, \"part\"]]\n",
    "\n",
    "            if not extended_end_row.empty:\n",
    "                end_idx = extended_end_row.index.min()\n",
    "            else:\n",
    "                end_idx = len(df)  # Default to full text if no better match is found\n",
    "        else:\n",
    "            end_idx = potential_end_idx  # Use valid end index if found\n",
    "    else:\n",
    "        end_idx = len(df)  # Default to full text if no end marker is found\n",
    "\n",
    "    # Assign extracted part\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if end_idx < len(df) else \"❌ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"❌ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_csv_{year}\"  # Change this to your actual directory\n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"❌ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str).unique())  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\n❌ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"📌 Available Parts: {failed_verdicts[-1]['all_parts']}\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"start_part\": start_part if start_part else \"❌ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"❌ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with start and end parts\n",
    "print(\"\\n=== Sample of Successful Extractions (Start & End Parts) ===\")\n",
    "print(final_df[[\"verdict\", \"start_part\", \"end_part\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\n✅ Process complete. Results saved as 'processed_verdicts.csv' and 'failed_verdicts.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "60154.03s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.62.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.5.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /sise/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /sise/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /sise/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Collecting exceptiongroup>=1.0.2 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in /sise/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.62.0-py3-none-any.whl (464 kB)\n",
      "Downloading anyio-4.5.2-py3-none-any.whl (89 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (338 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, jiter, h11, exceptiongroup, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.5.2 distro-1.9.0 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.8.2 openai-1.62.0 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts with API gpt-VERDICTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\n",
    "    \"טענות\", \"עמדת\", \"תסקיר\", \"שירות\", \"מבחן\", \"דיון\", \"התסקיר\",\n",
    "    \"טיעוני\", \"הצדדים\", \"צדדים\", \"והכרעה\",  \"ראיות\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "\n",
    "    # Find the first row where 'part' contains a start pattern (case-insensitive, partial match)\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    if start_row.empty:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "    start_idx = start_row.index.min()\n",
    "    start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Find the first row where 'part' contains an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Handle case where start and end are identical (wrong extraction range)\n",
    "    if not end_row.empty and end_row.index.min() == start_idx:\n",
    "        print(f\"⚠️ Warning: Start and End are the same for verdict '{df['verdict'].iloc[0]}'. Extending search.\")\n",
    "        end_row = df[df.index > start_idx + 1][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    end_idx = end_row.index.min() if not end_row.empty else len(df)\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if not end_row.empty else \"❌ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"❌ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "\n",
    "def extract_facts_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Sends extracted text to GPT API and extracts specific facts.\n",
    "    \"\"\"\n",
    "    if text == \"❌ No indictment facts found\":\n",
    "        return \"❌ No facts extracted\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Extract only the factual allegations from the provided legal text, preserving the original wording without summarizing, rephrasing, or omitting details. Present the extracted facts as a single paragraph rather than a structured list.\n",
    "\n",
    "    **Guidelines:**\n",
    "    - Do not include conclusions, arguments, or legal interpretations.\n",
    "    - Keep the extracted text **exactly as it appears** in the original source.\n",
    "    - Maintain coherence when merging multiple allegations into a single paragraph.\n",
    "\n",
    "    **Example:**\n",
    "    \n",
    "    **Input:**\n",
    "    הנאשם הורשע על פי הודאתו בעבירות של החזקת חלק של נשק או תחמושת, לפי סעיף 144 (א) לחוק העונשין, תשל\"ז 1977 (להלן: \"חוק העונשין\") ונשיאה/הובלת חלק של נשק או תחמושת, לפי סעיף 144(ב) לחוק העונשין. על פי הנטען בכתב האישום ביום 28.8.2022, בשעה 00:20 לערך, נהג הנאשם ברכב מסוג קיה ספורטג' נושא לוחית רישוי מספר 13-608-201 אל עבר מעבר הל\"ה בדרכו לשטחי האזור, כל זאת כאשר הוא נושא מתחת למושב הנהג ברכב שקית ובה 6 מכלולים של נשק מסוג M16. בנוסף בתא המטען של הרכב נשא הנאשם שבעה ארגזי תחמושת וארגז קרטון אשר הכילו יחדיו כ-9000 כדורים בקוטר 5.56 מ\"מ אשר היו מכוסים ומוסתרים.\n",
    "\n",
    "    **Expected Output:**\n",
    "    הנאשם הורשע על פי הודאתו בעבירות של החזקת חלק של נשק או תחמושת, לפי סעיף 144 (א) לחוק העונשין, תשל\"ז 1977 ונשיאה/הובלת חלק של נשק או תחמושת, לפי סעיף 144(ב) לחוק העונשין. על פי הנטען בכתב האישום, ביום 28.8.2022 בשעה 00:20 לערך, נהג הנאשם ברכב מסוג קיה ספורטג' עם לוחית רישוי מספר 13-608-201 לכיוון מעבר הל\"ה בדרכו לשטחי האזור, כאשר מתחת למושב הנהג ברכב הייתה שקית ובה 6 מכלולים של נשק מסוג M16. בנוסף, בתא המטען של הרכב נשא שבעה ארגזי תחמושת וארגז קרטון שהכילו יחדיו כ-9000 כדורים בקוטר 5.56 מ\"מ, שהיו מכוסים ומוסתרים.\n",
    "\n",
    "    **Input Text:**\n",
    "    {text}\n",
    "\n",
    "    **Extracted Facts:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract factual allegations from legal texts, ensuring no interpretation or rewording.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extracted_facts = response.choices[0].message.content.strip()\n",
    "    print(\"Input Text:\", text)\n",
    "    print(\"Extracted Facts:\", extracted_facts)\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "\n",
    "\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_csv_{year}\"  # Change this to your actual directory\n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Extract facts using GPT\n",
    "            extracted_gpt_facts = extract_facts_with_gpt(extracted_facts)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"❌ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str))  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\n❌ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"📌 Available Parts: {failed_verdicts[-1]['all_parts']}\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"extracted_gpt_facts\": extracted_gpt_facts,\n",
    "                \"start_part\": start_part if start_part else \"❌ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"❌ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_verdicts_with_gpt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with GPT output\n",
    "print(\"\\n=== Sample of Successful Extractions (GPT Facts) ===\")\n",
    "print(final_df[[\"verdict\", \"extracted_gpt_facts\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\n✅ Process complete. Results saved as 'processed_verdicts_with_gpt.csv' and 'failed_verdicts.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract indictment facts with API gpt-APPEALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # Replace with actual key\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "models = client.models.list()\n",
    "\n",
    "# Define start and end patterns based on the 'part' column (for partial matches)\n",
    "START_PARTS = [\n",
    "    \"עובדותם\", \"כללי\", \"כתב האישום\", \"האישום\", \"אישום\", \"רקע\", \"גזר\", \"דין\", \"פסק\",\"מבוא\",\"הרשעת\" ,\"בעניינו\",\"עבירות\",\"הורשע\",\"עובדות\"\n",
    "]\n",
    "\n",
    "END_PARTS = [\"אני מסכים\"\n",
    "    \"טענות\", \"עמדת\", \"תסקיר\", \"שירות\", \"מבחן\", \"דיון\", \"התסקיר\",\n",
    "    \"טיעוני\", \"הצדדים\", \"צדדים\", \"והכרעה\", \"ראיות\",\"הכרעה\"\n",
    "]\n",
    "\n",
    "def extract_indictment_facts(df):\n",
    "    \"\"\"\n",
    "    Extracts the 'Indictment Facts' section based on the 'part' column with partial matches.\n",
    "    Ensures:\n",
    "    - If start and end are the same, it extends the search.\n",
    "    - If no start is found, it attempts a secondary search in the text.\n",
    "    - The text **does not** include the content of the end part, only up to it.\n",
    "    \"\"\"\n",
    "    if df.empty or \"part\" not in df.columns or \"text\" not in df.columns:\n",
    "        return \"❌ No indictment facts found\", None, None\n",
    "\n",
    "    # Search for start part in 'part' column\n",
    "    start_row = df[df[\"part\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    if start_row.empty:\n",
    "        # Secondary search: check if the 'text' column contains possible indicators\n",
    "        text_match = df[df[\"text\"].str.contains('|'.join(START_PARTS), case=False, na=False, regex=True)]\n",
    "        if text_match.empty:\n",
    "            return \"❌ No indictment facts found (start section missing)\", None, None\n",
    "        else:\n",
    "            start_idx = text_match.index.min()\n",
    "            start_part_name = \"🔍 Found in text column\"\n",
    "    else:\n",
    "        start_idx = start_row.index.min()\n",
    "        start_part_name = df.loc[start_idx, \"part\"]\n",
    "\n",
    "    # Search for the first row containing an end pattern **after** the start index\n",
    "    end_row = df[df.index > start_idx][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "\n",
    "    # Handle case where start and end are identical (wrong extraction range)\n",
    "    if not end_row.empty and end_row.index.min() == start_idx:\n",
    "        print(f\"⚠️ Warning: Start and End are the same for verdict '{df['verdict'].iloc[0]}'. Extending search.\")\n",
    "        end_row = df[df.index > start_idx + 1][df[\"part\"].str.contains('|'.join(END_PARTS), case=False, na=False, regex=True)]\n",
    "    \n",
    "    end_idx = end_row.index.min() if not end_row.empty else len(df)\n",
    "    end_part_name = df.loc[end_idx, \"part\"] if not end_row.empty else \"❌ No end found (used full text)\"\n",
    "\n",
    "    # Extract text **only until** the end part, excluding it\n",
    "    extracted_text = \"\\n\".join(df.loc[start_idx:end_idx-1, \"text\"].dropna().astype(str))  # Exclude the last part\n",
    "\n",
    "    return extracted_text.strip() if extracted_text else \"❌ No indictment facts found\", start_part_name, end_part_name\n",
    "\n",
    "\n",
    "def extract_facts_with_gpt(text):\n",
    "    \"\"\"\n",
    "    Sends extracted text to GPT API and extracts the original indictment details from the case being appealed.\n",
    "    \"\"\"\n",
    "    if text == \"❌ No indictment facts found\":\n",
    "        return \"❌ No facts extracted\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    **Task:** Extract only the original indictment details of the case being appealed. Ignore all references to the appeal decision, legal arguments, and judicial reasoning. The extracted text should contain only the original facts that led to the indictment, exactly as they appear in the text.\n",
    "\n",
    "    **Guidelines:**\n",
    "    - Extract **only the factual allegations** from the original indictment.\n",
    "    - **Do not include** details about the appeal, court rulings, or sentencing decisions.\n",
    "    - Maintain the **exact wording** of the indictment without summarizing or omitting details.\n",
    "    - If the indictment contains multiple allegations, present them in a coherent paragraph.\n",
    "\n",
    "    **Example:**\n",
    "    \n",
    "    **Input:**\n",
    "    ע\"\"פ 761∕07 - ערעור על גזר דינו של בית המשפט המחוזי. \n",
    "    באחד מימיו של חודש יוני 2006, בשעות הערב, נהג הנאשם ברכב, וכאשר נעצר על ידי שוטרים לבדיקה, הוא נמצא מחזיק באקדח, מחסנית ותחמושת כשאלה עטופים בגרב ומוסתרים בתחתוניו.\n",
    "    כן נטען, כי הנאשם הציג בפני השוטרים תעודת זהות של אחר מתוך כוונה להונותם.\n",
    "    הנאשם הודה בעובדות האמורות, ובעקבות כך הורשע בעבירות של החזקת נשק שלא כדין והפרעה לשוטר במילוי תפקידו, עבירות לפי סעיפים 144 רישא ו-275 לחוק העונשין.\n",
    "\n",
    "    **Expected Output:**\n",
    "    באחד מימיו של חודש יוני 2006, בשעות הערב, נהג הנאשם ברכב, וכאשר נעצר על ידי שוטרים לבדיקה, נמצא מחזיק באקדח, מחסנית ותחמושת עטופים בגרב ומוסתרים בתחתוניו. בנוסף, הציג לשוטרים תעודת זהות של אחר בכוונה להונותם. על סמך עובדות אלה, הואשם בעבירות של החזקת נשק שלא כדין והפרעה לשוטר במילוי תפקידו לפי סעיפים 144 רישא ו-275 לחוק העונשין.\n",
    "\n",
    "    **Input Text:**\n",
    "    {text}\n",
    "\n",
    "    **Extracted Indictment Details:**\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\", \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI trained to extract factual allegations from legal texts, ensuring no interpretation or rewording.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    extracted_facts = response.choices[0].message.content.strip()\n",
    "    # print(\"Input Text:\", text)\n",
    "    print(\"Extracted Facts:\", extracted_facts)\n",
    "    return extracted_facts\n",
    "\n",
    "\n",
    "# Tracking statistics\n",
    "total_files = 0\n",
    "successful_extractions = 0\n",
    "failed_extractions = 0\n",
    "failed_verdicts = []\n",
    "extracted_results = []\n",
    "\n",
    "\n",
    "for year in [2018,2019,2020]:\n",
    "    csv_directory = f\"/home/liorkob/thesis/lcp/data/docx_citations_csv_{year}\"  \n",
    "\n",
    "    # Iterate through all CSV files in the directory\n",
    "    for filename in os.listdir(csv_directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            total_files += 1\n",
    "            file_path = os.path.join(csv_directory, filename)\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure necessary columns exist\n",
    "            if 'verdict' not in df.columns or 'text' not in df.columns or 'part' not in df.columns:\n",
    "                print(f\"Skipping {filename}, missing required columns.\")\n",
    "                continue\n",
    "\n",
    "            # Extract indictment facts based on 'part'\n",
    "            extracted_facts, start_part, end_part = extract_indictment_facts(df)\n",
    "\n",
    "            # Extract facts using GPT\n",
    "            extracted_gpt_facts = extract_facts_with_gpt(extracted_facts)\n",
    "\n",
    "            # Track statistics\n",
    "            if extracted_facts == \"❌ No indictment facts found\":\n",
    "                failed_extractions += 1\n",
    "                failed_verdicts.append({\n",
    "                    \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                    \"all_parts\": \"; \".join(df[\"part\"].dropna().astype(str))  # Store all parts for debugging\n",
    "                })\n",
    "                print(f\"\\n❌ **Failed Extraction for Verdict: {df['verdict'].iloc[0]}**\")\n",
    "                print(f\"📌 Available Parts: {failed_verdicts[-1]['all_parts']}.unique()\\n\")\n",
    "            else:\n",
    "                successful_extractions += 1\n",
    "\n",
    "            # Store results\n",
    "            extracted_results.append({\n",
    "                \"verdict\": df[\"verdict\"].iloc[0],\n",
    "                \"extracted_facts\": extracted_facts,\n",
    "                \"extracted_gpt_facts\": extracted_gpt_facts,\n",
    "                \"start_part\": start_part if start_part else \"❌ Not Found\",\n",
    "                \"end_part\": end_part if end_part else \"❌ Not Found\"\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "final_df = pd.DataFrame(extracted_results)\n",
    "failed_df = pd.DataFrame(failed_verdicts) if failed_verdicts else pd.DataFrame(columns=[\"verdict\", \"all_parts\"])\n",
    "\n",
    "# Save results\n",
    "final_df.to_csv(\"processed_appeals_with_gpt.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "failed_df.to_csv(\"failed_verdicts.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Display statistics\n",
    "stats = {\n",
    "    \"Total CSV Files Processed\": total_files,\n",
    "    \"Successful Extractions\": successful_extractions,\n",
    "    \"Failed Extractions\": failed_extractions\n",
    "}\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n=== Statistics ===\")\n",
    "print(pd.DataFrame([stats]))\n",
    "\n",
    "# Show failed verdicts (if any)\n",
    "if not failed_df.empty:\n",
    "    print(\"\\n=== Sample of Failed Verdicts ===\")\n",
    "    print(failed_df.head())  # Print first few rows for review\n",
    "\n",
    "# Show extracted results with GPT output\n",
    "print(\"\\n=== Sample of Successful Extractions (GPT Facts) ===\")\n",
    "print(final_df[[\"verdict\", \"extracted_gpt_facts\"]].head())  # Print first few rows\n",
    "\n",
    "print(\"\\n✅ Process complete. Results saved as 'processed_appeals_with_gpt.csv' and 'failed_verdicts.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judgeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
