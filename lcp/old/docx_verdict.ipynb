{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 7.94MB/s]                    \n",
      "2025-01-15 13:08:54 INFO: Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:08:54,662 - INFO - Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:08:54 INFO: Downloading default packages for language: he (Hebrew) ...\n",
      "2025-01-15 13:08:54,669 - INFO - Downloading default packages for language: he (Hebrew) ...\n",
      "2025-01-15 13:08:55 INFO: File exists: /home/liorkob/stanza_resources/he/default.zip\n",
      "2025-01-15 13:08:55,226 - INFO - File exists: /home/liorkob/stanza_resources/he/default.zip\n",
      "2025-01-15 13:08:57 INFO: Finished downloading models and saved to /home/liorkob/stanza_resources\n",
      "2025-01-15 13:08:57,481 - INFO - Finished downloading models and saved to /home/liorkob/stanza_resources\n",
      "2025-01-15 13:08:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "2025-01-15 13:08:57,483 - INFO - Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 8.00MB/s]                    \n",
      "2025-01-15 13:08:57 INFO: Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:08:57,712 - INFO - Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:09:07 INFO: Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | iahlt_charlm      |\n",
      "=================================\n",
      "\n",
      "2025-01-15 13:09:07,417 - INFO - Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | iahlt_charlm      |\n",
      "=================================\n",
      "\n",
      "2025-01-15 13:09:07 INFO: Using device: cuda\n",
      "2025-01-15 13:09:07,418 - INFO - Using device: cuda\n",
      "2025-01-15 13:09:07 INFO: Loading: tokenize\n",
      "2025-01-15 13:09:07,419 - INFO - Loading: tokenize\n",
      "2025-01-15 13:09:07 INFO: Loading: mwt\n",
      "2025-01-15 13:09:07,439 - INFO - Loading: mwt\n",
      "2025-01-15 13:09:07 INFO: Loading: pos\n",
      "2025-01-15 13:09:07,453 - INFO - Loading: pos\n",
      "2025-01-15 13:09:07 INFO: Loading: lemma\n",
      "2025-01-15 13:09:07,677 - INFO - Loading: lemma\n",
      "2025-01-15 13:09:07 INFO: Loading: depparse\n",
      "2025-01-15 13:09:07,703 - INFO - Loading: depparse\n",
      "2025-01-15 13:09:07 INFO: Loading: ner\n",
      "2025-01-15 13:09:07,878 - INFO - Loading: ner\n",
      "2025-01-15 13:09:08 INFO: Done loading processors!\n",
      "2025-01-15 13:09:08,198 - INFO - Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/pdocx/bin/python\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import re\n",
    "from docx.text.paragraph import Paragraph\n",
    "from docx.document import Document\n",
    "from docx.table import _Cell, Table\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "stanza.download('he')\n",
    "nlp = stanza.Pipeline('he')\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "# Modify property of Paragraph.text to include hyperlink text\n",
    "Paragraph.text = property(lambda self: get_paragraph_text(self))\n",
    "\n",
    "def get_paragraph_text(paragraph) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from paragraph, including hyperlink text.\n",
    "    \"\"\"\n",
    "    def get_xml_tag(element):\n",
    "        return \"%s:%s\" % (element.prefix, re.match(\"{.*}(.*)\", element.tag).group(1))\n",
    "\n",
    "    text_content = ''\n",
    "    run_count = 0\n",
    "    for child in paragraph._p:\n",
    "        tag = get_xml_tag(child)\n",
    "        if tag == \"w:r\":\n",
    "            text_content += paragraph.runs[run_count].text\n",
    "            run_count += 1\n",
    "        if tag == \"w:hyperlink\":\n",
    "            for sub_child in child:\n",
    "                if get_xml_tag(sub_child) == \"w:r\":\n",
    "                    text_content += sub_child.text\n",
    "    return text_content\n",
    "\n",
    "def is_block_bold(block) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the entire block/paragraph text is bold.\n",
    "    \"\"\"\n",
    "    if block.runs:\n",
    "        for run in block.runs:\n",
    "            if run.bold:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def iterate_block_items(parent):\n",
    "    \"\"\"\n",
    "    Iterate over paragraphs and tables in a document or cell.\n",
    "    \"\"\"\n",
    "    if isinstance(parent, Document):\n",
    "        parent_element = parent.element.body\n",
    "    elif isinstance(parent, _Cell):\n",
    "        parent_element = parent._tc\n",
    "    else:\n",
    "        # Log unsupported type and return\n",
    "        print(f\"Unsupported parent type: {type(parent)}\")\n",
    "        return\n",
    "\n",
    "    for child in parent_element.iterchildren():\n",
    "        try:\n",
    "            if isinstance(child, CT_P):\n",
    "                yield Paragraph(child, parent)\n",
    "            elif isinstance(child, CT_Tbl):\n",
    "                table = Table(child, parent)\n",
    "                for row in table.rows:\n",
    "                    for cell in row.cells:\n",
    "                        yield from iterate_block_items(cell)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing child element: {e}\")\n",
    "\n",
    "def extract_part_after_number_or_hebrew_letter(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract text following a pattern of number or Hebrew letter.\n",
    "    \"\"\"\n",
    "    pattern = r'^(?:[0-9\\u05D0-\\u05EA]+)\\.\\s*(.*)'\n",
    "    match = re.search(pattern, sentence)\n",
    "    return match.group(1).strip() if match else sentence\n",
    "\n",
    "def count_patterns_in_block(block) -> int:\n",
    "    \"\"\"\n",
    "    Count the number-dot or dot-number patterns in a block.\n",
    "    \"\"\"\n",
    "    pattern = r'\\s*(?:\\.\\d+|\\d+\\.)'\n",
    "    return len(re.findall(pattern, block.text))\n",
    "\n",
    "def count_consecutive_blocks_starting_with_number(blocks) -> int:\n",
    "    \"\"\"\n",
    "    Count consecutive blocks starting with a number or Hebrew letter.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for block in blocks:\n",
    "        if 'הנאשם' in block.text:\n",
    "            return 1\n",
    "        count += count_patterns_in_block(block)\n",
    "        if 'חקיקה שאוזכרה' in block.text:\n",
    "            break\n",
    "    return count\n",
    "\n",
    "def extract_name_after_word(text: str, word: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the words following a given word up to the end of the sentence.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(fr'{word}(?:,)?\\s*([\\u0590-\\u05FF\\s\\'\\(\\)-]+)')\n",
    "    match = pattern.search(text)\n",
    "    return match.group(1) if match else ''\n",
    "\n",
    "def extract_violations(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Extract violations from the text based on a pre-defined pattern.\n",
    "    \"\"\"\n",
    "\n",
    "    matches = re.findall(r\"(?:סעיף|סעיפים|ס'|סע')\\s*\\d+\\s*(?:\\([\\s\\S]*?\\))?.*?(?=\\s*(?:ב|ל)(?:חוק|פקודת))\\s*(?:ב|ל)(?:חוק|פקודת)\\s*ה?(?:עונשין|כניסה לישראל|סמים\\s+המסוכנים|\\w+)?\", text)\n",
    "    # matches = re.findall(r\"(?:סעיף|סעיפים|ס'|סע')\\s*\\d+\\s*(?:\\([\\s\\S]*?\\))?.*?(?=\\s*(?:ב|ל)(?:חוק|פקודת))\\s*(?:ב|ל)(?:חוק|פקודת)\\s*ה?(?:עונשין|כניסה לישראל|סמים\\s+המסוכנים|[^\\[]+)?\", text)\n",
    "\n",
    "    matches = [match.strip() for match in matches]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 3.90MB/s]                    \n",
      "2025-01-15 13:10:22 INFO: Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:10:22,029 - INFO - Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:10:22 INFO: Downloading default packages for language: he (Hebrew) ...\n",
      "2025-01-15 13:10:22,041 - INFO - Downloading default packages for language: he (Hebrew) ...\n",
      "2025-01-15 13:10:22 INFO: File exists: /home/liorkob/stanza_resources/he/default.zip\n",
      "2025-01-15 13:10:22,606 - INFO - File exists: /home/liorkob/stanza_resources/he/default.zip\n",
      "2025-01-15 13:10:24 INFO: Finished downloading models and saved to /home/liorkob/stanza_resources\n",
      "2025-01-15 13:10:24,954 - INFO - Finished downloading models and saved to /home/liorkob/stanza_resources\n",
      "2025-01-15 13:10:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "2025-01-15 13:10:24,957 - INFO - Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 8.24MB/s]                    \n",
      "2025-01-15 13:10:25 INFO: Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:10:25,173 - INFO - Downloaded file to /home/liorkob/stanza_resources/resources.json\n",
      "2025-01-15 13:10:33 INFO: Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | iahlt_charlm      |\n",
      "=================================\n",
      "\n",
      "2025-01-15 13:10:33,110 - INFO - Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | iahlt_charlm      |\n",
      "=================================\n",
      "\n",
      "2025-01-15 13:10:33 INFO: Using device: cuda\n",
      "2025-01-15 13:10:33,111 - INFO - Using device: cuda\n",
      "2025-01-15 13:10:33 INFO: Loading: tokenize\n",
      "2025-01-15 13:10:33,112 - INFO - Loading: tokenize\n",
      "2025-01-15 13:10:33 INFO: Loading: mwt\n",
      "2025-01-15 13:10:33,129 - INFO - Loading: mwt\n",
      "2025-01-15 13:10:33 INFO: Loading: pos\n",
      "2025-01-15 13:10:33,144 - INFO - Loading: pos\n",
      "2025-01-15 13:10:33 INFO: Loading: lemma\n",
      "2025-01-15 13:10:33,378 - INFO - Loading: lemma\n",
      "2025-01-15 13:10:33 INFO: Loading: depparse\n",
      "2025-01-15 13:10:33,408 - INFO - Loading: depparse\n",
      "2025-01-15 13:10:33 INFO: Loading: ner\n",
      "2025-01-15 13:10:33,590 - INFO - Loading: ner\n",
      "2025-01-15 13:10:33 INFO: Done loading processors!\n",
      "2025-01-15 13:10:33,910 - INFO - Done loading processors!\n",
      "2025-01-15 13:10:33,915 - INFO - Processing directory: /home/liorkob/thesis/nlp_course/lcp/docx_try\n",
      "2025-01-15 13:10:33,924 - INFO - Processing file: /home/liorkob/thesis/nlp_course/lcp/docx_try/SH-16-11-49772-358.docx\n",
      "2025-01-15 13:10:33,934 - ERROR - Error processing SH-16-11-49772-358.docx: isinstance() arg 2 must be a type, a tuple of types, or a union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is valid.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from docx import Document\n",
    "import logging\n",
    "\n",
    "\"\"\"\n",
    "This script processes `.docx` verdict files, extracting text from them, identifying and classifying specific sections of the document. It saves the results into CSV files with the extracted text, sections, and metadata for further analysis.\n",
    "\n",
    "### Key Functionalities:\n",
    "\n",
    "1. **Text Extraction and Preprocessing**:\n",
    "   - The script iterates through paragraphs in `.docx` files, using custom functions from `utils.py` to identify specific sections based on formatting (e.g., bold text).\n",
    "   - The extracted sections are stored in a dictionary along with the corresponding full sentences from the document.\n",
    "\n",
    "2. **Part Identification**:\n",
    "   - It processes bolded blocks of text as distinct \"parts\" or sections (e.g., titles or key sections) and appends them to a list.\n",
    "   - For each sentence, the script associates it with both the most recent part (stored as `part_single`) and a concatenation of all previous parts (stored as `part_concatenated`).\n",
    "\n",
    "3. **NLP Processing**:\n",
    "   - The Hebrew Stanza NLP pipeline is used to split the text into sentences, which are then stored in the output alongside the associated document sections.\n",
    "   - The script also applies filters to skip short paragraphs and unwanted patterns (e.g., references to certain case types).\n",
    "\n",
    "4. **Error Handling and Logging**:\n",
    "   - The script uses Python’s `logging` module to provide informative logs, including handling errors if a document can't be opened or processed.\n",
    "   - It catches and logs any exceptions during the processing of files.\n",
    "\n",
    "5. **CSV Output**:\n",
    "   - For each `.docx` file, the extracted data (including text, section titles, and concatenated sections) is saved to a CSV file.\n",
    "\n",
    "6. **Recursive Directory Processing**:\n",
    "   - The script recursively processes `.docx` files in a specified root directory (`selenium_downloads\\מרב גרינברג`), saving the results for each file in a corresponding output directory (`outputs\\merav_grinberg_preproccsed`).\n",
    "\n",
    "### Main Functions:\n",
    "\n",
    "- **doc_to_csv(doc_path: str, result_path: str)**:\n",
    "   - Processes a single `.docx` file, extracting text and metadata.\n",
    "   - Saves the results to a CSV file if a result path is provided.\n",
    "\n",
    "- **run()**:\n",
    "   - Iterates through all `.docx` files in the root directory.\n",
    "   - For each file, it calls `doc_to_csv` and saves the resulting DataFrame as a CSV.\n",
    "\n",
    "### Usage:\n",
    "The script is executed via the `run()` function, which processes all files in the specified directory. It logs the status and outputs CSV files containing preprocessed data for each document.\n",
    "\"\"\"\n",
    "\n",
    "stanza.download('he')\n",
    "nlp = stanza.Pipeline('he')\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "def validate_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        print(\"The file is valid.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating document: {e}\")\n",
    "        return False\n",
    "\n",
    "def doc_to_csv(doc_path: str = None, result_path: str = None):\n",
    "    logging.info(f\"Processing file: {doc_path}\")\n",
    "    \n",
    "    data = {'verdict': [], 'text': [], 'part_single': [], 'part_concatenated': []}\n",
    "    data['verdict'] = os.path.splitext(os.path.basename(doc_path))[0]\n",
    "    \n",
    "    # Ensure the file path ends with .docx\n",
    "    if not doc_path.lower().endswith('.docx'):\n",
    "        doc_path += '.docx'\n",
    "    \n",
    "    if not os.path.exists(doc_path):\n",
    "        raise FileNotFoundError(f\"File not found: {doc_path}\")\n",
    "    \n",
    "    try:\n",
    "        doc = Document(doc_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error opening document {doc_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    # Initialize the `part` variables\n",
    "    part_single = 'nothing'  # This stores only the current part\n",
    "    part_list = []  # This list will store all parts for concatenation\n",
    "\n",
    "    for block in iterate_block_items(doc):\n",
    "        flag = False\n",
    "\n",
    "        # Skip blocks that are too short or contain specific unwanted patterns\n",
    "        if len(block.text) <= 1 or 'ע\"פ' in block.text or 'ת\"פ' in block.text or 'עפ\"ג' in block.text:\n",
    "            continue\n",
    "        #if len(block.text) <= 1:\n",
    "         #   continue\n",
    "\n",
    "        # If the block is bold and meets specific conditions, treat it as a new \"part\"\n",
    "        if is_block_bold(block) and len(block.text.split(' ')) < 10 and not re.match(r'^\\d', block.text) and not re.match(r'[\\u0590-\\u05FF][^.)*]*[.)]', block.text):\n",
    "            # Update both `part_single` and `part_list`\n",
    "            part_single = block.text  # Update the current part\n",
    "            part_list.append(block.text)  # Append to the list of parts for concatenation\n",
    "            part_concatenated = ', '.join(part_list)  # Concatenate all parts\n",
    "        else:\n",
    "            # For non-bold blocks, process the text as a sentence and associate it with both `part_single` and `part_concatenated`\n",
    "            extracted_part_text = extract_part_after_number_or_hebrew_letter(block.text)\n",
    "            sentences = nlp(extracted_part_text)\n",
    "\n",
    "            for sentence in sentences.sentences:\n",
    "                text = sentence.text\n",
    "                if text.startswith('\"'):\n",
    "                    flag = True\n",
    "                    continue\n",
    "                if text.endswith('\".') or text.endswith('\"'):\n",
    "                    flag = False\n",
    "                    continue\n",
    "                if flag:\n",
    "                    continue\n",
    "                if text == part_single:\n",
    "                    continue\n",
    "                if len(block.text.split(' ')) > 3:\n",
    "                    # Append the text and its corresponding parts\n",
    "                    data['text'].append(text)\n",
    "                    data['part_single'].append(part_single)\n",
    "                    data['part_concatenated'].append(part_concatenated)\n",
    "\n",
    "    # Convert the data dictionary into a DataFrame\n",
    "    sentence_doc_df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the result if a result path is provided\n",
    "    if result_path:\n",
    "        result_path = os.path.join(result_path, 'preprocessing.csv')\n",
    "        sentence_doc_df.to_csv(result_path, index=False)\n",
    "        logging.info(f\"Saved preprocessed data to {result_path}\")\n",
    "    \n",
    "    return sentence_doc_df\n",
    "\n",
    "def run():\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    root_directory = \"/home/liorkob/thesis/nlp_course/lcp/docx_try\"\n",
    "\n",
    "    for root, _, files in os.walk(root_directory):\n",
    "        logging.info(f\"Processing directory: {root}\")\n",
    "        for file in files:\n",
    "            if not file.lower().endswith('.docx'):\n",
    "                continue\n",
    "            \n",
    "            input_path = os.path.join(root, file)\n",
    "            output_dir = root_directory\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                is_valid = validate_docx(input_path)\n",
    "                df = doc_to_csv(doc_path=input_path)\n",
    "                output_path = os.path.join(output_dir, f\"{os.path.splitext(file)[0]}.csv\")\n",
    "                df.to_csv(output_path, index=False)\n",
    "                logging.info(f\"Processed and saved: {output_path}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing {file}: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdocx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
