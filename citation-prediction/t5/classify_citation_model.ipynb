{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 🔧 CONFIG\n",
    "# ========================\n",
    "# model_path = \"/home/liorkob/M.Sc/thesis/t5/mt5-mlm-final\"\n",
    "# model_path = \"imvladikon/het5-base\"\n",
    "model_path = \"google/mt5-base\"\n",
    "# model_path=\"/home/liorkob/M.Sc/thesis/t5/mt5-punishment-regression\"\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# 🧠 Dataset for T5\n",
    "# ========================\n",
    "class T5CitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.inputs = df.apply(lambda row: f\"predict citation: {row['gpt_facts_a']} </s> {row['gpt_facts_b']}\", axis=1).tolist()\n",
    "        self.targets = df[\"label\"].apply(lambda l: \"yes\" if l == 1 else \"no\").tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_enc = self.tokenizer(self.inputs[idx], padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        target_enc = self.tokenizer(self.targets[idx], padding='max_length', truncation=True, max_length=4, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# 📥 Load Data\n",
    "# ========================\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "# model.gradient_checkpointing_enable()  # ✅ חיסכון בזיכרון\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "model.gradient_checkpointing_enable()  # ✅ חיסכון בזיכרון\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "train_dataset = T5CitationDataset(df_train, tokenizer, max_len=max_len)\n",
    "val_dataset = T5CitationDataset(df_val, tokenizer, max_len=max_len)\n",
    "test_dataset = T5CitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ========================\n",
    "# 🔁 Training Loop\n",
    "# ========================\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        torch.cuda.empty_cache()  # ✅ ריקון בין צעדים\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ========================\n",
    "# 📊 Evaluation\n",
    "# ========================\n",
    "def evaluate(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            label_texts = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "            preds.extend([1 if p.strip().lower() == \"yes\" else 0 for p in pred_texts])\n",
    "            labels.extend([1 if l.strip().lower() == \"yes\" else 0 for l in label_texts])\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"AUC-ROC: {roc_auc_score(labels, preds):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(labels, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(labels, preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(labels, preds):.4f}\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "print(\"\\n🔍 Validation Set:\")\n",
    "evaluate(model, val_loader, tokenizer, device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n🧪 Test Set:\")\n",
    "evaluate(model, test_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model and tokenizer...\n",
      "Loading data...\n",
      "\n",
      "🔍 TESTING DIFFERENT LEGAL PROMPTS:\n",
      "============================================================\n",
      "\n",
      "📋 Testing Prompt Version 1:\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3919 characters\n",
      "Prompt version: 1\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:06<00:00, 32.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 3.4868 (F1: 0.5009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:06<00:00, 31.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5009\n",
      "Precision: 0.3382\n",
      "Recall: 0.9655\n",
      "Accuracy: 0.3579\n",
      "\n",
      "Prediction Distribution: [ 41 828]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5095\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.05      0.10       579\n",
      "           1       0.34      0.97      0.50       290\n",
      "\n",
      "    accuracy                           0.36       869\n",
      "   macro avg       0.55      0.51      0.30       869\n",
      "weighted avg       0.62      0.36      0.23       869\n",
      "\n",
      "Prompt 1 Baseline F1: 0.5009\n",
      "\n",
      "📋 Testing Prompt Version 2:\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3919 characters\n",
      "Prompt version: 2\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:06<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 3.5317 (F1: 0.5085)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:06<00:00, 31.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5085\n",
      "Precision: 0.3589\n",
      "Recall: 0.8724\n",
      "Accuracy: 0.4373\n",
      "\n",
      "Prediction Distribution: [164 705]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5459\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.22      0.34       579\n",
      "           1       0.36      0.87      0.51       290\n",
      "\n",
      "    accuracy                           0.44       869\n",
      "   macro avg       0.57      0.55      0.43       869\n",
      "weighted avg       0.64      0.44      0.40       869\n",
      "\n",
      "Prompt 2 Baseline F1: 0.5085\n",
      "\n",
      "📋 Testing Prompt Version 3:\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3924 characters\n",
      "Prompt version: 3\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:06<00:00, 31.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 3.5402 (F1: 0.5142)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 31.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5142\n",
      "Precision: 0.3837\n",
      "Recall: 0.7793\n",
      "Accuracy: 0.5086\n",
      "\n",
      "Prediction Distribution: [280 589]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5762\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.37      0.50       579\n",
      "           1       0.38      0.78      0.51       290\n",
      "\n",
      "    accuracy                           0.51       869\n",
      "   macro avg       0.58      0.58      0.51       869\n",
      "weighted avg       0.64      0.51      0.51       869\n",
      "\n",
      "Prompt 3 Baseline F1: 0.5142\n",
      "\n",
      "🏆 BEST PROMPT VERSION: 3 (F1: 0.5142)\n",
      "\n",
      "Creating final datasets with prompt version 3...\n",
      "Legal Dataset created: 4052 samples\n",
      "Label distribution: [2699 1353]\n",
      "Sample input length: 1989 characters\n",
      "Prompt version: 3\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3924 characters\n",
      "Prompt version: 3\n",
      "Legal Dataset created: 870 samples\n",
      "Label distribution: [579 291]\n",
      "Sample input length: 2199 characters\n",
      "Prompt version: 3\n",
      "\n",
      "Baseline F1 (0.5142) needs improvement. Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1013/1013 [02:05<00:00,  8.07it/s, loss=0.3464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Average Loss = 3.2148\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:07<00:00, 29.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.6755 (F1: 0.5115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5115\n",
      "Precision: 0.3488\n",
      "Recall: 0.9586\n",
      "Accuracy: 0.3890\n",
      "\n",
      "Prediction Distribution: [ 72 797]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5311\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.10      0.18       579\n",
      "           1       0.35      0.96      0.51       290\n",
      "\n",
      "    accuracy                           0.39       869\n",
      "   macro avg       0.59      0.53      0.35       869\n",
      "weighted avg       0.67      0.39      0.29       869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1013/1013 [02:05<00:00,  8.10it/s, loss=0.3013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Average Loss = 0.3555\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:07<00:00, 29.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2674 (F1: 0.5227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5227\n",
      "Precision: 0.3763\n",
      "Recall: 0.8552\n",
      "Accuracy: 0.4787\n",
      "\n",
      "Prediction Distribution: [210 659]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5727\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.29      0.43       579\n",
      "           1       0.38      0.86      0.52       290\n",
      "\n",
      "    accuracy                           0.48       869\n",
      "   macro avg       0.59      0.57      0.47       869\n",
      "weighted avg       0.66      0.48      0.46       869\n",
      "\n",
      "✅ New best model! F1: 0.5227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1013/1013 [02:04<00:00,  8.11it/s, loss=0.4273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Average Loss = 0.3281\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:07<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2343 (F1: 0.5260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5260\n",
      "Precision: 0.3625\n",
      "Recall: 0.9586\n",
      "Accuracy: 0.4235\n",
      "\n",
      "Prediction Distribution: [102 767]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5570\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.16      0.26       579\n",
      "           1       0.36      0.96      0.53       290\n",
      "\n",
      "    accuracy                           0.42       869\n",
      "   macro avg       0.62      0.56      0.40       869\n",
      "weighted avg       0.71      0.42      0.35       869\n",
      "\n",
      "✅ New best model! F1: 0.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1013/1013 [02:05<00:00,  8.10it/s, loss=0.2293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Average Loss = 0.3010\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:07<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.6886 (F1: 0.5330)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 29.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5330\n",
      "Precision: 0.3947\n",
      "Recall: 0.8207\n",
      "Accuracy: 0.5201\n",
      "\n",
      "Prediction Distribution: [266 603]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5951\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.37      0.51       579\n",
      "           1       0.39      0.82      0.53       290\n",
      "\n",
      "    accuracy                           0.52       869\n",
      "   macro avg       0.60      0.60      0.52       869\n",
      "weighted avg       0.67      0.52      0.52       869\n",
      "\n",
      "✅ New best model! F1: 0.5330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1013/1013 [02:04<00:00,  8.11it/s, loss=0.3243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Average Loss = 0.3167\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:07<00:00, 29.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.5397 (F1: 0.5533)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5533\n",
      "Precision: 0.4239\n",
      "Recall: 0.7966\n",
      "Accuracy: 0.5708\n",
      "\n",
      "Prediction Distribution: [324 545]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.6271\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.46      0.59       579\n",
      "           1       0.42      0.80      0.55       290\n",
      "\n",
      "    accuracy                           0.57       869\n",
      "   macro avg       0.62      0.63      0.57       869\n",
      "weighted avg       0.69      0.57      0.58       869\n",
      "\n",
      "✅ New best model! F1: 0.5533\n",
      "\n",
      "================================================================================\n",
      "🧪 FINAL LEGAL CITATION PREDICTION TEST:\n",
      "================================================================================\n",
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 218/218 [00:07<00:00, 29.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.5047 (F1: 0.5338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|██████████| 218/218 [00:07<00:00, 29.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5338\n",
      "Precision: 0.4157\n",
      "Recall: 0.7457\n",
      "Accuracy: 0.5644\n",
      "\n",
      "Prediction Distribution: [348 522]\n",
      "True Label Distribution: [579 291]\n",
      "AUC-ROC: 0.6095\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.47      0.59       579\n",
      "           1       0.42      0.75      0.53       291\n",
      "\n",
      "    accuracy                           0.56       870\n",
      "   macro avg       0.60      0.61      0.56       870\n",
      "weighted avg       0.66      0.56      0.57       870\n",
      "\n",
      "\n",
      "🎯 FINAL RESULTS:\n",
      "Best Prompt Version: 3\n",
      "Baseline F1: 0.5142\n",
      "Final Test F1: 0.5338\n",
      "Final Test Accuracy: 0.5644\n",
      "Optimal Threshold: -6.5047\n",
      "\n",
      "📈 F1: 0.5338 - Consider fine-tuning hyperparameters or trying longer training\n",
      "\n",
      "💾 Results saved to 'legal_citation_results.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ========================\n",
    "# 🔧 CONFIG\n",
    "# ========================\n",
    "model_path = \"imvladikon/het5-base\"\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================\n",
    "# 🎯 CORE: Improved Logits Classification\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    \"\"\"Classify using threshold-based method\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=input_ids.device)\n",
    "        decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        yes_tokens = [259, 1903]  # כן\n",
    "        no_tokens = [1124]        # לא\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx]\n",
    "            \n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            \n",
    "            score_diff = yes_score - no_score\n",
    "            \n",
    "            if score_diff > threshold:\n",
    "                prediction = 1\n",
    "                predicted_text = \"כן\"\n",
    "            else:\n",
    "                prediction = 0\n",
    "                predicted_text = \"לא\"\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            scores.append({\n",
    "                'prediction': prediction,\n",
    "                'predicted_text': predicted_text,\n",
    "                'score_diff': score_diff,\n",
    "                'yes_score': yes_score,\n",
    "                'no_score': no_score\n",
    "            })\n",
    "        \n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, device, true_labels):\n",
    "    \"\"\"Find optimal threshold for balanced predictions\"\"\"\n",
    "    print(\"🔍 Finding best threshold...\")\n",
    "    \n",
    "    all_score_diffs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Collecting scores\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            _, scores = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=0.0\n",
    "            )\n",
    "            \n",
    "            for score in scores:\n",
    "                all_score_diffs.append(score['score_diff'])\n",
    "    \n",
    "    # Test thresholds\n",
    "    thresholds = np.linspace(min(all_score_diffs), max(all_score_diffs), 50)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch_preds, _ = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "# ========================\n",
    "# 🧠 IMPROVED Dataset with Specific Legal Prompts\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512, prompt_version=1):\n",
    "        \"\"\"\n",
    "        Dataset with legally-specific prompts for sentencing citation prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiple versions of detailed legal prompts\n",
    "        self.legal_prompts = {\n",
    "            1: {\n",
    "                \"hebrew\": \"\"\"משימה: חיזוי ציטוטים לתמיכה במדיניות גזר דין\n",
    "הקשר: בפסקי דין פליליים, שופטים מצטטים פסקי דין קודמים כדי לתמוך בטווח הענישה שהם מציעים. לא כל הציטוטים רלוונטיים - אנו מתמקדים רק בציטוטים התומכים בהחלטות טווח הענישה.\n",
    "שאלה: האם פסק דין א' יצטט פסק דין ב' כדי לתמוך במדיניות גזר הדין שלו, על בסיס עובדות כתב האישום?\"\"\",\n",
    "                \"english\": \"\"\"Task: Predict citations supporting sentencing policy decisions\n",
    "Context: In criminal verdicts, judges cite previous rulings to support their proposed sentencing range. Not all citations are relevant - we focus specifically on citations supporting sentencing range decisions.\n",
    "Question: Will verdict A cite verdict B to support its sentencing policy, based on indictment facts?\"\"\"\n",
    "            },\n",
    "            \n",
    "            2: {\n",
    "                \"hebrew\": \"\"\"ניתוח ציטוטים בפסקי דין פליליים\n",
    "מטרה: זיהוי ציטוטים הרלוונטיים למדיניות ענישה\n",
    "הגדרה: ציטוט רלוונטי = הפניה לפסק דין קודם המשמש כתקדים לטווח העונש המוצע\n",
    "מיקום: בדרך כלל בחלק \"מדיניות הענישה\" או \"טווח הענישה\" של פסק הדין\n",
    "שאלה: בהתבסס על עובדות כתב האישום, האם צפוי שפסק דין א' יצטט פסק דין ב' לתמיכה בטווח הענישה?\"\"\",\n",
    "                \"english\": \"\"\"Criminal verdict citation analysis\n",
    "Goal: Identify citations relevant to sentencing policy\n",
    "Definition: Relevant citation = reference to prior ruling used as precedent for proposed punishment range\n",
    "Location: Typically found in \"Sentencing Policy\" or \"Sentencing Range\" sections\n",
    "Question: Based on indictment facts, will verdict A likely cite verdict B to support sentencing range?\"\"\"\n",
    "            },\n",
    "            \n",
    "            3: {\n",
    "                \"hebrew\": \"\"\"מערכת חיזוי ציטוטים משפטיים מתמחה\n",
    "תחום: דין פלילי - מדיניות ענישה\n",
    "מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום\n",
    "קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים)\n",
    "פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום.\"\"\",\n",
    "                \"english\": \"\"\"Specialized legal citation prediction system\n",
    "Domain: Criminal law - sentencing policy\n",
    "Purpose: Predict citations between verdicts based on indictment facts similarity\n",
    "Criteria: Citation is relevant if it supports sentencing range decision (not procedures, definitions, or unrelated verdicts)\n",
    "Verdict A will cite verdict B if there is similarity in offenses and circumstances presented in indictments.\"\"\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        chosen_prompt = self.legal_prompts[prompt_version][\"hebrew\"]\n",
    "        \n",
    "        # Create more detailed inputs with legal context\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Format with detailed legal context\n",
    "            legal_input = f\"\"\"{chosen_prompt}\n",
    "\n",
    "עובדות כתב אישום - פסק דין א':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "עובדות כתב אישום - פסק דין ב':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "על בסיס דמיון העבירות והנסיבות, האם פסק דין א' יצטט פסק דין ב' לתמיכה במדיניות הענישה?\"\"\"\n",
    "            \n",
    "            self.inputs.append(legal_input)\n",
    "        \n",
    "        self.targets = df[\"label\"].apply(lambda l: \"כן\" if l == 1 else \"לא\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(f\"Legal Dataset created: {len(self.inputs)} samples\")\n",
    "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Sample input length: {len(self.inputs[0])} characters\")\n",
    "        print(f\"Prompt version: {prompt_version}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        \n",
    "        # Tokenize with longer sequences due to detailed prompt\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=5,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# 📊 Evaluation Function\n",
    "# ========================\n",
    "def evaluate_legal_model(model, dataloader, tokenizer, device, use_threshold_tuning=True):\n",
    "    \"\"\"Evaluate the legal citation model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect true labels\n",
    "    true_labels = []\n",
    "    for batch in dataloader:\n",
    "        true_labels.extend(batch[\"numeric_label\"].numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    if use_threshold_tuning:\n",
    "        best_threshold = find_best_threshold(model, tokenizer, dataloader, device, true_labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "    else:\n",
    "        best_threshold = None\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (No Threshold Tuning)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                generated = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=5\n",
    "                )\n",
    "                decoded_preds = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                predictions = [1 if p.strip() == \"כן\" else 0 for p in decoded_preds]\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend([{} for _ in predictions])  # <- דמmy score dicts\n",
    "\n",
    "    predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    print(f\"\\n📊 LEGAL CITATION PREDICTION RESULTS:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Distribution: {np.bincount(predictions)}\")\n",
    "    print(f\"True Label Distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "    if len(np.unique(predictions)) > 1 and len(np.unique(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predictions)\n",
    "        print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    return f1, {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'threshold': best_threshold,\n",
    "        'scores': all_confidence_scores  # ← תמיד קיים\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================\n",
    "# 📥 Load Everything\n",
    "# ========================\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# Try different prompt versions\n",
    "prompt_versions_to_try = [1, 2, 3]\n",
    "best_prompt_version = 1\n",
    "best_baseline_f1 = 0\n",
    "\n",
    "print(\"\\n🔍 TESTING DIFFERENT LEGAL PROMPTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt_version in prompt_versions_to_try:\n",
    "    print(f\"\\n📋 Testing Prompt Version {prompt_version}:\")\n",
    "    \n",
    "    # Create datasets with this prompt version\n",
    "    val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len, prompt_version=prompt_version)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Test baseline performance\n",
    "    baseline_f1, baseline_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "    \n",
    "    print(f\"Prompt {prompt_version} Baseline F1: {baseline_f1:.4f}\")\n",
    "    \n",
    "    if baseline_f1 > best_baseline_f1:\n",
    "        best_baseline_f1 = baseline_f1\n",
    "        best_prompt_version = prompt_version\n",
    "\n",
    "print(f\"\\n🏆 BEST PROMPT VERSION: {best_prompt_version} (F1: {best_baseline_f1:.4f})\")\n",
    "\n",
    "# ========================\n",
    "# 🏗️ Create Final Datasets with Best Prompt\n",
    "# ========================\n",
    "print(f\"\\nCreating final datasets with prompt version {best_prompt_version}...\")\n",
    "train_dataset = LegalSentencingCitationDataset(df_train, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ========================\n",
    "# 🔁 Training (if needed)\n",
    "# ========================\n",
    "if best_baseline_f1 < 0.6:\n",
    "    print(f\"\\nBaseline F1 ({best_baseline_f1:.4f}) needs improvement. Starting training...\")\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_val_f1 = best_baseline_f1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}: Average Loss = {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_legal_citation_model.pt\")\n",
    "            print(f\"✅ New best model! F1: {best_val_f1:.4f}\")\n",
    "else:\n",
    "    print(f\"Baseline F1 ({best_baseline_f1:.4f}) is already good!\")\n",
    "\n",
    "# ========================\n",
    "# 🧪 Final Test Evaluation\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🧪 FINAL LEGAL CITATION PREDICTION TEST:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_f1, test_metrics = evaluate_legal_model(model, test_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "\n",
    "print(f\"\\n🎯 FINAL RESULTS:\")\n",
    "print(f\"Best Prompt Version: {best_prompt_version}\")\n",
    "print(f\"Baseline F1: {best_baseline_f1:.4f}\")\n",
    "print(f\"Final Test F1: {test_f1:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Optimal Threshold: {test_metrics['threshold']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'best_prompt_version': best_prompt_version,\n",
    "    'baseline_f1': best_baseline_f1,\n",
    "    'test_f1': test_f1,\n",
    "    'test_accuracy': test_metrics['accuracy'],\n",
    "    'optimal_threshold': test_metrics['threshold'],\n",
    "    'model_type': 'legal_sentencing_citation_prediction'\n",
    "\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('legal_citation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if test_f1 > 0.6:\n",
    "    print(f\"\\n🎉 SUCCESS! Legal citation model achieved {test_f1:.4f} F1 score!\")\n",
    "    print(f\"🏆 The detailed legal prompts significantly improved performance!\")\n",
    "else:\n",
    "    print(f\"\\n📈 F1: {test_f1:.4f} - Consider fine-tuning hyperparameters or trying longer training\")\n",
    "\n",
    "print(f\"\\n💾 Results saved to 'legal_citation_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Showing 10 Sample Predictions:\n",
      "================================================================================\n",
      "\n",
      "📄 Input #379\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם הורשע על פי הודאתו בעבירות של החזקת חלק של נשק או תחמושת, לפי סעיף 144 (א) לחוק העונשין, תשל\"ז 1977 ונשיאה/הובלת חלק של נשק או תחמושת, לפי סעיף 144(ב) לחוק העונשין. על פי הנטען בכתב האישום, ביום 28.8.2022 בשעה 00:20 לערך, נהג הנאשם ברכב מסוג קיה ספורטג' עם לוחית רישוי מספר 13-608-201 לכיוון מעבר הל\"ה בדרכו לשטחי האזור, כאשר מתחת למושב הנהג ברכב הייתה שקית ובה 6 מכלולים של נשק מסוג M16. בנוסף, בתא המטען של הרכב נשא שבעה ארגזי תחמושת וארגז קרטון שהכילו יחדיו כ-9000 כדורים בקוטר 5.56 מ\"מ, שהיו מכוסים ומוסתרים. עובדות כתב אישום - פסק דין ב': הנאשם הורשע על יסוד הודאתו בהתאם להסדר טיעון דיוני בכתב אישום מתוקן בהחזקת סם שלא לצריכה עצמית, עבירה ...\n",
      "✅ True Label: לא\n",
      "🧠 Predicted: לא\n",
      "🧮 Score Diff: -6.7443 | Yes Score: 0.5521 | No Score: 7.2964\n",
      "================================================================================\n",
      "\n",
      "📄 Input #302\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם הורשע על פי הודאתו בעבירות המיוחסות לו בכתב האישום: החזקה או שימוש בסמים שלא לצריכה עצמית – עבירה לפי סעיף 7(א)+(ג) רישא לפקודת הסמים המסוכנים (נוסח חדש), התשל\"ג – 1973. גידול, ייצור הכנה והפקה של סם מסוכן – עבירה לפי סעיף 6 לפקודת הסמים המסוכנים (נוסח חדש), התשל\"ג – 1973. הצדדים הגיעו להסדר בדבר הסכמה עונשית, מאסר שריצויו יהיה בדרך של עבודות שירות לצד ענישה הצופה פני עתיד וקנס וכן לאמץ כל ענישה נוספת לרבות צו מבחן, כאשר כל אחד מהצדדים יוכל לטעון באשר למשך תקופת המאסר אותה ירצה הנאשם בעבודות שירות. המאשימה עותרת לתקופה שלא תפחת מתשעה חודשים לצד ענישה נלווית ואילו ב\"כ הנאשם טוען כי יש להסתפק בהמלצת שירות המבחן לצו של\"צ ולחילופין לנוכח הסכמ...\n",
      "✅ True Label: לא\n",
      "🧠 Predicted: כן\n",
      "🧮 Score Diff: -6.3289 | Yes Score: 0.4099 | No Score: 6.7388\n",
      "================================================================================\n",
      "\n",
      "📄 Input #843\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם 1 הודה והורשע בכתב אישום מתוקן (טרם מענה) מיום 14.7.2020 המייחס לו עבירה של הספקת סם מסוכן לפי סעיפים 13 ו- 19א לפקודת הסמים המסוכנים (נוסח חדש), תשל\"ג – 1973. על פי המתואר בכתב האישום המתוקן, בתאריך 18.6.2020 סמוך לשעה 21:35 קיבל הנאשם 1 שיחת טלפון מהנאשם 2 - עמו יש לו היכרות מוקדמת, ותיאם איתו שתגיע אליו בחורה בשם אולגה, לקחת ממנו סם מסוכן. בין הנאשם 1 לאולגה – לא הייתה היכרות מוקדמת. מיד ובהמשך, התקשר הנאשם 2 לאולגה ומסר לה להגיע לרחוב סחלב בעיר אילת (להלן: \"המקום\"). בשעה 23:08 או בסמוך לכך התקשר הנאשם 2 לנאשם 1 וביקש ממנו לגשת לרכבה של אולגה במקום. כמה דקות לאחר מכן, בשעה 23:13 או בסמוך לכך, יצא הנאשם 1 מביתו, ניגש לאולגה ומסר לה סם מ...\n",
      "✅ True Label: כן\n",
      "🧠 Predicted: לא\n",
      "🧮 Score Diff: -6.8922 | Yes Score: 0.3238 | No Score: 7.2160\n",
      "================================================================================\n",
      "\n",
      "📄 Input #236\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם 2 הודה בעובדות כתב אישום מתוקן, שגובש במסגרת הסדר דיוני לאחר שנשמעו מספר עדי תביעה, והורשע בעבירות של קשירת קשר לפשע לפי סעיף 499 (א)(1) לחוק העונשין, התשל\"ז – 1977 וייבוא סם מסוכן לפי סעיף 13 יחד עם סעיף 19א לפקודת הסמים המסוכנים [נוסח חדש], תשל\"ג- 1973. מכתב האישום המתוקן עולה, כי עובר ליום 19.10.17 או בסמוך לכך, קשר הנאשם קשר עם אחר לייבא ארצה סמים מסוכנים. בהתאם לקשר הזמין האחר ביום 19.10.17 כרטיס טיסה לבריסל עבור הנאשם. 3 ימים לאחר מכן, ביום 22.10.17, טס הנאשם מישראל לבריסל דרך פרנקפורט, על מנת לייבא לישראל סמים. בסמוך לאחר הגעתו לבריסל, במועד שאינו ידוע במדויק, קיבל הנאשם מאדם, שזהותו אינה ידועה, סמים כשהם מחולקים ל- 9 אריזות ומוסלק...\n",
      "✅ True Label: כן\n",
      "🧠 Predicted: כן\n",
      "🧮 Score Diff: -6.3554 | Yes Score: 0.6838 | No Score: 7.0393\n",
      "================================================================================\n",
      "\n",
      "📄 Input #50\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': ביום 11.6.17 הודה הנאשם בכתב האישום והורשע בעבירה של החזקת סם מסוכן שלא לצריכה עצמית לפי סעיף 7 (א) בצירוף סעיף 7 (ג) רישא לפקודת הסמים המסוכנים. ביום 21.1.16 בשעה 01:23, בקיוסק בו עבד בתל-אביב, החזיק הנאשם שלא לצריכתו העצמית 101 יחידות צ'מינקה ו-100 יחידות פלאקה. עובדות כתב אישום - פסק דין ב': ביום 2.6.19 הודה הנאשם והורשע (הרשעתו תוקנה ביום 5.1.20), במסגרת הסדר טיעון שלא כלל הסכמה עונשית, בעבירה אחת של ניסיון לסחר בסם מסוכן, ובעוד שש עשרה עבירות של סחר בסם מסוכן. בחלק מהמקרים מדובר היה בסם מסוג חשיש ובאחרים בסם מסוג קנאביס, הנאשם סחר או ניסה לסחור בסמים אלה בכמויות שנעו בין 1 גר' ל – 7 גר'. כל העבירות התרחשו בין החודשים יולי – נובמבר 2018. על...\n",
      "✅ True Label: לא\n",
      "🧠 Predicted: לא\n",
      "🧮 Score Diff: -6.8709 | Yes Score: 0.6103 | No Score: 7.4812\n",
      "================================================================================\n",
      "\n",
      "📄 Input #858\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם, נתנאל הרשקוביץ', (להלן: הנאשם) הורשע על פי הודאתו בעובדות כתב אישום מתוקן המייחס לו עבירות של החזקת סמים שלא לצריכה עצמית עבירה לפי סעיף 7 (א) ו- 7 (ג) רישא לפקודת הסמים המסוכנים (נוסח חדש) תשל\"ג – 1973 (להלן: פקודת הסמים) וכן עבירות של יצוא, יבוא, מסחר, הספקה סמים מסוכנים – לפי סעיפים 13 ו- 19א לפקודת הסמים (3 עבירות). על פי עובדות כתב האישום המתוקן, במועדים הרלוונטיים לכתב האישום, הנאשם (בן 21) החל לעסוק בסחר בסמים עבור בצע כסף. את הסמים המסוכנים היה מוכר הנאשם בפארקים ציבוריים, או מגרשי ספורט ברחבי העיר אילת אשר שם היו מבלים קטינים, אשר היווי עבור הנאשם קליינטים פוטנציאליים. מגרשי הספורט כונו בפי הנאשם ובפי קטינים אחרים בשם \"האספלט\" (...\n",
      "✅ True Label: כן\n",
      "🧠 Predicted: לא\n",
      "🧮 Score Diff: -6.8125 | Yes Score: 0.5675 | No Score: 7.3800\n",
      "================================================================================\n",
      "\n",
      "📄 Input #779\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם הורשע על-פי הודאתו במסגרת הסדר טיעון בכתב אישום מתוקן (להלן: \"כתב האישום\") בעבירה של החזקת סם מסוכן שלא לצריכה עצמית, עבירה לפי סעיף 7(א)+(ג) רישא לפקודת הסמים המסוכנים [נוסח חדש], התשל\"ג – 1973 (להלן: \"פקודת הסמים\"). במסגרת ההסדר לא הושגה הסכמה עונשית בין הצדדים ואלו טענו לעונש באופן חופשי. עם זאת, הצדדים הגיעו להבנה ביחס לבקשת חילוט של רכב שהגישה המאשימה ולפיה זה ישוחרר בכפוף להפקדת סך של 30,000 ₪ לטובת קרן החילוט של הסמים עובר למתן גזר הדין. מעובדות כתב האישום עולה כי ביום 01.05.2022 סמוך לשעה 13:30 החזיק הנאשם עבור אחר שזהותו אינה ידועה למאשימה קילוגרם של סם מסוכן מסוג קוקאין, אשר הוסלק בתא מטען ברכב מסוג \"טויוטה קורולה\" שבבעלותו (להל...\n",
      "✅ True Label: כן\n",
      "🧠 Predicted: כן\n",
      "🧮 Score Diff: -6.0273 | Yes Score: 0.6312 | No Score: 6.6585\n",
      "================================================================================\n",
      "\n",
      "📄 Input #799\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם הורשע על פי הודאתו בעבירה של סיוע לסחר בסמים מסוכנים, לפי סעיפים 13 ו – 19א לפקודת הסמים המסוכנים (נוסח חדש), תשל\"ג – 1973 (פקודת הסמים) + סעיף 31 לחוק העונשין, תשל\"ז – 1977 (חוק העונשין) ובעבירה של סיוע להחזקת סמים שלא לצריכה עצמית, לפי סעיפים 7(א) ו7(ג) לפקודת הסמים + סעיף 31 לחוק העונשין. על פי עובדות כתב האישום, ביום 21.4.2020, קיבל חננאל שמואל זגורי (חננאל) לידיו 371.17 גרם קנבוס מחולק לשקיות. חננאל ביקש מהנאשם, כי ישמש עבורו נהג במהלך ביצוע הסחר בסם, באמצעות רכב אותו שאל מאחר. חננאל הניח את הסם מתחת לסדין במושב האחורי ברכב ויחד עם הנאשם הגיעו ברכב לרחוב ריש לקיש בירושלים (המקום). זמן קצר קודם לכן ביקש ת\"ח לרכוש קנבוס באמצעות יישומון...\n",
      "✅ True Label: לא\n",
      "🧠 Predicted: לא\n",
      "🧮 Score Diff: -6.7578 | Yes Score: 0.7015 | No Score: 7.4593\n",
      "================================================================================\n",
      "\n",
      "📄 Input #777\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם הודה בעובדות כתב אישום מתוקן שהוגש במסגרת הסדר דיוני שלא כלל הסכמה לעונש, והורשע בעבירה של סחר בסמים, לפי סעיפים 13 ו- 19א לפקודת הסמים המסוכנים [נוסח חדש], התשל\"ג – 1973, בכך שמכר לסוכנת משטרתית ביום 25.12.2018, באמצעות יישומון \"טלגרם\", סם מסוכן מסוג קוקאין במשקל 0.8898 גרם נטו, בתמורה לתשלום בסך 800 ₪. בפועל, ביום 25.12.2018, באמצעות יישומון \"טלגרם\", מכר הנאשם לסוכנת משטרתית סם מסוכן מסוג קוקאין במשקל 0.8898 גרם נטו, תמורת תשלום בסך 800 ₪. עובדות כתב אישום - פסק דין ב': ת\"פ 20654-11-17, בו הורשע הנאשם בעבירות של סחר בסמים (שתי עבירות), לפי סעיף 13 + 19א לפקודת הסמים המסוכנים (נוסח חדש) תשל\"ג-1973 ובעבירות של החזקת סמים שלא לשימוש עצמי (...\n",
      "✅ True Label: לא\n",
      "🧠 Predicted: כן\n",
      "🧮 Score Diff: -6.1172 | Yes Score: 0.5684 | No Score: 6.6856\n",
      "================================================================================\n",
      "\n",
      "📄 Input #285\n",
      "--------------------------------------------------------------------------------\n",
      "🔹 Decoded Input:\n",
      "מערכת חיזוי ציטוטים משפטיים מתמחה תחום: דין פלילי - מדיניות ענישה מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים) פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום. עובדות כתב אישום - פסק דין א': הנאשם הורשע על פי הודאתו, במסגרת הסדר דיוני, בעבירות של גידול, ייצור הכנת סמים מסוכנים, לפי סעיף 6 לפקודת הסמים המסוכנים [נוסח חדש], התשל\"ג-1973 (להלן: \"פקודת הסמים המסוכנים\"), והחזקת כלים להכנת סם שלא לצריכה עצמית, לפי סעיף 10 רישא לפקודת הסמים המסוכנים. לפי עובדות כתב האישום, מספר חודשים עובר ליום 14.11.2022, בתאריך ובשעה שאינם ידועים במדוייק למאשימה, רכש הנאשם כלים לשם גידול סמים מסוכנים כמפורט: 16 מנורות חימום, 3 מזגנים, 2 מאווררים ו-3 מאווררי אוויר. בהמשך לאותן הנסיבות, ביום 14.11.2022, גידל הנאשם בחדרים שונים בבית אותו שכר בראשון לציון (להלן: \"הבית\") 176 שתילים של סם מסוכן מסוג קאנבוס, במשקל כולל של 10.6 ק\"ג. עובדות כתב אישום - פסק דין ב'...\n",
      "✅ True Label: כן\n",
      "🧠 Predicted: כן\n",
      "🧮 Score Diff: -5.2470 | Yes Score: 0.5642 | No Score: 5.8112\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def show_sample_predictions(dataset, predictions, scores, tokenizer, num_samples=10):\n",
    "    print(f\"\\n🔎 Showing {num_samples} Sample Predictions:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    indices = np.random.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        input_ids = dataset[idx][\"input_ids\"]\n",
    "        decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        label = dataset[idx][\"numeric_label\"]\n",
    "        prediction = predictions[idx]\n",
    "        score = scores[idx]\n",
    "        \n",
    "        print(f\"\\n📄 Input #{idx}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"🔹 Decoded Input:\\n{decoded_input[:1000]}...\")  # Truncate long text\n",
    "        print(f\"✅ True Label: {'כן' if label == 1 else 'לא'}\")\n",
    "        print(f\"🧠 Predicted: {'כן' if prediction == 1 else 'לא'}\")\n",
    "        print(f\"🧮 Score Diff: {score['score_diff']:.4f} | Yes Score: {score['yes_score']:.4f} | No Score: {score['no_score']:.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "show_sample_predictions(test_dataset, test_metrics[\"predictions\"], test_metrics[\"scores\"], tokenizer, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and combining datasets for k-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for k-fold CV: 5791\n",
      "\n",
      "================================================================================\n",
      "🔄 STARTING K-FOLD EVALUATION FOR: m5-mlm-final\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal Dataset created: 5791 samples\n",
      "Label distribution: [3857 1934]\n",
      "Sample input length: 2083 characters\n",
      "\n",
      "📁 FOLD 1/5\n",
      "Train samples: 4632, Test samples: 1159\n",
      "Loading fresh model for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/7:   0%|          | 0/927 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Fold 1, Epoch 1/7: 100%|██████████| 927/927 [03:48<00:00,  4.06it/s, loss=0.4440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2493 (F1: 0.5103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5103\n",
      "Precision: 0.3815\n",
      "Recall: 0.7705\n",
      "Accuracy: 0.5135\n",
      "\n",
      "Prediction Distribution: [311 616]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.5790\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.39      0.52       622\n",
      "           1       0.38      0.77      0.51       305\n",
      "\n",
      "    accuracy                           0.51       927\n",
      "   macro avg       0.58      0.58      0.51       927\n",
      "weighted avg       0.65      0.51      0.51       927\n",
      "\n",
      "Fold 1, Epoch 1: Val F1 = 0.5103, Best = 0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2/7: 100%|██████████| 927/927 [03:51<00:00,  4.00it/s, loss=0.2527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.8732 (F1: 0.5514)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5514\n",
      "Precision: 0.4462\n",
      "Recall: 0.7213\n",
      "Accuracy: 0.6138\n",
      "\n",
      "Prediction Distribution: [434 493]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.6412\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.56      0.66       622\n",
      "           1       0.45      0.72      0.55       305\n",
      "\n",
      "    accuracy                           0.61       927\n",
      "   macro avg       0.63      0.64      0.61       927\n",
      "weighted avg       0.69      0.61      0.62       927\n",
      "\n",
      "Fold 1, Epoch 2: Val F1 = 0.5514, Best = 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/7: 100%|██████████| 927/927 [03:51<00:00,  4.00it/s, loss=0.2994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.5635 (F1: 0.6349)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6349\n",
      "Precision: 0.6154\n",
      "Recall: 0.6557\n",
      "Accuracy: 0.7519\n",
      "\n",
      "Prediction Distribution: [602 325]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7274\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       622\n",
      "           1       0.62      0.66      0.63       305\n",
      "\n",
      "    accuracy                           0.75       927\n",
      "   macro avg       0.72      0.73      0.72       927\n",
      "weighted avg       0.76      0.75      0.75       927\n",
      "\n",
      "Fold 1, Epoch 3: Val F1 = 0.6349, Best = 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/7: 100%|██████████| 927/927 [03:51<00:00,  4.00it/s, loss=0.0180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.9943 (F1: 0.6707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6707\n",
      "Precision: 0.6268\n",
      "Recall: 0.7213\n",
      "Accuracy: 0.7670\n",
      "\n",
      "Prediction Distribution: [576 351]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7554\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       622\n",
      "           1       0.63      0.72      0.67       305\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.74      0.76      0.75       927\n",
      "weighted avg       0.78      0.77      0.77       927\n",
      "\n",
      "Fold 1, Epoch 4: Val F1 = 0.6707, Best = 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5/7: 100%|██████████| 927/927 [03:51<00:00,  4.00it/s, loss=0.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.2699 (F1: 0.6964)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6964\n",
      "Precision: 0.7647\n",
      "Recall: 0.6393\n",
      "Accuracy: 0.8166\n",
      "\n",
      "Prediction Distribution: [672 255]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7714\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87       622\n",
      "           1       0.76      0.64      0.70       305\n",
      "\n",
      "    accuracy                           0.82       927\n",
      "   macro avg       0.80      0.77      0.78       927\n",
      "weighted avg       0.81      0.82      0.81       927\n",
      "\n",
      "Fold 1, Epoch 5: Val F1 = 0.6964, Best = 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6/7: 100%|██████████| 927/927 [03:50<00:00,  4.02it/s, loss=0.0031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -7.6882 (F1: 0.7072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7072\n",
      "Precision: 0.6736\n",
      "Recall: 0.7443\n",
      "Accuracy: 0.7972\n",
      "\n",
      "Prediction Distribution: [590 337]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7837\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       622\n",
      "           1       0.67      0.74      0.71       305\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.78      0.78       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 1, Epoch 6: Val F1 = 0.7072, Best = 0.7072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7/7: 100%|██████████| 927/927 [03:49<00:00,  4.04it/s, loss=0.0451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.0102 (F1: 0.7141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7141\n",
      "Precision: 0.6307\n",
      "Recall: 0.8230\n",
      "Accuracy: 0.7832\n",
      "\n",
      "Prediction Distribution: [529 398]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7933\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.83       622\n",
      "           1       0.63      0.82      0.71       305\n",
      "\n",
      "    accuracy                           0.78       927\n",
      "   macro avg       0.76      0.79      0.77       927\n",
      "weighted avg       0.81      0.78      0.79       927\n",
      "\n",
      "Fold 1, Epoch 7: Val F1 = 0.7141, Best = 0.7141\n",
      "Using fixed threshold: -6.010154292291524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|██████████| 290/290 [00:22<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6981\n",
      "Precision: 0.6069\n",
      "Recall: 0.8217\n",
      "Accuracy: 0.7627\n",
      "\n",
      "Prediction Distribution: [635 524]\n",
      "True Label Distribution: [772 387]\n",
      "AUC-ROC: 0.7774\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80       772\n",
      "           1       0.61      0.82      0.70       387\n",
      "\n",
      "    accuracy                           0.76      1159\n",
      "   macro avg       0.75      0.78      0.75      1159\n",
      "weighted avg       0.80      0.76      0.77      1159\n",
      "\n",
      "✅ Fold 1 Results: F1 = 0.6981, Accuracy = 0.7627\n",
      "\n",
      "📁 FOLD 2/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/7: 100%|██████████| 927/927 [03:49<00:00,  4.04it/s, loss=0.3480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.6131 (F1: 0.5033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5033\n",
      "Precision: 0.3548\n",
      "Recall: 0.8660\n",
      "Accuracy: 0.4358\n",
      "\n",
      "Prediction Distribution: [180 747]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.5449\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.22      0.35       621\n",
      "           1       0.35      0.87      0.50       306\n",
      "\n",
      "    accuracy                           0.44       927\n",
      "   macro avg       0.56      0.54      0.43       927\n",
      "weighted avg       0.63      0.44      0.40       927\n",
      "\n",
      "Fold 2, Epoch 1: Val F1 = 0.5033, Best = 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 2/7: 100%|██████████| 927/927 [03:49<00:00,  4.05it/s, loss=0.2061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.7372 (F1: 0.5259)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5259\n",
      "Precision: 0.3964\n",
      "Recall: 0.7810\n",
      "Accuracy: 0.5351\n",
      "\n",
      "Prediction Distribution: [324 603]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.5974\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.41      0.54       621\n",
      "           1       0.40      0.78      0.53       306\n",
      "\n",
      "    accuracy                           0.54       927\n",
      "   macro avg       0.59      0.60      0.53       927\n",
      "weighted avg       0.66      0.54      0.54       927\n",
      "\n",
      "Fold 2, Epoch 2: Val F1 = 0.5259, Best = 0.5259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 3/7: 100%|██████████| 927/927 [03:48<00:00,  4.06it/s, loss=0.1631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.8495 (F1: 0.5811)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5811\n",
      "Precision: 0.4817\n",
      "Recall: 0.7320\n",
      "Accuracy: 0.6516\n",
      "\n",
      "Prediction Distribution: [462 465]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.6720\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.61      0.70       621\n",
      "           1       0.48      0.73      0.58       306\n",
      "\n",
      "    accuracy                           0.65       927\n",
      "   macro avg       0.65      0.67      0.64       927\n",
      "weighted avg       0.71      0.65      0.66       927\n",
      "\n",
      "Fold 2, Epoch 3: Val F1 = 0.5811, Best = 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 4/7: 100%|██████████| 927/927 [03:50<00:00,  4.03it/s, loss=0.4540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.8180 (F1: 0.6510)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6510\n",
      "Precision: 0.6690\n",
      "Recall: 0.6340\n",
      "Accuracy: 0.7756\n",
      "\n",
      "Prediction Distribution: [637 290]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7397\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       621\n",
      "           1       0.67      0.63      0.65       306\n",
      "\n",
      "    accuracy                           0.78       927\n",
      "   macro avg       0.75      0.74      0.74       927\n",
      "weighted avg       0.77      0.78      0.77       927\n",
      "\n",
      "Fold 2, Epoch 4: Val F1 = 0.6510, Best = 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5/7: 100%|██████████| 927/927 [03:47<00:00,  4.07it/s, loss=0.1693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.4289 (F1: 0.6919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6919\n",
      "Precision: 0.6697\n",
      "Recall: 0.7157\n",
      "Accuracy: 0.7896\n",
      "\n",
      "Prediction Distribution: [600 327]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7709\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       621\n",
      "           1       0.67      0.72      0.69       306\n",
      "\n",
      "    accuracy                           0.79       927\n",
      "   macro avg       0.76      0.77      0.77       927\n",
      "weighted avg       0.79      0.79      0.79       927\n",
      "\n",
      "Fold 2, Epoch 5: Val F1 = 0.6919, Best = 0.6919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6/7: 100%|██████████| 927/927 [03:50<00:00,  4.03it/s, loss=0.7968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.1773 (F1: 0.6978)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6978\n",
      "Precision: 0.7133\n",
      "Recall: 0.6830\n",
      "Accuracy: 0.8047\n",
      "\n",
      "Prediction Distribution: [634 293]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7739\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       621\n",
      "           1       0.71      0.68      0.70       306\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.78      0.77      0.78       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 2, Epoch 6: Val F1 = 0.6978, Best = 0.6978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 7/7: 100%|██████████| 927/927 [03:48<00:00,  4.05it/s, loss=0.0263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.6320 (F1: 0.7068)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7068\n",
      "Precision: 0.6696\n",
      "Recall: 0.7484\n",
      "Accuracy: 0.7950\n",
      "\n",
      "Prediction Distribution: [585 342]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7832\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       621\n",
      "           1       0.67      0.75      0.71       306\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.78      0.77       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 2, Epoch 7: Val F1 = 0.7068, Best = 0.7068\n",
      "Using fixed threshold: -6.631978000913348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|██████████| 290/290 [00:23<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7170\n",
      "Precision: 0.6566\n",
      "Recall: 0.7896\n",
      "Accuracy: 0.7927\n",
      "\n",
      "Prediction Distribution: [695 463]\n",
      "True Label Distribution: [773 385]\n",
      "AUC-ROC: 0.7920\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.84       773\n",
      "           1       0.66      0.79      0.72       385\n",
      "\n",
      "    accuracy                           0.79      1158\n",
      "   macro avg       0.77      0.79      0.78      1158\n",
      "weighted avg       0.81      0.79      0.80      1158\n",
      "\n",
      "✅ Fold 2 Results: F1 = 0.7170, Accuracy = 0.7927\n",
      "\n",
      "📁 FOLD 3/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1/7: 100%|██████████| 927/927 [03:49<00:00,  4.03it/s, loss=0.3951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.8456 (F1: 0.5148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5148\n",
      "Precision: 0.3639\n",
      "Recall: 0.8795\n",
      "Accuracy: 0.4509\n",
      "\n",
      "Prediction Distribution: [185 742]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.5591\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.24      0.37       620\n",
      "           1       0.36      0.88      0.51       307\n",
      "\n",
      "    accuracy                           0.45       927\n",
      "   macro avg       0.58      0.56      0.44       927\n",
      "weighted avg       0.66      0.45      0.42       927\n",
      "\n",
      "Fold 3, Epoch 1: Val F1 = 0.5148, Best = 0.5148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2/7: 100%|██████████| 927/927 [03:48<00:00,  4.06it/s, loss=0.3832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.7337 (F1: 0.5392)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5392\n",
      "Precision: 0.3920\n",
      "Recall: 0.8632\n",
      "Accuracy: 0.5113\n",
      "\n",
      "Prediction Distribution: [251 676]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.6001\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.34      0.48       620\n",
      "           1       0.39      0.86      0.54       307\n",
      "\n",
      "    accuracy                           0.51       927\n",
      "   macro avg       0.61      0.60      0.51       927\n",
      "weighted avg       0.69      0.51      0.50       927\n",
      "\n",
      "Fold 3, Epoch 2: Val F1 = 0.5392, Best = 0.5392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3/7: 100%|██████████| 927/927 [03:49<00:00,  4.04it/s, loss=0.1313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.5333 (F1: 0.5532)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5532\n",
      "Precision: 0.4600\n",
      "Recall: 0.6938\n",
      "Accuracy: 0.6289\n",
      "\n",
      "Prediction Distribution: [464 463]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.6453\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.60      0.68       620\n",
      "           1       0.46      0.69      0.55       307\n",
      "\n",
      "    accuracy                           0.63       927\n",
      "   macro avg       0.63      0.65      0.62       927\n",
      "weighted avg       0.69      0.63      0.64       927\n",
      "\n",
      "Fold 3, Epoch 3: Val F1 = 0.5532, Best = 0.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 4/7: 100%|██████████| 927/927 [03:48<00:00,  4.06it/s, loss=0.0893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.0634 (F1: 0.6291)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6291\n",
      "Precision: 0.6054\n",
      "Recall: 0.6547\n",
      "Accuracy: 0.7443\n",
      "\n",
      "Prediction Distribution: [595 332]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7217\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       620\n",
      "           1       0.61      0.65      0.63       307\n",
      "\n",
      "    accuracy                           0.74       927\n",
      "   macro avg       0.71      0.72      0.72       927\n",
      "weighted avg       0.75      0.74      0.75       927\n",
      "\n",
      "Fold 3, Epoch 4: Val F1 = 0.6291, Best = 0.6291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 5/7: 100%|██████████| 927/927 [03:49<00:00,  4.03it/s, loss=0.2186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.3259 (F1: 0.6747)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6747\n",
      "Precision: 0.6299\n",
      "Recall: 0.7264\n",
      "Accuracy: 0.7681\n",
      "\n",
      "Prediction Distribution: [573 354]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7575\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       620\n",
      "           1       0.63      0.73      0.67       307\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.74      0.76      0.75       927\n",
      "weighted avg       0.78      0.77      0.77       927\n",
      "\n",
      "Fold 3, Epoch 5: Val F1 = 0.6747, Best = 0.6747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6/7: 100%|██████████| 927/927 [03:49<00:00,  4.04it/s, loss=0.0262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.9222 (F1: 0.7014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7014\n",
      "Precision: 0.6810\n",
      "Recall: 0.7231\n",
      "Accuracy: 0.7961\n",
      "\n",
      "Prediction Distribution: [601 326]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7777\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85       620\n",
      "           1       0.68      0.72      0.70       307\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.78      0.77       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 3, Epoch 6: Val F1 = 0.7014, Best = 0.7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 7/7: 100%|██████████| 927/927 [03:48<00:00,  4.05it/s, loss=0.0519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.9139 (F1: 0.7071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7071\n",
      "Precision: 0.6477\n",
      "Recall: 0.7785\n",
      "Accuracy: 0.7864\n",
      "\n",
      "Prediction Distribution: [558 369]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7844\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83       620\n",
      "           1       0.65      0.78      0.71       307\n",
      "\n",
      "    accuracy                           0.79       927\n",
      "   macro avg       0.76      0.78      0.77       927\n",
      "weighted avg       0.80      0.79      0.79       927\n",
      "\n",
      "Fold 3, Epoch 7: Val F1 = 0.7071, Best = 0.7071\n",
      "Using fixed threshold: -6.91388987521736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|██████████| 290/290 [00:22<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6969\n",
      "Precision: 0.6450\n",
      "Recall: 0.7580\n",
      "Accuracy: 0.7694\n",
      "\n",
      "Prediction Distribution: [682 476]\n",
      "True Label Distribution: [753 405]\n",
      "AUC-ROC: 0.7668\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.81       753\n",
      "           1       0.64      0.76      0.70       405\n",
      "\n",
      "    accuracy                           0.77      1158\n",
      "   macro avg       0.75      0.77      0.76      1158\n",
      "weighted avg       0.78      0.77      0.77      1158\n",
      "\n",
      "✅ Fold 3 Results: F1 = 0.6969, Accuracy = 0.7694\n",
      "\n",
      "📁 FOLD 4/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1/7: 100%|██████████| 927/927 [03:48<00:00,  4.05it/s, loss=0.0610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.3705 (F1: 0.5292)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5292\n",
      "Precision: 0.3978\n",
      "Recall: 0.7905\n",
      "Accuracy: 0.5221\n",
      "\n",
      "Prediction Distribution: [301 626]\n",
      "True Label Distribution: [612 315]\n",
      "AUC-ROC: 0.5872\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.38      0.51       612\n",
      "           1       0.40      0.79      0.53       315\n",
      "\n",
      "    accuracy                           0.52       927\n",
      "   macro avg       0.59      0.59      0.52       927\n",
      "weighted avg       0.65      0.52      0.52       927\n",
      "\n",
      "Fold 4, Epoch 1: Val F1 = 0.5292, Best = 0.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 2/7: 100%|██████████| 927/927 [03:48<00:00,  4.05it/s, loss=0.3636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.7057 (F1: 0.5443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5443\n",
      "Precision: 0.4150\n",
      "Recall: 0.7905\n",
      "Accuracy: 0.5502\n",
      "\n",
      "Prediction Distribution: [327 600]\n",
      "True Label Distribution: [612 315]\n",
      "AUC-ROC: 0.6085\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.43      0.56       612\n",
      "           1       0.41      0.79      0.54       315\n",
      "\n",
      "    accuracy                           0.55       927\n",
      "   macro avg       0.61      0.61      0.55       927\n",
      "weighted avg       0.67      0.55      0.55       927\n",
      "\n",
      "Fold 4, Epoch 2: Val F1 = 0.5443, Best = 0.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 3/7: 100%|██████████| 927/927 [03:49<00:00,  4.04it/s, loss=0.3350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.8132 (F1: 0.5680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|██████████| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5680\n",
      "Precision: 0.4551\n",
      "Recall: 0.7556\n",
      "Accuracy: 0.6095\n",
      "\n",
      "Prediction Distribution: [404 523]\n",
      "True Label Distribution: [612 315]\n",
      "AUC-ROC: 0.6449\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.53      0.64       612\n",
      "           1       0.46      0.76      0.57       315\n",
      "\n",
      "    accuracy                           0.61       927\n",
      "   macro avg       0.63      0.64      0.61       927\n",
      "weighted avg       0.69      0.61      0.62       927\n",
      "\n",
      "Fold 4, Epoch 3: Val F1 = 0.5680, Best = 0.5680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 4/7: 100%|██████████| 927/927 [03:49<00:00,  4.05it/s, loss=0.4517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|██████████| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "\n",
    "# ========================\n",
    "# 🔧 CONFIG\n",
    "# ========================\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 1024\n",
    "epochs = 7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================\n",
    "# 🎯 CORE: Improved Logits Classification\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    \"\"\"Classify using threshold-based method\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=input_ids.device)\n",
    "        decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        yes_tokens = [259, 1903]  # כן\n",
    "        no_tokens = [1124]        # לא\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx]\n",
    "            \n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            \n",
    "            score_diff = yes_score - no_score\n",
    "            \n",
    "            if score_diff > threshold:\n",
    "                prediction = 1\n",
    "                predicted_text = \"כן\"\n",
    "            else:\n",
    "                prediction = 0\n",
    "                predicted_text = \"לא\"\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            scores.append({\n",
    "                'prediction': prediction,\n",
    "                'predicted_text': predicted_text,\n",
    "                'score_diff': score_diff,\n",
    "                'yes_score': yes_score,\n",
    "                'no_score': no_score\n",
    "            })\n",
    "        \n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, device, true_labels):\n",
    "    \"\"\"Find optimal threshold for balanced predictions\"\"\"\n",
    "    print(\"🔍 Finding best threshold...\")\n",
    "    \n",
    "    all_score_diffs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Collecting scores\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            _, scores = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=0.0\n",
    "            )\n",
    "            \n",
    "            for score in scores:\n",
    "                all_score_diffs.append(score['score_diff'])\n",
    "    \n",
    "    # Test thresholds\n",
    "    thresholds = np.linspace(min(all_score_diffs), max(all_score_diffs), 50)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch_preds, _ = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "# ========================\n",
    "# 🧠 IMPROVED Dataset with Specific Legal Prompts\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        \"\"\"\n",
    "        Dataset with legally-specific prompts for sentencing citation prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiple versions of detailed legal prompts\n",
    "        prompt = \"\"\"מערכת חיזוי ציטוטים משפטיים מתמחה\n",
    "תחום: דין פלילי - מדיניות ענישה\n",
    "מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום\n",
    "קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש (לא הליכים, הגדרות, או פסקי דין לא קשורים)\n",
    "פסק דין א' יצטט פסק דין ב' אם יש דמיון בעבירות ובנסיבות העוולות המוצגות בכתבי האישום.\n",
    "שאלה: בהתבסס על עובדות כתב האישום, האם צפוי שפסק דין א' יצטט פסק דין ב' לתמיכה בטווח הענישה?\n",
    "\"\"\"\n",
    "\n",
    "#                 \"english\": \"\"\"Specialized legal citation prediction system\n",
    "# Domain: Criminal law - sentencing policy\n",
    "# Purpose: Predict citations between verdicts based on indictment facts similarity\n",
    "# Criteria: Citation is relevant if it supports sentencing range decision (not procedures, definitions, or unrelated verdicts)\n",
    "# Verdict A will cite verdict B if there is similarity in offenses and circumstances presented in indictments.\n",
    "# Question: Based on indictment facts, will verdict A likely cite verdict B to support sentencing range?\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "        # Create more detailed inputs with legal context\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Format with detailed legal context\n",
    "            legal_input = f\"\"\"{prompt}\n",
    "\n",
    "עובדות כתב אישום - פסק דין א':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "עובדות כתב אישום - פסק דין ב':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "על בסיס דמיון העבירות והנסיבות, האם פסק דין א' יצטט פסק דין ב' לתמיכה במדיניות הענישה?\"\"\"\n",
    "            \n",
    "            self.inputs.append(legal_input)\n",
    "        \n",
    "        self.targets = df[\"label\"].apply(lambda l: \"כן\" if l == 1 else \"לא\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(f\"Legal Dataset created: {len(self.inputs)} samples\")\n",
    "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Sample input length: {len(self.inputs[0])} characters\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        \n",
    "        # Tokenize with longer sequences due to detailed prompt\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=5,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# 📊 Evaluation Function\n",
    "# ========================\n",
    "def evaluate_legal_model(model, dataloader, tokenizer, device, use_threshold_tuning=True, fix_threshold=None):\n",
    "    \"\"\"Evaluate the legal citation model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect true labels\n",
    "    true_labels = []\n",
    "    for batch in dataloader:\n",
    "        true_labels.extend(batch[\"numeric_label\"].numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    # THREE OPTIONS NOW:\n",
    "    if fix_threshold is not None:\n",
    "        # ✅ USE FIXED THRESHOLD (no tuning)\n",
    "        print(f\"Using fixed threshold: {fix_threshold}\")\n",
    "        best_threshold = fix_threshold\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Fixed Threshold)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "                \n",
    "    elif use_threshold_tuning:\n",
    "        # ❌ TUNE THRESHOLD ON THIS DATASET (causes data leakage if used on test set)\n",
    "        best_threshold = find_best_threshold(model, tokenizer, dataloader, device, true_labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Tuned Threshold)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "    else:\n",
    "        # 🔄 USE MODEL.GENERATE() (different approach)\n",
    "        best_threshold = None\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Generation)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                generated = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=5\n",
    "                )\n",
    "                decoded_preds = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                predictions = [1 if p.strip() == \"כן\" else 0 for p in decoded_preds]\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend([{} for _ in predictions])\n",
    "\n",
    "    predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    print(f\"\\n📊 LEGAL CITATION PREDICTION RESULTS:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Distribution: {np.bincount(predictions)}\")\n",
    "    print(f\"True Label Distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "    if len(np.unique(predictions)) > 1 and len(np.unique(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predictions)\n",
    "        print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    return f1, {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'threshold': best_threshold,\n",
    "        'scores': all_confidence_scores  # ← תמיד קיים\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# 📥 Load Everything\n",
    "# ========================\n",
    "\n",
    "# model_paths = [\"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\",\"imvladikon/het5-base\"]\n",
    "\n",
    "# for model_path in model_paths:\n",
    "#     print(f\"Loading {model_path} and tokenizer...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#     print(\"Loading data...\")\n",
    "#     df_train = pd.read_csv(train_file)\n",
    "#     df_val = pd.read_csv(val_file)\n",
    "#     df_test = pd.read_csv(test_file)\n",
    "\n",
    "#     train_dataset = LegalSentencingCitationDataset(df_train, tokenizer, max_len=max_len)\n",
    "#     val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len)\n",
    "#     test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "#     optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "#     best_val_f1 = 0\n",
    "#     best_val_threshold=0\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "        \n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "#         for batch in progress_bar:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "#             outputs = model(\n",
    "#                 input_ids=batch[\"input_ids\"],\n",
    "#                 attention_mask=batch[\"attention_mask\"],\n",
    "#                 labels=batch[\"labels\"]\n",
    "#             )\n",
    "            \n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "#         print(f\"\\nEpoch {epoch+1}: Average Loss = {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "#         # Validation\n",
    "#         val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "        \n",
    "#         if val_f1 > best_val_f1:\n",
    "#             best_val_f1 = val_f1\n",
    "#             torch.save(model.state_dict(), \"best_legal_clm_citation_model.pt\")\n",
    "#             best_val_threshold = val_metrics['threshold']\n",
    "#             print(f\"✅ New best model! F1: {best_val_f1:.4f}\")\n",
    "\n",
    "#     # ========================\n",
    "#     # 🧪 Final Test Evaluation\n",
    "#     # ========================\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"🧪 FINAL LEGAL CITATION PREDICTION TEST:\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     test_f1, test_metrics = evaluate_legal_model(model, test_loader, tokenizer, device, \n",
    "#                                             use_threshold_tuning=False, \n",
    "#                                             fix_threshold=best_val_threshold)\n",
    "\n",
    "#     print(f\"\\n🎯 FINAL RESULTS:\")\n",
    "#     print(f\"Best Prompt Version: {best_prompt_version}\")\n",
    "#     print(f\"Baseline F1: {best_baseline_f1:.4f}\")\n",
    "#     print(f\"Final Test F1: {test_f1:.4f}\")\n",
    "#     print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "#     print(f\"Optimal Threshold: {test_metrics['threshold']:.4f}\")\n",
    "\n",
    "#     # Save results\n",
    "#     results = {\n",
    "#         'best_prompt_version': best_prompt_version,\n",
    "#         'baseline_f1': best_baseline_f1,\n",
    "#         'test_f1': test_f1,\n",
    "#         'test_accuracy': test_metrics['accuracy'],\n",
    "#         'optimal_threshold': test_metrics['threshold'],\n",
    "#         'model_type': 'legal_sentencing_citation_prediction'\n",
    "\n",
    "#     }\n",
    "\n",
    "# K-fold cross-validation setup\n",
    "K_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Store results for statistical comparison\n",
    "model_fold_results = defaultdict(list)  # {model_name: [fold1_f1, fold2_f1, ...]}\n",
    "model_fold_accuracy = defaultdict(list)  # {model_name: [fold1_acc, fold2_acc, ...]}\n",
    "\n",
    "def reset_model_weights(model):\n",
    "    \"\"\"Reset model weights to initial state for each fold\"\"\"\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def create_k_fold_datasets(full_dataset, k_folds=K_FOLDS, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Create k-fold splits of the dataset\"\"\"\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)\n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    fold_splits = []\n",
    "    for train_indices, val_indices in kfold.split(indices):\n",
    "        fold_splits.append((train_indices.tolist(), val_indices.tolist()))\n",
    "    \n",
    "    return fold_splits\n",
    "\n",
    "# Load and prepare the full dataset (combine train, val, test for k-fold)\n",
    "print(\"Loading and combining datasets for k-fold cross-validation...\")\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# Combine all data for k-fold CV\n",
    "df_full = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "print(f\"Total samples for k-fold CV: {len(df_full)}\")\n",
    "\n",
    "model_paths = [\"/home/liorkob/M.Sc/thesis/t5/m5-mlm-final\", \"google/mt5-base\"]\n",
    "\n",
    "for model_idx, model_path in enumerate(model_paths):\n",
    "    model_name = model_path.split('/')[-1]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔄 STARTING K-FOLD EVALUATION FOR: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load tokenizer once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create full dataset for k-fold splitting\n",
    "    full_dataset = LegalSentencingCitationDataset(df_full, tokenizer, max_len=max_len)\n",
    "    fold_splits = create_k_fold_datasets(full_dataset, k_folds=K_FOLDS, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    fold_f1_scores = []\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for fold, (train_indices, test_indices) in enumerate(fold_splits):\n",
    "        print(f\"\\n📁 FOLD {fold + 1}/{K_FOLDS}\")\n",
    "        print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
    "        \n",
    "        # Create fold-specific datasets\n",
    "        train_fold_dataset = Subset(full_dataset, train_indices)\n",
    "        test_fold_dataset = Subset(full_dataset, test_indices)\n",
    "        \n",
    "        # Split training data into train/validation (80/20 split)\n",
    "        train_size = int(0.8 * len(train_indices))\n",
    "        val_size = len(train_indices) - train_size\n",
    "        \n",
    "        # Create train/val split from training indices\n",
    "        train_train_indices = train_indices[:train_size]\n",
    "        train_val_indices = train_indices[train_size:]\n",
    "        \n",
    "        train_train_dataset = Subset(full_dataset, train_train_indices)\n",
    "        train_val_dataset = Subset(full_dataset, train_val_indices)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(train_val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_fold_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Load fresh model for this fold\n",
    "        print(f\"Loading fresh model for fold {fold + 1}...\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        \n",
    "        best_val_f1 = 0\n",
    "        best_val_threshold = 0.5\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Training loop for this fold\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1}, Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"]\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Validation on this fold\n",
    "            val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "            \n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_val_threshold = val_metrics['threshold']\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}: Val F1 = {val_f1:.4f}, Best = {best_val_f1:.4f}\")\n",
    "        \n",
    "        # Load best model for testing\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation for this fold\n",
    "        test_f1, test_metrics = evaluate_legal_model(\n",
    "            model, test_loader, tokenizer, device,\n",
    "            use_threshold_tuning=False,\n",
    "            fix_threshold=best_val_threshold\n",
    "        )\n",
    "        \n",
    "        fold_f1_scores.append(test_f1)\n",
    "        fold_accuracies.append(test_metrics['accuracy'])\n",
    "        \n",
    "        print(f\"✅ Fold {fold+1} Results: F1 = {test_f1:.4f}, Accuracy = {test_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Store results for this model\n",
    "    model_fold_results[model_name] = fold_f1_scores\n",
    "    model_fold_accuracy[model_name] = fold_accuracies\n",
    "    \n",
    "    print(f\"\\n📊 {model_name} - K-Fold Summary:\")\n",
    "    print(f\"F1 Scores: {fold_f1_scores}\")\n",
    "    print(f\"Mean F1: {np.mean(fold_f1_scores):.4f} ± {np.std(fold_f1_scores):.4f}\")\n",
    "    print(f\"Accuracy: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} ± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# ========================\n",
    "# 📊 Statistical Significance Testing with K-Fold Results\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 K-FOLD STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_names = list(model_fold_results.keys())\n",
    "if len(model_names) == 2:\n",
    "    model1, model2 = model_names\n",
    "    \n",
    "    # Extract k-fold results\n",
    "    f1_scores_1 = model_fold_results[model1]\n",
    "    f1_scores_2 = model_fold_results[model2]\n",
    "    accuracy_scores_1 = model_fold_accuracy[model1]\n",
    "    accuracy_scores_2 = model_fold_accuracy[model2]\n",
    "    \n",
    "    print(f\"\\nComparing {model1} vs {model2}\")\n",
    "    print(f\"K-Fold validation with {K_FOLDS} folds\")\n",
    "    \n",
    "    # Paired t-test for F1 scores (each fold is a paired observation)\n",
    "    print(\"\\n--- F1 Score Comparison (K-Fold) ---\")\n",
    "    print(f\"{model1} - Mean F1: {np.mean(f1_scores_1):.4f} ± {np.std(f1_scores_1):.4f}\")\n",
    "    print(f\"{model2} - Mean F1: {np.mean(f1_scores_2):.4f} ± {np.std(f1_scores_2):.4f}\")\n",
    "    print(f\"Fold-wise F1 scores:\")\n",
    "    for i in range(K_FOLDS):\n",
    "        print(f\"  Fold {i+1}: {f1_scores_1[i]:.4f} vs {f1_scores_2[i]:.4f}\")\n",
    "    \n",
    "    f1_t_stat, f1_p_value = stats.ttest_rel(f1_scores_1, f1_scores_2)\n",
    "    print(f\"\\nPaired t-test (K-Fold): t={f1_t_stat:.4f}, p={f1_p_value:.6f}\")\n",
    "    \n",
    "    # Determine significance level\n",
    "    if f1_p_value < 0.001:\n",
    "        significance_f1 = \"***\"\n",
    "    elif f1_p_value < 0.01:\n",
    "        significance_f1 = \"**\"\n",
    "    elif f1_p_value < 0.05:\n",
    "        significance_f1 = \"*\"\n",
    "    else:\n",
    "        significance_f1 = \"ns\"\n",
    "    \n",
    "    print(f\"Significance: {significance_f1}\")\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    diff_f1 = np.array(f1_scores_1) - np.array(f1_scores_2)\n",
    "    cohens_d_f1 = np.mean(diff_f1) / np.std(diff_f1)\n",
    "    print(f\"Cohen's d (effect size): {cohens_d_f1:.4f}\")\n",
    "    \n",
    "    # Paired t-test for Accuracy\n",
    "    print(\"\\n--- Accuracy Comparison (K-Fold) ---\")\n",
    "    print(f\"{model1} - Mean Accuracy: {np.mean(accuracy_scores_1):.4f} ± {np.std(accuracy_scores_1):.4f}\")\n",
    "    print(f\"{model2} - Mean Accuracy: {np.mean(accuracy_scores_2):.4f} ± {np.std(accuracy_scores_2):.4f}\")\n",
    "    print(f\"Fold-wise Accuracy scores:\")\n",
    "    for i in range(K_FOLDS):\n",
    "        print(f\"  Fold {i+1}: {accuracy_scores_1[i]:.4f} vs {accuracy_scores_2[i]:.4f}\")\n",
    "    \n",
    "    acc_t_stat, acc_p_value = stats.ttest_rel(accuracy_scores_1, accuracy_scores_2)\n",
    "    print(f\"\\nPaired t-test (K-Fold): t={acc_t_stat:.4f}, p={acc_p_value:.6f}\")\n",
    "    \n",
    "    if acc_p_value < 0.001:\n",
    "        significance_acc = \"***\"\n",
    "    elif acc_p_value < 0.01:\n",
    "        significance_acc = \"**\"\n",
    "    elif acc_p_value < 0.05:\n",
    "        significance_acc = \"*\"\n",
    "    else:\n",
    "        significance_acc = \"ns\"\n",
    "    \n",
    "    print(f\"Significance: {significance_acc}\")\n",
    "    \n",
    "    # Effect size for accuracy\n",
    "    diff_acc = np.array(accuracy_scores_1) - np.array(accuracy_scores_2)\n",
    "    cohens_d_acc = np.mean(diff_acc) / np.std(diff_acc)\n",
    "    print(f\"Cohen's d (effect size): {cohens_d_acc:.4f}\")\n",
    "    \n",
    "    # Confidence intervals\n",
    "    from scipy.stats import t\n",
    "    alpha = 0.05\n",
    "    dof = K_FOLDS - 1\n",
    "    t_critical = t.ppf(1 - alpha/2, dof)\n",
    "    \n",
    "    f1_diff_mean = np.mean(diff_f1)\n",
    "    f1_diff_se = stats.sem(diff_f1)\n",
    "    f1_ci_lower = f1_diff_mean - t_critical * f1_diff_se\n",
    "    f1_ci_upper = f1_diff_mean + t_critical * f1_diff_se\n",
    "    \n",
    "    acc_diff_mean = np.mean(diff_acc)\n",
    "    acc_diff_se = stats.sem(diff_acc)\n",
    "    acc_ci_lower = acc_diff_mean - t_critical * acc_diff_se\n",
    "    acc_ci_upper = acc_diff_mean + t_critical * acc_diff_se\n",
    "    \n",
    "    print(f\"\\n95% Confidence Interval for F1 difference: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "    print(f\"95% Confidence Interval for Accuracy difference: [{acc_ci_lower:.4f}, {acc_ci_upper:.4f}]\")\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric)\n",
    "    print(\"\\n--- Non-parametric Tests ---\")\n",
    "    f1_wilcoxon_stat, f1_wilcoxon_p = stats.wilcoxon(f1_scores_1, f1_scores_2)\n",
    "    acc_wilcoxon_stat, acc_wilcoxon_p = stats.wilcoxon(accuracy_scores_1, accuracy_scores_2)\n",
    "    \n",
    "    print(f\"Wilcoxon signed-rank test (F1): p={f1_wilcoxon_p:.6f}\")\n",
    "    print(f\"Wilcoxon signed-rank test (Accuracy): p={acc_wilcoxon_p:.6f}\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"📋 K-FOLD STATISTICAL SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<15} {'Model 1':<15} {'Model 2':<15} {'p-value':<12} {'Significance':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'F1 Score':<15} {np.mean(f1_scores_1):<15.4f} {np.mean(f1_scores_2):<15.4f} {f1_p_value:<12.6f} {significance_f1:<12}\")\n",
    "    print(f\"{'Accuracy':<15} {np.mean(accuracy_scores_1):<15.4f} {np.mean(accuracy_scores_2):<15.4f} {acc_p_value:<12.6f} {significance_acc:<12}\")\n",
    "    \n",
    "    print(f\"\\nEffect Sizes (Cohen's d):\")\n",
    "    print(f\"F1 Score: {cohens_d_f1:.4f}\")\n",
    "    print(f\"Accuracy: {cohens_d_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n",
    "    print(\"Effect size interpretation: |d| < 0.2 (small), 0.2-0.8 (medium), > 0.8 (large)\")\n",
    "    \n",
    "    # Save detailed k-fold results\n",
    "    kfold_results = pd.DataFrame({\n",
    "        'fold': range(1, K_FOLDS + 1),\n",
    "        f'{model1}_f1': f1_scores_1,\n",
    "        f'{model2}_f1': f1_scores_2,\n",
    "        f'{model1}_accuracy': accuracy_scores_1,\n",
    "        f'{model2}_accuracy': accuracy_scores_2,\n",
    "        'f1_difference': diff_f1,\n",
    "        'accuracy_difference': diff_acc\n",
    "    })\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'statistic': ['mean', 'std', 'sem', 't_stat', 'p_value', 'cohens_d'],\n",
    "        f'{model1}_f1': [np.mean(f1_scores_1), np.std(f1_scores_1), stats.sem(f1_scores_1), \n",
    "                        f1_t_stat, f1_p_value, cohens_d_f1],\n",
    "        f'{model2}_f1': [np.mean(f1_scores_2), np.std(f1_scores_2), stats.sem(f1_scores_2), \n",
    "                        f1_t_stat, f1_p_value, -cohens_d_f1],\n",
    "        f'{model1}_accuracy': [np.mean(accuracy_scores_1), np.std(accuracy_scores_1), stats.sem(accuracy_scores_1),\n",
    "                              acc_t_stat, acc_p_value, cohens_d_acc],\n",
    "        f'{model2}_accuracy': [np.mean(accuracy_scores_2), np.std(accuracy_scores_2), stats.sem(accuracy_scores_2),\n",
    "                              acc_t_stat, acc_p_value, -cohens_d_acc]\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    kfold_results.to_csv('kfold_detailed_results.csv', index=False)\n",
    "    summary_stats.to_csv('kfold_statistical_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n💾 Results saved:\")\n",
    "    print(f\"  - Detailed k-fold results: 'kfold_detailed_results.csv'\")\n",
    "    print(f\"  - Statistical summary: 'kfold_statistical_summary.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"Note: Statistical comparison requires exactly 2 models for paired testing.\")\n",
    "\n",
    "print(f\"\\n🎯 K-FOLD CROSS-VALIDATION COMPLETED\")\n",
    "print(f\"Both models evaluated on the same {K_FOLDS} folds for fair comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
