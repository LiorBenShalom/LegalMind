{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ğŸ”§ CONFIG\n",
    "# ========================\n",
    "# model_path = \"/home/liorkob/M.Sc/thesis/t5/mt5-mlm-final\"\n",
    "# model_path = \"imvladikon/het5-base\"\n",
    "model_path = \"google/mt5-base\"\n",
    "# model_path=\"/home/liorkob/M.Sc/thesis/t5/mt5-punishment-regression\"\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ§  Dataset for T5\n",
    "# ========================\n",
    "class T5CitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.inputs = df.apply(lambda row: f\"predict citation: {row['gpt_facts_a']} </s> {row['gpt_facts_b']}\", axis=1).tolist()\n",
    "        self.targets = df[\"label\"].apply(lambda l: \"yes\" if l == 1 else \"no\").tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_enc = self.tokenizer(self.inputs[idx], padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        target_enc = self.tokenizer(self.targets[idx], padding='max_length', truncation=True, max_length=4, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“¥ Load Data\n",
    "# ========================\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "# model.gradient_checkpointing_enable()  # âœ… ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "model.gradient_checkpointing_enable()  # âœ… ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "train_dataset = T5CitationDataset(df_train, tokenizer, max_len=max_len)\n",
    "val_dataset = T5CitationDataset(df_val, tokenizer, max_len=max_len)\n",
    "test_dataset = T5CitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ========================\n",
    "# ğŸ” Training Loop\n",
    "# ========================\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        torch.cuda.empty_cache()  # âœ… ×¨×™×§×•×Ÿ ×‘×™×Ÿ ×¦×¢×“×™×\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Evaluation\n",
    "# ========================\n",
    "def evaluate(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            label_texts = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "            preds.extend([1 if p.strip().lower() == \"yes\" else 0 for p in pred_texts])\n",
    "            labels.extend([1 if l.strip().lower() == \"yes\" else 0 for l in label_texts])\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"AUC-ROC: {roc_auc_score(labels, preds):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(labels, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(labels, preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(labels, preds):.4f}\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "print(\"\\nğŸ” Validation Set:\")\n",
    "evaluate(model, val_loader, tokenizer, device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ§ª Test Set:\")\n",
    "evaluate(model, test_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model and tokenizer...\n",
      "Loading data...\n",
      "\n",
      "ğŸ” TESTING DIFFERENT LEGAL PROMPTS:\n",
      "============================================================\n",
      "\n",
      "ğŸ“‹ Testing Prompt Version 1:\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3919 characters\n",
      "Prompt version: 1\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:06<00:00, 32.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 3.4868 (F1: 0.5009)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:06<00:00, 31.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5009\n",
      "Precision: 0.3382\n",
      "Recall: 0.9655\n",
      "Accuracy: 0.3579\n",
      "\n",
      "Prediction Distribution: [ 41 828]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5095\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.05      0.10       579\n",
      "           1       0.34      0.97      0.50       290\n",
      "\n",
      "    accuracy                           0.36       869\n",
      "   macro avg       0.55      0.51      0.30       869\n",
      "weighted avg       0.62      0.36      0.23       869\n",
      "\n",
      "Prompt 1 Baseline F1: 0.5009\n",
      "\n",
      "ğŸ“‹ Testing Prompt Version 2:\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3919 characters\n",
      "Prompt version: 2\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:06<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 3.5317 (F1: 0.5085)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:06<00:00, 31.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5085\n",
      "Precision: 0.3589\n",
      "Recall: 0.8724\n",
      "Accuracy: 0.4373\n",
      "\n",
      "Prediction Distribution: [164 705]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5459\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.22      0.34       579\n",
      "           1       0.36      0.87      0.51       290\n",
      "\n",
      "    accuracy                           0.44       869\n",
      "   macro avg       0.57      0.55      0.43       869\n",
      "weighted avg       0.64      0.44      0.40       869\n",
      "\n",
      "Prompt 2 Baseline F1: 0.5085\n",
      "\n",
      "ğŸ“‹ Testing Prompt Version 3:\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3924 characters\n",
      "Prompt version: 3\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:06<00:00, 31.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 3.5402 (F1: 0.5142)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 31.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5142\n",
      "Precision: 0.3837\n",
      "Recall: 0.7793\n",
      "Accuracy: 0.5086\n",
      "\n",
      "Prediction Distribution: [280 589]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5762\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.37      0.50       579\n",
      "           1       0.38      0.78      0.51       290\n",
      "\n",
      "    accuracy                           0.51       869\n",
      "   macro avg       0.58      0.58      0.51       869\n",
      "weighted avg       0.64      0.51      0.51       869\n",
      "\n",
      "Prompt 3 Baseline F1: 0.5142\n",
      "\n",
      "ğŸ† BEST PROMPT VERSION: 3 (F1: 0.5142)\n",
      "\n",
      "Creating final datasets with prompt version 3...\n",
      "Legal Dataset created: 4052 samples\n",
      "Label distribution: [2699 1353]\n",
      "Sample input length: 1989 characters\n",
      "Prompt version: 3\n",
      "Legal Dataset created: 869 samples\n",
      "Label distribution: [579 290]\n",
      "Sample input length: 3924 characters\n",
      "Prompt version: 3\n",
      "Legal Dataset created: 870 samples\n",
      "Label distribution: [579 291]\n",
      "Sample input length: 2199 characters\n",
      "Prompt version: 3\n",
      "\n",
      "Baseline F1 (0.5142) needs improvement. Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:05<00:00,  8.07it/s, loss=0.3464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Average Loss = 3.2148\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.6755 (F1: 0.5115)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5115\n",
      "Precision: 0.3488\n",
      "Recall: 0.9586\n",
      "Accuracy: 0.3890\n",
      "\n",
      "Prediction Distribution: [ 72 797]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5311\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.10      0.18       579\n",
      "           1       0.35      0.96      0.51       290\n",
      "\n",
      "    accuracy                           0.39       869\n",
      "   macro avg       0.59      0.53      0.35       869\n",
      "weighted avg       0.67      0.39      0.29       869\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:05<00:00,  8.10it/s, loss=0.3013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Average Loss = 0.3555\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2674 (F1: 0.5227)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5227\n",
      "Precision: 0.3763\n",
      "Recall: 0.8552\n",
      "Accuracy: 0.4787\n",
      "\n",
      "Prediction Distribution: [210 659]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5727\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.29      0.43       579\n",
      "           1       0.38      0.86      0.52       290\n",
      "\n",
      "    accuracy                           0.48       869\n",
      "   macro avg       0.59      0.57      0.47       869\n",
      "weighted avg       0.66      0.48      0.46       869\n",
      "\n",
      "âœ… New best model! F1: 0.5227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:04<00:00,  8.11it/s, loss=0.4273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Average Loss = 0.3281\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2343 (F1: 0.5260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5260\n",
      "Precision: 0.3625\n",
      "Recall: 0.9586\n",
      "Accuracy: 0.4235\n",
      "\n",
      "Prediction Distribution: [102 767]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5570\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.16      0.26       579\n",
      "           1       0.36      0.96      0.53       290\n",
      "\n",
      "    accuracy                           0.42       869\n",
      "   macro avg       0.62      0.56      0.40       869\n",
      "weighted avg       0.71      0.42      0.35       869\n",
      "\n",
      "âœ… New best model! F1: 0.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:05<00:00,  8.10it/s, loss=0.2293]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Average Loss = 0.3010\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.6886 (F1: 0.5330)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5330\n",
      "Precision: 0.3947\n",
      "Recall: 0.8207\n",
      "Accuracy: 0.5201\n",
      "\n",
      "Prediction Distribution: [266 603]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.5951\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.37      0.51       579\n",
      "           1       0.39      0.82      0.53       290\n",
      "\n",
      "    accuracy                           0.52       869\n",
      "   macro avg       0.60      0.60      0.52       869\n",
      "weighted avg       0.67      0.52      0.52       869\n",
      "\n",
      "âœ… New best model! F1: 0.5330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1013/1013 [02:04<00:00,  8.11it/s, loss=0.3243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Average Loss = 0.3167\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.5397 (F1: 0.5533)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5533\n",
      "Precision: 0.4239\n",
      "Recall: 0.7966\n",
      "Accuracy: 0.5708\n",
      "\n",
      "Prediction Distribution: [324 545]\n",
      "True Label Distribution: [579 290]\n",
      "AUC-ROC: 0.6271\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.46      0.59       579\n",
      "           1       0.42      0.80      0.55       290\n",
      "\n",
      "    accuracy                           0.57       869\n",
      "   macro avg       0.62      0.63      0.57       869\n",
      "weighted avg       0.69      0.57      0.58       869\n",
      "\n",
      "âœ… New best model! F1: 0.5533\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª FINAL LEGAL CITATION PREDICTION TEST:\n",
      "================================================================================\n",
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.5047 (F1: 0.5338)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 218/218 [00:07<00:00, 29.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5338\n",
      "Precision: 0.4157\n",
      "Recall: 0.7457\n",
      "Accuracy: 0.5644\n",
      "\n",
      "Prediction Distribution: [348 522]\n",
      "True Label Distribution: [579 291]\n",
      "AUC-ROC: 0.6095\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.47      0.59       579\n",
      "           1       0.42      0.75      0.53       291\n",
      "\n",
      "    accuracy                           0.56       870\n",
      "   macro avg       0.60      0.61      0.56       870\n",
      "weighted avg       0.66      0.56      0.57       870\n",
      "\n",
      "\n",
      "ğŸ¯ FINAL RESULTS:\n",
      "Best Prompt Version: 3\n",
      "Baseline F1: 0.5142\n",
      "Final Test F1: 0.5338\n",
      "Final Test Accuracy: 0.5644\n",
      "Optimal Threshold: -6.5047\n",
      "\n",
      "ğŸ“ˆ F1: 0.5338 - Consider fine-tuning hyperparameters or trying longer training\n",
      "\n",
      "ğŸ’¾ Results saved to 'legal_citation_results.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ========================\n",
    "# ğŸ”§ CONFIG\n",
    "# ========================\n",
    "model_path = \"imvladikon/het5-base\"\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ¯ CORE: Improved Logits Classification\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    \"\"\"Classify using threshold-based method\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=input_ids.device)\n",
    "        decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        yes_tokens = [259, 1903]  # ×›×Ÿ\n",
    "        no_tokens = [1124]        # ×œ×\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx]\n",
    "            \n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            \n",
    "            score_diff = yes_score - no_score\n",
    "            \n",
    "            if score_diff > threshold:\n",
    "                prediction = 1\n",
    "                predicted_text = \"×›×Ÿ\"\n",
    "            else:\n",
    "                prediction = 0\n",
    "                predicted_text = \"×œ×\"\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            scores.append({\n",
    "                'prediction': prediction,\n",
    "                'predicted_text': predicted_text,\n",
    "                'score_diff': score_diff,\n",
    "                'yes_score': yes_score,\n",
    "                'no_score': no_score\n",
    "            })\n",
    "        \n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, device, true_labels):\n",
    "    \"\"\"Find optimal threshold for balanced predictions\"\"\"\n",
    "    print(\"ğŸ” Finding best threshold...\")\n",
    "    \n",
    "    all_score_diffs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Collecting scores\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            _, scores = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=0.0\n",
    "            )\n",
    "            \n",
    "            for score in scores:\n",
    "                all_score_diffs.append(score['score_diff'])\n",
    "    \n",
    "    # Test thresholds\n",
    "    thresholds = np.linspace(min(all_score_diffs), max(all_score_diffs), 50)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch_preds, _ = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "# ========================\n",
    "# ğŸ§  IMPROVED Dataset with Specific Legal Prompts\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512, prompt_version=1):\n",
    "        \"\"\"\n",
    "        Dataset with legally-specific prompts for sentencing citation prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiple versions of detailed legal prompts\n",
    "        self.legal_prompts = {\n",
    "            1: {\n",
    "                \"hebrew\": \"\"\"××©×™××”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×œ×ª××™×›×” ×‘××“×™× ×™×•×ª ×’×–×¨ ×“×™×Ÿ\n",
    "×”×§×©×¨: ×‘×¤×¡×§×™ ×“×™×Ÿ ×¤×œ×™×œ×™×™×, ×©×•×¤×˜×™× ××¦×˜×˜×™× ×¤×¡×§×™ ×“×™×Ÿ ×§×•×“××™× ×›×“×™ ×œ×ª××•×š ×‘×˜×•×•×— ×”×¢× ×™×©×” ×©×”× ××¦×™×¢×™×. ×œ× ×›×œ ×”×¦×™×˜×•×˜×™× ×¨×œ×•×•× ×˜×™×™× - ×× ×• ××ª××§×“×™× ×¨×§ ×‘×¦×™×˜×•×˜×™× ×”×ª×•××›×™× ×‘×”×—×œ×˜×•×ª ×˜×•×•×— ×”×¢× ×™×©×”.\n",
    "×©××œ×”: ×”×× ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×›×“×™ ×œ×ª××•×š ×‘××“×™× ×™×•×ª ×’×–×¨ ×”×“×™×Ÿ ×©×œ×•, ×¢×œ ×‘×¡×™×¡ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×?\"\"\",\n",
    "                \"english\": \"\"\"Task: Predict citations supporting sentencing policy decisions\n",
    "Context: In criminal verdicts, judges cite previous rulings to support their proposed sentencing range. Not all citations are relevant - we focus specifically on citations supporting sentencing range decisions.\n",
    "Question: Will verdict A cite verdict B to support its sentencing policy, based on indictment facts?\"\"\"\n",
    "            },\n",
    "            \n",
    "            2: {\n",
    "                \"hebrew\": \"\"\"× ×™×ª×•×— ×¦×™×˜×•×˜×™× ×‘×¤×¡×§×™ ×“×™×Ÿ ×¤×œ×™×œ×™×™×\n",
    "××˜×¨×”: ×–×™×”×•×™ ×¦×™×˜×•×˜×™× ×”×¨×œ×•×•× ×˜×™×™× ×œ××“×™× ×™×•×ª ×¢× ×™×©×”\n",
    "×”×’×“×¨×”: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ = ×”×¤× ×™×” ×œ×¤×¡×§ ×“×™×Ÿ ×§×•×“× ×”××©××© ×›×ª×§×“×™× ×œ×˜×•×•×— ×”×¢×•× ×© ×”××•×¦×¢\n",
    "××™×§×•×: ×‘×“×¨×š ×›×œ×œ ×‘×—×œ×§ \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\" ××• \"×˜×•×•×— ×”×¢× ×™×©×”\" ×©×œ ×¤×¡×§ ×”×“×™×Ÿ\n",
    "×©××œ×”: ×‘×”×ª×‘×¡×¡ ×¢×œ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×, ×”×× ×¦×¤×•×™ ×©×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘×˜×•×•×— ×”×¢× ×™×©×”?\"\"\",\n",
    "                \"english\": \"\"\"Criminal verdict citation analysis\n",
    "Goal: Identify citations relevant to sentencing policy\n",
    "Definition: Relevant citation = reference to prior ruling used as precedent for proposed punishment range\n",
    "Location: Typically found in \"Sentencing Policy\" or \"Sentencing Range\" sections\n",
    "Question: Based on indictment facts, will verdict A likely cite verdict B to support sentencing range?\"\"\"\n",
    "            },\n",
    "            \n",
    "            3: {\n",
    "                \"hebrew\": \"\"\"××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×”\n",
    "×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×”\n",
    "××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×\n",
    "×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×)\n",
    "×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×.\"\"\",\n",
    "                \"english\": \"\"\"Specialized legal citation prediction system\n",
    "Domain: Criminal law - sentencing policy\n",
    "Purpose: Predict citations between verdicts based on indictment facts similarity\n",
    "Criteria: Citation is relevant if it supports sentencing range decision (not procedures, definitions, or unrelated verdicts)\n",
    "Verdict A will cite verdict B if there is similarity in offenses and circumstances presented in indictments.\"\"\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        chosen_prompt = self.legal_prompts[prompt_version][\"hebrew\"]\n",
    "        \n",
    "        # Create more detailed inputs with legal context\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Format with detailed legal context\n",
    "            legal_input = f\"\"\"{chosen_prompt}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×”×¢×‘×™×¨×•×ª ×•×”× ×¡×™×‘×•×ª, ×”×× ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘××“×™× ×™×•×ª ×”×¢× ×™×©×”?\"\"\"\n",
    "            \n",
    "            self.inputs.append(legal_input)\n",
    "        \n",
    "        self.targets = df[\"label\"].apply(lambda l: \"×›×Ÿ\" if l == 1 else \"×œ×\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(f\"Legal Dataset created: {len(self.inputs)} samples\")\n",
    "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Sample input length: {len(self.inputs[0])} characters\")\n",
    "        print(f\"Prompt version: {prompt_version}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        \n",
    "        # Tokenize with longer sequences due to detailed prompt\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=5,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Evaluation Function\n",
    "# ========================\n",
    "def evaluate_legal_model(model, dataloader, tokenizer, device, use_threshold_tuning=True):\n",
    "    \"\"\"Evaluate the legal citation model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect true labels\n",
    "    true_labels = []\n",
    "    for batch in dataloader:\n",
    "        true_labels.extend(batch[\"numeric_label\"].numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    if use_threshold_tuning:\n",
    "        best_threshold = find_best_threshold(model, tokenizer, dataloader, device, true_labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "    else:\n",
    "        best_threshold = None\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (No Threshold Tuning)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                generated = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=5\n",
    "                )\n",
    "                decoded_preds = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                predictions = [1 if p.strip() == \"×›×Ÿ\" else 0 for p in decoded_preds]\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend([{} for _ in predictions])  # <- ×“×my score dicts\n",
    "\n",
    "    predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    print(f\"\\nğŸ“Š LEGAL CITATION PREDICTION RESULTS:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Distribution: {np.bincount(predictions)}\")\n",
    "    print(f\"True Label Distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "    if len(np.unique(predictions)) > 1 and len(np.unique(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predictions)\n",
    "        print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    return f1, {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'threshold': best_threshold,\n",
    "        'scores': all_confidence_scores  # â† ×ª××™×“ ×§×™×™×\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ğŸ“¥ Load Everything\n",
    "# ========================\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# Try different prompt versions\n",
    "prompt_versions_to_try = [1, 2, 3]\n",
    "best_prompt_version = 1\n",
    "best_baseline_f1 = 0\n",
    "\n",
    "print(\"\\nğŸ” TESTING DIFFERENT LEGAL PROMPTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt_version in prompt_versions_to_try:\n",
    "    print(f\"\\nğŸ“‹ Testing Prompt Version {prompt_version}:\")\n",
    "    \n",
    "    # Create datasets with this prompt version\n",
    "    val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len, prompt_version=prompt_version)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Test baseline performance\n",
    "    baseline_f1, baseline_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "    \n",
    "    print(f\"Prompt {prompt_version} Baseline F1: {baseline_f1:.4f}\")\n",
    "    \n",
    "    if baseline_f1 > best_baseline_f1:\n",
    "        best_baseline_f1 = baseline_f1\n",
    "        best_prompt_version = prompt_version\n",
    "\n",
    "print(f\"\\nğŸ† BEST PROMPT VERSION: {best_prompt_version} (F1: {best_baseline_f1:.4f})\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ—ï¸ Create Final Datasets with Best Prompt\n",
    "# ========================\n",
    "print(f\"\\nCreating final datasets with prompt version {best_prompt_version}...\")\n",
    "train_dataset = LegalSentencingCitationDataset(df_train, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ========================\n",
    "# ğŸ” Training (if needed)\n",
    "# ========================\n",
    "if best_baseline_f1 < 0.6:\n",
    "    print(f\"\\nBaseline F1 ({best_baseline_f1:.4f}) needs improvement. Starting training...\")\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_val_f1 = best_baseline_f1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}: Average Loss = {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_legal_citation_model.pt\")\n",
    "            print(f\"âœ… New best model! F1: {best_val_f1:.4f}\")\n",
    "else:\n",
    "    print(f\"Baseline F1 ({best_baseline_f1:.4f}) is already good!\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ§ª Final Test Evaluation\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª FINAL LEGAL CITATION PREDICTION TEST:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_f1, test_metrics = evaluate_legal_model(model, test_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "\n",
    "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "print(f\"Best Prompt Version: {best_prompt_version}\")\n",
    "print(f\"Baseline F1: {best_baseline_f1:.4f}\")\n",
    "print(f\"Final Test F1: {test_f1:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Optimal Threshold: {test_metrics['threshold']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'best_prompt_version': best_prompt_version,\n",
    "    'baseline_f1': best_baseline_f1,\n",
    "    'test_f1': test_f1,\n",
    "    'test_accuracy': test_metrics['accuracy'],\n",
    "    'optimal_threshold': test_metrics['threshold'],\n",
    "    'model_type': 'legal_sentencing_citation_prediction'\n",
    "\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('legal_citation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if test_f1 > 0.6:\n",
    "    print(f\"\\nğŸ‰ SUCCESS! Legal citation model achieved {test_f1:.4f} F1 score!\")\n",
    "    print(f\"ğŸ† The detailed legal prompts significantly improved performance!\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“ˆ F1: {test_f1:.4f} - Consider fine-tuning hyperparameters or trying longer training\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Results saved to 'legal_citation_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Showing 10 Sample Predictions:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #379\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª ×—×œ×§ ×©×œ × ×©×§ ××• ×ª×—××•×©×ª, ×œ×¤×™ ×¡×¢×™×£ 144 (×) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×– 1977 ×•× ×©×™××”/×”×•×‘×œ×ª ×—×œ×§ ×©×œ × ×©×§ ××• ×ª×—××•×©×ª, ×œ×¤×™ ×¡×¢×™×£ 144(×‘) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ. ×¢×œ ×¤×™ ×”× ×˜×¢×Ÿ ×‘×›×ª×‘ ×”××™×©×•×, ×‘×™×•× 28.8.2022 ×‘×©×¢×” 00:20 ×œ×¢×¨×š, × ×”×’ ×”× ××©× ×‘×¨×›×‘ ××¡×•×’ ×§×™×” ×¡×¤×•×¨×˜×’' ×¢× ×œ×•×—×™×ª ×¨×™×©×•×™ ××¡×¤×¨ 13-608-201 ×œ×›×™×•×•×Ÿ ××¢×‘×¨ ×”×œ\"×” ×‘×“×¨×›×• ×œ×©×˜×—×™ ×”××–×•×¨, ×›××©×¨ ××ª×—×ª ×œ××•×©×‘ ×”× ×”×’ ×‘×¨×›×‘ ×”×™×™×ª×” ×©×§×™×ª ×•×‘×” 6 ××›×œ×•×œ×™× ×©×œ × ×©×§ ××¡×•×’ M16. ×‘× ×•×¡×£, ×‘×ª× ×”××˜×¢×Ÿ ×©×œ ×”×¨×›×‘ × ×©× ×©×‘×¢×” ××¨×’×–×™ ×ª×—××•×©×ª ×•××¨×’×– ×§×¨×˜×•×Ÿ ×©×”×›×™×œ×• ×™×—×“×™×• ×›-9000 ×›×“×•×¨×™× ×‘×§×•×˜×¨ 5.56 ×\"×, ×©×”×™×• ××›×•×¡×™× ×•××•×¡×ª×¨×™×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘': ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×™×¡×•×“ ×”×•×“××ª×• ×‘×”×ª×× ×œ×”×¡×“×¨ ×˜×™×¢×•×Ÿ ×“×™×•× ×™ ×‘×›×ª×‘ ××™×©×•× ××ª×•×§×Ÿ ×‘×”×—×–×§×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª, ×¢×‘×™×¨×” ...\n",
      "âœ… True Label: ×œ×\n",
      "ğŸ§  Predicted: ×œ×\n",
      "ğŸ§® Score Diff: -6.7443 | Yes Score: 0.5521 | No Score: 7.2964\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #302\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×‘×™×¨×•×ª ×”××™×•×—×¡×•×ª ×œ×• ×‘×›×ª×‘ ×”××™×©×•×: ×”×—×–×§×” ××• ×©×™××•×© ×‘×¡××™× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª â€“ ×¢×‘×™×¨×” ×œ×¤×™ ×¡×¢×™×£ 7(×)+(×’) ×¨×™×©× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× (× ×•×¡×— ×—×“×©), ×”×ª×©×œ\"×’ â€“ 1973. ×’×™×“×•×œ, ×™×™×¦×•×¨ ×”×›× ×” ×•×”×¤×§×” ×©×œ ×¡× ××¡×•×›×Ÿ â€“ ×¢×‘×™×¨×” ×œ×¤×™ ×¡×¢×™×£ 6 ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× (× ×•×¡×— ×—×“×©), ×”×ª×©×œ\"×’ â€“ 1973. ×”×¦×“×“×™× ×”×’×™×¢×• ×œ×”×¡×“×¨ ×‘×“×‘×¨ ×”×¡×›××” ×¢×•× ×©×™×ª, ×××¡×¨ ×©×¨×™×¦×•×™×• ×™×”×™×” ×‘×“×¨×š ×©×œ ×¢×‘×•×“×•×ª ×©×™×¨×•×ª ×œ×¦×“ ×¢× ×™×©×” ×”×¦×•×¤×” ×¤× ×™ ×¢×ª×™×“ ×•×§× ×¡ ×•×›×Ÿ ×œ×××¥ ×›×œ ×¢× ×™×©×” × ×•×¡×¤×ª ×œ×¨×‘×•×ª ×¦×• ××‘×—×Ÿ, ×›××©×¨ ×›×œ ××—×“ ××”×¦×“×“×™× ×™×•×›×œ ×œ×˜×¢×•×Ÿ ×‘××©×¨ ×œ××©×š ×ª×§×•×¤×ª ×”×××¡×¨ ××•×ª×” ×™×¨×¦×” ×”× ××©× ×‘×¢×‘×•×“×•×ª ×©×™×¨×•×ª. ×”×××©×™××” ×¢×•×ª×¨×ª ×œ×ª×§×•×¤×” ×©×œ× ×ª×¤×—×ª ××ª×©×¢×” ×—×•×“×©×™× ×œ×¦×“ ×¢× ×™×©×” × ×œ×•×•×™×ª ×•××™×œ×• ×‘\"×› ×”× ××©× ×˜×•×¢×Ÿ ×›×™ ×™×© ×œ×”×¡×ª×¤×§ ×‘×”××œ×¦×ª ×©×™×¨×•×ª ×”××‘×—×Ÿ ×œ×¦×• ×©×œ\"×¦ ×•×œ×—×™×œ×•×¤×™×Ÿ ×œ× ×•×›×— ×”×¡×›×...\n",
      "âœ… True Label: ×œ×\n",
      "ğŸ§  Predicted: ×›×Ÿ\n",
      "ğŸ§® Score Diff: -6.3289 | Yes Score: 0.4099 | No Score: 6.7388\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #843\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× 1 ×”×•×“×” ×•×”×•×¨×©×¢ ×‘×›×ª×‘ ××™×©×•× ××ª×•×§×Ÿ (×˜×¨× ××¢× ×”) ××™×•× 14.7.2020 ×”××™×™×—×¡ ×œ×• ×¢×‘×™×¨×” ×©×œ ×”×¡×¤×§×ª ×¡× ××¡×•×›×Ÿ ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×•- 19× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× (× ×•×¡×— ×—×“×©), ×ª×©×œ\"×’ â€“ 1973. ×¢×œ ×¤×™ ×”××ª×•××¨ ×‘×›×ª×‘ ×”××™×©×•× ×”××ª×•×§×Ÿ, ×‘×ª××¨×™×š 18.6.2020 ×¡××•×š ×œ×©×¢×” 21:35 ×§×™×‘×œ ×”× ××©× 1 ×©×™×—×ª ×˜×œ×¤×•×Ÿ ××”× ××©× 2 - ×¢××• ×™×© ×œ×• ×”×™×›×¨×•×ª ××•×§×“××ª, ×•×ª×™×× ××™×ª×• ×©×ª×’×™×¢ ××œ×™×• ×‘×—×•×¨×” ×‘×©× ××•×œ×’×”, ×œ×§×—×ª ××× ×• ×¡× ××¡×•×›×Ÿ. ×‘×™×Ÿ ×”× ××©× 1 ×œ××•×œ×’×” â€“ ×œ× ×”×™×™×ª×” ×”×™×›×¨×•×ª ××•×§×“××ª. ××™×“ ×•×‘×”××©×š, ×”×ª×§×©×¨ ×”× ××©× 2 ×œ××•×œ×’×” ×•××¡×¨ ×œ×” ×œ×”×’×™×¢ ×œ×¨×—×•×‘ ×¡×—×œ×‘ ×‘×¢×™×¨ ××™×œ×ª (×œ×”×œ×Ÿ: \"×”××§×•×\"). ×‘×©×¢×” 23:08 ××• ×‘×¡××•×š ×œ×›×š ×”×ª×§×©×¨ ×”× ××©× 2 ×œ× ××©× 1 ×•×‘×™×§×© ××× ×• ×œ×’×©×ª ×œ×¨×›×‘×” ×©×œ ××•×œ×’×” ×‘××§×•×. ×›××” ×“×§×•×ª ×œ××—×¨ ××›×Ÿ, ×‘×©×¢×” 23:13 ××• ×‘×¡××•×š ×œ×›×š, ×™×¦× ×”× ××©× 1 ××‘×™×ª×•, × ×™×’×© ×œ××•×œ×’×” ×•××¡×¨ ×œ×” ×¡× ×...\n",
      "âœ… True Label: ×›×Ÿ\n",
      "ğŸ§  Predicted: ×œ×\n",
      "ğŸ§® Score Diff: -6.8922 | Yes Score: 0.3238 | No Score: 7.2160\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #236\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× 2 ×”×•×“×” ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× ××ª×•×§×Ÿ, ×©×’×•×‘×© ×‘××¡×’×¨×ª ×”×¡×“×¨ ×“×™×•× ×™ ×œ××—×¨ ×©× ×©××¢×• ××¡×¤×¨ ×¢×“×™ ×ª×‘×™×¢×”, ×•×”×•×¨×©×¢ ×‘×¢×‘×™×¨×•×ª ×©×œ ×§×©×™×¨×ª ×§×©×¨ ×œ×¤×©×¢ ×œ×¤×™ ×¡×¢×™×£ 499 (×)(1) ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×”×ª×©×œ\"×– â€“ 1977 ×•×™×™×‘×•× ×¡× ××¡×•×›×Ÿ ×œ×¤×™ ×¡×¢×™×£ 13 ×™×—×“ ×¢× ×¡×¢×™×£ 19× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×ª×©×œ\"×’- 1973. ××›×ª×‘ ×”××™×©×•× ×”××ª×•×§×Ÿ ×¢×•×œ×”, ×›×™ ×¢×•×‘×¨ ×œ×™×•× 19.10.17 ××• ×‘×¡××•×š ×œ×›×š, ×§×©×¨ ×”× ××©× ×§×©×¨ ×¢× ××—×¨ ×œ×™×™×‘× ××¨×¦×” ×¡××™× ××¡×•×›× ×™×. ×‘×”×ª×× ×œ×§×©×¨ ×”×–××™×Ÿ ×”××—×¨ ×‘×™×•× 19.10.17 ×›×¨×˜×™×¡ ×˜×™×¡×” ×œ×‘×¨×™×¡×œ ×¢×‘×•×¨ ×”× ××©×. 3 ×™××™× ×œ××—×¨ ××›×Ÿ, ×‘×™×•× 22.10.17, ×˜×¡ ×”× ××©× ××™×©×¨××œ ×œ×‘×¨×™×¡×œ ×“×¨×š ×¤×¨× ×§×¤×•×¨×˜, ×¢×œ ×× ×ª ×œ×™×™×‘× ×œ×™×©×¨××œ ×¡××™×. ×‘×¡××•×š ×œ××—×¨ ×”×’×¢×ª×• ×œ×‘×¨×™×¡×œ, ×‘××•×¢×“ ×©××™× ×• ×™×“×•×¢ ×‘××“×•×™×§, ×§×™×‘×œ ×”× ××©× ×××“×, ×©×–×”×•×ª×• ××™× ×” ×™×“×•×¢×”, ×¡××™× ×›×©×”× ××—×•×œ×§×™× ×œ- 9 ××¨×™×–×•×ª ×•××•×¡×œ×§...\n",
      "âœ… True Label: ×›×Ÿ\n",
      "ğŸ§  Predicted: ×›×Ÿ\n",
      "ğŸ§® Score Diff: -6.3554 | Yes Score: 0.6838 | No Score: 7.0393\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #50\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×‘×™×•× 11.6.17 ×”×•×“×” ×”× ××©× ×‘×›×ª×‘ ×”××™×©×•× ×•×”×•×¨×©×¢ ×‘×¢×‘×™×¨×” ×©×œ ×”×—×–×§×ª ×¡× ××¡×•×›×Ÿ ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª ×œ×¤×™ ×¡×¢×™×£ 7 (×) ×‘×¦×™×¨×•×£ ×¡×¢×™×£ 7 (×’) ×¨×™×©× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™×. ×‘×™×•× 21.1.16 ×‘×©×¢×” 01:23, ×‘×§×™×•×¡×§ ×‘×• ×¢×‘×“ ×‘×ª×œ-××‘×™×‘, ×”×—×–×™×§ ×”× ××©× ×©×œ× ×œ×¦×¨×™×›×ª×• ×”×¢×¦××™×ª 101 ×™×—×™×“×•×ª ×¦'××™× ×§×” ×•-100 ×™×—×™×“×•×ª ×¤×œ××§×”. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘': ×‘×™×•× 2.6.19 ×”×•×“×” ×”× ××©× ×•×”×•×¨×©×¢ (×”×¨×©×¢×ª×• ×ª×•×§× ×” ×‘×™×•× 5.1.20), ×‘××¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ ×©×œ× ×›×œ×œ ×”×¡×›××” ×¢×•× ×©×™×ª, ×‘×¢×‘×™×¨×” ××—×ª ×©×œ × ×™×¡×™×•×Ÿ ×œ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ, ×•×‘×¢×•×“ ×©×© ×¢×©×¨×” ×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡× ××¡×•×›×Ÿ. ×‘×—×œ×§ ××”××§×¨×™× ××“×•×‘×¨ ×”×™×” ×‘×¡× ××¡×•×’ ×—×©×™×© ×•×‘××—×¨×™× ×‘×¡× ××¡×•×’ ×§× ××‘×™×¡, ×”× ××©× ×¡×—×¨ ××• × ×™×¡×” ×œ×¡×—×•×¨ ×‘×¡××™× ××œ×” ×‘×›××•×™×•×ª ×©× ×¢×• ×‘×™×Ÿ 1 ×’×¨' ×œ â€“ 7 ×’×¨'. ×›×œ ×”×¢×‘×™×¨×•×ª ×”×ª×¨×—×©×• ×‘×™×Ÿ ×”×—×•×“×©×™× ×™×•×œ×™ â€“ × ×•×‘××‘×¨ 2018. ×¢×œ...\n",
      "âœ… True Label: ×œ×\n",
      "ğŸ§  Predicted: ×œ×\n",
      "ğŸ§® Score Diff: -6.8709 | Yes Score: 0.6103 | No Score: 7.4812\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #858\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©×, × ×ª× ××œ ×”×¨×©×§×•×‘×™×¥', (×œ×”×œ×Ÿ: ×”× ××©×) ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× ××ª×•×§×Ÿ ×”××™×™×—×¡ ×œ×• ×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª ×¡××™× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª ×¢×‘×™×¨×” ×œ×¤×™ ×¡×¢×™×£ 7 (×) ×•- 7 (×’) ×¨×™×©× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× (× ×•×¡×— ×—×“×©) ×ª×©×œ\"×’ â€“ 1973 (×œ×”×œ×Ÿ: ×¤×§×•×“×ª ×”×¡××™×) ×•×›×Ÿ ×¢×‘×™×¨×•×ª ×©×œ ×™×¦×•×, ×™×‘×•×, ××¡×—×¨, ×”×¡×¤×§×” ×¡××™× ××¡×•×›× ×™× â€“ ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×•- 19× ×œ×¤×§×•×“×ª ×”×¡××™× (3 ×¢×‘×™×¨×•×ª). ×¢×œ ×¤×™ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×”××ª×•×§×Ÿ, ×‘××•×¢×“×™× ×”×¨×œ×•×•× ×˜×™×™× ×œ×›×ª×‘ ×”××™×©×•×, ×”× ××©× (×‘×Ÿ 21) ×”×—×œ ×œ×¢×¡×•×§ ×‘×¡×—×¨ ×‘×¡××™× ×¢×‘×•×¨ ×‘×¦×¢ ×›×¡×£. ××ª ×”×¡××™× ×”××¡×•×›× ×™× ×”×™×” ××•×›×¨ ×”× ××©× ×‘×¤××¨×§×™× ×¦×™×‘×•×¨×™×™×, ××• ××’×¨×©×™ ×¡×¤×•×¨×˜ ×‘×¨×—×‘×™ ×”×¢×™×¨ ××™×œ×ª ××©×¨ ×©× ×”×™×• ××‘×œ×™× ×§×˜×™× ×™×, ××©×¨ ×”×™×•×•×™ ×¢×‘×•×¨ ×”× ××©× ×§×œ×™×™× ×˜×™× ×¤×•×˜× ×¦×™××œ×™×™×. ××’×¨×©×™ ×”×¡×¤×•×¨×˜ ×›×•× ×• ×‘×¤×™ ×”× ××©× ×•×‘×¤×™ ×§×˜×™× ×™× ××—×¨×™× ×‘×©× \"×”××¡×¤×œ×˜\" (...\n",
      "âœ… True Label: ×›×Ÿ\n",
      "ğŸ§  Predicted: ×œ×\n",
      "ğŸ§® Score Diff: -6.8125 | Yes Score: 0.5675 | No Score: 7.3800\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #779\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× ×”×•×¨×©×¢ ×¢×œ-×¤×™ ×”×•×“××ª×• ×‘××¡×’×¨×ª ×”×¡×“×¨ ×˜×™×¢×•×Ÿ ×‘×›×ª×‘ ××™×©×•× ××ª×•×§×Ÿ (×œ×”×œ×Ÿ: \"×›×ª×‘ ×”××™×©×•×\") ×‘×¢×‘×™×¨×” ×©×œ ×”×—×–×§×ª ×¡× ××¡×•×›×Ÿ ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª, ×¢×‘×™×¨×” ×œ×¤×™ ×¡×¢×™×£ 7(×)+(×’) ×¨×™×©× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×”×ª×©×œ\"×’ â€“ 1973 (×œ×”×œ×Ÿ: \"×¤×§×•×“×ª ×”×¡××™×\"). ×‘××¡×’×¨×ª ×”×”×¡×“×¨ ×œ× ×”×•×©×’×” ×”×¡×›××” ×¢×•× ×©×™×ª ×‘×™×Ÿ ×”×¦×“×“×™× ×•××œ×• ×˜×¢× ×• ×œ×¢×•× ×© ×‘××•×¤×Ÿ ×—×•×¤×©×™. ×¢× ×–××ª, ×”×¦×“×“×™× ×”×’×™×¢×• ×œ×”×‘× ×” ×‘×™×—×¡ ×œ×‘×§×©×ª ×—×™×œ×•×˜ ×©×œ ×¨×›×‘ ×©×”×’×™×©×” ×”×××©×™××” ×•×œ×¤×™×” ×–×” ×™×©×•×—×¨×¨ ×‘×›×¤×•×£ ×œ×”×¤×§×“×ª ×¡×š ×©×œ 30,000 â‚ª ×œ×˜×•×‘×ª ×§×¨×Ÿ ×”×—×™×œ×•×˜ ×©×œ ×”×¡××™× ×¢×•×‘×¨ ×œ××ª×Ÿ ×’×–×¨ ×”×“×™×Ÿ. ××¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×¢×•×œ×” ×›×™ ×‘×™×•× 01.05.2022 ×¡××•×š ×œ×©×¢×” 13:30 ×”×—×–×™×§ ×”× ××©× ×¢×‘×•×¨ ××—×¨ ×©×–×”×•×ª×• ××™× ×” ×™×“×•×¢×” ×œ×××©×™××” ×§×™×œ×•×’×¨× ×©×œ ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§×•×§××™×Ÿ, ××©×¨ ×”×•×¡×œ×§ ×‘×ª× ××˜×¢×Ÿ ×‘×¨×›×‘ ××¡×•×’ \"×˜×•×™×•×˜×” ×§×•×¨×•×œ×”\" ×©×‘×‘×¢×œ×•×ª×• (×œ×”×œ...\n",
      "âœ… True Label: ×›×Ÿ\n",
      "ğŸ§  Predicted: ×›×Ÿ\n",
      "ğŸ§® Score Diff: -6.0273 | Yes Score: 0.6312 | No Score: 6.6585\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #799\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×• ×‘×¢×‘×™×¨×” ×©×œ ×¡×™×•×¢ ×œ×¡×—×¨ ×‘×¡××™× ××¡×•×›× ×™×, ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×• â€“ 19× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× (× ×•×¡×— ×—×“×©), ×ª×©×œ\"×’ â€“ 1973 (×¤×§×•×“×ª ×”×¡××™×) + ×¡×¢×™×£ 31 ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ, ×ª×©×œ\"×– â€“ 1977 (×—×•×§ ×”×¢×•× ×©×™×Ÿ) ×•×‘×¢×‘×™×¨×” ×©×œ ×¡×™×•×¢ ×œ×”×—×–×§×ª ×¡××™× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª, ×œ×¤×™ ×¡×¢×™×¤×™× 7(×) ×•7(×’) ×œ×¤×§×•×“×ª ×”×¡××™× + ×¡×¢×™×£ 31 ×œ×—×•×§ ×”×¢×•× ×©×™×Ÿ. ×¢×œ ×¤×™ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×, ×‘×™×•× 21.4.2020, ×§×™×‘×œ ×—× × ××œ ×©××•××œ ×–×’×•×¨×™ (×—× × ××œ) ×œ×™×“×™×• 371.17 ×’×¨× ×§× ×‘×•×¡ ××—×•×œ×§ ×œ×©×§×™×•×ª. ×—× × ××œ ×‘×™×§×© ××”× ××©×, ×›×™ ×™×©××© ×¢×‘×•×¨×• × ×”×’ ×‘××”×œ×š ×‘×™×¦×•×¢ ×”×¡×—×¨ ×‘×¡×, ×‘×××¦×¢×•×ª ×¨×›×‘ ××•×ª×• ×©××œ ×××—×¨. ×—× × ××œ ×”× ×™×— ××ª ×”×¡× ××ª×—×ª ×œ×¡×“×™×Ÿ ×‘××•×©×‘ ×”××—×•×¨×™ ×‘×¨×›×‘ ×•×™×—×“ ×¢× ×”× ××©× ×”×’×™×¢×• ×‘×¨×›×‘ ×œ×¨×—×•×‘ ×¨×™×© ×œ×§×™×© ×‘×™×¨×•×©×œ×™× (×”××§×•×). ×–××Ÿ ×§×¦×¨ ×§×•×“× ×œ×›×Ÿ ×‘×™×§×© ×ª\"×— ×œ×¨×›×•×© ×§× ×‘×•×¡ ×‘×××¦×¢×•×ª ×™×™×©×•××•×Ÿ...\n",
      "âœ… True Label: ×œ×\n",
      "ğŸ§  Predicted: ×œ×\n",
      "ğŸ§® Score Diff: -6.7578 | Yes Score: 0.7015 | No Score: 7.4593\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #777\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× ×”×•×“×” ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× ××ª×•×§×Ÿ ×©×”×•×’×© ×‘××¡×’×¨×ª ×”×¡×“×¨ ×“×™×•× ×™ ×©×œ× ×›×œ×œ ×”×¡×›××” ×œ×¢×•× ×©, ×•×”×•×¨×©×¢ ×‘×¢×‘×™×¨×” ×©×œ ×¡×—×¨ ×‘×¡××™×, ×œ×¤×™ ×¡×¢×™×¤×™× 13 ×•- 19× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×”×ª×©×œ\"×’ â€“ 1973, ×‘×›×š ×©××›×¨ ×œ×¡×•×›× ×ª ××©×˜×¨×ª×™×ª ×‘×™×•× 25.12.2018, ×‘×××¦×¢×•×ª ×™×™×©×•××•×Ÿ \"×˜×œ×’×¨×\", ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§×•×§××™×Ÿ ×‘××©×§×œ 0.8898 ×’×¨× × ×˜×•, ×‘×ª××•×¨×” ×œ×ª×©×œ×•× ×‘×¡×š 800 â‚ª. ×‘×¤×•×¢×œ, ×‘×™×•× 25.12.2018, ×‘×××¦×¢×•×ª ×™×™×©×•××•×Ÿ \"×˜×œ×’×¨×\", ××›×¨ ×”× ××©× ×œ×¡×•×›× ×ª ××©×˜×¨×ª×™×ª ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§×•×§××™×Ÿ ×‘××©×§×œ 0.8898 ×’×¨× × ×˜×•, ×ª××•×¨×ª ×ª×©×œ×•× ×‘×¡×š 800 â‚ª. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘': ×ª\"×¤ 20654-11-17, ×‘×• ×”×•×¨×©×¢ ×”× ××©× ×‘×¢×‘×™×¨×•×ª ×©×œ ×¡×—×¨ ×‘×¡××™× (×©×ª×™ ×¢×‘×™×¨×•×ª), ×œ×¤×™ ×¡×¢×™×£ 13 + 19× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× (× ×•×¡×— ×—×“×©) ×ª×©×œ\"×’-1973 ×•×‘×¢×‘×™×¨×•×ª ×©×œ ×”×—×–×§×ª ×¡××™× ×©×œ× ×œ×©×™××•×© ×¢×¦××™ (...\n",
      "âœ… True Label: ×œ×\n",
      "ğŸ§  Predicted: ×›×Ÿ\n",
      "ğŸ§® Score Diff: -6.1172 | Yes Score: 0.5684 | No Score: 6.6856\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ Input #285\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ”¹ Decoded Input:\n",
      "××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×” ×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×” ××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•× ×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×) ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×': ×”× ××©× ×”×•×¨×©×¢ ×¢×œ ×¤×™ ×”×•×“××ª×•, ×‘××¡×’×¨×ª ×”×¡×“×¨ ×“×™×•× ×™, ×‘×¢×‘×™×¨×•×ª ×©×œ ×’×™×“×•×œ, ×™×™×¦×•×¨ ×”×›× ×ª ×¡××™× ××¡×•×›× ×™×, ×œ×¤×™ ×¡×¢×™×£ 6 ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™× [× ×•×¡×— ×—×“×©], ×”×ª×©×œ\"×’-1973 (×œ×”×œ×Ÿ: \"×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™×\"), ×•×”×—×–×§×ª ×›×œ×™× ×œ×”×›× ×ª ×¡× ×©×œ× ×œ×¦×¨×™×›×” ×¢×¦××™×ª, ×œ×¤×™ ×¡×¢×™×£ 10 ×¨×™×©× ×œ×¤×§×•×“×ª ×”×¡××™× ×”××¡×•×›× ×™×. ×œ×¤×™ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×, ××¡×¤×¨ ×—×•×“×©×™× ×¢×•×‘×¨ ×œ×™×•× 14.11.2022, ×‘×ª××¨×™×š ×•×‘×©×¢×” ×©××™× × ×™×“×•×¢×™× ×‘××“×•×™×™×§ ×œ×××©×™××”, ×¨×›×© ×”× ××©× ×›×œ×™× ×œ×©× ×’×™×“×•×œ ×¡××™× ××¡×•×›× ×™× ×›××¤×•×¨×˜: 16 ×× ×•×¨×•×ª ×—×™××•×, 3 ××–×’× ×™×, 2 ×××•×•×¨×¨×™× ×•-3 ×××•×•×¨×¨×™ ××•×•×™×¨. ×‘×”××©×š ×œ××•×ª×Ÿ ×”× ×¡×™×‘×•×ª, ×‘×™×•× 14.11.2022, ×’×™×“×œ ×”× ××©× ×‘×—×“×¨×™× ×©×•× ×™× ×‘×‘×™×ª ××•×ª×• ×©×›×¨ ×‘×¨××©×•×Ÿ ×œ×¦×™×•×Ÿ (×œ×”×œ×Ÿ: \"×”×‘×™×ª\") 176 ×©×ª×™×œ×™× ×©×œ ×¡× ××¡×•×›×Ÿ ××¡×•×’ ×§×× ×‘×•×¡, ×‘××©×§×œ ×›×•×œ×œ ×©×œ 10.6 ×§\"×’. ×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘'...\n",
      "âœ… True Label: ×›×Ÿ\n",
      "ğŸ§  Predicted: ×›×Ÿ\n",
      "ğŸ§® Score Diff: -5.2470 | Yes Score: 0.5642 | No Score: 5.8112\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def show_sample_predictions(dataset, predictions, scores, tokenizer, num_samples=10):\n",
    "    print(f\"\\nğŸ” Showing {num_samples} Sample Predictions:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    indices = np.random.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        input_ids = dataset[idx][\"input_ids\"]\n",
    "        decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        label = dataset[idx][\"numeric_label\"]\n",
    "        prediction = predictions[idx]\n",
    "        score = scores[idx]\n",
    "        \n",
    "        print(f\"\\nğŸ“„ Input #{idx}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"ğŸ”¹ Decoded Input:\\n{decoded_input[:1000]}...\")  # Truncate long text\n",
    "        print(f\"âœ… True Label: {'×›×Ÿ' if label == 1 else '×œ×'}\")\n",
    "        print(f\"ğŸ§  Predicted: {'×›×Ÿ' if prediction == 1 else '×œ×'}\")\n",
    "        print(f\"ğŸ§® Score Diff: {score['score_diff']:.4f} | Yes Score: {score['yes_score']:.4f} | No Score: {score['no_score']:.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "show_sample_predictions(test_dataset, test_metrics[\"predictions\"], test_metrics[\"scores\"], tokenizer, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and combining datasets for k-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for k-fold CV: 5791\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ STARTING K-FOLD EVALUATION FOR: m5-mlm-final\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal Dataset created: 5791 samples\n",
      "Label distribution: [3857 1934]\n",
      "Sample input length: 2083 characters\n",
      "\n",
      "ğŸ“ FOLD 1/5\n",
      "Train samples: 4632, Test samples: 1159\n",
      "Loading fresh model for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/7:   0%|          | 0/927 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Fold 1, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.06it/s, loss=0.4440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2493 (F1: 0.5103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5103\n",
      "Precision: 0.3815\n",
      "Recall: 0.7705\n",
      "Accuracy: 0.5135\n",
      "\n",
      "Prediction Distribution: [311 616]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.5790\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.39      0.52       622\n",
      "           1       0.38      0.77      0.51       305\n",
      "\n",
      "    accuracy                           0.51       927\n",
      "   macro avg       0.58      0.58      0.51       927\n",
      "weighted avg       0.65      0.51      0.51       927\n",
      "\n",
      "Fold 1, Epoch 1: Val F1 = 0.5103, Best = 0.5103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:51<00:00,  4.00it/s, loss=0.2527]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.8732 (F1: 0.5514)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5514\n",
      "Precision: 0.4462\n",
      "Recall: 0.7213\n",
      "Accuracy: 0.6138\n",
      "\n",
      "Prediction Distribution: [434 493]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.6412\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.56      0.66       622\n",
      "           1       0.45      0.72      0.55       305\n",
      "\n",
      "    accuracy                           0.61       927\n",
      "   macro avg       0.63      0.64      0.61       927\n",
      "weighted avg       0.69      0.61      0.62       927\n",
      "\n",
      "Fold 1, Epoch 2: Val F1 = 0.5514, Best = 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:51<00:00,  4.00it/s, loss=0.2994]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.5635 (F1: 0.6349)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6349\n",
      "Precision: 0.6154\n",
      "Recall: 0.6557\n",
      "Accuracy: 0.7519\n",
      "\n",
      "Prediction Distribution: [602 325]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7274\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       622\n",
      "           1       0.62      0.66      0.63       305\n",
      "\n",
      "    accuracy                           0.75       927\n",
      "   macro avg       0.72      0.73      0.72       927\n",
      "weighted avg       0.76      0.75      0.75       927\n",
      "\n",
      "Fold 1, Epoch 3: Val F1 = 0.6349, Best = 0.6349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:51<00:00,  4.00it/s, loss=0.0180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.9943 (F1: 0.6707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6707\n",
      "Precision: 0.6268\n",
      "Recall: 0.7213\n",
      "Accuracy: 0.7670\n",
      "\n",
      "Prediction Distribution: [576 351]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7554\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       622\n",
      "           1       0.63      0.72      0.67       305\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.74      0.76      0.75       927\n",
      "weighted avg       0.78      0.77      0.77       927\n",
      "\n",
      "Fold 1, Epoch 4: Val F1 = 0.6707, Best = 0.6707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:51<00:00,  4.00it/s, loss=0.0045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.2699 (F1: 0.6964)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6964\n",
      "Precision: 0.7647\n",
      "Recall: 0.6393\n",
      "Accuracy: 0.8166\n",
      "\n",
      "Prediction Distribution: [672 255]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7714\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87       622\n",
      "           1       0.76      0.64      0.70       305\n",
      "\n",
      "    accuracy                           0.82       927\n",
      "   macro avg       0.80      0.77      0.78       927\n",
      "weighted avg       0.81      0.82      0.81       927\n",
      "\n",
      "Fold 1, Epoch 5: Val F1 = 0.6964, Best = 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:50<00:00,  4.02it/s, loss=0.0031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -7.6882 (F1: 0.7072)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7072\n",
      "Precision: 0.6736\n",
      "Recall: 0.7443\n",
      "Accuracy: 0.7972\n",
      "\n",
      "Prediction Distribution: [590 337]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7837\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       622\n",
      "           1       0.67      0.74      0.71       305\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.78      0.78       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 1, Epoch 6: Val F1 = 0.7072, Best = 0.7072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.04it/s, loss=0.0451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.0102 (F1: 0.7141)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7141\n",
      "Precision: 0.6307\n",
      "Recall: 0.8230\n",
      "Accuracy: 0.7832\n",
      "\n",
      "Prediction Distribution: [529 398]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7933\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.83       622\n",
      "           1       0.63      0.82      0.71       305\n",
      "\n",
      "    accuracy                           0.78       927\n",
      "   macro avg       0.76      0.79      0.77       927\n",
      "weighted avg       0.81      0.78      0.79       927\n",
      "\n",
      "Fold 1, Epoch 7: Val F1 = 0.7141, Best = 0.7141\n",
      "Using fixed threshold: -6.010154292291524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:22<00:00, 12.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6981\n",
      "Precision: 0.6069\n",
      "Recall: 0.8217\n",
      "Accuracy: 0.7627\n",
      "\n",
      "Prediction Distribution: [635 524]\n",
      "True Label Distribution: [772 387]\n",
      "AUC-ROC: 0.7774\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.73      0.80       772\n",
      "           1       0.61      0.82      0.70       387\n",
      "\n",
      "    accuracy                           0.76      1159\n",
      "   macro avg       0.75      0.78      0.75      1159\n",
      "weighted avg       0.80      0.76      0.77      1159\n",
      "\n",
      "âœ… Fold 1 Results: F1 = 0.6981, Accuracy = 0.7627\n",
      "\n",
      "ğŸ“ FOLD 2/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.04it/s, loss=0.3480]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.6131 (F1: 0.5033)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5033\n",
      "Precision: 0.3548\n",
      "Recall: 0.8660\n",
      "Accuracy: 0.4358\n",
      "\n",
      "Prediction Distribution: [180 747]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.5449\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.22      0.35       621\n",
      "           1       0.35      0.87      0.50       306\n",
      "\n",
      "    accuracy                           0.44       927\n",
      "   macro avg       0.56      0.54      0.43       927\n",
      "weighted avg       0.63      0.44      0.40       927\n",
      "\n",
      "Fold 2, Epoch 1: Val F1 = 0.5033, Best = 0.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.05it/s, loss=0.2061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.7372 (F1: 0.5259)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5259\n",
      "Precision: 0.3964\n",
      "Recall: 0.7810\n",
      "Accuracy: 0.5351\n",
      "\n",
      "Prediction Distribution: [324 603]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.5974\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.41      0.54       621\n",
      "           1       0.40      0.78      0.53       306\n",
      "\n",
      "    accuracy                           0.54       927\n",
      "   macro avg       0.59      0.60      0.53       927\n",
      "weighted avg       0.66      0.54      0.54       927\n",
      "\n",
      "Fold 2, Epoch 2: Val F1 = 0.5259, Best = 0.5259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.06it/s, loss=0.1631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.8495 (F1: 0.5811)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5811\n",
      "Precision: 0.4817\n",
      "Recall: 0.7320\n",
      "Accuracy: 0.6516\n",
      "\n",
      "Prediction Distribution: [462 465]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.6720\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.61      0.70       621\n",
      "           1       0.48      0.73      0.58       306\n",
      "\n",
      "    accuracy                           0.65       927\n",
      "   macro avg       0.65      0.67      0.64       927\n",
      "weighted avg       0.71      0.65      0.66       927\n",
      "\n",
      "Fold 2, Epoch 3: Val F1 = 0.5811, Best = 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:50<00:00,  4.03it/s, loss=0.4540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.8180 (F1: 0.6510)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6510\n",
      "Precision: 0.6690\n",
      "Recall: 0.6340\n",
      "Accuracy: 0.7756\n",
      "\n",
      "Prediction Distribution: [637 290]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7397\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.83       621\n",
      "           1       0.67      0.63      0.65       306\n",
      "\n",
      "    accuracy                           0.78       927\n",
      "   macro avg       0.75      0.74      0.74       927\n",
      "weighted avg       0.77      0.78      0.77       927\n",
      "\n",
      "Fold 2, Epoch 4: Val F1 = 0.6510, Best = 0.6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:47<00:00,  4.07it/s, loss=0.1693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.4289 (F1: 0.6919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6919\n",
      "Precision: 0.6697\n",
      "Recall: 0.7157\n",
      "Accuracy: 0.7896\n",
      "\n",
      "Prediction Distribution: [600 327]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7709\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.83      0.84       621\n",
      "           1       0.67      0.72      0.69       306\n",
      "\n",
      "    accuracy                           0.79       927\n",
      "   macro avg       0.76      0.77      0.77       927\n",
      "weighted avg       0.79      0.79      0.79       927\n",
      "\n",
      "Fold 2, Epoch 5: Val F1 = 0.6919, Best = 0.6919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:50<00:00,  4.03it/s, loss=0.7968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.1773 (F1: 0.6978)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6978\n",
      "Precision: 0.7133\n",
      "Recall: 0.6830\n",
      "Accuracy: 0.8047\n",
      "\n",
      "Prediction Distribution: [634 293]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7739\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86       621\n",
      "           1       0.71      0.68      0.70       306\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.78      0.77      0.78       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 2, Epoch 6: Val F1 = 0.6978, Best = 0.6978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.05it/s, loss=0.0263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.6320 (F1: 0.7068)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7068\n",
      "Precision: 0.6696\n",
      "Recall: 0.7484\n",
      "Accuracy: 0.7950\n",
      "\n",
      "Prediction Distribution: [585 342]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7832\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84       621\n",
      "           1       0.67      0.75      0.71       306\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.78      0.77       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 2, Epoch 7: Val F1 = 0.7068, Best = 0.7068\n",
      "Using fixed threshold: -6.631978000913348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:23<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7170\n",
      "Precision: 0.6566\n",
      "Recall: 0.7896\n",
      "Accuracy: 0.7927\n",
      "\n",
      "Prediction Distribution: [695 463]\n",
      "True Label Distribution: [773 385]\n",
      "AUC-ROC: 0.7920\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.84       773\n",
      "           1       0.66      0.79      0.72       385\n",
      "\n",
      "    accuracy                           0.79      1158\n",
      "   macro avg       0.77      0.79      0.78      1158\n",
      "weighted avg       0.81      0.79      0.80      1158\n",
      "\n",
      "âœ… Fold 2 Results: F1 = 0.7170, Accuracy = 0.7927\n",
      "\n",
      "ğŸ“ FOLD 3/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.03it/s, loss=0.3951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.8456 (F1: 0.5148)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5148\n",
      "Precision: 0.3639\n",
      "Recall: 0.8795\n",
      "Accuracy: 0.4509\n",
      "\n",
      "Prediction Distribution: [185 742]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.5591\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.24      0.37       620\n",
      "           1       0.36      0.88      0.51       307\n",
      "\n",
      "    accuracy                           0.45       927\n",
      "   macro avg       0.58      0.56      0.44       927\n",
      "weighted avg       0.66      0.45      0.42       927\n",
      "\n",
      "Fold 3, Epoch 1: Val F1 = 0.5148, Best = 0.5148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.06it/s, loss=0.3832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.7337 (F1: 0.5392)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5392\n",
      "Precision: 0.3920\n",
      "Recall: 0.8632\n",
      "Accuracy: 0.5113\n",
      "\n",
      "Prediction Distribution: [251 676]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.6001\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.34      0.48       620\n",
      "           1       0.39      0.86      0.54       307\n",
      "\n",
      "    accuracy                           0.51       927\n",
      "   macro avg       0.61      0.60      0.51       927\n",
      "weighted avg       0.69      0.51      0.50       927\n",
      "\n",
      "Fold 3, Epoch 2: Val F1 = 0.5392, Best = 0.5392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.04it/s, loss=0.1313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.5333 (F1: 0.5532)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5532\n",
      "Precision: 0.4600\n",
      "Recall: 0.6938\n",
      "Accuracy: 0.6289\n",
      "\n",
      "Prediction Distribution: [464 463]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.6453\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.60      0.68       620\n",
      "           1       0.46      0.69      0.55       307\n",
      "\n",
      "    accuracy                           0.63       927\n",
      "   macro avg       0.63      0.65      0.62       927\n",
      "weighted avg       0.69      0.63      0.64       927\n",
      "\n",
      "Fold 3, Epoch 3: Val F1 = 0.5532, Best = 0.5532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.06it/s, loss=0.0893]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.0634 (F1: 0.6291)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6291\n",
      "Precision: 0.6054\n",
      "Recall: 0.6547\n",
      "Accuracy: 0.7443\n",
      "\n",
      "Prediction Distribution: [595 332]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7217\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       620\n",
      "           1       0.61      0.65      0.63       307\n",
      "\n",
      "    accuracy                           0.74       927\n",
      "   macro avg       0.71      0.72      0.72       927\n",
      "weighted avg       0.75      0.74      0.75       927\n",
      "\n",
      "Fold 3, Epoch 4: Val F1 = 0.6291, Best = 0.6291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.03it/s, loss=0.2186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.3259 (F1: 0.6747)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6747\n",
      "Precision: 0.6299\n",
      "Recall: 0.7264\n",
      "Accuracy: 0.7681\n",
      "\n",
      "Prediction Distribution: [573 354]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7575\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82       620\n",
      "           1       0.63      0.73      0.67       307\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.74      0.76      0.75       927\n",
      "weighted avg       0.78      0.77      0.77       927\n",
      "\n",
      "Fold 3, Epoch 5: Val F1 = 0.6747, Best = 0.6747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.04it/s, loss=0.0262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.9222 (F1: 0.7014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7014\n",
      "Precision: 0.6810\n",
      "Recall: 0.7231\n",
      "Accuracy: 0.7961\n",
      "\n",
      "Prediction Distribution: [601 326]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7777\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.85       620\n",
      "           1       0.68      0.72      0.70       307\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.78      0.77       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 3, Epoch 6: Val F1 = 0.7014, Best = 0.7014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.05it/s, loss=0.0519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.9139 (F1: 0.7071)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7071\n",
      "Precision: 0.6477\n",
      "Recall: 0.7785\n",
      "Accuracy: 0.7864\n",
      "\n",
      "Prediction Distribution: [558 369]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7844\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83       620\n",
      "           1       0.65      0.78      0.71       307\n",
      "\n",
      "    accuracy                           0.79       927\n",
      "   macro avg       0.76      0.78      0.77       927\n",
      "weighted avg       0.80      0.79      0.79       927\n",
      "\n",
      "Fold 3, Epoch 7: Val F1 = 0.7071, Best = 0.7071\n",
      "Using fixed threshold: -6.91388987521736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:22<00:00, 12.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6969\n",
      "Precision: 0.6450\n",
      "Recall: 0.7580\n",
      "Accuracy: 0.7694\n",
      "\n",
      "Prediction Distribution: [682 476]\n",
      "True Label Distribution: [753 405]\n",
      "AUC-ROC: 0.7668\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.81       753\n",
      "           1       0.64      0.76      0.70       405\n",
      "\n",
      "    accuracy                           0.77      1158\n",
      "   macro avg       0.75      0.77      0.76      1158\n",
      "weighted avg       0.78      0.77      0.77      1158\n",
      "\n",
      "âœ… Fold 3 Results: F1 = 0.6969, Accuracy = 0.7694\n",
      "\n",
      "ğŸ“ FOLD 4/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.05it/s, loss=0.0610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.3705 (F1: 0.5292)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5292\n",
      "Precision: 0.3978\n",
      "Recall: 0.7905\n",
      "Accuracy: 0.5221\n",
      "\n",
      "Prediction Distribution: [301 626]\n",
      "True Label Distribution: [612 315]\n",
      "AUC-ROC: 0.5872\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.38      0.51       612\n",
      "           1       0.40      0.79      0.53       315\n",
      "\n",
      "    accuracy                           0.52       927\n",
      "   macro avg       0.59      0.59      0.52       927\n",
      "weighted avg       0.65      0.52      0.52       927\n",
      "\n",
      "Fold 4, Epoch 1: Val F1 = 0.5292, Best = 0.5292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:48<00:00,  4.05it/s, loss=0.3636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.7057 (F1: 0.5443)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5443\n",
      "Precision: 0.4150\n",
      "Recall: 0.7905\n",
      "Accuracy: 0.5502\n",
      "\n",
      "Prediction Distribution: [327 600]\n",
      "True Label Distribution: [612 315]\n",
      "AUC-ROC: 0.6085\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.43      0.56       612\n",
      "           1       0.41      0.79      0.54       315\n",
      "\n",
      "    accuracy                           0.55       927\n",
      "   macro avg       0.61      0.61      0.55       927\n",
      "weighted avg       0.67      0.55      0.55       927\n",
      "\n",
      "Fold 4, Epoch 2: Val F1 = 0.5443, Best = 0.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.04it/s, loss=0.3350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.8132 (F1: 0.5680)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5680\n",
      "Precision: 0.4551\n",
      "Recall: 0.7556\n",
      "Accuracy: 0.6095\n",
      "\n",
      "Prediction Distribution: [404 523]\n",
      "True Label Distribution: [612 315]\n",
      "AUC-ROC: 0.6449\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.53      0.64       612\n",
      "           1       0.46      0.76      0.57       315\n",
      "\n",
      "    accuracy                           0.61       927\n",
      "   macro avg       0.63      0.64      0.61       927\n",
      "weighted avg       0.69      0.61      0.62       927\n",
      "\n",
      "Fold 4, Epoch 3: Val F1 = 0.5680, Best = 0.5680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:49<00:00,  4.05it/s, loss=0.4517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:18<00:00, 12.65it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "\n",
    "# ========================\n",
    "# ğŸ”§ CONFIG\n",
    "# ========================\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 1024\n",
    "epochs = 7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ¯ CORE: Improved Logits Classification\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    \"\"\"Classify using threshold-based method\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=input_ids.device)\n",
    "        decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        yes_tokens = [259, 1903]  # ×›×Ÿ\n",
    "        no_tokens = [1124]        # ×œ×\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx]\n",
    "            \n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            \n",
    "            score_diff = yes_score - no_score\n",
    "            \n",
    "            if score_diff > threshold:\n",
    "                prediction = 1\n",
    "                predicted_text = \"×›×Ÿ\"\n",
    "            else:\n",
    "                prediction = 0\n",
    "                predicted_text = \"×œ×\"\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            scores.append({\n",
    "                'prediction': prediction,\n",
    "                'predicted_text': predicted_text,\n",
    "                'score_diff': score_diff,\n",
    "                'yes_score': yes_score,\n",
    "                'no_score': no_score\n",
    "            })\n",
    "        \n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, device, true_labels):\n",
    "    \"\"\"Find optimal threshold for balanced predictions\"\"\"\n",
    "    print(\"ğŸ” Finding best threshold...\")\n",
    "    \n",
    "    all_score_diffs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Collecting scores\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            _, scores = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=0.0\n",
    "            )\n",
    "            \n",
    "            for score in scores:\n",
    "                all_score_diffs.append(score['score_diff'])\n",
    "    \n",
    "    # Test thresholds\n",
    "    thresholds = np.linspace(min(all_score_diffs), max(all_score_diffs), 50)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch_preds, _ = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "# ========================\n",
    "# ğŸ§  IMPROVED Dataset with Specific Legal Prompts\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        \"\"\"\n",
    "        Dataset with legally-specific prompts for sentencing citation prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiple versions of detailed legal prompts\n",
    "        prompt = \"\"\"××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×”\n",
    "×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×”\n",
    "××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×\n",
    "×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×)\n",
    "×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×.\n",
    "×©××œ×”: ×‘×”×ª×‘×¡×¡ ×¢×œ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×, ×”×× ×¦×¤×•×™ ×©×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘×˜×•×•×— ×”×¢× ×™×©×”?\n",
    "\"\"\"\n",
    "\n",
    "#                 \"english\": \"\"\"Specialized legal citation prediction system\n",
    "# Domain: Criminal law - sentencing policy\n",
    "# Purpose: Predict citations between verdicts based on indictment facts similarity\n",
    "# Criteria: Citation is relevant if it supports sentencing range decision (not procedures, definitions, or unrelated verdicts)\n",
    "# Verdict A will cite verdict B if there is similarity in offenses and circumstances presented in indictments.\n",
    "# Question: Based on indictment facts, will verdict A likely cite verdict B to support sentencing range?\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "        # Create more detailed inputs with legal context\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Format with detailed legal context\n",
    "            legal_input = f\"\"\"{prompt}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×”×¢×‘×™×¨×•×ª ×•×”× ×¡×™×‘×•×ª, ×”×× ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘××“×™× ×™×•×ª ×”×¢× ×™×©×”?\"\"\"\n",
    "            \n",
    "            self.inputs.append(legal_input)\n",
    "        \n",
    "        self.targets = df[\"label\"].apply(lambda l: \"×›×Ÿ\" if l == 1 else \"×œ×\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(f\"Legal Dataset created: {len(self.inputs)} samples\")\n",
    "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Sample input length: {len(self.inputs[0])} characters\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        \n",
    "        # Tokenize with longer sequences due to detailed prompt\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=5,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Evaluation Function\n",
    "# ========================\n",
    "def evaluate_legal_model(model, dataloader, tokenizer, device, use_threshold_tuning=True, fix_threshold=None):\n",
    "    \"\"\"Evaluate the legal citation model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect true labels\n",
    "    true_labels = []\n",
    "    for batch in dataloader:\n",
    "        true_labels.extend(batch[\"numeric_label\"].numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    # THREE OPTIONS NOW:\n",
    "    if fix_threshold is not None:\n",
    "        # âœ… USE FIXED THRESHOLD (no tuning)\n",
    "        print(f\"Using fixed threshold: {fix_threshold}\")\n",
    "        best_threshold = fix_threshold\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Fixed Threshold)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "                \n",
    "    elif use_threshold_tuning:\n",
    "        # âŒ TUNE THRESHOLD ON THIS DATASET (causes data leakage if used on test set)\n",
    "        best_threshold = find_best_threshold(model, tokenizer, dataloader, device, true_labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Tuned Threshold)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "    else:\n",
    "        # ğŸ”„ USE MODEL.GENERATE() (different approach)\n",
    "        best_threshold = None\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Generation)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                generated = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=5\n",
    "                )\n",
    "                decoded_preds = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                predictions = [1 if p.strip() == \"×›×Ÿ\" else 0 for p in decoded_preds]\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend([{} for _ in predictions])\n",
    "\n",
    "    predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    print(f\"\\nğŸ“Š LEGAL CITATION PREDICTION RESULTS:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Distribution: {np.bincount(predictions)}\")\n",
    "    print(f\"True Label Distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "    if len(np.unique(predictions)) > 1 and len(np.unique(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predictions)\n",
    "        print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    return f1, {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'threshold': best_threshold,\n",
    "        'scores': all_confidence_scores  # â† ×ª××™×“ ×§×™×™×\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“¥ Load Everything\n",
    "# ========================\n",
    "\n",
    "# model_paths = [\"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\",\"imvladikon/het5-base\"]\n",
    "\n",
    "# for model_path in model_paths:\n",
    "#     print(f\"Loading {model_path} and tokenizer...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#     print(\"Loading data...\")\n",
    "#     df_train = pd.read_csv(train_file)\n",
    "#     df_val = pd.read_csv(val_file)\n",
    "#     df_test = pd.read_csv(test_file)\n",
    "\n",
    "#     train_dataset = LegalSentencingCitationDataset(df_train, tokenizer, max_len=max_len)\n",
    "#     val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len)\n",
    "#     test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "#     optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "#     best_val_f1 = 0\n",
    "#     best_val_threshold=0\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "        \n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "#         for batch in progress_bar:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "#             outputs = model(\n",
    "#                 input_ids=batch[\"input_ids\"],\n",
    "#                 attention_mask=batch[\"attention_mask\"],\n",
    "#                 labels=batch[\"labels\"]\n",
    "#             )\n",
    "            \n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "#         print(f\"\\nEpoch {epoch+1}: Average Loss = {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "#         # Validation\n",
    "#         val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "        \n",
    "#         if val_f1 > best_val_f1:\n",
    "#             best_val_f1 = val_f1\n",
    "#             torch.save(model.state_dict(), \"best_legal_clm_citation_model.pt\")\n",
    "#             best_val_threshold = val_metrics['threshold']\n",
    "#             print(f\"âœ… New best model! F1: {best_val_f1:.4f}\")\n",
    "\n",
    "#     # ========================\n",
    "#     # ğŸ§ª Final Test Evaluation\n",
    "#     # ========================\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"ğŸ§ª FINAL LEGAL CITATION PREDICTION TEST:\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     test_f1, test_metrics = evaluate_legal_model(model, test_loader, tokenizer, device, \n",
    "#                                             use_threshold_tuning=False, \n",
    "#                                             fix_threshold=best_val_threshold)\n",
    "\n",
    "#     print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "#     print(f\"Best Prompt Version: {best_prompt_version}\")\n",
    "#     print(f\"Baseline F1: {best_baseline_f1:.4f}\")\n",
    "#     print(f\"Final Test F1: {test_f1:.4f}\")\n",
    "#     print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "#     print(f\"Optimal Threshold: {test_metrics['threshold']:.4f}\")\n",
    "\n",
    "#     # Save results\n",
    "#     results = {\n",
    "#         'best_prompt_version': best_prompt_version,\n",
    "#         'baseline_f1': best_baseline_f1,\n",
    "#         'test_f1': test_f1,\n",
    "#         'test_accuracy': test_metrics['accuracy'],\n",
    "#         'optimal_threshold': test_metrics['threshold'],\n",
    "#         'model_type': 'legal_sentencing_citation_prediction'\n",
    "\n",
    "#     }\n",
    "\n",
    "# K-fold cross-validation setup\n",
    "K_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Store results for statistical comparison\n",
    "model_fold_results = defaultdict(list)  # {model_name: [fold1_f1, fold2_f1, ...]}\n",
    "model_fold_accuracy = defaultdict(list)  # {model_name: [fold1_acc, fold2_acc, ...]}\n",
    "\n",
    "def reset_model_weights(model):\n",
    "    \"\"\"Reset model weights to initial state for each fold\"\"\"\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def create_k_fold_datasets(full_dataset, k_folds=K_FOLDS, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Create k-fold splits of the dataset\"\"\"\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)\n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    fold_splits = []\n",
    "    for train_indices, val_indices in kfold.split(indices):\n",
    "        fold_splits.append((train_indices.tolist(), val_indices.tolist()))\n",
    "    \n",
    "    return fold_splits\n",
    "\n",
    "# Load and prepare the full dataset (combine train, val, test for k-fold)\n",
    "print(\"Loading and combining datasets for k-fold cross-validation...\")\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# Combine all data for k-fold CV\n",
    "df_full = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "print(f\"Total samples for k-fold CV: {len(df_full)}\")\n",
    "\n",
    "model_paths = [\"/home/liorkob/M.Sc/thesis/t5/m5-mlm-final\", \"google/mt5-base\"]\n",
    "\n",
    "for model_idx, model_path in enumerate(model_paths):\n",
    "    model_name = model_path.split('/')[-1]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”„ STARTING K-FOLD EVALUATION FOR: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load tokenizer once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create full dataset for k-fold splitting\n",
    "    full_dataset = LegalSentencingCitationDataset(df_full, tokenizer, max_len=max_len)\n",
    "    fold_splits = create_k_fold_datasets(full_dataset, k_folds=K_FOLDS, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    fold_f1_scores = []\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for fold, (train_indices, test_indices) in enumerate(fold_splits):\n",
    "        print(f\"\\nğŸ“ FOLD {fold + 1}/{K_FOLDS}\")\n",
    "        print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
    "        \n",
    "        # Create fold-specific datasets\n",
    "        train_fold_dataset = Subset(full_dataset, train_indices)\n",
    "        test_fold_dataset = Subset(full_dataset, test_indices)\n",
    "        \n",
    "        # Split training data into train/validation (80/20 split)\n",
    "        train_size = int(0.8 * len(train_indices))\n",
    "        val_size = len(train_indices) - train_size\n",
    "        \n",
    "        # Create train/val split from training indices\n",
    "        train_train_indices = train_indices[:train_size]\n",
    "        train_val_indices = train_indices[train_size:]\n",
    "        \n",
    "        train_train_dataset = Subset(full_dataset, train_train_indices)\n",
    "        train_val_dataset = Subset(full_dataset, train_val_indices)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(train_val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_fold_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Load fresh model for this fold\n",
    "        print(f\"Loading fresh model for fold {fold + 1}...\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        \n",
    "        best_val_f1 = 0\n",
    "        best_val_threshold = 0.5\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Training loop for this fold\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1}, Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"]\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Validation on this fold\n",
    "            val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "            \n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_val_threshold = val_metrics['threshold']\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}: Val F1 = {val_f1:.4f}, Best = {best_val_f1:.4f}\")\n",
    "        \n",
    "        # Load best model for testing\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation for this fold\n",
    "        test_f1, test_metrics = evaluate_legal_model(\n",
    "            model, test_loader, tokenizer, device,\n",
    "            use_threshold_tuning=False,\n",
    "            fix_threshold=best_val_threshold\n",
    "        )\n",
    "        \n",
    "        fold_f1_scores.append(test_f1)\n",
    "        fold_accuracies.append(test_metrics['accuracy'])\n",
    "        \n",
    "        print(f\"âœ… Fold {fold+1} Results: F1 = {test_f1:.4f}, Accuracy = {test_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Store results for this model\n",
    "    model_fold_results[model_name] = fold_f1_scores\n",
    "    model_fold_accuracy[model_name] = fold_accuracies\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {model_name} - K-Fold Summary:\")\n",
    "    print(f\"F1 Scores: {fold_f1_scores}\")\n",
    "    print(f\"Mean F1: {np.mean(fold_f1_scores):.4f} Â± {np.std(fold_f1_scores):.4f}\")\n",
    "    print(f\"Accuracy: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} Â± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Statistical Significance Testing with K-Fold Results\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š K-FOLD STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_names = list(model_fold_results.keys())\n",
    "if len(model_names) == 2:\n",
    "    model1, model2 = model_names\n",
    "    \n",
    "    # Extract k-fold results\n",
    "    f1_scores_1 = model_fold_results[model1]\n",
    "    f1_scores_2 = model_fold_results[model2]\n",
    "    accuracy_scores_1 = model_fold_accuracy[model1]\n",
    "    accuracy_scores_2 = model_fold_accuracy[model2]\n",
    "    \n",
    "    print(f\"\\nComparing {model1} vs {model2}\")\n",
    "    print(f\"K-Fold validation with {K_FOLDS} folds\")\n",
    "    \n",
    "    # Paired t-test for F1 scores (each fold is a paired observation)\n",
    "    print(\"\\n--- F1 Score Comparison (K-Fold) ---\")\n",
    "    print(f\"{model1} - Mean F1: {np.mean(f1_scores_1):.4f} Â± {np.std(f1_scores_1):.4f}\")\n",
    "    print(f\"{model2} - Mean F1: {np.mean(f1_scores_2):.4f} Â± {np.std(f1_scores_2):.4f}\")\n",
    "    print(f\"Fold-wise F1 scores:\")\n",
    "    for i in range(K_FOLDS):\n",
    "        print(f\"  Fold {i+1}: {f1_scores_1[i]:.4f} vs {f1_scores_2[i]:.4f}\")\n",
    "    \n",
    "    f1_t_stat, f1_p_value = stats.ttest_rel(f1_scores_1, f1_scores_2)\n",
    "    print(f\"\\nPaired t-test (K-Fold): t={f1_t_stat:.4f}, p={f1_p_value:.6f}\")\n",
    "    \n",
    "    # Determine significance level\n",
    "    if f1_p_value < 0.001:\n",
    "        significance_f1 = \"***\"\n",
    "    elif f1_p_value < 0.01:\n",
    "        significance_f1 = \"**\"\n",
    "    elif f1_p_value < 0.05:\n",
    "        significance_f1 = \"*\"\n",
    "    else:\n",
    "        significance_f1 = \"ns\"\n",
    "    \n",
    "    print(f\"Significance: {significance_f1}\")\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    diff_f1 = np.array(f1_scores_1) - np.array(f1_scores_2)\n",
    "    cohens_d_f1 = np.mean(diff_f1) / np.std(diff_f1)\n",
    "    print(f\"Cohen's d (effect size): {cohens_d_f1:.4f}\")\n",
    "    \n",
    "    # Paired t-test for Accuracy\n",
    "    print(\"\\n--- Accuracy Comparison (K-Fold) ---\")\n",
    "    print(f\"{model1} - Mean Accuracy: {np.mean(accuracy_scores_1):.4f} Â± {np.std(accuracy_scores_1):.4f}\")\n",
    "    print(f\"{model2} - Mean Accuracy: {np.mean(accuracy_scores_2):.4f} Â± {np.std(accuracy_scores_2):.4f}\")\n",
    "    print(f\"Fold-wise Accuracy scores:\")\n",
    "    for i in range(K_FOLDS):\n",
    "        print(f\"  Fold {i+1}: {accuracy_scores_1[i]:.4f} vs {accuracy_scores_2[i]:.4f}\")\n",
    "    \n",
    "    acc_t_stat, acc_p_value = stats.ttest_rel(accuracy_scores_1, accuracy_scores_2)\n",
    "    print(f\"\\nPaired t-test (K-Fold): t={acc_t_stat:.4f}, p={acc_p_value:.6f}\")\n",
    "    \n",
    "    if acc_p_value < 0.001:\n",
    "        significance_acc = \"***\"\n",
    "    elif acc_p_value < 0.01:\n",
    "        significance_acc = \"**\"\n",
    "    elif acc_p_value < 0.05:\n",
    "        significance_acc = \"*\"\n",
    "    else:\n",
    "        significance_acc = \"ns\"\n",
    "    \n",
    "    print(f\"Significance: {significance_acc}\")\n",
    "    \n",
    "    # Effect size for accuracy\n",
    "    diff_acc = np.array(accuracy_scores_1) - np.array(accuracy_scores_2)\n",
    "    cohens_d_acc = np.mean(diff_acc) / np.std(diff_acc)\n",
    "    print(f\"Cohen's d (effect size): {cohens_d_acc:.4f}\")\n",
    "    \n",
    "    # Confidence intervals\n",
    "    from scipy.stats import t\n",
    "    alpha = 0.05\n",
    "    dof = K_FOLDS - 1\n",
    "    t_critical = t.ppf(1 - alpha/2, dof)\n",
    "    \n",
    "    f1_diff_mean = np.mean(diff_f1)\n",
    "    f1_diff_se = stats.sem(diff_f1)\n",
    "    f1_ci_lower = f1_diff_mean - t_critical * f1_diff_se\n",
    "    f1_ci_upper = f1_diff_mean + t_critical * f1_diff_se\n",
    "    \n",
    "    acc_diff_mean = np.mean(diff_acc)\n",
    "    acc_diff_se = stats.sem(diff_acc)\n",
    "    acc_ci_lower = acc_diff_mean - t_critical * acc_diff_se\n",
    "    acc_ci_upper = acc_diff_mean + t_critical * acc_diff_se\n",
    "    \n",
    "    print(f\"\\n95% Confidence Interval for F1 difference: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "    print(f\"95% Confidence Interval for Accuracy difference: [{acc_ci_lower:.4f}, {acc_ci_upper:.4f}]\")\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric)\n",
    "    print(\"\\n--- Non-parametric Tests ---\")\n",
    "    f1_wilcoxon_stat, f1_wilcoxon_p = stats.wilcoxon(f1_scores_1, f1_scores_2)\n",
    "    acc_wilcoxon_stat, acc_wilcoxon_p = stats.wilcoxon(accuracy_scores_1, accuracy_scores_2)\n",
    "    \n",
    "    print(f\"Wilcoxon signed-rank test (F1): p={f1_wilcoxon_p:.6f}\")\n",
    "    print(f\"Wilcoxon signed-rank test (Accuracy): p={acc_wilcoxon_p:.6f}\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“‹ K-FOLD STATISTICAL SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<15} {'Model 1':<15} {'Model 2':<15} {'p-value':<12} {'Significance':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'F1 Score':<15} {np.mean(f1_scores_1):<15.4f} {np.mean(f1_scores_2):<15.4f} {f1_p_value:<12.6f} {significance_f1:<12}\")\n",
    "    print(f\"{'Accuracy':<15} {np.mean(accuracy_scores_1):<15.4f} {np.mean(accuracy_scores_2):<15.4f} {acc_p_value:<12.6f} {significance_acc:<12}\")\n",
    "    \n",
    "    print(f\"\\nEffect Sizes (Cohen's d):\")\n",
    "    print(f\"F1 Score: {cohens_d_f1:.4f}\")\n",
    "    print(f\"Accuracy: {cohens_d_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n",
    "    print(\"Effect size interpretation: |d| < 0.2 (small), 0.2-0.8 (medium), > 0.8 (large)\")\n",
    "    \n",
    "    # Save detailed k-fold results\n",
    "    kfold_results = pd.DataFrame({\n",
    "        'fold': range(1, K_FOLDS + 1),\n",
    "        f'{model1}_f1': f1_scores_1,\n",
    "        f'{model2}_f1': f1_scores_2,\n",
    "        f'{model1}_accuracy': accuracy_scores_1,\n",
    "        f'{model2}_accuracy': accuracy_scores_2,\n",
    "        'f1_difference': diff_f1,\n",
    "        'accuracy_difference': diff_acc\n",
    "    })\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'statistic': ['mean', 'std', 'sem', 't_stat', 'p_value', 'cohens_d'],\n",
    "        f'{model1}_f1': [np.mean(f1_scores_1), np.std(f1_scores_1), stats.sem(f1_scores_1), \n",
    "                        f1_t_stat, f1_p_value, cohens_d_f1],\n",
    "        f'{model2}_f1': [np.mean(f1_scores_2), np.std(f1_scores_2), stats.sem(f1_scores_2), \n",
    "                        f1_t_stat, f1_p_value, -cohens_d_f1],\n",
    "        f'{model1}_accuracy': [np.mean(accuracy_scores_1), np.std(accuracy_scores_1), stats.sem(accuracy_scores_1),\n",
    "                              acc_t_stat, acc_p_value, cohens_d_acc],\n",
    "        f'{model2}_accuracy': [np.mean(accuracy_scores_2), np.std(accuracy_scores_2), stats.sem(accuracy_scores_2),\n",
    "                              acc_t_stat, acc_p_value, -cohens_d_acc]\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    kfold_results.to_csv('kfold_detailed_results.csv', index=False)\n",
    "    summary_stats.to_csv('kfold_statistical_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Results saved:\")\n",
    "    print(f\"  - Detailed k-fold results: 'kfold_detailed_results.csv'\")\n",
    "    print(f\"  - Statistical summary: 'kfold_statistical_summary.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"Note: Statistical comparison requires exactly 2 models for paired testing.\")\n",
    "\n",
    "print(f\"\\nğŸ¯ K-FOLD CROSS-VALIDATION COMPLETED\")\n",
    "print(f\"Both models evaluated on the same {K_FOLDS} folds for fair comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
