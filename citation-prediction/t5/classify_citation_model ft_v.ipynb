{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# ========================\n",
    "# ğŸ”§ CONFIG\n",
    "# ========================\n",
    "# model_path = \"/home/liorkob/M.Sc/thesis/t5/mt5-mlm-final\"\n",
    "# model_path = \"imvladikon/het5-base\"\n",
    "model_path = \"google/mt5-base\"\n",
    "# model_path=\"/home/liorkob/M.Sc/thesis/t5/mt5-punishment-regression\"\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ§  Dataset for T5\n",
    "# ========================\n",
    "class T5CitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.inputs = df.apply(lambda row: f\"predict citation: {row['gpt_facts_a']} </s> {row['gpt_facts_b']}\", axis=1).tolist()\n",
    "        self.targets = df[\"label\"].apply(lambda l: \"yes\" if l == 1 else \"no\").tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_enc = self.tokenizer(self.inputs[idx], padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        target_enc = self.tokenizer(self.targets[idx], padding='max_length', truncation=True, max_length=4, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": target_enc[\"input_ids\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“¥ Load Data\n",
    "# ========================\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/mt5-large')\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "# model.gradient_checkpointing_enable()  # âœ… ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "model.gradient_checkpointing_enable()  # âœ… ×—×™×¡×›×•×Ÿ ×‘×–×™×›×¨×•×Ÿ\n",
    "\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "train_dataset = T5CitationDataset(df_train, tokenizer, max_len=max_len)\n",
    "val_dataset = T5CitationDataset(df_val, tokenizer, max_len=max_len)\n",
    "test_dataset = T5CitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ========================\n",
    "# ğŸ” Training Loop\n",
    "# ========================\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        torch.cuda.empty_cache()  # âœ… ×¨×™×§×•×Ÿ ×‘×™×Ÿ ×¦×¢×“×™×\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Evaluation\n",
    "# ========================\n",
    "def evaluate(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            outputs = model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            label_texts = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "            preds.extend([1 if p.strip().lower() == \"yes\" else 0 for p in pred_texts])\n",
    "            labels.extend([1 if l.strip().lower() == \"yes\" else 0 for l in label_texts])\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"AUC-ROC: {roc_auc_score(labels, preds):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(labels, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(labels, preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(labels, preds):.4f}\")\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "print(\"\\nğŸ” Validation Set:\")\n",
    "evaluate(model, val_loader, tokenizer, device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ§ª Test Set:\")\n",
    "evaluate(model, test_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ========================\n",
    "# ğŸ”§ CONFIG\n",
    "# ========================\n",
    "model_path = \"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\"\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ¯ CORE: Improved Logits Classification\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    \"\"\"Classify using threshold-based method\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=input_ids.device)\n",
    "        decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        yes_tokens = [259, 1903]  # ×›×Ÿ\n",
    "        no_tokens = [1124]        # ×œ×\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx]\n",
    "            \n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            \n",
    "            score_diff = yes_score - no_score\n",
    "            \n",
    "            if score_diff > threshold:\n",
    "                prediction = 1\n",
    "                predicted_text = \"×›×Ÿ\"\n",
    "            else:\n",
    "                prediction = 0\n",
    "                predicted_text = \"×œ×\"\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            scores.append({\n",
    "                'prediction': prediction,\n",
    "                'predicted_text': predicted_text,\n",
    "                'score_diff': score_diff,\n",
    "                'yes_score': yes_score,\n",
    "                'no_score': no_score\n",
    "            })\n",
    "        \n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, device, true_labels):\n",
    "    \"\"\"Find optimal threshold for balanced predictions\"\"\"\n",
    "    print(\"ğŸ” Finding best threshold...\")\n",
    "    \n",
    "    all_score_diffs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Collecting scores\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            _, scores = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=0.0\n",
    "            )\n",
    "            \n",
    "            for score in scores:\n",
    "                all_score_diffs.append(score['score_diff'])\n",
    "    \n",
    "    # Test thresholds\n",
    "    thresholds = np.linspace(min(all_score_diffs), max(all_score_diffs), 50)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch_preds, _ = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "# ========================\n",
    "# ğŸ§  IMPROVED Dataset with Specific Legal Prompts\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512, prompt_version=1):\n",
    "        \"\"\"\n",
    "        Dataset with legally-specific prompts for sentencing citation prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiple versions of detailed legal prompts\n",
    "        self.legal_prompts = {\n",
    "            1: {\n",
    "                \"hebrew\": \"\"\"××©×™××”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×œ×ª××™×›×” ×‘××“×™× ×™×•×ª ×’×–×¨ ×“×™×Ÿ\n",
    "×”×§×©×¨: ×‘×¤×¡×§×™ ×“×™×Ÿ ×¤×œ×™×œ×™×™×, ×©×•×¤×˜×™× ××¦×˜×˜×™× ×¤×¡×§×™ ×“×™×Ÿ ×§×•×“××™× ×›×“×™ ×œ×ª××•×š ×‘×˜×•×•×— ×”×¢× ×™×©×” ×©×”× ××¦×™×¢×™×. ×œ× ×›×œ ×”×¦×™×˜×•×˜×™× ×¨×œ×•×•× ×˜×™×™× - ×× ×• ××ª××§×“×™× ×¨×§ ×‘×¦×™×˜×•×˜×™× ×”×ª×•××›×™× ×‘×”×—×œ×˜×•×ª ×˜×•×•×— ×”×¢× ×™×©×”.\n",
    "×©××œ×”: ×”×× ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×›×“×™ ×œ×ª××•×š ×‘××“×™× ×™×•×ª ×’×–×¨ ×”×“×™×Ÿ ×©×œ×•, ×¢×œ ×‘×¡×™×¡ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×?\"\"\",\n",
    "                \"english\": \"\"\"Task: Predict citations supporting sentencing policy decisions\n",
    "Context: In criminal verdicts, judges cite previous rulings to support their proposed sentencing range. Not all citations are relevant - we focus specifically on citations supporting sentencing range decisions.\n",
    "Question: Will verdict A cite verdict B to support its sentencing policy, based on indictment facts?\"\"\"\n",
    "            },\n",
    "            \n",
    "            2: {\n",
    "                \"hebrew\": \"\"\"× ×™×ª×•×— ×¦×™×˜×•×˜×™× ×‘×¤×¡×§×™ ×“×™×Ÿ ×¤×œ×™×œ×™×™×\n",
    "××˜×¨×”: ×–×™×”×•×™ ×¦×™×˜×•×˜×™× ×”×¨×œ×•×•× ×˜×™×™× ×œ××“×™× ×™×•×ª ×¢× ×™×©×”\n",
    "×”×’×“×¨×”: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ = ×”×¤× ×™×” ×œ×¤×¡×§ ×“×™×Ÿ ×§×•×“× ×”××©××© ×›×ª×§×“×™× ×œ×˜×•×•×— ×”×¢×•× ×© ×”××•×¦×¢\n",
    "××™×§×•×: ×‘×“×¨×š ×›×œ×œ ×‘×—×œ×§ \"××“×™× ×™×•×ª ×”×¢× ×™×©×”\" ××• \"×˜×•×•×— ×”×¢× ×™×©×”\" ×©×œ ×¤×¡×§ ×”×“×™×Ÿ\n",
    "×©××œ×”: ×‘×”×ª×‘×¡×¡ ×¢×œ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×, ×”×× ×¦×¤×•×™ ×©×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘×˜×•×•×— ×”×¢× ×™×©×”?\"\"\",\n",
    "                \"english\": \"\"\"Criminal verdict citation analysis\n",
    "Goal: Identify citations relevant to sentencing policy\n",
    "Definition: Relevant citation = reference to prior ruling used as precedent for proposed punishment range\n",
    "Location: Typically found in \"Sentencing Policy\" or \"Sentencing Range\" sections\n",
    "Question: Based on indictment facts, will verdict A likely cite verdict B to support sentencing range?\"\"\"\n",
    "            },\n",
    "            \n",
    "            3: {\n",
    "                \"hebrew\": \"\"\"××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×”\n",
    "×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×”\n",
    "××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×\n",
    "×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×)\n",
    "×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×.\"\"\",\n",
    "                \"english\": \"\"\"Specialized legal citation prediction system\n",
    "Domain: Criminal law - sentencing policy\n",
    "Purpose: Predict citations between verdicts based on indictment facts similarity\n",
    "Criteria: Citation is relevant if it supports sentencing range decision (not procedures, definitions, or unrelated verdicts)\n",
    "Verdict A will cite verdict B if there is similarity in offenses and circumstances presented in indictments.\"\"\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        chosen_prompt = self.legal_prompts[prompt_version][\"hebrew\"]\n",
    "        \n",
    "        # Create more detailed inputs with legal context\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Format with detailed legal context\n",
    "            legal_input = f\"\"\"{chosen_prompt}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×”×¢×‘×™×¨×•×ª ×•×”× ×¡×™×‘×•×ª, ×”×× ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘××“×™× ×™×•×ª ×”×¢× ×™×©×”?\"\"\"\n",
    "            \n",
    "            self.inputs.append(legal_input)\n",
    "        \n",
    "        self.targets = df[\"label\"].apply(lambda l: \"×›×Ÿ\" if l == 1 else \"×œ×\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(f\"Legal Dataset created: {len(self.inputs)} samples\")\n",
    "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Sample input length: {len(self.inputs[0])} characters\")\n",
    "        print(f\"Prompt version: {prompt_version}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        \n",
    "        # Tokenize with longer sequences due to detailed prompt\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=5,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Evaluation Function\n",
    "# ========================\n",
    "def evaluate_legal_model(model, dataloader, tokenizer, device, use_threshold_tuning=True):\n",
    "    \"\"\"Evaluate the legal citation model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect true labels\n",
    "    true_labels = []\n",
    "    for batch in dataloader:\n",
    "        true_labels.extend(batch[\"numeric_label\"].numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    if use_threshold_tuning:\n",
    "        best_threshold = find_best_threshold(model, tokenizer, dataloader, device, true_labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "    else:\n",
    "        best_threshold = None\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (No Threshold Tuning)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                generated = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=5\n",
    "                )\n",
    "                decoded_preds = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                predictions = [1 if p.strip() == \"×›×Ÿ\" else 0 for p in decoded_preds]\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend([{} for _ in predictions])  # <- ×“×my score dicts\n",
    "\n",
    "    predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    print(f\"\\nğŸ“Š LEGAL CITATION PREDICTION RESULTS:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Distribution: {np.bincount(predictions)}\")\n",
    "    print(f\"True Label Distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "    if len(np.unique(predictions)) > 1 and len(np.unique(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predictions)\n",
    "        print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    return f1, {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'threshold': best_threshold,\n",
    "        'scores': all_confidence_scores  # â† ×ª××™×“ ×§×™×™×\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“¥ Load Everything\n",
    "# ========================\n",
    "print(\"Loading model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# Try different prompt versions\n",
    "prompt_versions_to_try = [1, 2, 3]\n",
    "best_prompt_version = 1\n",
    "best_baseline_f1 = 0\n",
    "\n",
    "print(\"\\nğŸ” TESTING DIFFERENT LEGAL PROMPTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for prompt_version in prompt_versions_to_try:\n",
    "    print(f\"\\nğŸ“‹ Testing Prompt Version {prompt_version}:\")\n",
    "    \n",
    "    # Create datasets with this prompt version\n",
    "    val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len, prompt_version=prompt_version)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Test baseline performance\n",
    "    baseline_f1, baseline_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "    \n",
    "    print(f\"Prompt {prompt_version} Baseline F1: {baseline_f1:.4f}\")\n",
    "    \n",
    "    if baseline_f1 > best_baseline_f1:\n",
    "        best_baseline_f1 = baseline_f1\n",
    "        best_prompt_version = prompt_version\n",
    "\n",
    "print(f\"\\nğŸ† BEST PROMPT VERSION: {best_prompt_version} (F1: {best_baseline_f1:.4f})\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ—ï¸ Create Final Datasets with Best Prompt\n",
    "# ========================\n",
    "print(f\"\\nCreating final datasets with prompt version {best_prompt_version}...\")\n",
    "train_dataset = LegalSentencingCitationDataset(df_train, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len, prompt_version=best_prompt_version)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ========================\n",
    "# ğŸ” Training (if needed)\n",
    "# ========================\n",
    "if best_baseline_f1 < 0.6:\n",
    "    print(f\"\\nBaseline F1 ({best_baseline_f1:.4f}) needs improvement. Starting training...\")\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    best_val_f1 = best_baseline_f1\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}: Average Loss = {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_legal_clm_citation_model.pt\")\n",
    "            print(f\"âœ… New best model! F1: {best_val_f1:.4f}\")\n",
    "else:\n",
    "    print(f\"Baseline F1 ({best_baseline_f1:.4f}) is already good!\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ§ª Final Test Evaluation\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ§ª FINAL LEGAL CITATION PREDICTION TEST:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_f1, test_metrics = evaluate_legal_model(model, test_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "\n",
    "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "print(f\"Best Prompt Version: {best_prompt_version}\")\n",
    "print(f\"Baseline F1: {best_baseline_f1:.4f}\")\n",
    "print(f\"Final Test F1: {test_f1:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Optimal Threshold: {test_metrics['threshold']:.4f}\")\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'best_prompt_version': best_prompt_version,\n",
    "    'baseline_f1': best_baseline_f1,\n",
    "    'test_f1': test_f1,\n",
    "    'test_accuracy': test_metrics['accuracy'],\n",
    "    'optimal_threshold': test_metrics['threshold'],\n",
    "    'model_type': 'legal_sentencing_citation_prediction'\n",
    "\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('legal_citation_results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if test_f1 > 0.6:\n",
    "    print(f\"\\nğŸ‰ SUCCESS! Legal citation model achieved {test_f1:.4f} F1 score!\")\n",
    "    print(f\"ğŸ† The detailed legal prompts significantly improved performance!\")\n",
    "else:\n",
    "    print(f\"\\nğŸ“ˆ F1: {test_f1:.4f} - Consider fine-tuning hyperparameters or trying longer training\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Results saved to 'legal_citation_results.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_predictions(dataset, predictions, scores, tokenizer, num_samples=10):\n",
    "    print(f\"\\nğŸ” Showing {num_samples} Sample Predictions:\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    indices = np.random.choice(len(dataset), size=min(num_samples, len(dataset)), replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        input_ids = dataset[idx][\"input_ids\"]\n",
    "        decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "        label = dataset[idx][\"numeric_label\"]\n",
    "        prediction = predictions[idx]\n",
    "        score = scores[idx]\n",
    "        \n",
    "        print(f\"\\nğŸ“„ Input #{idx}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"ğŸ”¹ Decoded Input:\\n{decoded_input[:1000]}...\")  # Truncate long text\n",
    "        print(f\"âœ… True Label: {'×›×Ÿ' if label == 1 else '×œ×'}\")\n",
    "        print(f\"ğŸ§  Predicted: {'×›×Ÿ' if prediction == 1 else '×œ×'}\")\n",
    "        print(f\"ğŸ§® Score Diff: {score['score_diff']:.4f} | Yes Score: {score['yes_score']:.4f} | No Score: {score['no_score']:.4f}\")\n",
    "        print(\"=\" * 80)\n",
    "show_sample_predictions(test_dataset, test_metrics[\"predictions\"], test_metrics[\"scores\"], tokenizer, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading and combining datasets for k-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples for k-fold CV: 5791\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ STARTING K-FOLD EVALUATION FOR: het5-mlm-final\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal Dataset created: 5791 samples\n",
      "Label distribution: [3857 1934]\n",
      "Sample input length: 2083 characters\n",
      "\n",
      "ğŸ“ FOLD 1/5\n",
      "Train samples: 4632, Test samples: 1159\n",
      "Loading fresh model for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/7:   0%|          | 0/927 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "Fold 1, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.3768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.4985 (F1: 0.5173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5173\n",
      "Precision: 0.4155\n",
      "Recall: 0.6852\n",
      "Accuracy: 0.5793\n",
      "\n",
      "Prediction Distribution: [424 503]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.6063\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.53      0.63       622\n",
      "           1       0.42      0.69      0.52       305\n",
      "\n",
      "    accuracy                           0.58       927\n",
      "   macro avg       0.59      0.61      0.57       927\n",
      "weighted avg       0.66      0.58      0.59       927\n",
      "\n",
      "Fold 1, Epoch 1: Val F1 = 0.5173, Best = 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:44<00:00,  4.13it/s, loss=0.3974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.9965 (F1: 0.5325)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5325\n",
      "Precision: 0.4354\n",
      "Recall: 0.6852\n",
      "Accuracy: 0.6041\n",
      "\n",
      "Prediction Distribution: [447 480]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.6248\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.56      0.66       622\n",
      "           1       0.44      0.69      0.53       305\n",
      "\n",
      "    accuracy                           0.60       927\n",
      "   macro avg       0.61      0.62      0.59       927\n",
      "weighted avg       0.67      0.60      0.62       927\n",
      "\n",
      "Fold 1, Epoch 2: Val F1 = 0.5325, Best = 0.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.2365]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.7889 (F1: 0.6293)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6293\n",
      "Precision: 0.6325\n",
      "Recall: 0.6262\n",
      "Accuracy: 0.7573\n",
      "\n",
      "Prediction Distribution: [625 302]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7239\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       622\n",
      "           1       0.63      0.63      0.63       305\n",
      "\n",
      "    accuracy                           0.76       927\n",
      "   macro avg       0.73      0.72      0.72       927\n",
      "weighted avg       0.76      0.76      0.76       927\n",
      "\n",
      "Fold 1, Epoch 3: Val F1 = 0.6293, Best = 0.6293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.2577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.6116 (F1: 0.6756)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6756\n",
      "Precision: 0.6209\n",
      "Recall: 0.7410\n",
      "Accuracy: 0.7659\n",
      "\n",
      "Prediction Distribution: [563 364]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7596\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.78      0.82       622\n",
      "           1       0.62      0.74      0.68       305\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.74      0.76      0.75       927\n",
      "weighted avg       0.78      0.77      0.77       927\n",
      "\n",
      "Fold 1, Epoch 4: Val F1 = 0.6756, Best = 0.6756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.0538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.4795 (F1: 0.6912)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6912\n",
      "Precision: 0.6503\n",
      "Recall: 0.7377\n",
      "Accuracy: 0.7832\n",
      "\n",
      "Prediction Distribution: [581 346]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7716\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83       622\n",
      "           1       0.65      0.74      0.69       305\n",
      "\n",
      "    accuracy                           0.78       927\n",
      "   macro avg       0.76      0.77      0.76       927\n",
      "weighted avg       0.79      0.78      0.79       927\n",
      "\n",
      "Fold 1, Epoch 5: Val F1 = 0.6912, Best = 0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.0049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.5115 (F1: 0.7375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7375\n",
      "Precision: 0.7045\n",
      "Recall: 0.7738\n",
      "Accuracy: 0.8188\n",
      "\n",
      "Prediction Distribution: [592 335]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.8073\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86       622\n",
      "           1       0.70      0.77      0.74       305\n",
      "\n",
      "    accuracy                           0.82       927\n",
      "   macro avg       0.79      0.81      0.80       927\n",
      "weighted avg       0.82      0.82      0.82       927\n",
      "\n",
      "Fold 1, Epoch 6: Val F1 = 0.7375, Best = 0.7375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=1.0322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.2710 (F1: 0.7206)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7206\n",
      "Precision: 0.6985\n",
      "Recall: 0.7443\n",
      "Accuracy: 0.8101\n",
      "\n",
      "Prediction Distribution: [602 325]\n",
      "True Label Distribution: [622 305]\n",
      "AUC-ROC: 0.7934\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       622\n",
      "           1       0.70      0.74      0.72       305\n",
      "\n",
      "    accuracy                           0.81       927\n",
      "   macro avg       0.78      0.79      0.79       927\n",
      "weighted avg       0.81      0.81      0.81       927\n",
      "\n",
      "Fold 1, Epoch 7: Val F1 = 0.7206, Best = 0.7375\n",
      "Using fixed threshold: -6.511479732941607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:21<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7241\n",
      "Precision: 0.6659\n",
      "Recall: 0.7933\n",
      "Accuracy: 0.7981\n",
      "\n",
      "Prediction Distribution: [698 461]\n",
      "True Label Distribution: [772 387]\n",
      "AUC-ROC: 0.7969\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84       772\n",
      "           1       0.67      0.79      0.72       387\n",
      "\n",
      "    accuracy                           0.80      1159\n",
      "   macro avg       0.78      0.80      0.78      1159\n",
      "weighted avg       0.81      0.80      0.80      1159\n",
      "\n",
      "âœ… Fold 1 Results: F1 = 0.7241, Accuracy = 0.7981\n",
      "\n",
      "ğŸ“ FOLD 2/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.4902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.7694 (F1: 0.5064)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5064\n",
      "Precision: 0.3434\n",
      "Recall: 0.9641\n",
      "Accuracy: 0.3797\n",
      "\n",
      "Prediction Distribution: [ 68 859]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.5279\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.09      0.17       621\n",
      "           1       0.34      0.96      0.51       306\n",
      "\n",
      "    accuracy                           0.38       927\n",
      "   macro avg       0.59      0.53      0.34       927\n",
      "weighted avg       0.67      0.38      0.28       927\n",
      "\n",
      "Fold 2, Epoch 1: Val F1 = 0.5064, Best = 0.5064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.2292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2334 (F1: 0.5320)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5320\n",
      "Precision: 0.3693\n",
      "Recall: 0.9510\n",
      "Accuracy: 0.4477\n",
      "\n",
      "Prediction Distribution: [139 788]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.5753\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.20      0.33       621\n",
      "           1       0.37      0.95      0.53       306\n",
      "\n",
      "    accuracy                           0.45       927\n",
      "   macro avg       0.63      0.58      0.43       927\n",
      "weighted avg       0.72      0.45      0.39       927\n",
      "\n",
      "Fold 2, Epoch 2: Val F1 = 0.5320, Best = 0.5320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.3140]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.6795 (F1: 0.6238)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6238\n",
      "Precision: 0.6139\n",
      "Recall: 0.6340\n",
      "Accuracy: 0.7476\n",
      "\n",
      "Prediction Distribution: [611 316]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7188\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81       621\n",
      "           1       0.61      0.63      0.62       306\n",
      "\n",
      "    accuracy                           0.75       927\n",
      "   macro avg       0.72      0.72      0.72       927\n",
      "weighted avg       0.75      0.75      0.75       927\n",
      "\n",
      "Fold 2, Epoch 3: Val F1 = 0.6238, Best = 0.6238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.3868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -4.4860 (F1: 0.6335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6335\n",
      "Precision: 0.5603\n",
      "Recall: 0.7288\n",
      "Accuracy: 0.7217\n",
      "\n",
      "Prediction Distribution: [529 398]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7235\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.72      0.78       621\n",
      "           1       0.56      0.73      0.63       306\n",
      "\n",
      "    accuracy                           0.72       927\n",
      "   macro avg       0.70      0.72      0.70       927\n",
      "weighted avg       0.75      0.72      0.73       927\n",
      "\n",
      "Fold 2, Epoch 4: Val F1 = 0.6335, Best = 0.6335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.17it/s, loss=0.1263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -6.0377 (F1: 0.6912)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6912\n",
      "Precision: 0.6283\n",
      "Recall: 0.7680\n",
      "Accuracy: 0.7735\n",
      "\n",
      "Prediction Distribution: [553 374]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7721\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.82       621\n",
      "           1       0.63      0.77      0.69       306\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.75      0.77      0.76       927\n",
      "weighted avg       0.79      0.77      0.78       927\n",
      "\n",
      "Fold 2, Epoch 5: Val F1 = 0.6912, Best = 0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.0801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.6298 (F1: 0.7128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7128\n",
      "Precision: 0.6488\n",
      "Recall: 0.7908\n",
      "Accuracy: 0.7896\n",
      "\n",
      "Prediction Distribution: [554 373]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7899\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83       621\n",
      "           1       0.65      0.79      0.71       306\n",
      "\n",
      "    accuracy                           0.79       927\n",
      "   macro avg       0.77      0.79      0.77       927\n",
      "weighted avg       0.81      0.79      0.79       927\n",
      "\n",
      "Fold 2, Epoch 6: Val F1 = 0.7128, Best = 0.7128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 7/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.0130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.9754 (F1: 0.7095)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.7095\n",
      "Precision: 0.6667\n",
      "Recall: 0.7582\n",
      "Accuracy: 0.7950\n",
      "\n",
      "Prediction Distribution: [579 348]\n",
      "True Label Distribution: [621 306]\n",
      "AUC-ROC: 0.7857\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       621\n",
      "           1       0.67      0.76      0.71       306\n",
      "\n",
      "    accuracy                           0.80       927\n",
      "   macro avg       0.77      0.79      0.78       927\n",
      "weighted avg       0.80      0.80      0.80       927\n",
      "\n",
      "Fold 2, Epoch 7: Val F1 = 0.7095, Best = 0.7128\n",
      "Using fixed threshold: -5.629789388909632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Fixed Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290/290 [00:22<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6952\n",
      "Precision: 0.6258\n",
      "Recall: 0.7818\n",
      "Accuracy: 0.7720\n",
      "\n",
      "Prediction Distribution: [677 481]\n",
      "True Label Distribution: [773 385]\n",
      "AUC-ROC: 0.7745\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.77      0.82       773\n",
      "           1       0.63      0.78      0.70       385\n",
      "\n",
      "    accuracy                           0.77      1158\n",
      "   macro avg       0.75      0.77      0.76      1158\n",
      "weighted avg       0.79      0.77      0.78      1158\n",
      "\n",
      "âœ… Fold 2 Results: F1 = 0.6952, Accuracy = 0.7720\n",
      "\n",
      "ğŸ“ FOLD 3/5\n",
      "Train samples: 4633, Test samples: 1158\n",
      "Loading fresh model for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.4294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -3.4036 (F1: 0.5017)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5017\n",
      "Precision: 0.3395\n",
      "Recall: 0.9609\n",
      "Accuracy: 0.3679\n",
      "\n",
      "Prediction Distribution: [ 58 869]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.5176\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.07      0.14       620\n",
      "           1       0.34      0.96      0.50       307\n",
      "\n",
      "    accuracy                           0.37       927\n",
      "   macro avg       0.57      0.52      0.32       927\n",
      "weighted avg       0.64      0.37      0.26       927\n",
      "\n",
      "Fold 3, Epoch 1: Val F1 = 0.5017, Best = 0.5017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 2/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.16it/s, loss=0.3617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.1464 (F1: 0.5471)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5471\n",
      "Precision: 0.3971\n",
      "Recall: 0.8795\n",
      "Accuracy: 0.5178\n",
      "\n",
      "Prediction Distribution: [247 680]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.6091\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.34      0.48       620\n",
      "           1       0.40      0.88      0.55       307\n",
      "\n",
      "    accuracy                           0.52       927\n",
      "   macro avg       0.62      0.61      0.52       927\n",
      "weighted avg       0.70      0.52      0.51       927\n",
      "\n",
      "Fold 3, Epoch 2: Val F1 = 0.5471, Best = 0.5471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 3/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.4393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.9755 (F1: 0.5950)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.5950\n",
      "Precision: 0.4698\n",
      "Recall: 0.8111\n",
      "Accuracy: 0.6343\n",
      "\n",
      "Prediction Distribution: [397 530]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.6789\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.55      0.67       620\n",
      "           1       0.47      0.81      0.59       307\n",
      "\n",
      "    accuracy                           0.63       927\n",
      "   macro avg       0.66      0.68      0.63       927\n",
      "weighted avg       0.73      0.63      0.64       927\n",
      "\n",
      "Fold 3, Epoch 3: Val F1 = 0.5950, Best = 0.5950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 4/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.5062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.2056 (F1: 0.6583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6583\n",
      "Precision: 0.6372\n",
      "Recall: 0.6808\n",
      "Accuracy: 0.7659\n",
      "\n",
      "Prediction Distribution: [599 328]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7444\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82       620\n",
      "           1       0.64      0.68      0.66       307\n",
      "\n",
      "    accuracy                           0.77       927\n",
      "   macro avg       0.74      0.74      0.74       927\n",
      "weighted avg       0.77      0.77      0.77       927\n",
      "\n",
      "Fold 3, Epoch 4: Val F1 = 0.6583, Best = 0.6583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 5/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:43<00:00,  4.15it/s, loss=0.0741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: -5.0696 (F1: 0.6686)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Legal Evaluation (Tuned Threshold): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š LEGAL CITATION PREDICTION RESULTS:\n",
      "F1 Score: 0.6686\n",
      "Precision: 0.5934\n",
      "Recall: 0.7655\n",
      "Accuracy: 0.7487\n",
      "\n",
      "Prediction Distribution: [531 396]\n",
      "True Label Distribution: [620 307]\n",
      "AUC-ROC: 0.7529\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.80       620\n",
      "           1       0.59      0.77      0.67       307\n",
      "\n",
      "    accuracy                           0.75       927\n",
      "   macro avg       0.73      0.75      0.73       927\n",
      "weighted avg       0.77      0.75      0.75       927\n",
      "\n",
      "Fold 3, Epoch 5: Val F1 = 0.6686, Best = 0.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 6/7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927/927 [03:42<00:00,  4.16it/s, loss=0.5029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Finding best threshold...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232/232 [00:17<00:00, 13.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "\n",
    "# ========================\n",
    "# ğŸ”§ CONFIG\n",
    "# ========================\n",
    "train_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\"\n",
    "val_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\"\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 1024\n",
    "epochs = 7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ¯ CORE: Improved Logits Classification\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    \"\"\"Classify using threshold-based method\"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        decoder_input_ids = torch.zeros((batch_size, 1), dtype=torch.long, device=input_ids.device)\n",
    "        decoder_input_ids[:, 0] = tokenizer.pad_token_id\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids\n",
    "        )\n",
    "        \n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        yes_tokens = [259, 1903]  # ×›×Ÿ\n",
    "        no_tokens = [1124]        # ×œ×\n",
    "        \n",
    "        predictions = []\n",
    "        scores = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            batch_logits = logits[batch_idx]\n",
    "            \n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            \n",
    "            score_diff = yes_score - no_score\n",
    "            \n",
    "            if score_diff > threshold:\n",
    "                prediction = 1\n",
    "                predicted_text = \"×›×Ÿ\"\n",
    "            else:\n",
    "                prediction = 0\n",
    "                predicted_text = \"×œ×\"\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            scores.append({\n",
    "                'prediction': prediction,\n",
    "                'predicted_text': predicted_text,\n",
    "                'score_diff': score_diff,\n",
    "                'yes_score': yes_score,\n",
    "                'no_score': no_score\n",
    "            })\n",
    "        \n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, device, true_labels):\n",
    "    \"\"\"Find optimal threshold for balanced predictions\"\"\"\n",
    "    print(\"ğŸ” Finding best threshold...\")\n",
    "    \n",
    "    all_score_diffs = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Collecting scores\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            _, scores = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=0.0\n",
    "            )\n",
    "            \n",
    "            for score in scores:\n",
    "                all_score_diffs.append(score['score_diff'])\n",
    "    \n",
    "    # Test thresholds\n",
    "    thresholds = np.linspace(min(all_score_diffs), max(all_score_diffs), 50)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            batch_preds, _ = classify_with_threshold_search(\n",
    "                model, tokenizer,\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                threshold=threshold\n",
    "            )\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_threshold\n",
    "\n",
    "# ========================\n",
    "# ğŸ§  IMPROVED Dataset with Specific Legal Prompts\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        \"\"\"\n",
    "        Dataset with legally-specific prompts for sentencing citation prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        # Multiple versions of detailed legal prompts\n",
    "        prompt = \"\"\"××¢×¨×›×ª ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ××©×¤×˜×™×™× ××ª××—×”\n",
    "×ª×—×•×: ×“×™×Ÿ ×¤×œ×™×œ×™ - ××“×™× ×™×•×ª ×¢× ×™×©×”\n",
    "××˜×¨×”: ×—×™×–×•×™ ×¦×™×˜×•×˜×™× ×‘×™×Ÿ ×¤×¡×§×™ ×“×™×Ÿ ×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×‘×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×\n",
    "×§×¨×™×˜×¨×™×•× ×™×: ×¦×™×˜×•×˜ ×¨×œ×•×•× ×˜×™ ×× ×”×•× ×ª×•××š ×‘×”×—×œ×˜×ª ×˜×•×•×— ×”×¢×•× ×© (×œ× ×”×œ×™×›×™×, ×”×’×“×¨×•×ª, ××• ×¤×¡×§×™ ×“×™×Ÿ ×œ× ×§×©×•×¨×™×)\n",
    "×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×× ×™×© ×“××™×•×Ÿ ×‘×¢×‘×™×¨×•×ª ×•×‘× ×¡×™×‘×•×ª ×”×¢×•×•×œ×•×ª ×”××•×¦×’×•×ª ×‘×›×ª×‘×™ ×”××™×©×•×.\n",
    "×©××œ×”: ×‘×”×ª×‘×¡×¡ ×¢×œ ×¢×•×‘×“×•×ª ×›×ª×‘ ×”××™×©×•×, ×”×× ×¦×¤×•×™ ×©×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘×˜×•×•×— ×”×¢× ×™×©×”?\n",
    "\"\"\"\n",
    "\n",
    "#                 \"english\": \"\"\"Specialized legal citation prediction system\n",
    "# Domain: Criminal law - sentencing policy\n",
    "# Purpose: Predict citations between verdicts based on indictment facts similarity\n",
    "# Criteria: Citation is relevant if it supports sentencing range decision (not procedures, definitions, or unrelated verdicts)\n",
    "# Verdict A will cite verdict B if there is similarity in offenses and circumstances presented in indictments.\n",
    "# Question: Based on indictment facts, will verdict A likely cite verdict B to support sentencing range?\"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "        # Create more detailed inputs with legal context\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            # Format with detailed legal context\n",
    "            legal_input = f\"\"\"{prompt}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "×¢×•×‘×“×•×ª ×›×ª×‘ ××™×©×•× - ×¤×¡×§ ×“×™×Ÿ ×‘':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "×¢×œ ×‘×¡×™×¡ ×“××™×•×Ÿ ×”×¢×‘×™×¨×•×ª ×•×”× ×¡×™×‘×•×ª, ×”×× ×¤×¡×§ ×“×™×Ÿ ×' ×™×¦×˜×˜ ×¤×¡×§ ×“×™×Ÿ ×‘' ×œ×ª××™×›×” ×‘××“×™× ×™×•×ª ×”×¢× ×™×©×”?\"\"\"\n",
    "            \n",
    "            self.inputs.append(legal_input)\n",
    "        \n",
    "        self.targets = df[\"label\"].apply(lambda l: \"×›×Ÿ\" if l == 1 else \"×œ×\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        print(f\"Legal Dataset created: {len(self.inputs)} samples\")\n",
    "        print(f\"Label distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Sample input length: {len(self.inputs[0])} characters\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "        \n",
    "        # Tokenize with longer sequences due to detailed prompt\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_len, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=5,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Evaluation Function\n",
    "# ========================\n",
    "def evaluate_legal_model(model, dataloader, tokenizer, device, use_threshold_tuning=True, fix_threshold=None):\n",
    "    \"\"\"Evaluate the legal citation model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect true labels\n",
    "    true_labels = []\n",
    "    for batch in dataloader:\n",
    "        true_labels.extend(batch[\"numeric_label\"].numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_confidence_scores = []\n",
    "\n",
    "    # THREE OPTIONS NOW:\n",
    "    if fix_threshold is not None:\n",
    "        # âœ… USE FIXED THRESHOLD (no tuning)\n",
    "        print(f\"Using fixed threshold: {fix_threshold}\")\n",
    "        best_threshold = fix_threshold\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Fixed Threshold)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "                \n",
    "    elif use_threshold_tuning:\n",
    "        # âŒ TUNE THRESHOLD ON THIS DATASET (causes data leakage if used on test set)\n",
    "        best_threshold = find_best_threshold(model, tokenizer, dataloader, device, true_labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Tuned Threshold)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "                predictions, confidence_scores = classify_with_threshold_search(\n",
    "                    model, tokenizer,\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    threshold=best_threshold\n",
    "                )\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend(confidence_scores)\n",
    "    else:\n",
    "        # ğŸ”„ USE MODEL.GENERATE() (different approach)\n",
    "        best_threshold = None\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Legal Evaluation (Generation)\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                generated = model.generate(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_length=5\n",
    "                )\n",
    "                decoded_preds = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "                predictions = [1 if p.strip() == \"×›×Ÿ\" else 0 for p in decoded_preds]\n",
    "\n",
    "                all_predictions.extend(predictions)\n",
    "                all_confidence_scores.extend([{} for _ in predictions])\n",
    "\n",
    "    predictions = np.array(all_predictions)\n",
    "\n",
    "    # Calculate metrics\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    accuracy = np.mean(predictions == true_labels)\n",
    "\n",
    "    print(f\"\\nğŸ“Š LEGAL CITATION PREDICTION RESULTS:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nPrediction Distribution: {np.bincount(predictions)}\")\n",
    "    print(f\"True Label Distribution: {np.bincount(true_labels)}\")\n",
    "\n",
    "    if len(np.unique(predictions)) > 1 and len(np.unique(true_labels)) > 1:\n",
    "        auc = roc_auc_score(true_labels, predictions)\n",
    "        print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predictions))\n",
    "\n",
    "    return f1, {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': predictions,\n",
    "        'threshold': best_threshold,\n",
    "        'scores': all_confidence_scores  # â† ×ª××™×“ ×§×™×™×\n",
    "    }\n",
    "\n",
    "# ========================\n",
    "# ğŸ“¥ Load Everything\n",
    "# ========================\n",
    "\n",
    "# model_paths = [\"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\",\"imvladikon/het5-base\"]\n",
    "\n",
    "# for model_path in model_paths:\n",
    "#     print(f\"Loading {model_path} and tokenizer...\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#     model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#     print(\"Loading data...\")\n",
    "#     df_train = pd.read_csv(train_file)\n",
    "#     df_val = pd.read_csv(val_file)\n",
    "#     df_test = pd.read_csv(test_file)\n",
    "\n",
    "#     train_dataset = LegalSentencingCitationDataset(df_train, tokenizer, max_len=max_len)\n",
    "#     val_dataset = LegalSentencingCitationDataset(df_val, tokenizer, max_len=max_len)\n",
    "#     test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "#     optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "#     best_val_f1 = 0\n",
    "#     best_val_threshold=0\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "        \n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "#         for batch in progress_bar:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "#             outputs = model(\n",
    "#                 input_ids=batch[\"input_ids\"],\n",
    "#                 attention_mask=batch[\"attention_mask\"],\n",
    "#                 labels=batch[\"labels\"]\n",
    "#             )\n",
    "            \n",
    "#             loss = outputs.loss\n",
    "#             loss.backward()\n",
    "            \n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "#         print(f\"\\nEpoch {epoch+1}: Average Loss = {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "#         # Validation\n",
    "#         val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "        \n",
    "#         if val_f1 > best_val_f1:\n",
    "#             best_val_f1 = val_f1\n",
    "#             torch.save(model.state_dict(), \"best_legal_clm_citation_model.pt\")\n",
    "#             best_val_threshold = val_metrics['threshold']\n",
    "#             print(f\"âœ… New best model! F1: {best_val_f1:.4f}\")\n",
    "\n",
    "#     # ========================\n",
    "#     # ğŸ§ª Final Test Evaluation\n",
    "#     # ========================\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"ğŸ§ª FINAL LEGAL CITATION PREDICTION TEST:\")\n",
    "#     print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     test_f1, test_metrics = evaluate_legal_model(model, test_loader, tokenizer, device, \n",
    "#                                             use_threshold_tuning=False, \n",
    "#                                             fix_threshold=best_val_threshold)\n",
    "\n",
    "#     print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "#     print(f\"Best Prompt Version: {best_prompt_version}\")\n",
    "#     print(f\"Baseline F1: {best_baseline_f1:.4f}\")\n",
    "#     print(f\"Final Test F1: {test_f1:.4f}\")\n",
    "#     print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "#     print(f\"Optimal Threshold: {test_metrics['threshold']:.4f}\")\n",
    "\n",
    "#     # Save results\n",
    "#     results = {\n",
    "#         'best_prompt_version': best_prompt_version,\n",
    "#         'baseline_f1': best_baseline_f1,\n",
    "#         'test_f1': test_f1,\n",
    "#         'test_accuracy': test_metrics['accuracy'],\n",
    "#         'optimal_threshold': test_metrics['threshold'],\n",
    "#         'model_type': 'legal_sentencing_citation_prediction'\n",
    "\n",
    "#     }\n",
    "\n",
    "# K-fold cross-validation setup\n",
    "K_FOLDS = 5\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Store results for statistical comparison\n",
    "model_fold_results = defaultdict(list)  # {model_name: [fold1_f1, fold2_f1, ...]}\n",
    "model_fold_accuracy = defaultdict(list)  # {model_name: [fold1_acc, fold2_acc, ...]}\n",
    "\n",
    "def reset_model_weights(model):\n",
    "    \"\"\"Reset model weights to initial state for each fold\"\"\"\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "def create_k_fold_datasets(full_dataset, k_folds=K_FOLDS, random_seed=RANDOM_SEED):\n",
    "    \"\"\"Create k-fold splits of the dataset\"\"\"\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_seed)\n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    fold_splits = []\n",
    "    for train_indices, val_indices in kfold.split(indices):\n",
    "        fold_splits.append((train_indices.tolist(), val_indices.tolist()))\n",
    "    \n",
    "    return fold_splits\n",
    "\n",
    "# Load and prepare the full dataset (combine train, val, test for k-fold)\n",
    "print(\"Loading and combining datasets for k-fold cross-validation...\")\n",
    "df_train = pd.read_csv(train_file)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_test = pd.read_csv(test_file)\n",
    "\n",
    "# Combine all data for k-fold CV\n",
    "df_full = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "print(f\"Total samples for k-fold CV: {len(df_full)}\")\n",
    "\n",
    "model_paths = [\"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\", \"imvladikon/het5-base\"]\n",
    "\n",
    "for model_idx, model_path in enumerate(model_paths):\n",
    "    model_name = model_path.split('/')[-1]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”„ STARTING K-FOLD EVALUATION FOR: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load tokenizer once\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create full dataset for k-fold splitting\n",
    "    full_dataset = LegalSentencingCitationDataset(df_full, tokenizer, max_len=max_len)\n",
    "    fold_splits = create_k_fold_datasets(full_dataset, k_folds=K_FOLDS, random_seed=RANDOM_SEED)\n",
    "    \n",
    "    fold_f1_scores = []\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for fold, (train_indices, test_indices) in enumerate(fold_splits):\n",
    "        print(f\"\\nğŸ“ FOLD {fold + 1}/{K_FOLDS}\")\n",
    "        print(f\"Train samples: {len(train_indices)}, Test samples: {len(test_indices)}\")\n",
    "        \n",
    "        # Create fold-specific datasets\n",
    "        train_fold_dataset = Subset(full_dataset, train_indices)\n",
    "        test_fold_dataset = Subset(full_dataset, test_indices)\n",
    "        \n",
    "        # Split training data into train/validation (80/20 split)\n",
    "        train_size = int(0.8 * len(train_indices))\n",
    "        val_size = len(train_indices) - train_size\n",
    "        \n",
    "        # Create train/val split from training indices\n",
    "        train_train_indices = train_indices[:train_size]\n",
    "        train_val_indices = train_indices[train_size:]\n",
    "        \n",
    "        train_train_dataset = Subset(full_dataset, train_train_indices)\n",
    "        train_val_dataset = Subset(full_dataset, train_val_indices)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(train_val_dataset, batch_size=batch_size)\n",
    "        test_loader = DataLoader(test_fold_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Load fresh model for this fold\n",
    "        print(f\"Loading fresh model for fold {fold + 1}...\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        \n",
    "        best_val_f1 = 0\n",
    "        best_val_threshold = 0.5\n",
    "        best_model_state = None\n",
    "        \n",
    "        # Training loop for this fold\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_loader, desc=f\"Fold {fold+1}, Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"labels\"]\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Validation on this fold\n",
    "            val_f1, val_metrics = evaluate_legal_model(model, val_loader, tokenizer, device, use_threshold_tuning=True)\n",
    "            \n",
    "            if val_f1 > best_val_f1:\n",
    "                best_val_f1 = val_f1\n",
    "                best_val_threshold = val_metrics['threshold']\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            print(f\"Fold {fold+1}, Epoch {epoch+1}: Val F1 = {val_f1:.4f}, Best = {best_val_f1:.4f}\")\n",
    "        \n",
    "        # Load best model for testing\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # Test evaluation for this fold\n",
    "        test_f1, test_metrics = evaluate_legal_model(\n",
    "            model, test_loader, tokenizer, device,\n",
    "            use_threshold_tuning=False,\n",
    "            fix_threshold=best_val_threshold\n",
    "        )\n",
    "        \n",
    "        fold_f1_scores.append(test_f1)\n",
    "        fold_accuracies.append(test_metrics['accuracy'])\n",
    "        \n",
    "        print(f\"âœ… Fold {fold+1} Results: F1 = {test_f1:.4f}, Accuracy = {test_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Store results for this model\n",
    "    model_fold_results[model_name] = fold_f1_scores\n",
    "    model_fold_accuracy[model_name] = fold_accuracies\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {model_name} - K-Fold Summary:\")\n",
    "    print(f\"F1 Scores: {fold_f1_scores}\")\n",
    "    print(f\"Mean F1: {np.mean(fold_f1_scores):.4f} Â± {np.std(fold_f1_scores):.4f}\")\n",
    "    print(f\"Accuracy: {fold_accuracies}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(fold_accuracies):.4f} Â± {np.std(fold_accuracies):.4f}\")\n",
    "\n",
    "# ========================\n",
    "# ğŸ“Š Statistical Significance Testing with K-Fold Results\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š K-FOLD STATISTICAL SIGNIFICANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model_names = list(model_fold_results.keys())\n",
    "if len(model_names) == 2:\n",
    "    model1, model2 = model_names\n",
    "    \n",
    "    # Extract k-fold results\n",
    "    f1_scores_1 = model_fold_results[model1]\n",
    "    f1_scores_2 = model_fold_results[model2]\n",
    "    accuracy_scores_1 = model_fold_accuracy[model1]\n",
    "    accuracy_scores_2 = model_fold_accuracy[model2]\n",
    "    \n",
    "    print(f\"\\nComparing {model1} vs {model2}\")\n",
    "    print(f\"K-Fold validation with {K_FOLDS} folds\")\n",
    "    \n",
    "    # Paired t-test for F1 scores (each fold is a paired observation)\n",
    "    print(\"\\n--- F1 Score Comparison (K-Fold) ---\")\n",
    "    print(f\"{model1} - Mean F1: {np.mean(f1_scores_1):.4f} Â± {np.std(f1_scores_1):.4f}\")\n",
    "    print(f\"{model2} - Mean F1: {np.mean(f1_scores_2):.4f} Â± {np.std(f1_scores_2):.4f}\")\n",
    "    print(f\"Fold-wise F1 scores:\")\n",
    "    for i in range(K_FOLDS):\n",
    "        print(f\"  Fold {i+1}: {f1_scores_1[i]:.4f} vs {f1_scores_2[i]:.4f}\")\n",
    "    \n",
    "    f1_t_stat, f1_p_value = stats.ttest_rel(f1_scores_1, f1_scores_2)\n",
    "    print(f\"\\nPaired t-test (K-Fold): t={f1_t_stat:.4f}, p={f1_p_value:.6f}\")\n",
    "    \n",
    "    # Determine significance level\n",
    "    if f1_p_value < 0.001:\n",
    "        significance_f1 = \"***\"\n",
    "    elif f1_p_value < 0.01:\n",
    "        significance_f1 = \"**\"\n",
    "    elif f1_p_value < 0.05:\n",
    "        significance_f1 = \"*\"\n",
    "    else:\n",
    "        significance_f1 = \"ns\"\n",
    "    \n",
    "    print(f\"Significance: {significance_f1}\")\n",
    "    \n",
    "    # Effect size (Cohen's d for paired samples)\n",
    "    diff_f1 = np.array(f1_scores_1) - np.array(f1_scores_2)\n",
    "    cohens_d_f1 = np.mean(diff_f1) / np.std(diff_f1)\n",
    "    print(f\"Cohen's d (effect size): {cohens_d_f1:.4f}\")\n",
    "    \n",
    "    # Paired t-test for Accuracy\n",
    "    print(\"\\n--- Accuracy Comparison (K-Fold) ---\")\n",
    "    print(f\"{model1} - Mean Accuracy: {np.mean(accuracy_scores_1):.4f} Â± {np.std(accuracy_scores_1):.4f}\")\n",
    "    print(f\"{model2} - Mean Accuracy: {np.mean(accuracy_scores_2):.4f} Â± {np.std(accuracy_scores_2):.4f}\")\n",
    "    print(f\"Fold-wise Accuracy scores:\")\n",
    "    for i in range(K_FOLDS):\n",
    "        print(f\"  Fold {i+1}: {accuracy_scores_1[i]:.4f} vs {accuracy_scores_2[i]:.4f}\")\n",
    "    \n",
    "    acc_t_stat, acc_p_value = stats.ttest_rel(accuracy_scores_1, accuracy_scores_2)\n",
    "    print(f\"\\nPaired t-test (K-Fold): t={acc_t_stat:.4f}, p={acc_p_value:.6f}\")\n",
    "    \n",
    "    if acc_p_value < 0.001:\n",
    "        significance_acc = \"***\"\n",
    "    elif acc_p_value < 0.01:\n",
    "        significance_acc = \"**\"\n",
    "    elif acc_p_value < 0.05:\n",
    "        significance_acc = \"*\"\n",
    "    else:\n",
    "        significance_acc = \"ns\"\n",
    "    \n",
    "    print(f\"Significance: {significance_acc}\")\n",
    "    \n",
    "    # Effect size for accuracy\n",
    "    diff_acc = np.array(accuracy_scores_1) - np.array(accuracy_scores_2)\n",
    "    cohens_d_acc = np.mean(diff_acc) / np.std(diff_acc)\n",
    "    print(f\"Cohen's d (effect size): {cohens_d_acc:.4f}\")\n",
    "    \n",
    "    # Confidence intervals\n",
    "    from scipy.stats import t\n",
    "    alpha = 0.05\n",
    "    dof = K_FOLDS - 1\n",
    "    t_critical = t.ppf(1 - alpha/2, dof)\n",
    "    \n",
    "    f1_diff_mean = np.mean(diff_f1)\n",
    "    f1_diff_se = stats.sem(diff_f1)\n",
    "    f1_ci_lower = f1_diff_mean - t_critical * f1_diff_se\n",
    "    f1_ci_upper = f1_diff_mean + t_critical * f1_diff_se\n",
    "    \n",
    "    acc_diff_mean = np.mean(diff_acc)\n",
    "    acc_diff_se = stats.sem(diff_acc)\n",
    "    acc_ci_lower = acc_diff_mean - t_critical * acc_diff_se\n",
    "    acc_ci_upper = acc_diff_mean + t_critical * acc_diff_se\n",
    "    \n",
    "    print(f\"\\n95% Confidence Interval for F1 difference: [{f1_ci_lower:.4f}, {f1_ci_upper:.4f}]\")\n",
    "    print(f\"95% Confidence Interval for Accuracy difference: [{acc_ci_lower:.4f}, {acc_ci_upper:.4f}]\")\n",
    "    \n",
    "    # Wilcoxon signed-rank test (non-parametric)\n",
    "    print(\"\\n--- Non-parametric Tests ---\")\n",
    "    f1_wilcoxon_stat, f1_wilcoxon_p = stats.wilcoxon(f1_scores_1, f1_scores_2)\n",
    "    acc_wilcoxon_stat, acc_wilcoxon_p = stats.wilcoxon(accuracy_scores_1, accuracy_scores_2)\n",
    "    \n",
    "    print(f\"Wilcoxon signed-rank test (F1): p={f1_wilcoxon_p:.6f}\")\n",
    "    print(f\"Wilcoxon signed-rank test (Accuracy): p={acc_wilcoxon_p:.6f}\")\n",
    "    \n",
    "    # Summary table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ“‹ K-FOLD STATISTICAL SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Metric':<15} {'Model 1':<15} {'Model 2':<15} {'p-value':<12} {'Significance':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'F1 Score':<15} {np.mean(f1_scores_1):<15.4f} {np.mean(f1_scores_2):<15.4f} {f1_p_value:<12.6f} {significance_f1:<12}\")\n",
    "    print(f\"{'Accuracy':<15} {np.mean(accuracy_scores_1):<15.4f} {np.mean(accuracy_scores_2):<15.4f} {acc_p_value:<12.6f} {significance_acc:<12}\")\n",
    "    \n",
    "    print(f\"\\nEffect Sizes (Cohen's d):\")\n",
    "    print(f\"F1 Score: {cohens_d_f1:.4f}\")\n",
    "    print(f\"Accuracy: {cohens_d_acc:.4f}\")\n",
    "    \n",
    "    print(\"\\n*** p < 0.001, ** p < 0.01, * p < 0.05, ns = not significant\")\n",
    "    print(\"Effect size interpretation: |d| < 0.2 (small), 0.2-0.8 (medium), > 0.8 (large)\")\n",
    "    \n",
    "    # Save detailed k-fold results\n",
    "    kfold_results = pd.DataFrame({\n",
    "        'fold': range(1, K_FOLDS + 1),\n",
    "        f'{model1}_f1': f1_scores_1,\n",
    "        f'{model2}_f1': f1_scores_2,\n",
    "        f'{model1}_accuracy': accuracy_scores_1,\n",
    "        f'{model2}_accuracy': accuracy_scores_2,\n",
    "        'f1_difference': diff_f1,\n",
    "        'accuracy_difference': diff_acc\n",
    "    })\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'statistic': ['mean', 'std', 'sem', 't_stat', 'p_value', 'cohens_d'],\n",
    "        f'{model1}_f1': [np.mean(f1_scores_1), np.std(f1_scores_1), stats.sem(f1_scores_1), \n",
    "                        f1_t_stat, f1_p_value, cohens_d_f1],\n",
    "        f'{model2}_f1': [np.mean(f1_scores_2), np.std(f1_scores_2), stats.sem(f1_scores_2), \n",
    "                        f1_t_stat, f1_p_value, -cohens_d_f1],\n",
    "        f'{model1}_accuracy': [np.mean(accuracy_scores_1), np.std(accuracy_scores_1), stats.sem(accuracy_scores_1),\n",
    "                              acc_t_stat, acc_p_value, cohens_d_acc],\n",
    "        f'{model2}_accuracy': [np.mean(accuracy_scores_2), np.std(accuracy_scores_2), stats.sem(accuracy_scores_2),\n",
    "                              acc_t_stat, acc_p_value, -cohens_d_acc]\n",
    "    })\n",
    "    \n",
    "    # Save results\n",
    "    kfold_results.to_csv('kfold_detailed_results.csv', index=False)\n",
    "    summary_stats.to_csv('kfold_statistical_summary.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Results saved:\")\n",
    "    print(f\"  - Detailed k-fold results: 'kfold_detailed_results.csv'\")\n",
    "    print(f\"  - Statistical summary: 'kfold_statistical_summary.csv'\")\n",
    "\n",
    "else:\n",
    "    print(\"Note: Statistical comparison requires exactly 2 models for paired testing.\")\n",
    "\n",
    "print(f\"\\nğŸ¯ K-FOLD CROSS-VALIDATION COMPLETED\")\n",
    "print(f\"Both models evaluated on the same {K_FOLDS} folds for fair comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4gemma_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
