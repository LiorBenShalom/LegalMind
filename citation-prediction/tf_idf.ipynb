{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Loading and preprocessing data...\n",
      "ğŸ“Š Dataset shape: (2502, 7)\n",
      "ğŸ“ˆ Low range: 0.0 - 144.0\n",
      "ğŸ“ˆ High range: 2.0 - 192.0\n",
      "ğŸ”„ Training set size: 2001\n",
      "ğŸ”„ Test set size: 501\n",
      "ğŸ”¤ Creating TF-IDF features...\n",
      "ğŸ“ TF-IDF feature shape: (2001, 10000)\n",
      "ğŸ¤– Training baseline models...\n",
      "  ğŸ“Š Training Multi-output Logistic Regression...\n",
      "  âš ï¸ Logistic regression failed: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n",
      "  ğŸŒ² Training Random Forest Regressor...\n",
      "ğŸ¯ Evaluating models...\n",
      "\n",
      "ğŸ“Š Random Baseline Results:\n",
      "  ğŸ“ˆ MAE: 68.516\n",
      "  ğŸ“ˆ RMSE: 83.149\n",
      "  ğŸ“ˆ RÂ²: -24.729\n",
      "  ğŸ“Š LOW - MAE: 62.024, RÂ²: -31.220\n",
      "  ğŸ“Š HIGH - MAE: 75.009, RÂ²: -18.237\n",
      "\n",
      "ğŸ“Š Random Forest Results:\n",
      "  ğŸ“ˆ MAE: 9.135\n",
      "  ğŸ“ˆ RMSE: 13.298\n",
      "  ğŸ“ˆ RÂ²: 0.411\n",
      "  ğŸ“Š LOW - MAE: 7.052, RÂ²: 0.410\n",
      "  ğŸ“Š HIGH - MAE: 11.219, RÂ²: 0.412\n",
      "\n",
      "ğŸ” Sample Predictions (first 10 test cases):\n",
      "True Low   True High  RF Low     RF High   \n",
      "6.0        18.0       7.9        25.5      \n",
      "24.0       48.0       12.3       22.2      \n",
      "10.0       24.0       12.2       32.4      \n",
      "36.0       84.0       16.1       34.0      \n",
      "12.0       24.0       8.9        22.2      \n",
      "6.0        18.0       7.9        21.1      \n",
      "1.0        12.0       9.4        24.6      \n",
      "18.0       36.0       10.1       22.6      \n",
      "24.0       48.0       14.3       25.8      \n",
      "0.0        12.0       3.3        13.0      \n",
      "\n",
      "ğŸ’¾ Saving models to ./baseline_punishment_model...\n",
      "âœ… Models and results saved successfully!\n",
      "\n",
      "ğŸ” Top 20 Important Features (Random Forest):\n",
      "   1. ×œ×™×©×¨××œ               (importance: 0.0765)\n",
      "   2. ×”×›× ×”                 (importance: 0.0353)\n",
      "   3. ××¡×•×’ ×§×•×§××™×Ÿ          (importance: 0.0320)\n",
      "   4. ×™×™×‘×•×                (importance: 0.0202)\n",
      "   5. ××¡×•×›×Ÿ ×©×œ×            (importance: 0.0185)\n",
      "   6. ×˜×¡                   (importance: 0.0139)\n",
      "   7. ×§×•×§××™×Ÿ               (importance: 0.0139)\n",
      "   8. ×™×™×‘×•× ×¡×             (importance: 0.0126)\n",
      "   9. 144 ×¨×™×©×             (importance: 0.0122)\n",
      "  10. ×¢×                   (importance: 0.0114)\n",
      "  11. × ×•×©× ×¢××•             (importance: 0.0088)\n",
      "  12. ×—×‘×™×œ×•×ª               (importance: 0.0073)\n",
      "  13. ×”×¡×›××” ×œ×¢× ×™×™×Ÿ         (importance: 0.0059)\n",
      "  14. ×’×¨×                  (importance: 0.0054)\n",
      "  15. ×”×¨×•××™×Ÿ               (importance: 0.0050)\n",
      "  16. ×”×—×©××œ                (importance: 0.0048)\n",
      "  17. ×¢××•                  (importance: 0.0048)\n",
      "  18. ×‘××©×§×œ                (importance: 0.0047)\n",
      "  19. ×•×¢×“                  (importance: 0.0046)\n",
      "  20. ×”×¡×“×¨                 (importance: 0.0046)\n",
      "\n",
      "ğŸ“‹ SUMMARY:\n",
      "ğŸ¯ Best model: Random Forest\n",
      "ğŸ“Š Improvement over random: -86.7% MAE reduction\n",
      "ğŸ”¤ TF-IDF features: 10,000\n",
      "ğŸ“ˆ Best RÂ²: 0.411\n",
      "âœ… Model shows promise - significantly better than random!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "data_path = \"/home/liorkob/M.Sc/thesis/pre-train/punishment_prediction/merged_output.csv\"\n",
    "model_save_path = \"./baseline_punishment_model\"\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "# TF-IDF parameters\n",
    "max_features = 10000\n",
    "min_df = 2\n",
    "max_df = 0.95\n",
    "ngram_range = (1, 2)  # unigrams and bigrams\n",
    "\n",
    "print(\"ğŸ”§ Loading and preprocessing data...\")\n",
    "\n",
    "# ========== LOAD DATA ==========\n",
    "df = pd.read_csv(data_path).dropna(subset=[\"extracted_gpt_facts\", \"low\", \"high\"])\n",
    "\n",
    "# Convert targets to numeric\n",
    "df[\"low\"] = pd.to_numeric(df[\"low\"], errors='coerce')\n",
    "df[\"high\"] = pd.to_numeric(df[\"high\"], errors='coerce')\n",
    "\n",
    "# Remove rows where conversion failed\n",
    "df = df.dropna(subset=[\"low\", \"high\"])\n",
    "\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"ğŸ“ˆ Low range: {df['low'].min():.1f} - {df['low'].max():.1f}\")\n",
    "print(f\"ğŸ“ˆ High range: {df['high'].min():.1f} - {df['high'].max():.1f}\")\n",
    "\n",
    "# ========== PREPARE FEATURES AND TARGETS ==========\n",
    "X_text = df[\"extracted_gpt_facts\"].astype(str)\n",
    "y = df[[\"low\", \"high\"]].values\n",
    "\n",
    "# Split the data\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”„ Training set size: {len(X_train_text)}\")\n",
    "print(f\"ğŸ”„ Test set size: {len(X_test_text)}\")\n",
    "\n",
    "# ========== TF-IDF VECTORIZATION ==========\n",
    "print(\"ğŸ”¤ Creating TF-IDF features...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=max_features,\n",
    "    min_df=min_df,\n",
    "    max_df=max_df,\n",
    "    ngram_range=ngram_range,\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf.transform(X_test_text)\n",
    "\n",
    "print(f\"ğŸ“ TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# ========== MODEL TRAINING ==========\n",
    "print(\"ğŸ¤– Training baseline models...\")\n",
    "\n",
    "# Create save directory\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Approach 1: Multi-output Logistic Regression (treating as regression)\n",
    "print(\"  ğŸ“Š Training Multi-output Logistic Regression...\")\n",
    "logistic_model = MultiOutputRegressor(\n",
    "    LogisticRegression(max_iter=1000, random_state=random_state)\n",
    ")\n",
    "\n",
    "# Scale targets for better logistic regression performance\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "try:\n",
    "    logistic_model.fit(X_train_tfidf, y_train_scaled)\n",
    "    logistic_pred_scaled = logistic_model.predict(X_test_tfidf)\n",
    "    logistic_pred = scaler_y.inverse_transform(logistic_pred_scaled)\n",
    "    logistic_success = True\n",
    "except Exception as e:\n",
    "    print(f\"  âš ï¸ Logistic regression failed: {e}\")\n",
    "    logistic_success = False\n",
    "\n",
    "# Approach 2: Random Forest (usually more robust for this type of problem)\n",
    "print(\"  ğŸŒ² Training Random Forest Regressor...\")\n",
    "rf_model = MultiOutputRegressor(\n",
    "    RandomForestRegressor(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "rf_pred = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# ========== EVALUATION ==========\n",
    "def evaluate_predictions(y_true, y_pred, model_name):\n",
    "    print(f\"\\nğŸ“Š {model_name} Results:\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"  ğŸ“ˆ MAE: {mae:.3f}\")\n",
    "    print(f\"  ğŸ“ˆ RMSE: {rmse:.3f}\")\n",
    "    print(f\"  ğŸ“ˆ RÂ²: {r2:.3f}\")\n",
    "    \n",
    "    # Per-target metrics\n",
    "    for i, target in enumerate(['low', 'high']):\n",
    "        target_mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        target_r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "        print(f\"  ğŸ“Š {target.upper()} - MAE: {target_mae:.3f}, RÂ²: {target_r2:.3f}\")\n",
    "    \n",
    "    return mae, rmse, r2\n",
    "\n",
    "# Evaluate models\n",
    "print(\"ğŸ¯ Evaluating models...\")\n",
    "\n",
    "# Random baseline (for comparison)\n",
    "random_pred = np.random.uniform(\n",
    "    low=[y_train[:, 0].min(), y_train[:, 1].min()],\n",
    "    high=[y_train[:, 0].max(), y_train[:, 1].max()],\n",
    "    size=y_test.shape\n",
    ")\n",
    "random_mae, random_rmse, random_r2 = evaluate_predictions(y_test, random_pred, \"Random Baseline\")\n",
    "\n",
    "# Random Forest\n",
    "rf_mae, rf_rmse, rf_r2 = evaluate_predictions(y_test, rf_pred, \"Random Forest\")\n",
    "\n",
    "# Logistic Regression (if successful)\n",
    "if logistic_success:\n",
    "    log_mae, log_rmse, log_r2 = evaluate_predictions(y_test, logistic_pred, \"Logistic Regression\")\n",
    "\n",
    "# ========== SAMPLE PREDICTIONS ==========\n",
    "print(f\"\\nğŸ” Sample Predictions (first 10 test cases):\")\n",
    "print(f\"{'True Low':<10} {'True High':<10} {'RF Low':<10} {'RF High':<10}\", end=\"\")\n",
    "if logistic_success:\n",
    "    print(f\" {'LR Low':<10} {'LR High':<10}\")\n",
    "else:\n",
    "    print()\n",
    "\n",
    "for i in range(min(10, len(y_test))):\n",
    "    print(f\"{y_test[i,0]:<10.1f} {y_test[i,1]:<10.1f} {rf_pred[i,0]:<10.1f} {rf_pred[i,1]:<10.1f}\", end=\"\")\n",
    "    if logistic_success:\n",
    "        print(f\" {logistic_pred[i,0]:<10.1f} {logistic_pred[i,1]:<10.1f}\")\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "# ========== SAVE MODELS ==========\n",
    "print(f\"\\nğŸ’¾ Saving models to {model_save_path}...\")\n",
    "\n",
    "# Save the best performing model and components\n",
    "with open(f\"{model_save_path}/tfidf_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "with open(f\"{model_save_path}/random_forest_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "if logistic_success:\n",
    "    with open(f\"{model_save_path}/logistic_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(logistic_model, f)\n",
    "    with open(f\"{model_save_path}/target_scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler_y, f)\n",
    "\n",
    "# Save results summary\n",
    "results = {\n",
    "    'random_baseline': {'mae': random_mae, 'rmse': random_rmse, 'r2': random_r2},\n",
    "    'random_forest': {'mae': rf_mae, 'rmse': rf_rmse, 'r2': rf_r2}\n",
    "}\n",
    "\n",
    "if logistic_success:\n",
    "    results['logistic_regression'] = {'mae': log_mae, 'rmse': log_rmse, 'r2': log_r2}\n",
    "\n",
    "with open(f\"{model_save_path}/results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(\"âœ… Models and results saved successfully!\")\n",
    "\n",
    "# ========== FEATURE IMPORTANCE ==========\n",
    "print(f\"\\nğŸ” Top 20 Important Features (Random Forest):\")\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Get feature importance for both outputs\n",
    "importance_low = rf_model.estimators_[0].feature_importances_\n",
    "importance_high = rf_model.estimators_[1].feature_importances_\n",
    "\n",
    "# Combined importance\n",
    "combined_importance = (importance_low + importance_high) / 2\n",
    "top_indices = np.argsort(combined_importance)[-20:][::-1]\n",
    "\n",
    "for i, idx in enumerate(top_indices):\n",
    "    print(f\"  {i+1:2d}. {feature_names[idx]:<20} (importance: {combined_importance[idx]:.4f})\")\n",
    "\n",
    "# ========== SUMMARY ==========\n",
    "print(f\"\\nğŸ“‹ SUMMARY:\")\n",
    "print(f\"ğŸ¯ Best model: {'Random Forest' if rf_r2 > (log_r2 if logistic_success else -1) else 'Logistic Regression'}\")\n",
    "print(f\"ğŸ“Š Improvement over random: {((rf_mae - random_mae) / random_mae * 100):.1f}% MAE reduction\")\n",
    "print(f\"ğŸ”¤ TF-IDF features: {X_train_tfidf.shape[1]:,}\")\n",
    "print(f\"ğŸ“ˆ Best RÂ²: {max(rf_r2, log_r2 if logistic_success else rf_r2):.3f}\")\n",
    "\n",
    "if rf_r2 > 0.1:  # Arbitrary threshold for \"decent\" performance\n",
    "    print(\"âœ… Model shows promise - significantly better than random!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Model performance is limited - may need feature engineering or different approach\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judgeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
