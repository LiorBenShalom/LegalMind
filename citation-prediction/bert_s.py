import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel, AutoTokenizer
from torch.optim import Adam
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from scipy.stats import ttest_rel, t
from tqdm import tqdm
import warnings
import random
warnings.filterwarnings('ignore')

def set_seed(seed=42):
    """Set all random seeds for reproducibility"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

class CrossEncoderVerdictDataset(Dataset):
    def __init__(self, df, tokenizer, max_len=512):
        self.df = df.reset_index(drop=True)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        text = f"[CLS] {row['gpt_facts_a']} [SEP] {row['gpt_facts_b']} [SEP]"
        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': enc['input_ids'].squeeze(),
            'attention_mask': enc['attention_mask'].squeeze(),
            'label': torch.tensor(row['label'], dtype=torch.float)
        }

class CrossEncoderHeBERT(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        self.encoder = AutoModel.from_pretrained(model_name)
        hidden = self.encoder.config.hidden_size
        self.classifier = nn.Sequential(
            nn.Linear(hidden, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1)
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled = outputs.last_hidden_state[:, 0]  # [CLS] token
        return self.classifier(pooled).squeeze(-1)

def train_model_fold(model, train_loader, val_loader, optimizer, device, epochs=25, patience=5, verbose=False):
    """Train model for one fold with early stopping"""
    criterion = nn.BCEWithLogitsLoss()
    best_auc = 0
    no_improve = 0
    
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=2, verbose=verbose
    )
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        
        for batch in train_loader:
            for key in batch:
                batch[key] = batch[key].to(device)

            logits = model(batch['input_ids'], batch['attention_mask'])
            loss = criterion(logits, batch['label'])

            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            total_loss += loss.item()

        # Validation
        model.eval()
        val_probs, val_labels = [], []
        with torch.no_grad():
            for batch in val_loader:
                for key in batch:
                    batch[key] = batch[key].to(device)
                logits = model(batch['input_ids'], batch['attention_mask'])
                prob = torch.sigmoid(logits).cpu().numpy()
                label = batch['label'].cpu().numpy()
                val_probs.extend(prob)
                val_labels.extend(label)
        
        val_auc = roc_auc_score(val_labels, val_probs)
        scheduler.step(val_auc)
        
        if verbose:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}, Val AUC: {val_auc:.4f}, LR: {current_lr:.2e}")
        
        if val_auc > best_auc + 1e-4:
            best_auc = val_auc
            no_improve = 0
            best_state = model.state_dict().copy()
        else:
            no_improve += 1
            if no_improve >= patience:
                if verbose:
                    print("Early stopping triggered")
                break
    
    model.load_state_dict(best_state)
    return model, best_auc

def evaluate_model_fold(model, test_loader, device):
    """Evaluate model on test set and return AUC"""
    model.eval()
    probs, targets = [], []
    
    with torch.no_grad():
        for batch in test_loader:
            for key in batch:
                batch[key] = batch[key].to(device)
            logits = model(batch['input_ids'], batch['attention_mask'])
            prob = torch.sigmoid(logits).cpu().numpy()
            label = batch['label'].cpu().numpy()
            probs.extend(prob)
            targets.extend(label)
    
    auc = roc_auc_score(targets, probs)
    return auc

def evaluate_non_trained_model(model_name, tokenizer, test_df, device, batch_size=8):
    """Evaluate a non-trained model (only pretrained + random classifier)"""
    test_dataset = CrossEncoderVerdictDataset(test_df, tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)
    
    model = CrossEncoderHeBERT(model_name).to(device)
    auc = evaluate_model_fold(model, test_loader, device)
    
    del model
    torch.cuda.empty_cache()
    return auc

def run_clean_mlm_comparison(df_full, baseline_model_name, finetuned_model_name, k=5, epochs=25, patience=5, batch_size=8, random_state=42):
    """
    Clean MLM comparison: baseline vs fine-tuned models
    Shows: 1) Non-trained baseline performance, 2) Statistical comparison of trained models
    """
    
    set_seed(random_state)
    
    # Initialize tokenizers
    baseline_tokenizer = AutoTokenizer.from_pretrained(baseline_model_name)
    finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_name)
    
    # Set up k-fold
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)
    
    # Results storage
    baseline_trained_aucs = []
    finetuned_trained_aucs = []
    baseline_non_trained_aucs = []
    fold_results = []
    
    print(f"ğŸš€ Starting Clean {k}-Fold MLM Analysis")
    print(f"Baseline Model: {baseline_model_name}")
    print(f"Fine-tuned Model: {finetuned_model_name}")
    print("=" * 80)
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(df_full, df_full['label'])):
        print(f"\nğŸ“ FOLD {fold + 1}/{k}")
        print("-" * 40)
        
        set_seed(random_state + fold)
        
        # Split data
        df_train_fold = df_full.iloc[train_idx].reset_index(drop=True)
        df_test_fold = df_full.iloc[test_idx].reset_index(drop=True)
        
        # Further split training into train/val (80/20)
        train_size = int(0.8 * len(df_train_fold))
        df_train = df_train_fold[:train_size]
        df_val = df_train_fold[train_size:]
        
        print(f"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test_fold)}")
        
        # === NON-TRAINED BASELINE (for reference) ===
        if fold == 0:  # Only do this once as reference
            print("\nğŸ”µ Evaluating Non-Trained Baseline (Reference)...")
            baseline_non_trained_auc = evaluate_non_trained_model(
                baseline_model_name, baseline_tokenizer, df_test_fold, device, batch_size
            )
            baseline_non_trained_aucs.append(baseline_non_trained_auc)
            print(f"Non-Trained Baseline AUC: {baseline_non_trained_auc:.4f} (Reference)")
        
        # === TRAINED BASELINE MODEL ===
        print("\nğŸ”µ Training Baseline Model...")
        
        train_dataset_base = CrossEncoderVerdictDataset(df_train, baseline_tokenizer)
        val_dataset_base = CrossEncoderVerdictDataset(df_val, baseline_tokenizer)
        test_dataset_base = CrossEncoderVerdictDataset(df_test_fold, baseline_tokenizer)
        
        train_loader_base = DataLoader(train_dataset_base, batch_size=batch_size, shuffle=True)
        val_loader_base = DataLoader(val_dataset_base, batch_size=batch_size)
        test_loader_base = DataLoader(test_dataset_base, batch_size=batch_size)
        
        baseline_model = CrossEncoderHeBERT(baseline_model_name).to(device)
        baseline_optimizer = Adam(baseline_model.parameters(), lr=1e-5, weight_decay=1e-4)
        
        baseline_model, baseline_val_auc = train_model_fold(
            baseline_model, train_loader_base, val_loader_base, 
            baseline_optimizer, device, epochs, patience, verbose=False
        )
        
        baseline_trained_auc = evaluate_model_fold(baseline_model, test_loader_base, device)
        baseline_trained_aucs.append(baseline_trained_auc)
        print(f"Trained Baseline - Val AUC: {baseline_val_auc:.4f}, Test AUC: {baseline_trained_auc:.4f}")
        
        del baseline_model, baseline_optimizer
        torch.cuda.empty_cache()
        
        # === TRAINED FINE-TUNED MODEL ===
        set_seed(random_state + fold)
        print("\nğŸŸ¢ Training Fine-tuned Model...")
        
        train_dataset_ft = CrossEncoderVerdictDataset(df_train, finetuned_tokenizer)
        val_dataset_ft = CrossEncoderVerdictDataset(df_val, finetuned_tokenizer)
        test_dataset_ft = CrossEncoderVerdictDataset(df_test_fold, finetuned_tokenizer)
        
        train_loader_ft = DataLoader(train_dataset_ft, batch_size=batch_size, shuffle=True)
        val_loader_ft = DataLoader(val_dataset_ft, batch_size=batch_size)
        test_loader_ft = DataLoader(test_dataset_ft, batch_size=batch_size)
        
        finetuned_model = CrossEncoderHeBERT(finetuned_model_name).to(device)
        finetuned_optimizer = Adam(finetuned_model.parameters(), lr=1e-5, weight_decay=1e-4)
        
        finetuned_model, finetuned_val_auc = train_model_fold(
            finetuned_model, train_loader_ft, val_loader_ft, 
            finetuned_optimizer, device, epochs, patience, verbose=False
        )
        
        finetuned_trained_auc = evaluate_model_fold(finetuned_model, test_loader_ft, device)
        finetuned_trained_aucs.append(finetuned_trained_auc)
        print(f"Trained Fine-tuned - Val AUC: {finetuned_val_auc:.4f}, Test AUC: {finetuned_trained_auc:.4f}")
        
        # Calculate improvement
        improvement = finetuned_trained_auc - baseline_trained_auc
        print(f"Improvement: {improvement:+.4f}")
        
        # Store fold results
        fold_results.append({
            'fold': fold + 1,
            'baseline_trained_auc': baseline_trained_auc,
            'finetuned_trained_auc': finetuned_trained_auc,
            'improvement': improvement,
            'baseline_val_auc': baseline_val_auc,
            'finetuned_val_auc': finetuned_val_auc
        })
        
        del finetuned_model, finetuned_optimizer
        torch.cuda.empty_cache()
    
    # === STATISTICAL ANALYSIS ===
    print("\n" + "=" * 80)
    print("ğŸ“Š STATISTICAL ANALYSIS")
    print("=" * 80)
    
    baseline_trained_aucs = np.array(baseline_trained_aucs)
    finetuned_trained_aucs = np.array(finetuned_trained_aucs)
    improvements = finetuned_trained_aucs - baseline_trained_aucs
    
    # Summary statistics
    print(f"\nğŸ“ˆ Summary Statistics:")
    if baseline_non_trained_aucs:
        print(f"Non-Trained Baseline (Reference): {baseline_non_trained_aucs[0]:.4f}")
    print(f"Trained Baseline:     {baseline_trained_aucs.mean():.4f} Â± {baseline_trained_aucs.std():.4f}")
    print(f"Trained Fine-tuned:   {finetuned_trained_aucs.mean():.4f} Â± {finetuned_trained_aucs.std():.4f}")
    print(f"Mean Improvement:     {improvements.mean():+.4f} Â± {improvements.std():.4f}")
    
    # THE ONLY T-TEST: Baseline Trained vs Fine-tuned Trained
    t_stat, p_value = ttest_rel(finetuned_trained_aucs, baseline_trained_aucs)
    
    print(f"\nğŸ§ª Paired T-Test Results (Trained Models Only):")
    print(f"t-statistic: {t_stat:.4f}")
    print(f"p-value: {p_value:.2e}")
    
    # Significance interpretation
    alpha = 0.05
    if p_value < alpha:
        significance = "âœ… SIGNIFICANT"
        interpretation = f"MLM fine-tuning provides statistically significant improvement (p < {alpha})"
    else:
        significance = "âŒ NOT SIGNIFICANT"
        interpretation = f"MLM fine-tuning improvement is not statistically significant (p â‰¥ {alpha})"
    
    print(f"Result: {significance}")
    print(f"Interpretation: {interpretation}")
    
    # Effect size (Cohen's d)
    pooled_std = np.sqrt((baseline_trained_aucs.var() + finetuned_trained_aucs.var()) / 2)
    cohens_d = improvements.mean() / pooled_std
    print(f"Effect Size (Cohen's d): {cohens_d:.4f}")
    
    # Effect size interpretation
    if abs(cohens_d) < 0.2:
        effect_size_interp = "negligible"
    elif abs(cohens_d) < 0.5:
        effect_size_interp = "small"
    elif abs(cohens_d) < 0.8:
        effect_size_interp = "medium"
    else:
        effect_size_interp = "large"
    
    print(f"Effect Size Interpretation: {effect_size_interp}")
    
    # Fold-wise results table
    print(f"\nğŸ“‹ Fold-wise Results:")
    print("Fold | Baseline | Fine-tuned | Improvement | Base Val | FT Val")
    print("-" * 70)
    for result in fold_results:
        print(f"{result['fold']:4d} | {result['baseline_trained_auc']:8.4f} | {result['finetuned_trained_auc']:10.4f} | "
              f"{result['improvement']:+10.4f} | {result['baseline_val_auc']:8.4f} | {result['finetuned_val_auc']:6.4f}")
    
    # Confidence interval
    confidence_level = 0.95
    df_ci = len(improvements) - 1
    t_critical = t.ppf((1 + confidence_level) / 2, df_ci)
    margin_error = t_critical * (improvements.std() / np.sqrt(len(improvements)))
    ci_lower = improvements.mean() - margin_error
    ci_upper = improvements.mean() + margin_error
    
    print(f"\nğŸ¯ {confidence_level*100}% Confidence Interval for Mean Improvement:")
    print(f"[{ci_lower:+.4f}, {ci_upper:+.4f}]")
    
    return {
        'baseline_trained_aucs': baseline_trained_aucs,
        'finetuned_trained_aucs': finetuned_trained_aucs,
        'baseline_non_trained_reference': baseline_non_trained_aucs[0] if baseline_non_trained_aucs else None,
        'improvements': improvements,
        't_statistic': t_stat,
        'p_value': p_value,
        'cohens_d': cohens_d,
        'effect_size_interpretation': effect_size_interp,
        'mean_improvement': improvements.mean(),
        'std_improvement': improvements.std(),
        'confidence_interval': (ci_lower, ci_upper),
        'fold_results': fold_results,
        'is_significant': p_value < 0.05
    }

# === MAIN EXECUTION ===
if __name__ == "__main__":
    set_seed(42)
    
    # Load datasets
    print("ğŸ“‚ Loading datasets...")
    df_train = pd.read_csv("/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv")
    df_val = pd.read_csv("/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv")
    df_test = pd.read_csv("/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv")
    
    df_full = pd.concat([df_train, df_val, df_test], ignore_index=True)
    print(f"Total dataset size: {len(df_full)} samples")
    print(f"Label distribution: {df_full['label'].value_counts().to_dict()}")
    
    # Define experiments
    experiments = [
        {
            "name": "HeBERT MLM vs Baseline",
            "baseline": "avichr/heBERT",
            "finetuned": "/home/liorkob/M.Sc/thesis/pre-train/models/hebert-mlm-3k-drugs/final"
        },
        {
            "name": "Legal-HeBERT MLM vs Baseline", 
            "baseline": "avichr/Legal-heBERT",
            "finetuned": "/home/liorkob/M.Sc/thesis/pre-train/models/Legal-heBERT-mlm-3k-drugs/final"
        },
        {
            "name": "mBERT MLM vs Baseline",
            "baseline": "bert-base-multilingual-cased",
            "finetuned": "/home/liorkob/M.Sc/thesis/pre-train/models/mBERT-mlm-3k-drugs/final"
        }
    ]
    
    # Run clean experiments
    all_results = {}
    
    for exp in experiments:
        print(f"\n\nğŸš¨ Running Clean Experiment: {exp['name']}")
        print("=" * 100)
        
        results = run_clean_mlm_comparison(
            df_full=df_full,
            baseline_model_name=exp["baseline"],
            finetuned_model_name=exp["finetuned"],
            k=5,
            epochs=25,
            patience=5,
            batch_size=8,
            random_state=42
        )
        
        all_results[exp['name']] = results
        
        print(f"\nğŸ“Œ {exp['name']} CLEAN SUMMARY:")
        if results['baseline_non_trained_reference']:
            print(f"Non-Trained Reference: {results['baseline_non_trained_reference']:.4f} AUC")
        print(f"Mean AUC improvement: {results['mean_improvement']:+.4f}")
        print(f"P-value: {results['p_value']:.2e}")
        if results['is_significant']:
            print("âœ… Statistically significant improvement!")
        else:
            print("âŒ Not statistically significant.")
        
        print("\n" + "="*50 + " END OF CLEAN EXPERIMENT " + "="*50)
    
    # Final clean summary
    print(f"\n\nğŸ† FINAL CLEAN SUMMARY")
    print("=" * 80)
    print("ğŸ“‹ Clean MLM Fine-tuning Results:")
    print()
    print("| Model | Non-Trained | Baseline | Fine-tuned | Improvement | p-value | Cohen's d | Significant |")
    print("|-------|-------------|----------|------------|-------------|---------|-----------|-------------|")
    
    for name, results in all_results.items():
        model_short = name.split()[0]
        non_trained = results['baseline_non_trained_reference'] if results['baseline_non_trained_reference'] else 0.50
        baseline_mean = results['baseline_trained_aucs'].mean()
        finetuned_mean = results['finetuned_trained_aucs'].mean()
        improvement = results['mean_improvement']
        p_val = results['p_value']
        cohens_d = results['cohens_d']
        significant = "âœ…" if results['is_significant'] else "âŒ"
        
        print(f"| {model_short} | {non_trained:.3f} | {baseline_mean:.3f} | {finetuned_mean:.3f} | {improvement:+.3f} | {p_val:.3f} | {cohens_d:.3f} | {significant} |")
    
    print(f"\nğŸ¯ Key Findings:")
    significant_models = [name for name, results in all_results.items() if results['is_significant']]
    if significant_models:
        print(f"âœ… Significant improvements found in: {', '.join(significant_models)}")
    else:
        print("âŒ No statistically significant improvements found")
    
    print(f"\nğŸ“Š All models start around 0.50 AUC without training (random performance)")
    print(f"ğŸ“ˆ Training provides massive improvement (~0.30+ AUC)")
    print(f"ğŸ”¬ MLM fine-tuning provides additional small improvements where significant")