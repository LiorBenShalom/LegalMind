{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install umap-learn\n",
    "# !pip install ipywidgets\n",
    "# !pip install Pillow\n",
    "# !pip install umap\n",
    "# !pip install gensim\n",
    "# !pip install \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load HeBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "model = AutoModel.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Mean pooling over token embeddings (excluding special tokens)\n",
    "    attention = inputs['attention_mask'].unsqueeze(-1)\n",
    "    embedding = (outputs.last_hidden_state * attention).sum(1) / attention.sum(1)\n",
    "    return embedding.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "verdict_paragraphs = df[\"extracted_gpt_facts\"].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([get_embedding(text) for text in verdict_paragraphs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 top words: ['הנאשם', 'של', 'סם', 'גרם', 'מסוכן', 'מסוג', 'במשקל', 'סעיף', 'לפי', 'קנבוס']\n",
      "Cluster 1 top words: ['הנאשם', 'של', 'לפי', 'סם', 'מסוכן', 'סעיף', 'על', 'במשקל', 'ביום', 'גרם']\n",
      "Cluster 2 top words: ['הנאשם', 'של', 'את', 'על', 'המתלונן', 'החבילה', 'עם', 'סם', 'כי', 'סעיף']\n",
      "Cluster 3 top words: ['הנאשם', 'של', 'סם', 'מסוג', 'סעיף', 'לפי', 'על', 'מסוכן', 'גרם', 'במשקל']\n",
      "Cluster 4 top words: ['הנאשם', 'של', 'הסוכן', 'נאשם', 'את', 'מסוג', 'סם', 'מסוכן', 'גרם', 'על']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for i in range(5):\n",
    "    cluster_texts = [text for text, label in zip(verdict_paragraphs, labels) if label == i]\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(cluster_texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    mean_scores = np.asarray(X.mean(axis=0)).flatten()\n",
    "    top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "    top_words = [terms[ind] for ind in top_indices]\n",
    "    print(f\"Cluster {i} top words:\", top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 top words: ['הנאשם', 'של', 'סם', 'גרם', 'מסוכן', 'מסוג', 'במשקל', 'סעיף', 'לפי', 'קנבוס']\n",
      "Cluster 1 top words: ['הנאשם', 'של', 'לפי', 'סם', 'מסוכן', 'סעיף', 'על', 'במשקל', 'ביום', 'גרם']\n",
      "Cluster 2 top words: ['הנאשם', 'של', 'את', 'על', 'המתלונן', 'החבילה', 'עם', 'סם', 'כי', 'סעיף']\n",
      "Cluster 3 top words: ['הנאשם', 'של', 'סם', 'מסוג', 'סעיף', 'לפי', 'על', 'מסוכן', 'גרם', 'במשקל']\n",
      "Cluster 4 top words: ['הנאשם', 'של', 'הסוכן', 'נאשם', 'את', 'מסוג', 'סם', 'מסוכן', 'גרם', 'על']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "k=5\n",
    "for i in range(k):\n",
    "    cluster_texts = [text for text, label in zip(verdict_paragraphs, labels) if label == i]\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(cluster_texts)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    mean_scores = np.asarray(X.mean(axis=0)).flatten()\n",
    "    top_indices = mean_scores.argsort()[-10:][::-1]\n",
    "    top_words = [terms[ind] for ind in top_indices]\n",
    "    print(f\"Cluster {i} top words:\", top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 2 -words cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 unique words: ['הנאשם', 'ביום', 'האישום', 'הסוכן', 'סמים', 'לו', 'לאחר', 'כתב', 'הורשע', 'לנאשם']\n",
      "Cluster 1 unique words: ['המפגש', 'כדין', 'טלפוני', 'ובתמורה', 'תיאום', 'לסך', 'בביתו', 'שוחחו', 'לביתו', 'ימכור']\n",
      "Cluster 2 unique words: ['הסמים', 'סעיף', 'לפקודת', 'המסוכנים', 'שלא', 'עבירות', 'סחר', 'עבירה', 'עצמית', 'לצריכה']\n",
      "Cluster 3 unique words: ['בסם', 'הודאתו', 'בעובדות', 'לרכוש', 'קנאביס', 'טיעון', 'עובדות', 'מתוקן', 'השניים', 'העביר']\n",
      "Cluster 4 unique words: ['סם', 'מסוג', 'מסוכן', 'גרם', 'במשקל', 'נטו', 'קוקאין', 'קנבוס', 'מכר', 'תמורת']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "# Function to reconstruct full Hebrew words\n",
    "def get_full_words(tokens):\n",
    "    words = []\n",
    "    current = \"\"\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"##\"):\n",
    "            current += tok[2:]\n",
    "        else:\n",
    "            if current:\n",
    "                words.append(current)\n",
    "            current = tok\n",
    "    if current:\n",
    "        words.append(current)\n",
    "    return words\n",
    "\n",
    "# Hebrew stopwords (custom list)\n",
    "hebrew_stopwords = {\n",
    "    'של', 'על', 'את', 'כי', 'עם', 'זה', 'גם', 'אם', 'או', 'היה', 'היא', 'הוא', 'הם' ,\"פי\", \"לו\"\n",
    "    'אבל', 'אני', 'אנחנו', 'אתם', 'אתן', 'אין', 'כל', 'לא', 'כן', 'יש', 'מה', 'מי', 'בו',\n",
    "    'כך', 'לפי', 'ללא', 'וכן', 'עד', 'רק', 'כמו', 'מאוד', 'זאת', 'הזו', 'אותו', 'אותה'\n",
    "}\n",
    "\n",
    "def is_meaningful(word):\n",
    "    return bool(re.search(r'[א-ת]', word)) and not word.isdigit() and len(word) >= 2\n",
    "\n",
    "# 1. Load and tokenize\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "verdict_paragraphs = df[\"extracted_gpt_facts\"].dropna().tolist()\n",
    "\n",
    "tokenized_sentences = []\n",
    "for p in verdict_paragraphs:\n",
    "    tokens = tokenizer.tokenize(p)\n",
    "    full_words = get_full_words(tokens)\n",
    "    filtered = [w for w in full_words if is_meaningful(w) and w not in hebrew_stopwords]\n",
    "    tokenized_sentences.append(filtered)\n",
    "\n",
    "# 2. Train Word2Vec\n",
    "w2v_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=5)\n",
    "\n",
    "# 3. Choose words to cluster\n",
    "words = list(w2v_model.wv.index_to_key)[:500]  # top N frequent words\n",
    "word_vectors = [w2v_model.wv[word] for word in words]\n",
    "\n",
    "# 4. Cluster\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init='auto')  # n_init='auto' for sklearn >= 1.2\n",
    "labels = kmeans.fit_predict(word_vectors)\n",
    "\n",
    "# 5. Group words by cluster\n",
    "clusters = defaultdict(list)\n",
    "for word, label in zip(words, labels):\n",
    "    clusters[label].append(word)\n",
    "\n",
    "# 6. Print top words per cluster\n",
    "# for cluster_id in sorted(clusters.keys()):\n",
    "#     print(f\"Cluster {cluster_id} top words: {clusters[cluster_id][:10]}\")\n",
    "from collections import Counter\n",
    "\n",
    "# Flatten all cluster words to count frequency\n",
    "all_words = [word for word_list in clusters.values() for word in word_list]\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Keep only words that appear in one cluster\n",
    "unique_clusters = {\n",
    "    cid: [w for w in word_list if word_counts[w] == 1][:10]  # top 10 unique words\n",
    "    for cid, word_list in clusters.items()\n",
    "}\n",
    "\n",
    "# Print unique words per cluster\n",
    "for cluster_id in sorted(unique_clusters.keys()):\n",
    "    print(f\"Cluster {cluster_id} unique words: {unique_clusters[cluster_id]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try 3 - embedding api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index 30: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens, however you requested 9287 tokens (9287 in your prompt; 0 for the completion). Please reduce your prompt; or completion length.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [01:26<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 5:\n",
      "- הנאשם הורשע בתיק זה, במסגרת הכרעת דין נפרדת שניתנה בתאריך 10.9.23 בביצוע העבירות הבאות: במסגרת האישום השלישי שעניינו ניסיון התנקשות בזאב רוזנשטיין בתאריך 30.6.03, כפי שעוד יפורט – בביצוע עבירה של הספק ...\n",
      "- הנאשם הורשע על פי הודאתו ובמסגרת הסדר טיעון בעובדות כתב האישום המתוקן שעניינן עבירות של ניסיון לייבוא סם מסוכן – עבירה לפי סעיף 13 לפקודת הסמים המסוכנים [נוסח חדש] התשל\"ג-1973 והחזקת סם מסוכן שלא לצרי ...\n",
      "\n",
      "Cluster 2:\n",
      "- נאשם 1 (להלן: \"הנאשם\") הורשע על יסוד הודאתו במסגרת הסדר טיעון, לאחר ניהול חלקי של הוכחות, בארבע עבירות של סחר בסם מסוכן לפי סעיף 13 בצירוף סעיף 19א לפקודת הסמים המסוכנים [נוסח חדש], תשל\"ג-1973. עיקר ע ...\n",
      "- הנאשם הורשע על פי הודאתו ובמסגרת הסדר טיעון בעובדות כתב האישום המתוקן ובתיק המצורף בכתב האישום המתוקן אף הוא. במועדים הרלוונטיים לאירועים שיפורטו להלן, שימש נ.פ. 163-21 כסוכן סמוי של משטרת ישראל, כשמט ...\n",
      "\n",
      "Cluster 4:\n",
      "- ביום 9.1.24 הורשע הנאשם לאחר שמיעת ראיות, בהכרעת דין מפורטת, בעבירות של ייבוא סם מסוכן, לפי סעיף 13 בצירוף סעיף 19א לפקודת הסמים המסוכנים [נוסח חדש], תשל\"ג-1973, החזקת סם מסוכן שלא לצריכה עצמית לפי סע ...\n",
      "- הנאשם 1 (להלן – \"הנאשם\"), הורשע על יסוד הודאתו בעובדות כתב אישום מתוקן, בעבירות שלהלן: יבוא סמים מסוכנים – עבירה לפי סעיפים 13 ו-19א לפקודת הסמים המסוכנים [נוסח חדש], תשל\"ג-1973; אספקת סם – עבירה לפי  ...\n",
      "\n",
      "Cluster 6:\n",
      "- הנאשם הורשע לאחר ניהול הוכחות, בעבירות הבאות: ייצור, הכנה והפקת סמים מסוכנים, לפי סעיף 6 לפקודת הסמים המסוכנים [נוסח חדש], התשל\"ג-1973; החזקת סם שלא לצריכה עצמית, לפי סעיף 7(א) ו-(ג) רישא לפקודה; החזק ...\n",
      "- הנאשם 2 מחמוד אדכידק הורשע על פי הודאתו, במסגרת הסדר טיעון, בכתב אישום מתוקן, בעבירות של סחיטה באיומים, לפי סעיף 428 רישא לחוק העונשין, תשל\"ז-1977, וסחיטה בכוח לפי סעיף 427 רישא לחוק. בנוסף, צירף הנאש ...\n",
      "\n",
      "Cluster 3:\n",
      "- הנאשם הורשע במסגרת הסדר דיוני בעובדות כתב אישום מתוקן בעבירות ייצור, הכנה והפקת סם מסוכן לפי סעיף 6 לפקודת הסמים המסוכנים, התשל\"ג-1973, ונטילת חשמל, לפי סעיף 400 לחוק העונשין, תשל\"ז-1977. כמפורט בעובד ...\n",
      "- נאשם 3 הודה בכתב אישום מתוקן במסגרת הסדר חלקי שלא כלל הסדר אודות העונש, בעבירות של סיוע ליצור, הכנה והפקה לפי סעיף 6 לפקודת הסמים המסוכנים [נוסח חדש], תשל\"ג-1973 וסיוע לנטילת חשמל לפי סעיף 400 + 31 לח ...\n",
      "\n",
      "Cluster 0:\n",
      "- הנאשם הודה בכתב האישום המתוקן בביצוע תשע עבירות סחר בסמים, לפי סעיפים 13 ו-19א לפקודת הסמים המסוכנים; החזקת סם שלא לצריכה עצמית לפי סעיפים 7(א)(ג) רישא לפקודה, ונהיגה בשכרות, לפי סעיף 39א לפקודת התעבו ...\n",
      "- הנאשם הורשע על פי הודאתו בעבירות הבאות: 2 עבירות של סחר בסמים בצוותא, לפי סעיפים 13 ו-19א לפקודת הסמים המסוכנים בצירוף סעיף 29(א) לחוק העונשין, התשל\"ז-1977; החזקת סם שלא לצריכה עצמית, לפי סעיפים 7(א)( ...\n",
      "\n",
      "Cluster 1:\n",
      "- במסגרת הסדר טיעון הורשע הנאשם על פי הודאתו בשני כתבי אישום מתוקנים והופנה לשירות המבחן לקבלת תסקיר. לא גובשה הסכמה לעניין העונש. ת\"פ 59578-12-21: בתאריך 23.10.19 רכש הנאשם קטנוע שנגנב על ידי אחר, בתמו ...\n",
      "- הנאשם הורשע על פי הודאתו, בעבירות המיוחסות לו בכתב אישום מתוקן בעבירות של הספקת סם מסוכן (ריבוי עבירות) – לפי סעיפים 13 + 19א לפקודת הסמים המסוכנים, (נוסח חדש) תשל\"ג – 1973, נהיגה בזמן פסילה – לפי סעי ...\n",
      "\n",
      "🔹 Cluster 5 top TF-IDF words: ['את', 'גרם', 'הנאשם', 'כי', 'לפי', 'סם', 'סעיף', 'על', 'עם', 'של']\n",
      "\n",
      "🔹 Cluster 2 top TF-IDF words: ['את', 'במשקל', 'גרם', 'הנאשם', 'הסוכן', 'כי', 'מסוג', 'מסוכן', 'סם', 'של']\n",
      "\n",
      "🔹 Cluster 4 top TF-IDF words: ['את', 'ביום', 'הנאשם', 'הסמים', 'מסוג', 'מסוכן', 'סם', 'על', 'עם', 'של']\n",
      "\n",
      "🔹 Cluster 6 top TF-IDF words: ['במשקל', 'גרם', 'הנאשם', 'הסמים', 'לפי', 'מסוג', 'מסוכן', 'סם', 'על', 'של']\n",
      "\n",
      "🔹 Cluster 3 top TF-IDF words: ['את', 'הנאשם', 'לפי', 'מסוג', 'מסוכן', 'סם', 'סעיף', 'על', 'קנבוס', 'של']\n",
      "\n",
      "🔹 Cluster 0 top TF-IDF words: ['ביום', 'גרם', 'הנאשם', 'כי', 'מהאישום', 'מכר', 'עולה', 'קנביס', 'של', 'תמורת']\n",
      "\n",
      "🔹 Cluster 1 top TF-IDF words: ['2019', 'ביום', 'במשקל', 'גרם', 'הנאשם', 'מסוג', 'מסוכן', 'סם', 'של', 'תמורת']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "key = \"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\"  # replace with your key\n",
    "client = OpenAI(api_key=key)\n",
    "# Load data\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "verdicts = df[\"extracted_gpt_facts\"].dropna().tolist()\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Get embeddings (can be slow — use tqdm for progress bar)\n",
    "from tqdm import tqdm\n",
    "embeddings = []\n",
    "\n",
    "for i, v in enumerate(tqdm(verdicts)):\n",
    "    try:\n",
    "        emb = get_embedding(v)\n",
    "        if emb is None:\n",
    "            print(f\"Warning: embedding failed at index {i}\")\n",
    "            emb = [0] * 1536\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")\n",
    "        emb = [0] * 1536\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Remove None entries\n",
    "valid_pairs = [(v, e) for v, e in zip(verdicts, embeddings) if e is not None]\n",
    "verdicts_clean, embeddings_clean = zip(*valid_pairs)\n",
    "\n",
    "# Cluster\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings_clean)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster_to_verdicts = defaultdict(list)\n",
    "for label, verdict in zip(labels, verdicts_clean):\n",
    "    cluster_to_verdicts[label].append(verdict)\n",
    "\n",
    "# Print top 2 examples per cluster\n",
    "for cluster_id, v_list in cluster_to_verdicts.items():\n",
    "    print(f\"\\nCluster {cluster_id}:\")\n",
    "    for example in v_list[:2]:\n",
    "        print(\"-\", example[:200], \"...\")\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# תוצאה: מילים בולטות לכל אשכול\n",
    "for cluster_id, texts in cluster_to_verdicts.items():\n",
    "    vectorizer = TfidfVectorizer(max_features=10, stop_words='english')  # אין סטופ וורדס לעברית, אז תוכל להסיר ידנית\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    top_words = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\n🔹 Cluster {cluster_id} top TF-IDF words: {list(top_words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdicts over 8192 tokens: 3/354\n",
      "Verdict names with too many tokens:\n",
      "- SH-20-02-48152-11\n",
      "- SH-22-11-64232-785\n",
      "- SH-22-11-64232-785\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "# Count how many verdicts exceed 8192 tokens\n",
    "too_long_count = 0\n",
    "token_counts = []\n",
    "\n",
    "for text in verdicts:\n",
    "    tokens = encoding.encode(text)\n",
    "    token_counts.append(len(tokens))\n",
    "    if len(tokens) > 8192:\n",
    "        too_long_count += 1\n",
    "\n",
    "print(f\"Verdicts over 8192 tokens: {too_long_count}/{len(verdicts)}\")\n",
    "long_indices = []\n",
    "long_verdicts = []\n",
    "\n",
    "for i, text in enumerate(verdicts):\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > 8192:\n",
    "        long_indices.append(i)\n",
    "        long_verdicts.append(text)\n",
    "\n",
    "# Assuming df and verdicts are aligned (i.e., df[\"extracted_gpt_facts\"].dropna() == verdicts)\n",
    "long_verdict_names = df[\"verdict\"].dropna().iloc[long_indices].tolist()\n",
    "\n",
    "print(\"Verdict names with too many tokens:\")\n",
    "for name in long_verdict_names:\n",
    "    print(\"-\", name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
