{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for citation based data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "tag_dir = '/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdicts_tagged_citations'\n",
    "gpt_facts_path = '/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv'\n",
    "output_path = '/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdict_pairs_with_similarity.csv'\n",
    "\n",
    "# Load verdict facts\n",
    "verdict_facts = pd.read_csv(gpt_facts_path)\n",
    "facts_dict = dict(zip(verdict_facts['verdict'], verdict_facts['extracted_gpt_facts']))\n",
    "\n",
    "# Collect data\n",
    "rows = []\n",
    "for file in os.listdir(tag_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        verdict_a = file.replace('.csv', '')\n",
    "        file_path = os.path.join(tag_dir, file)\n",
    "        \n",
    "        # Skip empty files\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            if row['predicted_label'] == 1:\n",
    "                verdict_b = row['citation']\n",
    "                a_facts = facts_dict.get(verdict_a, \"\")\n",
    "                b_facts = facts_dict.get(verdict_b, \"\")\n",
    "                rows.append([verdict_a, a_facts, verdict_b, b_facts, 1])\n",
    "\n",
    "# Save result\n",
    "result_df = pd.DataFrame(rows, columns=[\n",
    "    'verdict_a_name', 'verdict_a_extracted_gpt_facts',\n",
    "    'verdict_b_name', 'verdict_b_extracted_gpt_facts', 'similarity_score'\n",
    "])\n",
    "result_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### genrate non similar pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ========== Setup ==========\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ========== Load Similar Pairs ==========\n",
    "df_pairs = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/valid_pairs_with_log.csv\")\n",
    "valid_pairs = df_pairs[df_pairs[\"log\"] != \"missing cited verdict\"].copy()\n",
    "similar_pairs = set(tuple(sorted([a, b])) for a, b in zip(valid_pairs[\"verdict_a\"], valid_pairs[\"verdict_b\"]))\n",
    "\n",
    "# Build similarity graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(similar_pairs)\n",
    "\n",
    "# ========== Load Facts from All Sources ==========\n",
    "facts1 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\")  \n",
    "# facts2 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_appeals_with_gpt_2.csv\")\n",
    "\n",
    "pairs_facts_a = df_pairs[[\"verdict_a\", \"gpt_facts_a\"]].rename(columns={\"verdict_a\": \"verdict\", \"gpt_facts_a\": \"extracted_gpt_facts\"})\n",
    "pairs_facts_b = df_pairs[[\"verdict_b\", \"gpt_facts_b\"]].rename(columns={\"verdict_b\": \"verdict\", \"gpt_facts_b\": \"extracted_gpt_facts\"})\n",
    "all_facts_df = pd.concat([facts1, pairs_facts_a, pairs_facts_b])\n",
    "\n",
    "# all_facts_df = pd.concat([facts1, facts2, pairs_facts_a, pairs_facts_b])\n",
    "all_facts_df = all_facts_df.dropna(subset=[\"verdict\", \"extracted_gpt_facts\"]).drop_duplicates(subset=\"verdict\")\n",
    "facts_dict = dict(zip(all_facts_df[\"verdict\"], all_facts_df[\"extracted_gpt_facts\"]))\n",
    "\n",
    "# ========== Prepare Verdict Sets ==========\n",
    "verdicts_in_pairs = set(valid_pairs[\"verdict_a\"]) | set(valid_pairs[\"verdict_b\"])\n",
    "all_verdicts = set(all_facts_df[\"verdict\"])\n",
    "extra_verdicts = list(all_verdicts - verdicts_in_pairs)\n",
    "combined_verdicts = list(verdicts_in_pairs | set(extra_verdicts))\n",
    "\n",
    "# ========== Generate Candidate Non-Similar Pairs ==========\n",
    "def generate_candidate_pairs(verdicts, existing_pairs, G, num_pairs):\n",
    "    non_similar = set()\n",
    "    attempts = 0\n",
    "    max_attempts = num_pairs * 20\n",
    "\n",
    "    while len(non_similar) < num_pairs and attempts < max_attempts:\n",
    "        a, b = random.sample(list(verdicts), 2)\n",
    "        pair = tuple(sorted([a, b]))\n",
    "        attempts += 1\n",
    "\n",
    "        if pair in existing_pairs or G.has_edge(*pair):\n",
    "            continue\n",
    "\n",
    "        # ✅ Only check transitive path if both nodes exist in graph\n",
    "        if a in G and b in G:\n",
    "            if nx.has_path(G, a, b):\n",
    "                continue\n",
    "\n",
    "        non_similar.add(pair)\n",
    "\n",
    "    return list(non_similar)\n",
    "\n",
    "# ========== GPT Verification ==========\n",
    "def verify_with_gpt(pair, facts_dict):\n",
    "    print(f\"Checking: {pair}, verified so far: {len(verified_non_similar)}\")\n",
    "\n",
    "    fact_a = facts_dict.get(pair[0], \"\")\n",
    "    fact_b = facts_dict.get(pair[1], \"\")\n",
    "\n",
    "    if not fact_a or not fact_b:\n",
    "        return False\n",
    "    prompt = f\"\"\"אתה עוזר משפטי. תפקידך לבדוק האם שתי מערכות עובדתיות מתארות **מצבים משפטיים דומים**, כך שניתן יהיה להסתמך על האחד לצורך גזירת העונש בשני.\n",
    "\n",
    "התייחס רק לנסיבות שקשורות ישירות לביצוע העבירה – כגון סוג העבירה, מהות המעשה, אופן הביצוע, משך הזמן, כוונה פלילית, ומאפיינים רלוונטיים של הנאשם או הקורבן *שנוגעים למעשה עצמו*.\n",
    "\n",
    "מאפיינים כלליים כמו גיל הנאשם, מקום המגורים או תוצאה מקרית שאינה נובעת מהמעשה – אינם רלוונטיים לדמיון המשפטי.\n",
    "\n",
    "ענה רק \"כן\" או \"לא\". אל תסביר. אל תוסיף סימני פיסוק.\n",
    "\n",
    "עובדות א: {fact_a}\n",
    "עובדות ב: {fact_b}\n",
    "תשובה:\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to analyz legal text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        print(answer)\n",
    "        return answer == \"לא\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"🚨 GPT API error: {e}\")\n",
    "        return \n",
    "\n",
    "# ========== Main Loop ==========\n",
    "target_count = 2000\n",
    "verified_non_similar = set()\n",
    "used_pairs = set()\n",
    "\n",
    "pbar = tqdm(total=target_count, desc=\"🔍 Verifying with GPT\")\n",
    "\n",
    "while len(verified_non_similar) < target_count:\n",
    "    needed = (target_count - len(verified_non_similar)) * 2\n",
    "    candidate_pairs = generate_candidate_pairs(combined_verdicts, similar_pairs | used_pairs, G, needed)\n",
    "\n",
    "    for pair in candidate_pairs:\n",
    "        if pair in used_pairs:\n",
    "            continue\n",
    "        used_pairs.add(pair)\n",
    "\n",
    "        if verify_with_gpt(pair, facts_dict):\n",
    "            verified_non_similar.add(pair)\n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ========== Save Output with Facts ==========\n",
    "pairs_list = []\n",
    "for a, b in verified_non_similar:\n",
    "    pairs_list.append({\n",
    "        \"verdict_a\": a,\n",
    "        \"verdict_b\": b,\n",
    "        \"gpt_facts_a\": facts_dict.get(a, \"\"),\n",
    "        \"gpt_facts_b\": facts_dict.get(b, \"\"),\n",
    "        \"label\": 0\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(pairs_list)\n",
    "output_df.to_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verified_non_similar_pairs_2.csv\", index=False)\n",
    "print(f\"\\n✅ Saved {len(output_df)} pairs with facts to verified_non_similar_pairs_2.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSVs\n",
    "similar = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/valid_pairs_with_log.csv\")\n",
    "non_similar = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verified_non_similar_pairs_2.csv\")\n",
    "\n",
    "# Filter relevant rows\n",
    "similar = similar[similar[\"log\"] != \"missing cited verdict\"].copy()\n",
    "similar[\"label\"] = 1\n",
    "\n",
    "non_similar = non_similar[non_similar[\"label\"] == 0].copy()\n",
    "non_similar[\"label\"] = 0\n",
    "\n",
    "# Select only required columns\n",
    "cols = [\"verdict_a\", \"verdict_b\", \"gpt_facts_a\", \"gpt_facts_b\", \"label\"]\n",
    "similar = similar[cols]\n",
    "non_similar = non_similar[cols]\n",
    "\n",
    "print(f\"✅ Similar pairs: {len(similar)}\")\n",
    "print(f\"✅ Non-similar pairs: {len(non_similar)}\")\n",
    "\n",
    "# Normalize order to detect duplicates\n",
    "def normalize_pair(row):\n",
    "    a, b = row['verdict_a'], row['verdict_b']\n",
    "    return tuple(sorted((a, b)))\n",
    "\n",
    "similar['pair_key'] = similar.apply(normalize_pair, axis=1)\n",
    "non_similar['pair_key'] = non_similar.apply(normalize_pair, axis=1)\n",
    "\n",
    "# Combine and remove duplicates\n",
    "data = pd.concat([similar, non_similar], ignore_index=True)\n",
    "print(f\"📦 Total before removing duplicates: {len(data)}\")\n",
    "\n",
    "data = data.drop_duplicates(subset='pair_key').drop(columns='pair_key')\n",
    "print(f\"🧹 Total after removing duplicates: {len(data)}\")\n",
    "\n",
    "# Shuffle\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "data.to_csv(\"data_pairs_5k.csv\", index=False)\n",
    "\n",
    "# Print distribution\n",
    "print(\"📊 Label distribution:\")\n",
    "print(data[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to test-tain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Load ===\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "df_pos = df[df['label'] == 1]\n",
    "df_neg = df[df['label'] == 0]\n",
    "df_pos_train, df_pos_val = train_test_split(df_pos, test_size=0.3, random_state=42)\n",
    "df_neg_train, df_neg_val = train_test_split(df_neg, test_size=0.3, random_state=42)\n",
    "df_train = pd.concat([df_pos_train, df_neg_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val = pd.concat([df_pos_val, df_neg_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.5, random_state=42, stratify=df_val['label'])\n",
    "\n",
    "# Save splits\n",
    "df_train.to_csv(\"crossencoder_train.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_val.to_csv(\"crossencoder_val.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_test.to_csv(\"crossencoder_test.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEncoderHeBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cross-Encoder Dataset ---\n",
    "class CrossEncoderVerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = f\"[CLS] {row['gpt_facts_a']} [SEP] {row['gpt_facts_b']} [SEP]\"\n",
    "        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# --- Cross-Encoder Model ---\n",
    "class CrossEncoderHeBERT(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        return self.classifier(pooled).squeeze(-1)\n",
    "\n",
    "# --- Training Loop ---\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_cross_encoder_with_early_stopping(model, train_loader, val_loader, optimizer, device, epochs=10, patience=3):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_auc = 0\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = criterion(logits, batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label'].long()).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_probs, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for key in batch:\n",
    "                    batch[key] = batch[key].to(device)\n",
    "                logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                label = batch['label'].cpu().numpy()\n",
    "                val_probs.extend(prob)\n",
    "                val_labels.extend(label)\n",
    "        val_auc = roc_auc_score(val_labels, val_probs)\n",
    "        print(f\"Epoch {epoch+1} | Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_crossencoder_ft_mlm.pt\")\n",
    "            print(\"✅ New best model saved.\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"⏹️ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def train_cross_encoder(model, dataloader, optimizer, device, epochs=15):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = criterion(logits, batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label'].long()).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(dataloader):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0\n",
    "    for t in np.linspace(0.01, 0.99, 100):\n",
    "        preds = (np.array(y_probs) >= t).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_cross_encoder(model, dataloader, device):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "\n",
    "    preds = (np.array(probs) >= 0.5).astype(int)\n",
    "    print(f\"[Default @0.5] AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "    print(f\"[Default @0.5] F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Precision: {precision_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "    best_thresh, best_f1 = find_best_threshold(targets, probs)\n",
    "    best_preds = (np.array(probs) >= best_thresh).astype(int)\n",
    "    print(f\"🔍 Best threshold: {best_thresh:.4f}\")\n",
    "    print(f\"[Best] F1 Score: {f1_score(targets, best_preds):.4f}\")\n",
    "    print(f\"[Best] Precision: {precision_score(targets, best_preds):.4f}\")\n",
    "    print(f\"[Best] Recall: {recall_score(targets, best_preds):.4f}\")\n",
    "    \n",
    "    return probs, targets, best_thresh  \n",
    "\n",
    "def evaluate_with_threshold(model, dataloader, device, threshold):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "    print(\"-------test-------\")       \n",
    "    preds = (np.array(probs) >= 0.5).astype(int)\n",
    "    print(f\"[Default @0.5] AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "    print(f\"[Default @0.5] F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Precision: {precision_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "    preds = (np.array(probs) >= threshold).astype(int)\n",
    "    print(f\"[Test @{threshold:.4f}] AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "    print(f\"[Test @{threshold:.4f}] F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "    print(f\"[Test @{threshold:.4f}] Precision: {precision_score(targets, preds):.4f}\")\n",
    "    print(f\"[Test @{threshold:.4f}] Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "# # --- Usage ---\n",
    "# model_name = \"/home/liorkob/M.Sc/thesis/pre-train/mlm/Legal-heBERT-mlm-3k-drugs/final\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/liorkob/M.Sc/thesis/pre-train/mlm/Legal-heBERT-mlm-3k-drugs/final\")\n",
    "\n",
    "# model_name = \"/home/liorkob/M.Sc/thesis/pre-train/hebert-mlm-3k-drugs-punishment\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/liorkob/M.Sc/thesis/pre-train/hebert-mlm-3k-drugs-punishment\")\n",
    "\n",
    "\n",
    "# model_name =\"avichr/Legal-heBERT\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"avichr/Legal-heBERT\")\n",
    "# # df_train, df_val = train_test_split(df, stratify=df.label, test_size=0.2, random_state=42)\n",
    "# df_train = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\")\n",
    "# df_val = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\")\n",
    "# df_test= pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\")\n",
    "\n",
    "\n",
    "# train_dataset = CrossEncoderVerdictDataset(df_train, tokenizer)\n",
    "# val_dataset = CrossEncoderVerdictDataset(df_val, tokenizer)\n",
    "# test_dataset = CrossEncoderVerdictDataset(df_test, tokenizer)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# model = CrossEncoderHeBERT(model_name).to(device)\n",
    "# optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# train_cross_encoder_with_early_stopping(model, train_loader, val_loader, optimizer, device, epochs=20, patience=5)\n",
    "\n",
    "# print(\"✅ CrossEncoderHeBERT model saved.\")\n",
    "# # model.load_state_dict(torch.load(\"best_crossencoder.pt\"))  \n",
    "\n",
    "# _, _, best_thresh = evaluate_cross_encoder(model, val_loader, device)\n",
    "# evaluate_with_threshold(model, test_loader, device, best_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from scipy.stats import ttest_rel\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Your existing classes (unchanged) ---\n",
    "class CrossEncoderVerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = f\"[CLS] {row['gpt_facts_a']} [SEP] {row['gpt_facts_b']} [SEP]\"\n",
    "        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class CrossEncoderHeBERT(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        return self.classifier(pooled).squeeze(-1)\n",
    "\n",
    "# --- Modified training function for k-fold ---\n",
    "def train_model_fold(model, train_loader, val_loader, optimizer, device, epochs=15, patience=3, verbose=False):\n",
    "    \"\"\"Train model for one fold with early stopping\"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_auc = 0\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = criterion(logits, batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_probs, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for key in batch:\n",
    "                    batch[key] = batch[key].to(device)\n",
    "                logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                label = batch['label'].cpu().numpy()\n",
    "                val_probs.extend(prob)\n",
    "                val_labels.extend(label)\n",
    "        \n",
    "        val_auc = roc_auc_score(val_labels, val_probs)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}, Val AUC: {val_auc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            no_improve = 0\n",
    "            best_state = model.state_dict().copy()\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                if verbose:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "def evaluate_model_fold(model, test_loader, device):\n",
    "    \"\"\"Evaluate model on test set and return AUC\"\"\"\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "    \n",
    "    auc = roc_auc_score(targets, probs)\n",
    "    return auc, probs, targets\n",
    "\n",
    "def run_kfold_comparison(df_full, baseline_model_name, finetuned_model_name, k=5, epochs=15, patience=3, batch_size=8, random_state=42):\n",
    "    \"\"\"\n",
    "    Run k-fold cross-validation comparing baseline vs fine-tuned model\n",
    "    \n",
    "    Args:\n",
    "        df_full: Full dataset DataFrame\n",
    "        baseline_model_name: Name/path of baseline model\n",
    "        finetuned_model_name: Name/path of fine-tuned model\n",
    "        k: Number of folds\n",
    "        epochs: Max epochs per fold\n",
    "        patience: Early stopping patience\n",
    "        batch_size: Batch size\n",
    "        random_state: Random seed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize tokenizers\n",
    "    baseline_tokenizer = AutoTokenizer.from_pretrained(baseline_model_name)\n",
    "    finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_name)\n",
    "    \n",
    "    # Set up k-fold\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    baseline_aucs = []\n",
    "    finetuned_aucs = []\n",
    "    fold_results = []\n",
    "    \n",
    "    print(f\"🚀 Starting {k}-Fold Cross-Validation\")\n",
    "    print(f\"Baseline Model: {baseline_model_name}\")\n",
    "    print(f\"Fine-tuned Model: {finetuned_model_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(df_full, df_full['label'])):\n",
    "        print(f\"\\n📁 FOLD {fold + 1}/{k}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Split data\n",
    "        df_train_fold = df_full.iloc[train_idx].reset_index(drop=True)\n",
    "        df_test_fold = df_full.iloc[test_idx].reset_index(drop=True)\n",
    "        \n",
    "        # Further split training into train/val (80/20)\n",
    "        train_size = int(0.8 * len(df_train_fold))\n",
    "        df_train = df_train_fold[:train_size]\n",
    "        df_val = df_train_fold[train_size:]\n",
    "        \n",
    "        print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test_fold)}\")\n",
    "        \n",
    "        # === BASELINE MODEL ===\n",
    "        print(\"\\n🔵 Training Baseline Model...\")\n",
    "        \n",
    "        # Create datasets and loaders for baseline\n",
    "        train_dataset_base = CrossEncoderVerdictDataset(df_train, baseline_tokenizer)\n",
    "        val_dataset_base = CrossEncoderVerdictDataset(df_val, baseline_tokenizer)\n",
    "        test_dataset_base = CrossEncoderVerdictDataset(df_test_fold, baseline_tokenizer)\n",
    "        \n",
    "        train_loader_base = DataLoader(train_dataset_base, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_base = DataLoader(val_dataset_base, batch_size=batch_size)\n",
    "        test_loader_base = DataLoader(test_dataset_base, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize and train baseline model\n",
    "        baseline_model = CrossEncoderHeBERT(baseline_model_name).to(device)\n",
    "        baseline_optimizer = Adam(baseline_model.parameters(), lr=2e-5)\n",
    "        \n",
    "        baseline_model = train_model_fold(\n",
    "            baseline_model, train_loader_base, val_loader_base, \n",
    "            baseline_optimizer, device, epochs, patience, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate baseline\n",
    "        baseline_auc, _, _ = evaluate_model_fold(baseline_model, test_loader_base, device)\n",
    "        baseline_aucs.append(baseline_auc)\n",
    "        print(f\"Baseline AUC: {baseline_auc:.4f}\")\n",
    "        \n",
    "        # Clean up baseline model\n",
    "        del baseline_model, baseline_optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # === FINE-TUNED MODEL ===\n",
    "        print(\"\\n🟢 Training Fine-tuned Model...\")\n",
    "        \n",
    "        # Create datasets and loaders for fine-tuned model\n",
    "        train_dataset_ft = CrossEncoderVerdictDataset(df_train, finetuned_tokenizer)\n",
    "        val_dataset_ft = CrossEncoderVerdictDataset(df_val, finetuned_tokenizer)\n",
    "        test_dataset_ft = CrossEncoderVerdictDataset(df_test_fold, finetuned_tokenizer)\n",
    "        \n",
    "        train_loader_ft = DataLoader(train_dataset_ft, batch_size=batch_size, shuffle=True)\n",
    "        val_loader_ft = DataLoader(val_dataset_ft, batch_size=batch_size)\n",
    "        test_loader_ft = DataLoader(test_dataset_ft, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize and train fine-tuned model\n",
    "        finetuned_model = CrossEncoderHeBERT(finetuned_model_name).to(device)\n",
    "        finetuned_optimizer = Adam(finetuned_model.parameters(), lr=2e-5)\n",
    "        \n",
    "        finetuned_model = train_model_fold(\n",
    "            finetuned_model, train_loader_ft, val_loader_ft, \n",
    "            finetuned_optimizer, device, epochs, patience, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Evaluate fine-tuned\n",
    "        finetuned_auc, _, _ = evaluate_model_fold(finetuned_model, test_loader_ft, device)\n",
    "        finetuned_aucs.append(finetuned_auc)\n",
    "        print(f\"Fine-tuned AUC: {finetuned_auc:.4f}\")\n",
    "        \n",
    "        # Calculate improvement\n",
    "        improvement = finetuned_auc - baseline_auc\n",
    "        print(f\"Improvement: {improvement:+.4f}\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_results.append({\n",
    "            'fold': fold + 1,\n",
    "            'baseline_auc': baseline_auc,\n",
    "            'finetuned_auc': finetuned_auc,\n",
    "            'improvement': improvement\n",
    "        })\n",
    "        \n",
    "        # Clean up fine-tuned model\n",
    "        del finetuned_model, finetuned_optimizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # === STATISTICAL ANALYSIS ===\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 STATISTICAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    baseline_aucs = np.array(baseline_aucs)\n",
    "    finetuned_aucs = np.array(finetuned_aucs)\n",
    "    improvements = finetuned_aucs - baseline_aucs\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n📈 Summary Statistics:\")\n",
    "    print(f\"Baseline AUC:    {baseline_aucs.mean():.4f} ± {baseline_aucs.std():.4f}\")\n",
    "    print(f\"Fine-tuned AUC:  {finetuned_aucs.mean():.4f} ± {finetuned_aucs.std():.4f}\")\n",
    "    print(f\"Mean Improvement: {improvements.mean():+.4f} ± {improvements.std():.4f}\")\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = ttest_rel(finetuned_aucs, baseline_aucs)\n",
    "    \n",
    "    print(f\"\\n🧪 Paired T-Test Results:\")\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.2e}\")\n",
    "    \n",
    "    # Significance interpretation\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        significance = \"✅ SIGNIFICANT\"\n",
    "        interpretation = f\"The improvement is statistically significant (p < {alpha})\"\n",
    "    else:\n",
    "        significance = \"❌ NOT SIGNIFICANT\"\n",
    "        interpretation = f\"The improvement is not statistically significant (p ≥ {alpha})\"\n",
    "    \n",
    "    print(f\"Result: {significance}\")\n",
    "    print(f\"Interpretation: {interpretation}\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((baseline_aucs.var() + finetuned_aucs.var()) / 2)\n",
    "    cohens_d = improvements.mean() / pooled_std\n",
    "    print(f\"Effect Size (Cohen's d): {cohens_d:.4f}\")\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_size_interp = \"negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_size_interp = \"small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_size_interp = \"medium\"\n",
    "    else:\n",
    "        effect_size_interp = \"large\"\n",
    "    \n",
    "    print(f\"Effect Size Interpretation: {effect_size_interp}\")\n",
    "    \n",
    "    # Fold-wise results table\n",
    "    print(f\"\\n📋 Fold-wise Results:\")\n",
    "    print(\"Fold | Baseline | Fine-tuned | Improvement\")\n",
    "    print(\"-\" * 45)\n",
    "    for result in fold_results:\n",
    "        print(f\"{result['fold']:4d} | {result['baseline_auc']:8.4f} | {result['finetuned_auc']:10.4f} | {result['improvement']:+10.4f}\")\n",
    "    \n",
    "    # Confidence interval for mean improvement\n",
    "    from scipy.stats import t\n",
    "    confidence_level = 0.95\n",
    "    df_ci = len(improvements) - 1\n",
    "    t_critical = t.ppf((1 + confidence_level) / 2, df_ci)\n",
    "    margin_error = t_critical * (improvements.std() / np.sqrt(len(improvements)))\n",
    "    ci_lower = improvements.mean() - margin_error\n",
    "    ci_upper = improvements.mean() + margin_error\n",
    "    \n",
    "    print(f\"\\n🎯 {confidence_level*100}% Confidence Interval for Mean Improvement:\")\n",
    "    print(f\"[{ci_lower:+.4f}, {ci_upper:+.4f}]\")\n",
    "    \n",
    "    return {\n",
    "        'baseline_aucs': baseline_aucs,\n",
    "        'finetuned_aucs': finetuned_aucs,\n",
    "        'improvements': improvements,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'mean_improvement': improvements.mean(),\n",
    "        'std_improvement': improvements.std(),\n",
    "        'confidence_interval': (ci_lower, ci_upper),\n",
    "        'fold_results': fold_results\n",
    "    }\n",
    "\n",
    "# # === MAIN EXECUTION ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load your full dataset\n",
    "#     print(\"📂 Loading datasets...\")\n",
    "#     df_train = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\")\n",
    "#     df_val = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\")\n",
    "#     df_test = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\")\n",
    "    \n",
    "#     # Combine all splits for k-fold CV\n",
    "#     df_full = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "#     print(f\"Total dataset size: {len(df_full)} samples\")\n",
    "#     print(f\"Label distribution: {df_full['label'].value_counts().to_dict()}\")\n",
    "    \n",
    "#     # Define model names/paths\n",
    "#     baseline_model_name = \"avichr/Legal-heBERT\"  # Your baseline\n",
    "    \n",
    "#     # Choose your fine-tuned model (uncomment the one you want to test)\n",
    "#     finetuned_model_name = \"/home/liorkob/M.Sc/thesis/pre-train/mlm/Legal-heBERT-mlm-3k-drugs/final\"\n",
    "#     # finetuned_model_name = \"/home/liorkob/M.Sc/thesis/pre-train/hebert-mlm-3k-drugs-punishment\"\n",
    "    \n",
    "#     # Run k-fold comparison\n",
    "#     results = run_kfold_comparison(\n",
    "#         df_full=df_full,\n",
    "#         baseline_model_name=baseline_model_name,\n",
    "#         finetuned_model_name=finetuned_model_name,\n",
    "#         k=5,  # 5-fold CV\n",
    "#         epochs=15,\n",
    "#         patience=3,\n",
    "#         batch_size=8,\n",
    "#         random_state=42\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\n🏁 Analysis Complete!\")\n",
    "#     print(f\"Your fine-tuned HeBERT shows a mean improvement of {results['mean_improvement']:+.4f} AUC-ROC\")\n",
    "    \n",
    "#     if results['p_value'] < 0.05:\n",
    "#         print(\"🎉 The improvement is statistically significant!\")\n",
    "#     else:\n",
    "#         print(\"⚠️  The improvement is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading datasets...\n",
      "Total dataset size: 5791 samples\n",
      "Label distribution: {0: 3857, 1: 1934}\n",
      "\n",
      "\n",
      "🚨 Running Experiment: HeBERT MLM vs Baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting 5-Fold Cross-Validation\n",
      "Baseline Model: avichr/heBERT\n",
      "Fine-tuned Model: /home/liorkob/M.Sc/thesis/pre-train/models/hebert-mlm-3k-drugs/final\n",
      "================================================================================\n",
      "\n",
      "📁 FOLD 1/5\n",
      "----------------------------------------\n",
      "Train: 3705, Val: 927, Test: 1159\n",
      "\n",
      "🔵 Training Baseline Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline AUC: 0.8258\n",
      "\n",
      "🟢 Training Fine-tuned Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/liorkob/M.Sc/thesis/pre-train/models/hebert-mlm-3k-drugs/final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"HeBERT MLM vs Baseline\",\n",
    "        \"baseline\": \"avichr/heBERT\",\n",
    "        \"finetuned\": \"/home/liorkob/M.Sc/thesis/pre-train/models/hebert-mlm-3k-drugs/final\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mBERT MLM vs Baseline\",\n",
    "        \"baseline\": \"bert-base-multilingual-cased\",\n",
    "        \"finetuned\": \"/home/liorkob/M.Sc/thesis/pre-train/models/mBERT-mlm-3k-drugs/final\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Legal-HeBERT MLM vs Baseline\",\n",
    "        \"baseline\": \"avichr/Legal-heBERT\",\n",
    "        \"finetuned\": \"/home/liorkob/M.Sc/thesis/pre-train/models/Legal-heBERT-mlm-3k-drugs/final\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for exp in experiments:\n",
    "        # Load your full dataset\n",
    "    print(\"📂 Loading datasets...\")\n",
    "    df_train = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\")\n",
    "    df_val = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\")\n",
    "    df_test = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\")\n",
    "    \n",
    "    # Combine all splits for k-fold CV\n",
    "    df_full = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "    print(f\"Total dataset size: {len(df_full)} samples\")\n",
    "    print(f\"Label distribution: {df_full['label'].value_counts().to_dict()}\")\n",
    "\n",
    "    print(f\"\\n\\n🚨 Running Experiment: {exp['name']}\")\n",
    "    results = run_kfold_comparison(\n",
    "        df_full=df_full,\n",
    "        baseline_model_name=exp[\"baseline\"],\n",
    "        finetuned_model_name=exp[\"finetuned\"],\n",
    "        k=5,\n",
    "        epochs=15,\n",
    "        patience=3,\n",
    "        batch_size=8,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"📌 {exp['name']} mean AUC improvement: {results['mean_improvement']:+.4f}\")\n",
    "    if results[\"p_value\"] < 0.05:\n",
    "        print(\"✅ Statistically significant improvement!\\n\")\n",
    "    else:\n",
    "        print(\"❌ Not statistically significant.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Loading model: /home/liorkob/M.Sc/thesis/t5/het5-mlm-final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Scoring: 100%|██████████| 218/218 [00:15<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Threshold: 4.7972 (F1: 0.5074)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Eval: 100%|██████████| 218/218 [00:16<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test Results:\n",
      "F1: 0.5074365704286964\n",
      "Precision: 0.3403755868544601\n",
      "Recall: 0.9965635738831615\n",
      "Accuracy: 0.3528735632183908\n",
      "AUC-ROC: 0.5129622705339815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.03      0.06       579\n",
      "           1       0.34      1.00      0.51       291\n",
      "\n",
      "    accuracy                           0.35       870\n",
      "   macro avg       0.64      0.51      0.28       870\n",
      "weighted avg       0.74      0.35      0.21       870\n",
      "\n",
      "\n",
      "📦 Loading model: imvladikon/het5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/liorkob/.conda/envs/4gemma_v2/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Scoring: 100%|██████████| 218/218 [00:15<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best Threshold: 3.5125 (F1: 0.5030)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Eval: 100%|██████████| 218/218 [00:15<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Test Results:\n",
      "F1: 0.5030461270670148\n",
      "Precision: 0.3368298368298368\n",
      "Recall: 0.993127147766323\n",
      "Accuracy: 0.34367816091954023\n",
      "AUC-ROC: 0.5051991524669267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.02      0.03       579\n",
      "           1       0.34      0.99      0.50       291\n",
      "\n",
      "    accuracy                           0.34       870\n",
      "   macro avg       0.59      0.51      0.27       870\n",
      "weighted avg       0.67      0.34      0.19       870\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========================\n",
    "# CONFIG\n",
    "# ========================\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 4\n",
    "max_len = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========================\n",
    "# Dataset\n",
    "# ========================\n",
    "class LegalSentencingCitationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        prompt = \"\"\"מערכת חיזוי ציטוטים משפטיים מתמחה\n",
    "תחום: דין פלילי - מדיניות ענישה\n",
    "מטרה: חיזוי ציטוטים בין פסקי דין על בסיס דמיון בעובדות כתב האישום\n",
    "קריטריונים: ציטוט רלוונטי אם הוא תומך בהחלטת טווח העונש\n",
    "שאלה: בהתבסס על עובדות כתב האישום, האם צפוי שפסק דין א' יצטט פסק דין ב'?\"\"\"\n",
    "\n",
    "        self.inputs = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = f\"\"\"{prompt}\n",
    "\n",
    "עובדות כתב אישום - פסק דין א':\n",
    "{row['gpt_facts_a']}\n",
    "\n",
    "עובדות כתב אישום - פסק דין ב':\n",
    "{row['gpt_facts_b']}\n",
    "\n",
    "על בסיס דמיון העבירות והנסיבות, האם פסק דין א' יצטט פסק דין ב'?\"\"\"\n",
    "            self.inputs.append(text)\n",
    "\n",
    "        self.targets = df[\"label\"].apply(lambda l: \"כן\" if l == 1 else \"לא\").tolist()\n",
    "        self.labels = df[\"label\"].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_enc = self.tokenizer(\n",
    "            self.inputs[idx], padding='max_length', truncation=True,\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            self.targets[idx], padding='max_length', truncation=True,\n",
    "            max_length=5, return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = target_enc[\"input_ids\"].squeeze(0)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "            \"numeric_label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# ========================\n",
    "# Inference Function\n",
    "# ========================\n",
    "def classify_with_threshold_search(model, tokenizer, input_ids, attention_mask, threshold=0.0):\n",
    "    with torch.no_grad():\n",
    "        decoder_input_ids = torch.full((input_ids.shape[0], 1),\n",
    "                                       tokenizer.pad_token_id, dtype=torch.long, device=input_ids.device)\n",
    "        logits = model(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       decoder_input_ids=decoder_input_ids).logits[:, -1, :]\n",
    "\n",
    "        yes_tokens = [259, 1903]\n",
    "        no_tokens = [1124]\n",
    "\n",
    "        predictions, scores = [], []\n",
    "        for batch_logits in logits:\n",
    "            yes_score = torch.mean(batch_logits[yes_tokens]).item()\n",
    "            no_score = torch.mean(batch_logits[no_tokens]).item()\n",
    "            score_diff = yes_score - no_score\n",
    "            predictions.append(1 if score_diff > threshold else 0)\n",
    "            scores.append(score_diff)\n",
    "        return predictions, scores\n",
    "\n",
    "def find_best_threshold(model, tokenizer, dataloader, labels):\n",
    "    all_diffs = []\n",
    "    for batch in tqdm(dataloader, desc=\"Scoring\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        _, diffs = classify_with_threshold_search(model, tokenizer,\n",
    "                                                  batch[\"input_ids\"],\n",
    "                                                  batch[\"attention_mask\"])\n",
    "        all_diffs.extend(diffs)\n",
    "\n",
    "    best_f1, best_th = 0, 0\n",
    "    for th in np.linspace(min(all_diffs), max(all_diffs), 50):\n",
    "        preds = []\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            batch_preds, _ = classify_with_threshold_search(model, tokenizer,\n",
    "                                                            batch[\"input_ids\"],\n",
    "                                                            batch[\"attention_mask\"],\n",
    "                                                            threshold=th)\n",
    "            preds.extend(batch_preds)\n",
    "        f1 = f1_score(labels, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_th = th\n",
    "\n",
    "    print(f\"✅ Best Threshold: {best_th:.4f} (F1: {best_f1:.4f})\")\n",
    "    return best_th\n",
    "\n",
    "def evaluate(model_path):\n",
    "    print(f\"\\n📦 Loading model: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    df_test = pd.read_csv(test_file)\n",
    "    test_dataset = LegalSentencingCitationDataset(df_test, tokenizer, max_len=max_len)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    true_labels = test_dataset.labels\n",
    "    best_th = find_best_threshold(model, tokenizer, test_loader, true_labels)\n",
    "\n",
    "    all_preds = []\n",
    "    for batch in tqdm(test_loader, desc=\"Final Eval\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        preds, _ = classify_with_threshold_search(model, tokenizer,\n",
    "                                                  batch[\"input_ids\"],\n",
    "                                                  batch[\"attention_mask\"],\n",
    "                                                  threshold=best_th)\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "    print(\"\\n📊 Test Results:\")\n",
    "    print(\"F1:\", f1_score(true_labels, all_preds))\n",
    "    print(\"Precision:\", precision_score(true_labels, all_preds, zero_division=0))\n",
    "    print(\"Recall:\", recall_score(true_labels, all_preds, zero_division=0))\n",
    "    print(\"Accuracy:\", np.mean(np.array(all_preds) == true_labels))\n",
    "    if len(set(all_preds)) > 1:\n",
    "        print(\"AUC-ROC:\", roc_auc_score(true_labels, all_preds))\n",
    "    print(classification_report(true_labels, all_preds))\n",
    "\n",
    "# ========================\n",
    "# Run on baseline models\n",
    "# ========================\n",
    "if __name__ == \"__main__\":\n",
    "    baseline_models = [\n",
    "        \"/home/liorkob/M.Sc/thesis/t5/het5-mlm-final\",\n",
    "        \"imvladikon/het5-base\"\n",
    "    ]\n",
    "    for path in baseline_models:\n",
    "        evaluate(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "\n",
      "====================\n",
      "🔍 Evaluating: HeBERT (baseline)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.49111807732497387\n",
      "Precision: 0.35285285285285284\n",
      "Recall: 0.8075601374570447\n",
      "Accuracy: 0.44022988505747124\n",
      "AUC-ROC: 0.548706443744102\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.26      0.38       579\n",
      "         1.0       0.35      0.81      0.49       291\n",
      "\n",
      "    accuracy                           0.44       870\n",
      "   macro avg       0.54      0.53      0.43       870\n",
      "weighted avg       0.60      0.44      0.42       870\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "🔍 Evaluating: HeBERT-MLM-3K-Drugs (fine-tuned)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/liorkob/M.Sc/thesis/pre-train/models/hebert-mlm-3k-drugs/final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.300187617260788\n",
      "Precision: 0.3305785123966942\n",
      "Recall: 0.27491408934707906\n",
      "Accuracy: 0.5712643678160919\n",
      "AUC-ROC: 0.48208191632688185\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.72      0.69       579\n",
      "         1.0       0.33      0.27      0.30       291\n",
      "\n",
      "    accuracy                           0.57       870\n",
      "   macro avg       0.50      0.50      0.50       870\n",
      "weighted avg       0.55      0.57      0.56       870\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "🔍 Evaluating: mBERT (baseline)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5012919896640826\n",
      "Precision: 0.33448275862068966\n",
      "Recall: 1.0\n",
      "Accuracy: 0.33448275862068966\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00       579\n",
      "         1.0       0.33      1.00      0.50       291\n",
      "\n",
      "    accuracy                           0.33       870\n",
      "   macro avg       0.17      0.50      0.25       870\n",
      "weighted avg       0.11      0.33      0.17       870\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "🔍 Evaluating: mBERT-MLM-3K-Drugs (fine-tuned)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/liorkob/M.Sc/thesis/pre-train/models/mBERT-mlm-3k-drugs/final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Accuracy: 0.6655172413793103\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      1.00      0.80       579\n",
      "         1.0       0.00      0.00      0.00       291\n",
      "\n",
      "    accuracy                           0.67       870\n",
      "   macro avg       0.33      0.50      0.40       870\n",
      "weighted avg       0.44      0.67      0.53       870\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "🔍 Evaluating: Legal-HeBERT (baseline)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/Legal-heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.38605898123324395\n",
      "Precision: 0.31648351648351647\n",
      "Recall: 0.4948453608247423\n",
      "Accuracy: 0.4735632183908046\n",
      "AUC-ROC: 0.49145938310512854\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.46      0.54       579\n",
      "         1.0       0.32      0.49      0.39       291\n",
      "\n",
      "    accuracy                           0.47       870\n",
      "   macro avg       0.48      0.48      0.46       870\n",
      "weighted avg       0.54      0.47      0.49       870\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "🔍 Evaluating: Legal-HeBERT-MLM-3K-Drugs (fine-tuned)\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at /home/liorkob/M.Sc/thesis/pre-train/models/Legal-heBERT-mlm-3k-drugs/final and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.3780663780663781\n",
      "Precision: 0.32587064676616917\n",
      "Recall: 0.45017182130584193\n",
      "Accuracy: 0.5045977011494253\n",
      "AUC-ROC: 0.47610526503213857\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.53      0.59       579\n",
      "         1.0       0.33      0.45      0.38       291\n",
      "\n",
      "    accuracy                           0.50       870\n",
      "   macro avg       0.49      0.49      0.48       870\n",
      "weighted avg       0.55      0.50      0.52       870\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_file = \"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\"\n",
    "batch_size = 8\n",
    "max_len = 512\n",
    "\n",
    "models_to_evaluate = [\n",
    "    (\"avichr/heBERT\", \"HeBERT (baseline)\"),\n",
    "    (\"/home/liorkob/M.Sc/thesis/pre-train/models/hebert-mlm-3k-drugs/final\", \"HeBERT-MLM-3K-Drugs (fine-tuned)\"),\n",
    "    (\"bert-base-multilingual-cased\", \"mBERT (baseline)\"),\n",
    "    (\"/home/liorkob/M.Sc/thesis/pre-train/models/mBERT-mlm-3k-drugs/final\", \"mBERT-MLM-3K-Drugs (fine-tuned)\"),\n",
    "    (\"avichr/Legal-heBERT\", \"Legal-HeBERT (baseline)\"),\n",
    "    (\"/home/liorkob/M.Sc/thesis/pre-train/models/Legal-heBERT-mlm-3k-drugs/final\", \"Legal-HeBERT-MLM-3K-Drugs (fine-tuned)\")\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class CrossEncoderVerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = f\"[CLS] {row['gpt_facts_a']} [SEP] {row['gpt_facts_b']} [SEP]\"\n",
    "        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# -----------------------\n",
    "# Model\n",
    "# -----------------------\n",
    "class CrossEncoderModel(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        hidden = encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(pooled).squeeze(-1)\n",
    "# -----------------------\n",
    "# Evaluation\n",
    "# -----------------------\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    probs, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            label = batch[\"label\"].cpu().numpy()\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            probs.extend(prob)\n",
    "            labels.extend(label)\n",
    "\n",
    "    probs = np.array(probs)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(\"F1:\", f1_score(labels, preds))\n",
    "    print(\"Precision:\", precision_score(labels, preds, zero_division=0))\n",
    "    print(\"Recall:\", recall_score(labels, preds, zero_division=0))\n",
    "    print(\"Accuracy:\", np.mean(preds == labels))\n",
    "    if len(set(preds)) > 1 and len(set(labels)) > 1:\n",
    "        print(\"AUC-ROC:\", roc_auc_score(labels, probs))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(labels, preds))\n",
    "\n",
    "# -----------------------\n",
    "# Main\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df_test = pd.read_csv(test_file)\n",
    "\n",
    "    for model_path, label in models_to_evaluate:\n",
    "        print(f\"\\n\\n====================\")\n",
    "        print(f\"🔍 Evaluating: {label}\")\n",
    "        print(f\"====================\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        test_dataset = CrossEncoderVerdictDataset(df_test, tokenizer, max_len=max_len)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        from transformers import AutoModel, AutoTokenizer\n",
    "        encoder = AutoModel.from_pretrained(model_path, trust_remote_code=True)\n",
    "        model = CrossEncoderModel(encoder).to(device)\n",
    "        evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
