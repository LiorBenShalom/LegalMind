{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liorkob/.conda/envs/judgeEnv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data prep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for tagged data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved to: /home/liorkob/M.Sc/thesis/data/drugs/gt_similarity_with_facts.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "sim_df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/similarity_gt_drugs.csv\")\n",
    "facts_df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/processed_verdicts_with_gpt.csv\")\n",
    "\n",
    "# Make sure file paths are consistent\n",
    "facts_df[\"verdict\"] = facts_df[\"verdict\"].str.strip()\n",
    "sim_df[\"verdict_a\"] = sim_df[\"verdict_1\"].str.strip()\n",
    "sim_df[\"verdict_b\"] = sim_df[\"verdict_2\"].str.strip()\n",
    "\n",
    "# Merge facts\n",
    "sim_df = sim_df.merge(facts_df.rename(columns={\"verdict\": \"verdict_a\", \"extracted_gpt_facts\": \"gpt_facts_a\"}), on=\"verdict_a\", how=\"left\")\n",
    "sim_df = sim_df.merge(facts_df.rename(columns={\"verdict\": \"verdict_b\", \"extracted_gpt_facts\": \"gpt_facts_b\"}), on=\"verdict_b\", how=\"left\")\n",
    "\n",
    "# Remap similarity\n",
    "sim_df[\"label\"] = sim_df[\"Similarity\"].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "# Save result\n",
    "output_path = \"/home/liorkob/M.Sc/thesis/data/drugs/gt_similarity_with_facts.csv\"\n",
    "sim_df[[\"verdict_a\", \"verdict_b\", \"gpt_facts_a\", \"gpt_facts_b\", \"label\"]].to_csv(output_path, index=False)\n",
    "print(f\"âœ… Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for citation based data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "tag_dir = '/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdicts_tagged_citations'\n",
    "gpt_facts_path = '/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv'\n",
    "output_path = '/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verdict_pairs_with_similarity.csv'\n",
    "\n",
    "# Load verdict facts\n",
    "verdict_facts = pd.read_csv(gpt_facts_path)\n",
    "facts_dict = dict(zip(verdict_facts['verdict'], verdict_facts['extracted_gpt_facts']))\n",
    "\n",
    "# Collect data\n",
    "rows = []\n",
    "for file in os.listdir(tag_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        verdict_a = file.replace('.csv', '')\n",
    "        file_path = os.path.join(tag_dir, file)\n",
    "        \n",
    "        # Skip empty files\n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except pd.errors.EmptyDataError:\n",
    "            continue\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            if row['predicted_label'] == 1:\n",
    "                verdict_b = row['citation']\n",
    "                a_facts = facts_dict.get(verdict_a, \"\")\n",
    "                b_facts = facts_dict.get(verdict_b, \"\")\n",
    "                rows.append([verdict_a, a_facts, verdict_b, b_facts, 1])\n",
    "\n",
    "# Save result\n",
    "result_df = pd.DataFrame(rows, columns=[\n",
    "    'verdict_a_name', 'verdict_a_extracted_gpt_facts',\n",
    "    'verdict_b_name', 'verdict_b_extracted_gpt_facts', 'similarity_score'\n",
    "])\n",
    "result_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### genrate non similar pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# ========== Setup ==========\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# ========== Load Similar Pairs ==========\n",
    "df_pairs = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/valid_pairs_with_log.csv\")\n",
    "valid_pairs = df_pairs[df_pairs[\"log\"] != \"missing cited verdict\"].copy()\n",
    "similar_pairs = set(tuple(sorted([a, b])) for a, b in zip(valid_pairs[\"verdict_a\"], valid_pairs[\"verdict_b\"]))\n",
    "\n",
    "# Build similarity graph\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(similar_pairs)\n",
    "\n",
    "# ========== Load Facts from All Sources ==========\n",
    "facts1 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/processed_verdicts_with_gpt.csv\")  \n",
    "# facts2 = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/5k/gpt/processed_appeals_with_gpt_2.csv\")\n",
    "\n",
    "pairs_facts_a = df_pairs[[\"verdict_a\", \"gpt_facts_a\"]].rename(columns={\"verdict_a\": \"verdict\", \"gpt_facts_a\": \"extracted_gpt_facts\"})\n",
    "pairs_facts_b = df_pairs[[\"verdict_b\", \"gpt_facts_b\"]].rename(columns={\"verdict_b\": \"verdict\", \"gpt_facts_b\": \"extracted_gpt_facts\"})\n",
    "all_facts_df = pd.concat([facts1, pairs_facts_a, pairs_facts_b])\n",
    "\n",
    "# all_facts_df = pd.concat([facts1, facts2, pairs_facts_a, pairs_facts_b])\n",
    "all_facts_df = all_facts_df.dropna(subset=[\"verdict\", \"extracted_gpt_facts\"]).drop_duplicates(subset=\"verdict\")\n",
    "facts_dict = dict(zip(all_facts_df[\"verdict\"], all_facts_df[\"extracted_gpt_facts\"]))\n",
    "\n",
    "# ========== Prepare Verdict Sets ==========\n",
    "verdicts_in_pairs = set(valid_pairs[\"verdict_a\"]) | set(valid_pairs[\"verdict_b\"])\n",
    "all_verdicts = set(all_facts_df[\"verdict\"])\n",
    "extra_verdicts = list(all_verdicts - verdicts_in_pairs)\n",
    "combined_verdicts = list(verdicts_in_pairs | set(extra_verdicts))\n",
    "\n",
    "# ========== Generate Candidate Non-Similar Pairs ==========\n",
    "def generate_candidate_pairs(verdicts, existing_pairs, G, num_pairs):\n",
    "    non_similar = set()\n",
    "    attempts = 0\n",
    "    max_attempts = num_pairs * 20\n",
    "\n",
    "    while len(non_similar) < num_pairs and attempts < max_attempts:\n",
    "        a, b = random.sample(list(verdicts), 2)\n",
    "        pair = tuple(sorted([a, b]))\n",
    "        attempts += 1\n",
    "\n",
    "        if pair in existing_pairs or G.has_edge(*pair):\n",
    "            continue\n",
    "\n",
    "        # âœ… Only check transitive path if both nodes exist in graph\n",
    "        if a in G and b in G:\n",
    "            if nx.has_path(G, a, b):\n",
    "                continue\n",
    "\n",
    "        non_similar.add(pair)\n",
    "\n",
    "    return list(non_similar)\n",
    "\n",
    "# ========== GPT Verification ==========\n",
    "def verify_with_gpt(pair, facts_dict):\n",
    "    print(f\"Checking: {pair}, verified so far: {len(verified_non_similar)}\")\n",
    "\n",
    "    fact_a = facts_dict.get(pair[0], \"\")\n",
    "    fact_b = facts_dict.get(pair[1], \"\")\n",
    "\n",
    "    if not fact_a or not fact_b:\n",
    "        return False\n",
    "    prompt = f\"\"\"××ª×” ×¢×•×–×¨ ××©×¤×˜×™. ×ª×¤×§×™×“×š ×œ×‘×“×•×§ ×”×× ×©×ª×™ ××¢×¨×›×•×ª ×¢×•×‘×“×ª×™×•×ª ××ª××¨×•×ª **××¦×‘×™× ××©×¤×˜×™×™× ×“×•××™×**, ×›×š ×©× ×™×ª×Ÿ ×™×”×™×” ×œ×”×¡×ª××š ×¢×œ ×”××—×“ ×œ×¦×•×¨×š ×’×–×™×¨×ª ×”×¢×•× ×© ×‘×©× ×™.\n",
    "\n",
    "×”×ª×™×™×—×¡ ×¨×§ ×œ× ×¡×™×‘×•×ª ×©×§×©×•×¨×•×ª ×™×©×™×¨×•×ª ×œ×‘×™×¦×•×¢ ×”×¢×‘×™×¨×” â€“ ×›×’×•×Ÿ ×¡×•×’ ×”×¢×‘×™×¨×”, ××”×•×ª ×”××¢×©×”, ××•×¤×Ÿ ×”×‘×™×¦×•×¢, ××©×š ×”×–××Ÿ, ×›×•×•× ×” ×¤×œ×™×œ×™×ª, ×•×××¤×™×™× ×™× ×¨×œ×•×•× ×˜×™×™× ×©×œ ×”× ××©× ××• ×”×§×•×¨×‘×Ÿ *×©× ×•×’×¢×™× ×œ××¢×©×” ×¢×¦××•*.\n",
    "\n",
    "×××¤×™×™× ×™× ×›×œ×œ×™×™× ×›××• ×’×™×œ ×”× ××©×, ××§×•× ×”××’×•×¨×™× ××• ×ª×•×¦××” ××§×¨×™×ª ×©××™× ×” × ×•×‘×¢×ª ××”××¢×©×” â€“ ××™× × ×¨×œ×•×•× ×˜×™×™× ×œ×“××™×•×Ÿ ×”××©×¤×˜×™.\n",
    "\n",
    "×¢× ×” ×¨×§ \"×›×Ÿ\" ××• \"×œ×\". ××œ ×ª×¡×‘×™×¨. ××œ ×ª×•×¡×™×£ ×¡×™×× ×™ ×¤×™×¡×•×§.\n",
    "\n",
    "×¢×•×‘×“×•×ª ×: {fact_a}\n",
    "×¢×•×‘×“×•×ª ×‘: {fact_b}\n",
    "×ª×©×•×‘×”:\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\", \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an AI trained to analyz legal text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        print(answer)\n",
    "        return answer == \"×œ×\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"ğŸš¨ GPT API error: {e}\")\n",
    "        return \n",
    "\n",
    "# ========== Main Loop ==========\n",
    "target_count = 2000\n",
    "verified_non_similar = set()\n",
    "used_pairs = set()\n",
    "\n",
    "pbar = tqdm(total=target_count, desc=\"ğŸ” Verifying with GPT\")\n",
    "\n",
    "while len(verified_non_similar) < target_count:\n",
    "    needed = (target_count - len(verified_non_similar)) * 2\n",
    "    candidate_pairs = generate_candidate_pairs(combined_verdicts, similar_pairs | used_pairs, G, needed)\n",
    "\n",
    "    for pair in candidate_pairs:\n",
    "        if pair in used_pairs:\n",
    "            continue\n",
    "        used_pairs.add(pair)\n",
    "\n",
    "        if verify_with_gpt(pair, facts_dict):\n",
    "            verified_non_similar.add(pair)\n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# ========== Save Output with Facts ==========\n",
    "pairs_list = []\n",
    "for a, b in verified_non_similar:\n",
    "    pairs_list.append({\n",
    "        \"verdict_a\": a,\n",
    "        \"verdict_b\": b,\n",
    "        \"gpt_facts_a\": facts_dict.get(a, \"\"),\n",
    "        \"gpt_facts_b\": facts_dict.get(b, \"\"),\n",
    "        \"label\": 0\n",
    "    })\n",
    "\n",
    "output_df = pd.DataFrame(pairs_list)\n",
    "output_df.to_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verified_non_similar_pairs_2.csv\", index=False)\n",
    "print(f\"\\nâœ… Saved {len(output_df)} pairs with facts to verified_non_similar_pairs_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Similar pairs: 1934\n",
      "âœ… Non-similar pairs: 3860\n",
      "ğŸ“¦ Total before removing duplicates: 5794\n",
      "ğŸ§¹ Total after removing duplicates: 5791\n",
      "ğŸ“Š Label distribution:\n",
      "label\n",
      "0    3857\n",
      "1    1934\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSVs\n",
    "similar = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/valid_pairs_with_log.csv\")\n",
    "non_similar = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs_3k/gpt/verified_non_similar_pairs_2.csv\")\n",
    "\n",
    "# Filter relevant rows\n",
    "similar = similar[similar[\"log\"] != \"missing cited verdict\"].copy()\n",
    "similar[\"label\"] = 1\n",
    "\n",
    "non_similar = non_similar[non_similar[\"label\"] == 0].copy()\n",
    "non_similar[\"label\"] = 0\n",
    "\n",
    "# Select only required columns\n",
    "cols = [\"verdict_a\", \"verdict_b\", \"gpt_facts_a\", \"gpt_facts_b\", \"label\"]\n",
    "similar = similar[cols]\n",
    "non_similar = non_similar[cols]\n",
    "\n",
    "print(f\"âœ… Similar pairs: {len(similar)}\")\n",
    "print(f\"âœ… Non-similar pairs: {len(non_similar)}\")\n",
    "\n",
    "# Normalize order to detect duplicates\n",
    "def normalize_pair(row):\n",
    "    a, b = row['verdict_a'], row['verdict_b']\n",
    "    return tuple(sorted((a, b)))\n",
    "\n",
    "similar['pair_key'] = similar.apply(normalize_pair, axis=1)\n",
    "non_similar['pair_key'] = non_similar.apply(normalize_pair, axis=1)\n",
    "\n",
    "# Combine and remove duplicates\n",
    "data = pd.concat([similar, non_similar], ignore_index=True)\n",
    "print(f\"ğŸ“¦ Total before removing duplicates: {len(data)}\")\n",
    "\n",
    "data = data.drop_duplicates(subset='pair_key').drop(columns='pair_key')\n",
    "print(f\"ğŸ§¹ Total after removing duplicates: {len(data)}\")\n",
    "\n",
    "# Shuffle\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "data.to_csv(\"data_pairs_5k.csv\", index=False)\n",
    "\n",
    "# Print distribution\n",
    "print(\"ğŸ“Š Label distribution:\")\n",
    "print(data[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split to test-tain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Load ===\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "df_pos = df[df['label'] == 1]\n",
    "df_neg = df[df['label'] == 0]\n",
    "df_pos_train, df_pos_val = train_test_split(df_pos, test_size=0.3, random_state=42)\n",
    "df_neg_train, df_neg_val = train_test_split(df_neg, test_size=0.3, random_state=42)\n",
    "df_train = pd.concat([df_pos_train, df_neg_train]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val = pd.concat([df_pos_val, df_neg_val]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_val, df_test = train_test_split(df_val, test_size=0.5, random_state=42, stratify=df_val['label'])\n",
    "\n",
    "# Save splits\n",
    "df_train.to_csv(\"crossencoder_train.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_val.to_csv(\"crossencoder_val.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "df_test.to_csv(\"crossencoder_test.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiameseHeBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = '/home/liorkob/M.Sc/thesis/similarity-model/hebert-mlm-verdicts/final'\n",
    "# model_name=\"avichr/heBERT\"\n",
    "# ----- Model -----\n",
    "class SiameseHeBERT(nn.Module):\n",
    "    def __init__(self, model_name=model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "    def forward(self, ids_a, mask_a, ids_b, mask_b):\n",
    "        out_a = self.encoder(ids_a, attention_mask=mask_a)\n",
    "        out_b = self.encoder(ids_b, attention_mask=mask_b)\n",
    "        vec_a = self.mean_pooling(out_a, mask_a)\n",
    "        vec_b = self.mean_pooling(out_b, mask_b)\n",
    "        combined = torch.cat([vec_a, vec_b], dim=1)\n",
    "        return self.classifier(combined).squeeze(-1)\n",
    "\n",
    "# # ----- Dataset -----\n",
    "class VerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc_a = self.tokenizer(row['gpt_facts_a'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        enc_b = self.tokenizer(row['gpt_facts_b'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids_a': enc_a['input_ids'].squeeze(),\n",
    "            'attention_mask_a': enc_a['attention_mask'].squeeze(),\n",
    "            'input_ids_b': enc_b['input_ids'].squeeze(),\n",
    "            'attention_mask_b': enc_b['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ----- Contrastive Loss -----\n",
    "def contrastive_loss(embedding_a, embedding_b, label, margin=1.0):\n",
    "    euclidean_distance = torch.norm(embedding_a - embedding_b, dim=1)\n",
    "    loss = label * euclidean_distance.pow(2) + \\\n",
    "           (1 - label) * torch.clamp(margin - euclidean_distance, min=0).pow(2)\n",
    "    return loss.mean()\n",
    "\n",
    "# ----- Train Function -----\n",
    "def train(model, dataloader, val_loader, optimizer, device, epochs=20, patience=5):\n",
    "    model.train()\n",
    "    best_auc = 0\n",
    "    no_improve_epochs = 0\n",
    "    best_model_path = \"best_siamese_hebert.pt\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            out_a = model.encoder(batch['input_ids_a'], attention_mask=batch['attention_mask_a'])\n",
    "            out_b = model.encoder(batch['input_ids_b'], attention_mask=batch['attention_mask_b'])\n",
    "\n",
    "            emb_a = model.mean_pooling(out_a, batch['attention_mask_a'])\n",
    "            emb_b = model.mean_pooling(out_b, batch['attention_mask_b'])\n",
    "\n",
    "            loss = contrastive_loss(emb_a, emb_b, batch['label'], margin=2)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            dists = torch.norm(emb_a - emb_b, dim=1)\n",
    "            preds = (dists < 0.5).long()\n",
    "            correct += (preds == batch['label'].long()).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "        # Validation AUC\n",
    "        val_probs, val_targets = collect_predictions(model, val_loader, device)\n",
    "        val_auc = roc_auc_score(val_targets, val_probs)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(dataloader):.4f}, Accuracy: {correct / total:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(\"âœ… Model improved and saved.\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"âš ï¸ No improvement for {no_improve_epochs} epoch(s).\")\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"â¹ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# ----- Evaluation Function -----\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids_a'], batch['attention_mask_a'],\n",
    "                           batch['input_ids_b'], batch['attention_mask_b'])\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label'].long()).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "    return correct / total\n",
    "\n",
    "# ----- Prediction Collection -----\n",
    "def collect_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids_a'], batch['attention_mask_a'],\n",
    "                           batch['input_ids_b'], batch['attention_mask_b'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "    return np.array(probs), np.array(targets)\n",
    "\n",
    "# ----- Main Run -----\n",
    "# df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "\n",
    "# # Stratified splits\n",
    "# df_train, df_temp = train_test_split(df, test_size=0.3, stratify=df[\"label\"], random_state=42)\n",
    "# df_val, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp[\"label\"], random_state=42)\n",
    "df_train = pd.read_csv(\"crossencoder_train.csv\")\n",
    "df_val = pd.read_csv(\"crossencoder_val.csv\")\n",
    "df_test = pd.read_csv(\"crossencoder_test.csv\")\n",
    "\n",
    "# Tokenizer and datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "train_dataset = VerdictDataset(df_train, tokenizer)\n",
    "val_dataset = VerdictDataset(df_val, tokenizer)\n",
    "test_dataset = VerdictDataset(df_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Initialize and train\n",
    "model = SiameseHeBERT().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=5e-5)\n",
    "train(model, train_loader,val_loader ,optimizer, device)\n",
    "\n",
    "# --- Validation Evaluation ---\n",
    "val_acc = evaluate(model, val_loader, device)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "val_probs, val_targets = collect_predictions(model, val_loader, device)\n",
    "val_preds = (val_probs >= 0.5).astype(int)\n",
    "print(f\"Validation AUC-ROC: {roc_auc_score(val_targets, val_probs):.4f}\")\n",
    "print(f\"Validation F1 Score: {f1_score(val_targets, val_preds):.4f}\")\n",
    "print(f\"Validation Precision: {precision_score(val_targets, val_preds):.4f}\")\n",
    "print(f\"Validation Recall: {recall_score(val_targets, val_preds):.4f}\")\n",
    "\n",
    "val_probs, val_targets = collect_predictions(model, val_loader, device)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "best_thresh = 0.5\n",
    "best_f1 = 0.0\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (val_probs >= t).astype(int)\n",
    "    f1 = f1_score(val_targets, preds)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"ğŸ” Best threshold by F1: {best_thresh:.4f} (F1: {best_f1:.4f})\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "f1_scores = [f1_score(val_targets, (val_probs >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "plt.plot(thresholds, f1_scores)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score vs Threshold\")\n",
    "plt.show()\n",
    "\n",
    "# --- Save model ---\n",
    "torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/siamese_hebert.pt\")\n",
    "print(\"âœ… Model saved to siamese_hebert.pt\")\n",
    "\n",
    "# --- Test Evaluation ---\n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "test_probs, test_targets = collect_predictions(model, test_loader, device)\n",
    "\n",
    "\n",
    "test_preds = (test_probs >= best_thresh).astype(int)\n",
    "print(f\"Test AUC-ROC: {roc_auc_score(test_targets, test_probs):.4f}\")\n",
    "print(f\"Test F1 Score: {f1_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(test_targets, test_preds):.4f}\")\n",
    "\n",
    "\n",
    "# --- Save model ---\n",
    "torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/siamese_hebert.pt\")\n",
    "print(\"âœ… Model saved to siamese_hebert.pt\")\n",
    "\n",
    "# --- Test Evaluation ---\n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "test_probs, test_targets = collect_predictions(model, test_loader, device)\n",
    "\n",
    "\n",
    "test_preds = (test_probs >= best_thresh).astype(int)\n",
    "print(f\"Test AUC-ROC: {roc_auc_score(test_targets, test_probs):.4f}\")\n",
    "print(f\"Test F1 Score: {f1_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"Test Precision: {precision_score(test_targets, test_preds):.4f}\")\n",
    "print(f\"Test Recall: {recall_score(test_targets, test_preds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = '/home/liorkob/M.Sc/thesis/similarity-model/hebert-mlm-verdicts/final'\n",
    "# model_name=\"avichr/heBERT\"\n",
    "# ----- Model -----\n",
    "class SiameseHeBERT(nn.Module):\n",
    "    def __init__(self, model_name=model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "    def forward(self, ids_a, mask_a, ids_b, mask_b):\n",
    "        out_a = self.encoder(ids_a, attention_mask=mask_a)\n",
    "        out_b = self.encoder(ids_b, attention_mask=mask_b)\n",
    "        vec_a = self.mean_pooling(out_a, mask_a)\n",
    "        vec_b = self.mean_pooling(out_b, mask_b)\n",
    "        combined = torch.cat([vec_a, vec_b], dim=1)\n",
    "        return self.classifier(combined).squeeze(-1)\n",
    "\n",
    "# # ----- Dataset -----\n",
    "class VerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc_a = self.tokenizer(row['gpt_facts_a'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        enc_b = self.tokenizer(row['gpt_facts_b'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids_a': enc_a['input_ids'].squeeze(),\n",
    "            'attention_mask_a': enc_a['attention_mask'].squeeze(),\n",
    "            'input_ids_b': enc_b['input_ids'].squeeze(),\n",
    "            'attention_mask_b': enc_b['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "# --- Triplet Dataset ---\n",
    "class TripletVerdictDataset(Dataset):\n",
    "    def __init__(self, df_pos, df_neg, tokenizer, max_len=128):\n",
    "        self.df_pos = df_pos.reset_index(drop=True)\n",
    "        self.df_neg = df_neg.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_pos = self.df_pos.iloc[idx]\n",
    "        anchor_text = row_pos['gpt_facts_a']\n",
    "        pos_text = row_pos['gpt_facts_b']\n",
    "\n",
    "        # Select negative at random\n",
    "        neg_text = self.df_neg.sample(1).iloc[0]['gpt_facts_b']\n",
    "\n",
    "        def encode(text1, text2):\n",
    "            return self.tokenizer(text1, text2, padding=\"max_length\", truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        anc_enc = encode(anchor_text, pos_text)\n",
    "        pos_enc = encode(anchor_text, pos_text)\n",
    "        neg_enc = encode(anchor_text, neg_text)\n",
    "\n",
    "        return {\n",
    "            'anchor_ids': anc_enc['input_ids'].squeeze(),\n",
    "            'anchor_mask': anc_enc['attention_mask'].squeeze(),\n",
    "            'positive_ids': pos_enc['input_ids'].squeeze(),\n",
    "            'positive_mask': pos_enc['attention_mask'].squeeze(),\n",
    "            'negative_ids': neg_enc['input_ids'].squeeze(),\n",
    "            'negative_mask': neg_enc['attention_mask'].squeeze(),\n",
    "        }\n",
    "\n",
    "# --- Triplet Loss ---\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    pos_dist = torch.norm(anchor - positive, dim=1)\n",
    "    neg_dist = torch.norm(anchor - negative, dim=1)\n",
    "    loss = torch.clamp(pos_dist - neg_dist + margin, min=0.0)\n",
    "    return loss.mean()\n",
    "\n",
    "def compute_auc_on_val(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_dists, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            out_a = model.encoder(batch['input_ids_a'], attention_mask=batch['attention_mask_a'])\n",
    "            out_b = model.encoder(batch['input_ids_b'], attention_mask=batch['attention_mask_b'])\n",
    "            emb_a = model.mean_pooling(out_a, batch['attention_mask_a'])\n",
    "            emb_b = model.mean_pooling(out_b, batch['attention_mask_b'])\n",
    "            dists = torch.norm(emb_a - emb_b, dim=1).cpu().numpy()\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            all_dists.extend(dists)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    return roc_auc_score(all_labels, -np.array(all_dists))  # minus = closer â†’ more similar\n",
    "\n",
    "def train_triplet_with_early_stopping(model, train_loader, val_loader, optimizer, device, epochs=30, patience=5):\n",
    "    best_auc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            anchor_out = model.encoder(batch['anchor_ids'], attention_mask=batch['anchor_mask'])\n",
    "            positive_out = model.encoder(batch['positive_ids'], attention_mask=batch['positive_mask'])\n",
    "            negative_out = model.encoder(batch['negative_ids'], attention_mask=batch['negative_mask'])\n",
    "\n",
    "            anchor_vec = model.mean_pooling(anchor_out, batch['anchor_mask'])\n",
    "            positive_vec = model.mean_pooling(positive_out, batch['positive_mask'])\n",
    "            negative_vec = model.mean_pooling(negative_out, batch['negative_mask'])\n",
    "\n",
    "            loss = triplet_loss(anchor_vec, positive_vec, negative_vec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Triplet Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation AUC\n",
    "        auc = compute_auc_on_val(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1} | Validation AUC: {auc:.4f}\")\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/best_triplet_auc.pt\")\n",
    "            print(\"âœ… New best model saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"â¹ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "# --- Training with Triplet Loss ---\n",
    "def train_triplet(model, dataloader, optimizer, device, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            anchor_out = model.encoder(batch['anchor_ids'], attention_mask=batch['anchor_mask'])\n",
    "            positive_out = model.encoder(batch['positive_ids'], attention_mask=batch['positive_mask'])\n",
    "            negative_out = model.encoder(batch['negative_ids'], attention_mask=batch['negative_mask'])\n",
    "\n",
    "            anchor_vec = model.mean_pooling(anchor_out, batch['anchor_mask'])\n",
    "            positive_vec = model.mean_pooling(positive_out, batch['positive_mask'])\n",
    "            negative_vec = model.mean_pooling(negative_out, batch['negative_mask'])\n",
    "\n",
    "            loss = triplet_loss(anchor_vec, positive_vec, negative_vec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Triplet Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# --- Evaluation (unchanged from original) ---\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, roc_curve\n",
    "\n",
    "def evaluate_triplet_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_dists, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            out_a = model.encoder(batch['input_ids_a'], attention_mask=batch['attention_mask_a'])\n",
    "            out_b = model.encoder(batch['input_ids_b'], attention_mask=batch['attention_mask_b'])\n",
    "            emb_a = model.mean_pooling(out_a, batch['attention_mask_a'])\n",
    "            emb_b = model.mean_pooling(out_b, batch['attention_mask_b'])\n",
    "            dists = torch.norm(emb_a - emb_b, dim=1).cpu().numpy()\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            all_dists.extend(dists)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    all_probs = -np.array(all_dists)  # more similar = higher\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Find best F1 threshold\n",
    "    thresholds = np.arange(all_probs.min(), all_probs.max(), 0.01)\n",
    "    best_thresh = 0\n",
    "    best_f1 = 0\n",
    "    f1_scores = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        preds = (all_probs >= t).astype(int)\n",
    "        f1 = f1_score(all_labels, preds)\n",
    "        f1_scores.append(f1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "\n",
    "    # Plot F1 vs threshold\n",
    "    plt.plot(thresholds, f1_scores)\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score vs. Threshold\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Final predictions with best F1 threshold\n",
    "    final_preds = (all_probs >= best_thresh).astype(int)\n",
    "\n",
    "    print(f\"ğŸ“Š Best threshold by F1: {best_thresh:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(all_labels, all_probs):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(all_labels, final_preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, final_preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(all_labels, final_preds):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Run All ---\n",
    "# df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "# df_pos = df[df.label == 1]\n",
    "# df_neg = df[df.label == 0]\n",
    "\n",
    "# # Splits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# df_pos_train, df_pos_val = train_test_split(df_pos, test_size=0.3, random_state=42)\n",
    "# df_neg_train, df_neg_val = train_test_split(df_neg, test_size=0.3, random_state=42)\n",
    "df_train = pd.read_csv(\"crossencoder_train.csv\")\n",
    "df_val = pd.read_csv(\"crossencoder_val.csv\")\n",
    "df_test = pd.read_csv(\"crossencoder_test.csv\")\n",
    "\n",
    "df_pos_train = df_train[df_train.label == 1]\n",
    "df_neg_train = df_train[df_train.label == 0]\n",
    "df_pos_val =df_val[df_val.label == 1]\n",
    "df_neg_val = df_val[df_val.label == 0]\n",
    "df_pos_test =df_test[df_test.label == 1]\n",
    "df_neg_test = df_test[df_test.label == 0]\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "# Triplet Train Set\n",
    "triplet_dataset = TripletVerdictDataset(df_pos_train, df_neg_train, tokenizer)\n",
    "triplet_loader = DataLoader(triplet_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Standard Val Set for Evaluation\n",
    "val_dataset = VerdictDataset(pd.concat([df_pos_val, df_neg_val]), tokenizer)\n",
    "test_dataset = VerdictDataset(pd.concat([df_pos_test, df_neg_test]), tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader=DataLoader(test_dataset, batch_size=8)\n",
    "# Model & Optimizer\n",
    "model = SiameseHeBERT().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train & Evaluate\n",
    "train_triplet_with_early_stopping(model, triplet_loader, val_loader, optimizer, device, epochs=30, patience=5)\n",
    "# train_triplet(model, triplet_loader, optimizer, device)\n",
    "evaluate_triplet_embeddings(model, val_loader, device)\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/siamese_hebert_triplet.pt\")\n",
    "print(\"âœ… Triplet model saved.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Test Evaluation ---\n",
    "print(\" --- Test Evaluation ---\")\n",
    "evaluate_triplet_embeddings(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NT-Xent (Normalized Temperature-scaled Cross Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = '/home/liorkob/M.Sc/thesis/similarity-model/hebert-mlm-verdicts/final'\n",
    "# model_name=\"avichr/heBERT\"\n",
    "# ----- Model -----\n",
    "class SiameseHeBERT(nn.Module):\n",
    "    def __init__(self, model_name=model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "    def forward(self, ids_a, mask_a, ids_b, mask_b):\n",
    "        out_a = self.encoder(ids_a, attention_mask=mask_a)\n",
    "        out_b = self.encoder(ids_b, attention_mask=mask_b)\n",
    "        vec_a = self.mean_pooling(out_a, mask_a)\n",
    "        vec_b = self.mean_pooling(out_b, mask_b)\n",
    "        combined = torch.cat([vec_a, vec_b], dim=1)\n",
    "        return self.classifier(combined).squeeze(-1)\n",
    "\n",
    "# # ----- Dataset -----\n",
    "class VerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc_a = self.tokenizer(row['gpt_facts_a'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        enc_b = self.tokenizer(row['gpt_facts_b'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids_a': enc_a['input_ids'].squeeze(),\n",
    "            'attention_mask_a': enc_a['attention_mask'].squeeze(),\n",
    "            'input_ids_b': enc_b['input_ids'].squeeze(),\n",
    "            'attention_mask_b': enc_b['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "class ContrastiveVerdictDataset(Dataset):\n",
    "    def __init__(self, df_pos, tokenizer, max_len=128):\n",
    "        self.df_pos = df_pos.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df_pos.iloc[idx]\n",
    "        anchor_text = row['gpt_facts_a']\n",
    "        pos_text = row['gpt_facts_b']\n",
    "\n",
    "        def encode(text):\n",
    "            return self.tokenizer(text, padding=\"max_length\", truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        anc_enc = encode(anchor_text)\n",
    "        pos_enc = encode(pos_text)\n",
    "\n",
    "        return {\n",
    "            'anchor_ids': anc_enc['input_ids'].squeeze(),\n",
    "            'anchor_mask': anc_enc['attention_mask'].squeeze(),\n",
    "            'positive_ids': pos_enc['input_ids'].squeeze(),\n",
    "            'positive_mask': pos_enc['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# --- Triplet Loss ---\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def nt_xent_loss(embeddings_a, embeddings_b, temperature=0.5):\n",
    "    \"\"\"\n",
    "    embeddings_a: (batch_size, hidden_size)\n",
    "    embeddings_b: (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = embeddings_a.size(0)\n",
    "\n",
    "    # Normalize\n",
    "    emb_a = F.normalize(embeddings_a, dim=1)\n",
    "    emb_b = F.normalize(embeddings_b, dim=1)\n",
    "\n",
    "    # ×›×œ ×–×•×’×•×ª ×”×§×•×¡×™× ×•×¡\n",
    "    representations = torch.cat([emb_a, emb_b], dim=0)  # (2N, D)\n",
    "    similarity_matrix = torch.matmul(representations, representations.T)  # (2N, 2N)\n",
    "\n",
    "    # × ×•×¨××œ×™×–×¦×™×” ×¢× ×˜××¤×³\n",
    "    sim = similarity_matrix / temperature\n",
    "\n",
    "    # ××¡×›×•×ª â€“ ×œ×–×”×•×ª ××™ ×—×™×•×‘×™ ×•××™ ×©×œ×™×œ×™\n",
    "    labels = torch.arange(batch_size, device=embeddings_a.device)\n",
    "    labels = torch.cat([labels + batch_size, labels], dim=0)\n",
    "\n",
    "    # ×œ× ×œ×”×¡×›×™× ×œ×”×©×•×•×ª ×“×•×’××” ×¢× ×¢×¦××”\n",
    "    mask = torch.eye(2 * batch_size, device=embeddings_a.device).bool()\n",
    "    sim = sim.masked_fill(mask, -1e9)\n",
    "\n",
    "    # ×§×¨×•×¡Ö¾×× ×˜×¨×•×¤×™\n",
    "    loss = F.cross_entropy(sim, labels)\n",
    "    return loss\n",
    "\n",
    "def compute_auc_on_val(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_dists, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            out_a = model.encoder(batch['input_ids_a'], attention_mask=batch['attention_mask_a'])\n",
    "            out_b = model.encoder(batch['input_ids_b'], attention_mask=batch['attention_mask_b'])\n",
    "            emb_a = model.mean_pooling(out_a, batch['attention_mask_a'])\n",
    "            emb_b = model.mean_pooling(out_b, batch['attention_mask_b'])\n",
    "            dists = torch.norm(emb_a - emb_b, dim=1).cpu().numpy()\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            all_dists.extend(dists)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    return roc_auc_score(all_labels, -np.array(all_dists))  # minus = closer â†’ more similar\n",
    "\n",
    "def train_triplet_with_early_stopping(model, train_loader, val_loader, optimizer, device, epochs=30, patience=5):\n",
    "    best_auc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            anchor_out = model.encoder(batch['anchor_ids'], attention_mask=batch['anchor_mask'])\n",
    "            positive_out = model.encoder(batch['positive_ids'], attention_mask=batch['positive_mask'])\n",
    "\n",
    "            anchor_vec = model.mean_pooling(anchor_out, batch['anchor_mask'])\n",
    "            positive_vec = model.mean_pooling(positive_out, batch['positive_mask'])\n",
    "\n",
    "            anchor_vec = model.mean_pooling(anchor_out, batch['anchor_mask'])\n",
    "            positive_vec = model.mean_pooling(positive_out, batch['positive_mask'])\n",
    "\n",
    "            loss = nt_xent_loss(anchor_vec, positive_vec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Triplet Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "        # Validation AUC\n",
    "        auc = compute_auc_on_val(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1} | Validation AUC: {auc:.4f}\")\n",
    "\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/best_triplet_auc.pt\")\n",
    "            print(\"âœ… New best model saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"â¹ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "# --- Training with Triplet Loss ---\n",
    "def train_triplet(model, dataloader, optimizer, device, epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            anchor_out = model.encoder(batch['anchor_ids'], attention_mask=batch['anchor_mask'])\n",
    "            positive_out = model.encoder(batch['positive_ids'], attention_mask=batch['positive_mask'])\n",
    "            negative_out = model.encoder(batch['negative_ids'], attention_mask=batch['negative_mask'])\n",
    "\n",
    "            anchor_vec = model.mean_pooling(anchor_out, batch['anchor_mask'])\n",
    "            positive_vec = model.mean_pooling(positive_out, batch['positive_mask'])\n",
    "            negative_vec = model.mean_pooling(negative_out, batch['negative_mask'])\n",
    "\n",
    "            anchor_vec = model.mean_pooling(anchor_out, batch['anchor_mask'])\n",
    "            positive_vec = model.mean_pooling(positive_out, batch['positive_mask'])\n",
    "\n",
    "            loss = nt_xent_loss(anchor_vec, positive_vec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Triplet Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# --- Evaluation (unchanged from original) ---\n",
    "def evaluate_triplet_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_dists, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            out_a = model.encoder(batch['input_ids_a'], attention_mask=batch['attention_mask_a'])\n",
    "            out_b = model.encoder(batch['input_ids_b'], attention_mask=batch['attention_mask_b'])\n",
    "            emb_a = model.mean_pooling(out_a, batch['attention_mask_a'])\n",
    "            emb_b = model.mean_pooling(out_b, batch['attention_mask_b'])\n",
    "            dists = torch.norm(emb_a - emb_b, dim=1).cpu().numpy()\n",
    "            labels = batch['label'].cpu().numpy()\n",
    "            all_dists.extend(dists)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    all_probs = -np.array(all_dists)\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "    best_thresh = thresholds[np.argmax(tpr - fpr)]\n",
    "    preds = (all_probs >= -16.797714).astype(int)\n",
    "\n",
    "    print(f\"Best threshold: {best_thresh:.4f}\")\n",
    "    print(f\"AUC-ROC: {roc_auc_score(all_labels, all_probs):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(all_labels, preds):.4f}\")\n",
    "    print(f\"Precision: {precision_score(all_labels, preds):.4f}\")\n",
    "    print(f\"Recall: {recall_score(all_labels, preds):.4f}\")\n",
    "    return best_thresh\n",
    "\n",
    "# --- Run All ---\n",
    "# df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "# df_pos = df[df.label == 1]\n",
    "# df_neg = df[df.label == 0]\n",
    "\n",
    "# # Splits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# df_pos_train, df_pos_val = train_test_split(df_pos, test_size=0.3, random_state=42)\n",
    "# df_neg_train, df_neg_val = train_test_split(df_neg, test_size=0.3, random_state=42)\n",
    "df_train = pd.read_csv(\"crossencoder_train.csv\")\n",
    "df_val = pd.read_csv(\"crossencoder_val.csv\")\n",
    "df_test = pd.read_csv(\"crossencoder_test.csv\")\n",
    "\n",
    "df_pos_train = df_train[df_train.label == 1]\n",
    "df_neg_train = df_train[df_train.label == 0]\n",
    "df_pos_val =df_val[df_val.label == 1]\n",
    "df_neg_val = df_val[df_val.label == 0]\n",
    "df_pos_test =df_test[df_test.label == 1]\n",
    "df_neg_test = df_test[df_test.label == 0]\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "# Triplet Train Set\n",
    "triplet_dataset = ContrastiveVerdictDataset(df_pos_train, tokenizer)\n",
    "triplet_loader = DataLoader(triplet_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Standard Val Set for Evaluation\n",
    "val_dataset = VerdictDataset(pd.concat([df_pos_val, df_neg_val]), tokenizer)\n",
    "test_dataset = VerdictDataset(pd.concat([df_pos_test, df_neg_test]), tokenizer)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader=DataLoader(test_dataset, batch_size=8)\n",
    "# Model & Optimizer\n",
    "model = SiameseHeBERT().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train & Evaluate\n",
    "train_triplet_with_early_stopping(model, triplet_loader, val_loader, optimizer, device, epochs=30, patience=5)\n",
    "# train_triplet(model, triplet_loader, optimizer, device)\n",
    "best_thresh=evaluate_triplet_embeddings(model, val_loader, device)\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/siamese_hebert_triplet.pt\")\n",
    "print(\"âœ… Triplet model saved.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Test Evaluation ---\n",
    "print(\" --- Test Evaluation ---\")\n",
    "evaluate_triplet_embeddings(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEncoderHeBERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/Legal-heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:01<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.6136, Accuracy: 0.6671\n",
      "Epoch 1 | Validation AUC: 0.7424\n",
      "âœ… New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:01<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.5048, Accuracy: 0.7562\n",
      "Epoch 2 | Validation AUC: 0.8000\n",
      "âœ… New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:02<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.4262, Accuracy: 0.8013\n",
      "Epoch 3 | Validation AUC: 0.8199\n",
      "âœ… New best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:02<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.3235, Accuracy: 0.8630\n",
      "Epoch 4 | Validation AUC: 0.8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:02<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.2653, Accuracy: 0.8875\n",
      "Epoch 5 | Validation AUC: 0.8127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:02<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.1969, Accuracy: 0.9218\n",
      "Epoch 6 | Validation AUC: 0.7992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:02<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.1581, Accuracy: 0.9314\n",
      "Epoch 7 | Validation AUC: 0.7911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 507/507 [01:02<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.1177, Accuracy: 0.9457\n",
      "Epoch 8 | Validation AUC: 0.7861\n",
      "â¹ï¸ Early stopping triggered.\n",
      "âœ… CrossEncoderHeBERT model saved.\n",
      "[Default @0.5] AUC-ROC: 0.7861\n",
      "[Default @0.5] F1 Score: 0.6139\n",
      "[Default @0.5] Precision: 0.6763\n",
      "[Default @0.5] Recall: 0.5621\n",
      "ğŸ” Best threshold: 0.0199\n",
      "[Best] F1 Score: 0.6440\n",
      "[Best] Precision: 0.5843\n",
      "[Best] Recall: 0.7172\n",
      "-------test-------\n",
      "[Default @0.5] AUC-ROC: 0.7995\n",
      "[Default @0.5] F1 Score: 0.6047\n",
      "[Default @0.5] Precision: 0.6434\n",
      "[Default @0.5] Recall: 0.5704\n",
      "[Test @0.0199] AUC-ROC: 0.7995\n",
      "[Test @0.0199] F1 Score: 0.6557\n",
      "[Test @0.0199] Precision: 0.5789\n",
      "[Test @0.0199] Recall: 0.7560\n"
     ]
    }
   ],
   "source": [
    "# --- Cross-Encoder Dataset ---\n",
    "class CrossEncoderVerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = f\"[CLS] {row['gpt_facts_a']} [SEP] {row['gpt_facts_b']} [SEP]\"\n",
    "        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# --- Cross-Encoder Model ---\n",
    "class CrossEncoderHeBERT(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]  # [CLS] token\n",
    "        return self.classifier(pooled).squeeze(-1)\n",
    "\n",
    "# --- Training Loop ---\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_cross_encoder_with_early_stopping(model, train_loader, val_loader, optimizer, device, epochs=10, patience=3):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_auc = 0\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = criterion(logits, batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label'].long()).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {total_loss / len(train_loader):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_probs, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                for key in batch:\n",
    "                    batch[key] = batch[key].to(device)\n",
    "                logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "                prob = torch.sigmoid(logits).cpu().numpy()\n",
    "                label = batch['label'].cpu().numpy()\n",
    "                val_probs.extend(prob)\n",
    "                val_labels.extend(label)\n",
    "        val_auc = roc_auc_score(val_labels, val_probs)\n",
    "        print(f\"Epoch {epoch+1} | Validation AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_crossencoder_ft_mlm.pt\")\n",
    "            print(\"âœ… New best model saved.\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(\"â¹ï¸ Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "def train_cross_encoder(model, dataloader, optimizer, device, epochs=15):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            loss = criterion(logits, batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label'].long()).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(dataloader):.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_threshold(y_true, y_probs):\n",
    "    best_thresh = 0.5\n",
    "    best_f1 = 0\n",
    "    for t in np.linspace(0.01, 0.99, 100):\n",
    "        preds = (np.array(y_probs) >= t).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = t\n",
    "    return best_thresh, best_f1\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_cross_encoder(model, dataloader, device):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "\n",
    "    preds = (np.array(probs) >= 0.5).astype(int)\n",
    "    print(f\"[Default @0.5] AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "    print(f\"[Default @0.5] F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Precision: {precision_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "    best_thresh, best_f1 = find_best_threshold(targets, probs)\n",
    "    best_preds = (np.array(probs) >= best_thresh).astype(int)\n",
    "    print(f\"ğŸ” Best threshold: {best_thresh:.4f}\")\n",
    "    print(f\"[Best] F1 Score: {f1_score(targets, best_preds):.4f}\")\n",
    "    print(f\"[Best] Precision: {precision_score(targets, best_preds):.4f}\")\n",
    "    print(f\"[Best] Recall: {recall_score(targets, best_preds):.4f}\")\n",
    "    \n",
    "    return probs, targets, best_thresh  \n",
    "\n",
    "def evaluate_with_threshold(model, dataloader, device, threshold):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "    print(\"-------test-------\")       \n",
    "    preds = (np.array(probs) >= 0.5).astype(int)\n",
    "    print(f\"[Default @0.5] AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "    print(f\"[Default @0.5] F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Precision: {precision_score(targets, preds):.4f}\")\n",
    "    print(f\"[Default @0.5] Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "    preds = (np.array(probs) >= threshold).astype(int)\n",
    "    print(f\"[Test @{threshold:.4f}] AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "    print(f\"[Test @{threshold:.4f}] F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "    print(f\"[Test @{threshold:.4f}] Precision: {precision_score(targets, preds):.4f}\")\n",
    "    print(f\"[Test @{threshold:.4f}] Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "# # --- Usage ---\n",
    "# model_name = \"/home/liorkob/M.Sc/thesis/pre-train/mlm/Legal-heBERT-mlm-3k-drugs/final\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/liorkob/M.Sc/thesis/pre-train/mlm/Legal-heBERT-mlm-3k-drugs/final\")\n",
    "\n",
    "# model_name = \"/home/liorkob/M.Sc/thesis/pre-train/hebert-mlm-3k-drugs-punishment\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/home/liorkob/M.Sc/thesis/pre-train/hebert-mlm-3k-drugs-punishment\")\n",
    "\n",
    "\n",
    "model_name =\"avichr/Legal-heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/Legal-heBERT\")\n",
    "# df_train, df_val = train_test_split(df, stratify=df.label, test_size=0.2, random_state=42)\n",
    "df_train = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_train.csv\")\n",
    "df_val = pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_val.csv\")\n",
    "df_test= pd.read_csv(\"/home/liorkob/M.Sc/thesis/citation-prediction/data_splits/crossencoder_test.csv\")\n",
    "\n",
    "\n",
    "train_dataset = CrossEncoderVerdictDataset(df_train, tokenizer)\n",
    "val_dataset = CrossEncoderVerdictDataset(df_val, tokenizer)\n",
    "test_dataset = CrossEncoderVerdictDataset(df_test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "model = CrossEncoderHeBERT(model_name).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "train_cross_encoder_with_early_stopping(model, train_loader, val_loader, optimizer, device, epochs=20, patience=5)\n",
    "\n",
    "print(\"âœ… CrossEncoderHeBERT model saved.\")\n",
    "# model.load_state_dict(torch.load(\"best_crossencoder.pt\"))  \n",
    "\n",
    "_, _, best_thresh = evaluate_cross_encoder(model, val_loader, device)\n",
    "evaluate_with_threshold(model, test_loader, device, best_thresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explain_crossencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# def explain_crossencoder_test_set(model_name, tokenizer, test_df, output_csv=\"cross_attention_explanations.csv\", max_len=512, top_k=10):\n",
    "#     print(\"ğŸ“¥ Loading model with attention output...\")\n",
    "#     model = AutoModel.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     explanations = []\n",
    "\n",
    "#     for i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "#         text_a = row['gpt_facts_a']\n",
    "#         text_b = row['gpt_facts_b']\n",
    "#         label = row['label']\n",
    "\n",
    "#         full_text = f\"[CLS] {text_a} [SEP] {text_b} [SEP]\"\n",
    "#         inputs = tokenizer(full_text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_len).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#             attentions = outputs.attentions\n",
    "\n",
    "#         # ×©×›×‘×ª attention ××—×¨×•× ×”, ×××•×¦×¢ ×¢×œ ×›×œ ×”×¨××©×™×\n",
    "#         attn = attentions[-1][0].mean(dim=0)  # shape: [seq_len, seq_len]\n",
    "#         attn_scores = attn.sum(dim=0)  # ×›××” ×›×œ ××™×œ×” ×§×™×‘×œ×” attention\n",
    "\n",
    "#         # top tokens\n",
    "#         top_ids = torch.topk(attn_scores, k=top_k).indices.tolist()\n",
    "#         tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "#         stop_tokens = {'[PAD]', '[CLS]', '[SEP]', ',', '.', '!', '?', '×”× ××©×', '×‘×™×ª', '××©×¤×˜'}\n",
    "\n",
    "#         top_words = [\n",
    "#     f\"{tokens[idx]}:{attn_scores[idx].item():.2f}\"\n",
    "#     for idx in top_ids\n",
    "#     if tokens[idx] not in stop_tokens and tokens[idx].strip() != ''\n",
    "# ]\n",
    "\n",
    "\n",
    "#         explanations.append({\n",
    "#             \"index\": i,\n",
    "#             \"label\": label,\n",
    "#             \"text_a\": text_a,\n",
    "#             \"text_b\": text_b,\n",
    "#             \"top_attention_words\": \", \".join(top_words)\n",
    "#         })\n",
    "\n",
    "#     # Save to CSV\n",
    "#     with open(output_csv, mode='w', encoding='utf-8-sig', newline='') as f:\n",
    "#         writer = csv.DictWriter(f, fieldnames=[\"index\", \"label\", \"text_a\", \"text_b\", \"top_attention_words\"])\n",
    "#         writer.writeheader()\n",
    "#         writer.writerows(explanations)\n",
    "\n",
    "#     print(f\"âœ… Explanations saved to {output_csv}\")\n",
    "\n",
    "# df_test= pd.read_csv(\"crossencoder_test.csv\"),\n",
    "# explain_crossencoder_test_set(\n",
    "#     model_name=\"/home/liorkob/M.Sc/thesis/similarity-model/hebert-mlm-verdicts/final\",\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\"),\n",
    "#     df_test= df_test,\n",
    "#     output_csv=\"crossencoder_explanations_test.csv\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from captum) (3.9.4)\n",
      "Requirement already satisfied: numpy<2.0 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from captum) (1.26.3)\n",
      "Requirement already satisfied: packaging in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from captum) (24.2)\n",
      "Requirement already satisfied: torch>=1.10 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from captum) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from captum) (4.67.1)\n",
      "Requirement already satisfied: filelock in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (4.12.2)\n",
      "Requirement already satisfied: networkx in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from torch>=1.10->captum) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->captum) (12.8.93)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from matplotlib->captum) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from matplotlib->captum) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from matplotlib->captum) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from matplotlib->captum) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from matplotlib->captum) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from matplotlib->captum) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /sise/home/liorkob/.local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->captum) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /sise/home/liorkob/.conda/envs/new_env/lib/python3.9/site-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n",
      "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install captum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_524687/912036053.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
      "Some weights of BertModel were not initialized from the model checkpoint at avichr/heBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Token importances for Base HeBERT:\n",
      "×”×•×¨×©×¢           -> -0.1466\n",
      "×¢×œ              -> 0.0562\n",
      "×¤×™              -> -0.0754\n",
      "×”×•×“             -> -0.0682\n",
      "×‘×¢×‘×™×¨×”          -> -0.1348\n",
      "×©×¢× ×™            -> 0.1274\n",
      ",               -> -0.0552\n",
      ")               -> -0.0706\n",
      "7               -> -0.1301\n",
      "×¡×™×¤             -> -0.0526\n",
      ",               -> -0.1001\n",
      ".               -> -0.0586\n",
      ",               -> -0.0688\n",
      "##×™××•×©          -> 0.0669\n",
      ".               -> -0.0549\n",
      ".               -> -0.0773\n",
      ".               -> -0.0594\n",
      "08              -> -0.0612\n",
      ".               -> -0.1421\n",
      ".               -> -0.1316\n",
      "×›×ª×‘             -> 0.1299\n",
      "×”××‘×—×Ÿ           -> -0.0748\n",
      "×‘×¡×•×¤×•           -> -0.0792\n",
      "×”×©× ×™×™×”          -> -0.1448\n",
      "×¢×‘×•×“×•×ª          -> -0.0638\n",
      "×”×™×™×¦            -> 0.0590\n",
      "×”×ª× ×”×’×•×ª         -> -0.0539\n",
      "×”× ××©×           -> -0.0897\n",
      "##×•×—            -> -0.0586\n",
      "×”×•×¨×©×¢           -> -0.0716\n",
      "×‘×¢×‘×™×¨×•×ª         -> -0.1355\n",
      "×¡×™×¤             -> -0.0540\n",
      "× ×•×¡×—            -> 0.1637\n",
      "×¡×›×™×Ÿ            -> -0.1418\n",
      "1977            -> -0.0557\n",
      "×”× ×˜×¢×Ÿ           -> -0.1293\n",
      "09              -> -0.0616\n",
      "×¡×§              -> -0.0524\n",
      "××•×§             -> -0.0738\n",
      "×”×‘×¨×™×›×”          -> -0.0682\n",
      "×‘××•×©×‘           -> -0.1096\n",
      "×”×—×–×™×§           -> -0.1551\n",
      ".               -> -0.2121\n",
      "× ×˜×¢×Ÿ            -> -0.0784\n",
      "×¡×›×™×Ÿ            -> -0.1130\n",
      "×‘×¦×‘×¢            -> -0.0737\n",
      "##ber           -> -0.2806\n",
      "×œ×“              -> 0.0589\n",
      "##×§×•×¨           -> -0.0857\n",
      "×œ×—×ª×•×š           -> -0.0581\n",
      "×‘×™×ª×•            -> -0.1489\n",
      "×”×•×›×™×—           -> -0.3326\n",
      "× ×˜×¢×Ÿ            -> -0.1379\n",
      "×¤×œ×¤×œ            -> -0.0506\n",
      ".               -> -0.1204\n",
      "\n",
      "ğŸ” Token importances for MLM HeBERT:\n",
      "×”×•×¨×©×¢           -> -0.0741\n",
      "×”×•×“             -> -0.0874\n",
      "×‘×¢×‘×™×¨×”          -> -0.0630\n",
      "×”×—×–×§×ª           -> 0.0521\n",
      ",               -> 0.0530\n",
      "×’               -> 0.0808\n",
      "##×•×›× ×™×         -> -0.2099\n",
      ")               -> 0.0735\n",
      ",               -> 0.0523\n",
      "×œ×”×œ×Ÿ            -> 0.1333\n",
      "×”××ª×•×§           -> 0.0721\n",
      "##×™××•×©          -> 0.0626\n",
      "×—×©              -> -0.0559\n",
      "##××™×Ÿ           -> -0.0526\n",
      "×”×•×“             -> -0.0593\n",
      "×œ×©×™×¨×•×ª          -> 0.1694\n",
      "×”××•×ª            -> 0.2414\n",
      "##×¢×Ÿ            -> 0.0628\n",
      "×”×ª×™×™×¦×‘          -> 0.1094\n",
      "×”×•×—×œ×£           -> 0.0573\n",
      "##×•×’            -> 0.0635\n",
      "×”×¤× ×™×”           -> 0.0767\n",
      "×—×•×–×¨×ª           -> 0.0752\n",
      "×”×ª× ×”×’×•×ª         -> -0.0527\n",
      "×—×™×•×‘×™×ª          -> 0.0975\n",
      "##×•×—            -> -0.2138\n",
      "×‘×¢×‘×™×¨×•×ª         -> -0.0627\n",
      "09              -> 0.0553\n",
      ":               -> 0.0664\n",
      "##×•×“×™×”          -> -0.0622\n",
      "×”×‘×¨×™×›×”          -> -0.2527\n",
      "×¢×¨×•×’×•×ª          -> -0.1499\n",
      "×”×—×–×™×§           -> 0.0630\n",
      "× ×˜×¢×Ÿ            -> -0.1306\n",
      "×‘××›             -> 0.0730\n",
      "×¡×›×™×Ÿ            -> 0.5683\n",
      "×›×™×ª             -> 0.0733\n",
      "×œ×—×ª×•×š           -> -0.1274\n",
      "×‘×™×ª×•            -> 0.0733\n",
      "×”×•×›×™×—           -> -0.3035\n",
      "##×”             -> -0.0617\n",
      "×›×©×¨×”            -> -0.0579\n",
      "× ×˜×¢×Ÿ            -> -0.1518\n",
      "××ª×§             -> 0.0558\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from captum.attr import IntegratedGradients\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class CrossEncoderHeBERT(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return self.classifier(pooled).squeeze(-1)\n",
    "\n",
    "# base model + tokenizer path\n",
    "model_base_path = \"avichr/heBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_base_path)\n",
    "\n",
    "# map names to checkpoint paths\n",
    "model_ckpts = {\n",
    "    \"Base HeBERT\": \"/home/liorkob/M.Sc/thesis/similarity-model/best_crossencoder.pt\",\n",
    "    \"MLM HeBERT\": \"/home/liorkob/M.Sc/thesis/similarity-model/best_crossencoder_ft_mlm.pt\"\n",
    "}\n",
    "\n",
    "# Load models with their checkpoints\n",
    "models = {}\n",
    "for name, ckpt_path in model_ckpts.items():\n",
    "    model = CrossEncoderHeBERT(model_base_path).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "    models[name] = model\n",
    "\n",
    "# Load example\n",
    "df = pd.read_csv(\"crossencoder_test.csv\")\n",
    "example = df.iloc[0]\n",
    "text_a = example[\"gpt_facts_a\"]\n",
    "text_b = example[\"gpt_facts_b\"]\n",
    "full_text = f\"[CLS] {text_a} [SEP] {text_b} [SEP]\"\n",
    "\n",
    "# Explain\n",
    "def explain_with_ig(model, tokenizer, text):\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    \n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "    # ×—×™×œ×•×¥ ×”×××‘×¨×“×™× ×’×™× ×œ×¤× ×™ Captum\n",
    "    inputs_embeds = model.encoder.embeddings(input_ids)\n",
    "\n",
    "    def forward_func(inputs_embeds, attention_mask):\n",
    "        outputs = model.encoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return torch.sigmoid(model.classifier(pooled).squeeze(-1))\n",
    "\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    attributions, _ = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        n_steps=10,\n",
    "        internal_batch_size=2,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    scores = attributions.sum(dim=2).squeeze(0)\n",
    "    scores = scores / torch.norm(scores)\n",
    "    return list(zip(tokens, scores.detach().cpu().numpy()))\n",
    "\n",
    "# Compare models\n",
    "for name in [\"Base HeBERT\", \"MLM HeBERT\"]:\n",
    "    print(f\"\\nğŸ” Token importances for {name}:\")\n",
    "    token_attribs = explain_with_ig(models[name], tokenizer, full_text)\n",
    "    for token, score in token_attribs:\n",
    "        if abs(score) > 0.05 and token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            print(f\"{token:15} -> {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Comparing verdicts: ×ª×¤_66547-03-23 â‡„ ×ª×¤_65068-11-22 | Label: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# --- ×”×’×“×¨×•×ª ×‘×¡×™×¡ ---\n",
    "device = \"cpu\"  # ×›×“×™ ×œ×”×™×× ×¢ ×-CUDA OOM\n",
    "\n",
    "# ×˜×¢×Ÿ CSV\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "\n",
    "# ×˜×¢×Ÿ ×˜×•×§× ×™×™×–×¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "\n",
    "# ×¤×•× ×§×¦×™×™×ª ×”×¡×‘×¨ ×¢× Captum\n",
    "def explain_with_ig(model, tokenizer, text, device=\"cpu\"):\n",
    "    model = model.to(device)  # ×•×“××™ ×©×”××•×“×œ ×¢×œ ××•×ª×• device\n",
    "\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "    inputs_embeds = model.encoder.embeddings(input_ids)\n",
    "\n",
    "    def forward_func(inputs_embeds, attention_mask):\n",
    "        outputs = model.encoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state[:, 0]\n",
    "        return torch.sigmoid(model.classifier(pooled).squeeze(-1))\n",
    "\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    attributions, _ = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        n_steps=10,\n",
    "        internal_batch_size=2,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    scores = attributions.sum(dim=2).squeeze(0)\n",
    "    scores = scores / torch.norm(scores)\n",
    "    return list(zip(tokens, scores.detach().cpu().numpy()))\n",
    "\n",
    "# ×”×©×•×•××” ×’×¨×¤×™×ª\n",
    "def compare_token_importances(base_scores, mlm_scores, top_k=20):\n",
    "    base_scores = [(t, s) for t, s in base_scores if t not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]]\n",
    "    mlm_scores = [(t, s) for t, s in mlm_scores if t not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]]\n",
    "\n",
    "    tokens = [t for t, _ in base_scores]\n",
    "    base_dict = dict(base_scores)\n",
    "    mlm_dict = dict(mlm_scores)\n",
    "    shared = [(t, base_dict[t], mlm_dict[t]) for t in tokens if t in mlm_dict]\n",
    "\n",
    "    shared = sorted(shared, key=lambda x: abs(x[1] - x[2]), reverse=True)[:top_k]\n",
    "\n",
    "    tokens = [x[0] for x in shared]\n",
    "    base_vals = [x[1] for x in shared]\n",
    "    mlm_vals = [x[2] for x in shared]\n",
    "\n",
    "    x = np.arange(len(tokens))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.barh(x - width/2, base_vals, width, label='Base HeBERT')\n",
    "    plt.barh(x + width/2, mlm_vals, width, label='MLM HeBERT')\n",
    "    plt.yticks(x, tokens)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"Attribution Score\")\n",
    "    plt.title(\"Token Importance Comparison (Base vs MLM)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ×¤×•× ×§×¦×™×” ×©××¨×™×¦×” ×”×›×œ ×¢×œ ×©×•×¨×ª ×–×•×’ ×ª×™×§×™×\n",
    "def explain_case_pair(df_row, models, tokenizer, device=\"cpu\", top_k=20):\n",
    "    text_a = df_row[\"gpt_facts_a\"]\n",
    "    text_b = df_row[\"gpt_facts_b\"]\n",
    "    label = df_row[\"label\"]\n",
    "    verdict_a = df_row[\"verdict_a\"]\n",
    "    verdict_b = df_row[\"verdict_b\"]\n",
    "    full_text = f\"[CLS] {text_a} [SEP] {text_b} [SEP]\"\n",
    "\n",
    "    print(f\"\\nğŸ” Comparing verdicts: {verdict_a} â‡„ {verdict_b} | Label: {label}\")\n",
    "\n",
    "    base_expl = explain_with_ig(models[\"Base HeBERT\"], tokenizer, full_text, device)\n",
    "    mlm_expl = explain_with_ig(models[\"MLM HeBERT\"], tokenizer, full_text, device)\n",
    "\n",
    "    compare_token_importances(base_expl, mlm_expl, top_k=top_k)\n",
    "explain_case_pair(df.iloc[0], models, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # ====== Evaluate on Drugs Dataset ======\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Evaluate on Drugs Dataset ======\n",
    "drugs_df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/data/drugs/gt_similarity_with_facts.csv\")\n",
    "# drugs_dataset = VerdictDataset(drugs_df, tokenizer)\n",
    "drugs_dataset = CrossEncoderVerdictDataset(drugs_df, tokenizer)\n",
    "drugs_loader = DataLoader(drugs_dataset, batch_size=8)\n",
    "\n",
    "# Load trained model\n",
    "# model = SiameseHeBERT().to(device)\n",
    "# model.load_state_dict(torch.load(\"/home/liorkob/M.Sc/thesis/similarity-model/best_triplet_auc.pt\"))\n",
    "\n",
    "model = CrossEncoderHeBERT(model_name).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/liorkob/M.Sc/thesis/similarity-model/best_crossencoder.pt\"))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def collect_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            out_a = model.encoder(batch['input_ids_a'], attention_mask=batch['attention_mask_a'])\n",
    "            out_b = model.encoder(batch['input_ids_b'], attention_mask=batch['attention_mask_b'])\n",
    "\n",
    "            emb_a = model.mean_pooling(out_a, batch['attention_mask_a'])\n",
    "            emb_b = model.mean_pooling(out_b, batch['attention_mask_b'])\n",
    "\n",
    "            # ×›×›×œ ×©×§×¨×•×‘ ×™×•×ª×¨ â†’ ×¡×‘×™×¨ ×™×•×ª×¨ ×œ×“××™×•×Ÿ â‡’ × ×©×ª××© ×‘Ö¾××™× ×•×¡ ××¨×—×§ ×›Ö¾\"×¡×™×›×•×™\"\n",
    "            dists = torch.norm(emb_a - emb_b, dim=1)\n",
    "            probs = -dists  # × × ×¨××œ ×× × ×¨×¦×” later\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "    return np.array(all_probs), np.array(all_labels)\n",
    "\n",
    "\n",
    "Evaluate\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "probs, labels = collect_predictions(model, drugs_loader, device)\n",
    "fpr, tpr, thresholds = roc_curve(labels, probs)\n",
    "# × ×—×©×‘ ××ª ×”-threshold ×©×××§×¡× TPRâ€“FPR\n",
    "best_idx = (tpr - fpr).argmax()\n",
    "best_thresh = thresholds[best_idx]\n",
    "print(\"Best threshold:\", best_thresh)\n",
    "preds = (probs >= -20).astype(int)\n",
    "print(\"\\nğŸ“Š Evaluation on Drugs Dataset\")\n",
    "print(f\"âœ… Accuracy: {(preds == targets).mean():.4f}\")\n",
    "print(f\"âœ… AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "print(f\"âœ… Precision: {precision_score(targets, preds):.4f}\")\n",
    "print(f\"âœ… Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "# # Get predictions on drugs dataset\n",
    "# probs, targets = [], []\n",
    "# with torch.no_grad():\n",
    "#     for batch in drugs_loader:\n",
    "#         for key in batch:\n",
    "#             batch[key] = batch[key].to(device)\n",
    "#         logits = model(batch['input_ids_a'], batch['attention_mask_a'],\n",
    "#                batch['input_ids_b'], batch['attention_mask_b'])\n",
    "    \n",
    "#         # logits = model(batch['input_ids'], batch['attention_mask'])\n",
    "#         prob = torch.sigmoid(logits).cpu().numpy()\n",
    "#         label = batch['label'].cpu().numpy()\n",
    "#         probs.extend(prob)\n",
    "#         targets.extend(label)\n",
    "\n",
    "# # Use best threshold from previous validation\n",
    "# preds = (np.array(probs) >= 0.0694).astype(int)\n",
    "\n",
    "# print(\"\\nğŸ“Š Evaluation on Drugs Dataset\")\n",
    "# print(f\"âœ… Accuracy: {(preds == targets).mean():.4f}\")\n",
    "# print(f\"âœ… AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "# print(f\"âœ… F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "# print(f\"âœ… Precision: {precision_score(targets, preds):.4f}\")\n",
    "# print(f\"âœ… Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_attention_batch(val_dataset, tokenizer, model_name=\"avichr/heBERT\", window_size=20, max_pairs=50):\n",
    "    from transformers import AutoModel\n",
    "    import torch\n",
    "\n",
    "    print(\"ğŸ“¥ Loading BERT with attention output...\")\n",
    "    attention_model = AutoModel.from_pretrained(model_name, output_attentions=True).to(device)\n",
    "    attention_model.eval()\n",
    "\n",
    "    for i in range(min(len(val_dataset), max_pairs)):\n",
    "        sample = val_dataset[i]\n",
    "        text_a = tokenizer.decode(sample['input_ids_a'], skip_special_tokens=True)\n",
    "        text_b = tokenizer.decode(sample['input_ids_b'], skip_special_tokens=True)\n",
    "\n",
    "        # Tokenize pair\n",
    "        inputs = tokenizer(text_a, text_b, return_tensors='pt', truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = attention_model(**inputs)\n",
    "            attentions = outputs.attentions  # [layer][batch, heads, tokens, tokens]\n",
    "\n",
    "        # Use last layer, mean over heads\n",
    "        last_attn = attentions[-1][0]  # [heads, tokens, tokens]\n",
    "        mean_attn = last_attn.mean(dim=0)  # [tokens, tokens]\n",
    "\n",
    "        # Split query/doc by SEP token\n",
    "        sep_idx = (inputs['input_ids'][0] == tokenizer.sep_token_id).nonzero()[0].item()\n",
    "        attn_q_to_d = mean_attn[:sep_idx, sep_idx:]  # [q_len, d_len]\n",
    "        scores = attn_q_to_d.sum(dim=0)  # [d_len]\n",
    "\n",
    "        # Sliding window\n",
    "        max_score, max_start = -1, 0\n",
    "        for j in range(len(scores) - window_size + 1):\n",
    "            score = scores[j:j+window_size].sum().item()\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_start = j\n",
    "\n",
    "        doc_input_ids = inputs['input_ids'][0][sep_idx+1:]\n",
    "        selected_ids = doc_input_ids[max_start:max_start+window_size]\n",
    "        explanation = tokenizer.decode(selected_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\nğŸ” Pair #{i+1}:\")\n",
    "        print(\"ğŸ“„ GPT Facts A:\", text_a.replace(\"\\n\", \" \"), \"...\")\n",
    "        print(\"ğŸ“„ GPT Facts B:\", text_b.replace(\"\\n\", \" \"), \"...\")\n",
    "        print(\"ğŸ’¡ Most matching text in B:\", explanation)\n",
    "\n",
    "# ğŸ” Explain attention for sample validation pairs\n",
    "explain_attention_batch(val_dataset, tokenizer, model_name=model_name, window_size=20, max_pairs=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #2 model - cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GroupKFold\n",
    "# import pickle\n",
    "# df = pd.read_csv(\"data_pairs_5k.csv\")\n",
    "\n",
    "# # Define grouping key â€” each verdict appears in A or B\n",
    "# df['group'] = df['verdict_a'] + \" || \" + df['verdict_b']  # You can also just use verdict_a\n",
    "\n",
    "# # Option 1: Group by verdict_a only (more conservative)\n",
    "# groups = df['verdict_a']\n",
    "\n",
    "# gkf = GroupKFold(n_splits=2)\n",
    "# splits = list(gkf.split(df, df['label'], groups=groups))\n",
    "\n",
    "# # Save splits to use consistently\n",
    "# with open(\"fold_indices_clean.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(splits, f)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the deduplicated data\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "\n",
    "# Group by verdict_a (conservative approach)\n",
    "groups = df['verdict_a']\n",
    "print(f\"ğŸ“‚ Total unique verdict_a groups: {groups.nunique()}\")\n",
    "\n",
    "# Create GroupKFold splits\n",
    "gkf = GroupKFold(n_splits=2)\n",
    "splits = list(gkf.split(df, df['label'], groups=groups))\n",
    "\n",
    "# Save splits for reuse\n",
    "with open(\"fold_indices_clean.pkl\", \"wb\") as f:\n",
    "    pickle.dump(splits, f)\n",
    "\n",
    "print(f\"âœ… Saved {len(splits)} GroupKFold splits to fold_indices_clean.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name='/home/liorkob/M.Sc/thesis/similarity-model/hebert-mlm-verdicts/final'\n",
    "# model_name=\"avichr/heBERT\"\n",
    "# ----- Model -----\n",
    "class SiameseHeBERT(nn.Module):\n",
    "    def __init__(self, model_name=model_name):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden = self.encoder.config.hidden_size\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Binary output (logit)\n",
    "        )\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())\n",
    "        return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)\n",
    "\n",
    "    def forward(self, ids_a, mask_a, ids_b, mask_b):\n",
    "        out_a = self.encoder(ids_a, attention_mask=mask_a)\n",
    "        out_b = self.encoder(ids_b, attention_mask=mask_b)\n",
    "        vec_a = self.mean_pooling(out_a, mask_a)\n",
    "        vec_b = self.mean_pooling(out_b, mask_b)\n",
    "        combined = torch.cat([vec_a, vec_b], dim=1)\n",
    "        return self.classifier(combined).squeeze(-1)\n",
    "\n",
    "# ----- Dataset -----\n",
    "class VerdictDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.df = df.copy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc_a = self.tokenizer(row['gpt_facts_a'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        enc_b = self.tokenizer(row['gpt_facts_b'], truncation=True, padding='max_length',\n",
    "                               max_length=self.max_len, return_tensors='pt')\n",
    "        return {\n",
    "            'input_ids_a': enc_a['input_ids'].squeeze(),\n",
    "            'attention_mask_a': enc_a['attention_mask'].squeeze(),\n",
    "            'input_ids_b': enc_b['input_ids'].squeeze(),\n",
    "            'attention_mask_b': enc_b['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(row['label'], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# ----- Train Function -----\n",
    "def train(model, dataloader, optimizer, device,pos_weight_tensor,epochs=15):\n",
    "    model.train()\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids_a'], batch['attention_mask_a'],\n",
    "                          batch['input_ids_b'], batch['attention_mask_b'])\n",
    "            loss = loss_fn(logits, batch['label'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label']).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "        print(f\"Loss: {total_loss / len(dataloader):.4f}, Accuracy: {correct / total:.4f}\")\n",
    "\n",
    "# ----- Evaluation Function -----\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            logits = model(batch['input_ids_a'], batch['attention_mask_a'],\n",
    "                          batch['input_ids_b'], batch['attention_mask_b'])\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == batch['label']).sum().item()\n",
    "            total += batch['label'].size(0)\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# ----- Main K-Fold Script -----\n",
    "df = pd.read_csv(\"/home/liorkob/M.Sc/thesis/similarity-model/data_pairs_5k.csv\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"avichr/heBERT\")\n",
    "dataset = VerdictDataset(df, tokenizer)\n",
    "print(\"Similarity scores:\", df['label'].unique())\n",
    "print(\"Label distribution:\", dataset.df['label'].value_counts())\n",
    "\n",
    "# ----- Compute pos_weight -----\n",
    "labels = dataset.df['label']\n",
    "num_pos = (labels == 1).sum()\n",
    "num_neg = (labels == 0).sum()\n",
    "pos_weight_value = num_neg / num_pos\n",
    "pos_weight_tensor = torch.tensor(pos_weight_value, dtype=torch.float).to(device)\n",
    "\n",
    "fold_accuracies = []\n",
    "\n",
    "\n",
    "with open(\"/home/liorkob/M.Sc/thesis/similarity-model/fold_indices_clean.pkl\", \"rb\") as f:\n",
    "    splits = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(splits):\n",
    "    print(f\"\\n--- Fold {fold + 1} ---\")\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    test_subset = Subset(dataset, test_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=8)\n",
    "\n",
    "    model = SiameseHeBERT().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "    train(model, train_loader, optimizer, device,pos_weight_tensor)\n",
    "    acc = evaluate(model, test_loader, device)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    fold_accuracies.append(acc)\n",
    "\n",
    "print(f\"\\nAverage k-Fold Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
    "\n",
    "# Save the final model from the last fold\n",
    "torch.save(model.state_dict(), \"/home/liorkob/M.Sc/thesis/similarity-model/siamese_hebert.pt\")\n",
    "print(\"âœ… Model saved to siamese_hebert.pt\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "acc = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "fold_accuracies.append(acc)\n",
    "\n",
    "\n",
    "def collect_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    probs, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "            logits = model(batch['input_ids_a'], batch['attention_mask_a'],\n",
    "                           batch['input_ids_b'], batch['attention_mask_b'])\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "            label = batch['label'].cpu().numpy()\n",
    "            probs.extend(prob)\n",
    "            targets.extend(label)\n",
    "    return np.array(probs), np.array(targets)\n",
    "\n",
    "probs, targets = collect_predictions(model, test_loader, device)\n",
    "print(f\"AUC-ROC: {roc_auc_score(targets, probs):.4f}\")\n",
    "preds = (probs >= 0.5).astype(int)  # Add this line\n",
    "\n",
    "print(f\"F1 Score: {f1_score(targets, preds):.4f}\")\n",
    "print(f\"Precision: {precision_score(targets, preds):.4f}\")\n",
    "print(f\"Recall: {recall_score(targets, preds):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# × ×‘×—×¨ batch ×œ×‘×“×™×§×”\n",
    "batch = next(iter(test_loader))\n",
    "for key in batch:\n",
    "    batch[key] = batch[key].to(device)\n",
    "\n",
    "# ×™×¦×™×¨×ª embedding ×©×œ input_ids_a\n",
    "with torch.no_grad():\n",
    "    embedded_inputs = model.encoder.embeddings(batch['input_ids_a'])  # shape: [batch, seq_len, hidden]\n",
    "\n",
    "class EmbeddingWrapper(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.encoder = base_model.encoder  # HeBERT\n",
    "        self.pool = base_model.mean_pooling\n",
    "        self.classifier = base_model.classifier\n",
    "\n",
    "    def forward(self, embedded_input):\n",
    "        # ×¦×¨×™ attention mask ×©××ª××™× ×œÖ¾embedding\n",
    "        attention_mask = (embedded_input.abs().sum(-1) > 0).bool().to(embedded_input.device)\n",
    "\n",
    "        # ×”×¢×‘×¨ ××ª ×”Ö¾embedding ×œÖ¾AutoModel ×‘××§×•× input_ids\n",
    "        output = self.encoder(inputs_embeds=embedded_input, attention_mask=attention_mask)\n",
    "        pooled = self.pool(output, attention_mask)\n",
    "\n",
    "        # ×›××• ×§×•×“× - ×¦×“ B ××¤×¡\n",
    "        combined = torch.cat([pooled, torch.zeros_like(pooled)], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ×¢×˜×™×¤×ª ×”××•×“×œ\n",
    "wrapped_model = EmbeddingWrapper(model).to(device)\n",
    "\n",
    "# ×”×¤×¢×œ×ª SHAP\n",
    "explainer = shap.DeepExplainer(wrapped_model, embedded_inputs)\n",
    "shap_values = explainer.shap_values(embedded_inputs, check_additivity=False)\n",
    "print(np.array(shap_values).shape)\n",
    "\n",
    "\n",
    "all_explanations = []\n",
    "\n",
    "# × × ×™×— ×©×™×© ×œ×š batch ×‘×’×•×“×œ 8 ×›××• ×‘×“×•×’××” ×©×œ×š\n",
    "for i in range(len(batch['input_ids_a'])):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_a'][i].cpu())\n",
    "    attention_mask = batch['attention_mask_a'][i].cpu().bool().numpy()\n",
    "\n",
    "    shap_tensor_i = shap_values[i][:, :, 0]  # ×¦×•×¨×”: [seq_len, embedding_dim]\n",
    "    shap_tensor_valid = shap_tensor_i[attention_mask, :]\n",
    "    token_level_values = shap_tensor_valid.mean(axis=1)\n",
    "\n",
    "    valid_tokens = np.array(tokens)[attention_mask]\n",
    "\n",
    "    all_explanations.append((valid_tokens, token_level_values))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ×”× ×—×ª ×¦×•×¨×ª SHAP values: [batch, seq_len, embedding_dim, 1]\n",
    "shap_tensor = shap_values[0][:, :, 0]  # ×¦×•×¨×”: [128, 768]\n",
    "print(\"Fixed SHAP shape:\", shap_tensor.shape)\n",
    "\n",
    "# × ×™×§×•×™ padding\n",
    "attention_mask = batch['attention_mask_a'][0]\n",
    "valid_indices = attention_mask.bool().cpu().numpy()\n",
    "\n",
    "# ×¡×™× ×•×Ÿ ×˜×•×§× ×™× ×××™×ª×™×™×\n",
    "shap_tensor_valid = shap_tensor[valid_indices, :]  # [valid_seq_len, 768]\n",
    "token_level_values = shap_tensor_valid.mean(axis=1)  # ×¦×™×•×Ÿ ××—×“ ×œ×›×œ ×˜×•×§×Ÿ\n",
    "\n",
    "# ×˜×•×§× ×™×\n",
    "tokens = tokenizer.convert_ids_to_tokens(batch['input_ids_a'][0].cpu())\n",
    "valid_tokens = np.array(tokens)[valid_indices]\n",
    "\n",
    "# ×™×¦×™×¨×ª ×”×¡×‘×¨ ×•×”×¦×’×”\n",
    "explanation = shap.Explanation(values=token_level_values, data=valid_tokens, base_values=0)\n",
    "shap.plots.text(explanation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "token_scores = defaultdict(list)\n",
    "\n",
    "# × × ×™×— ×©×™×© ×œ×š ×¨×©×™××” ×©×œ ×–×•×’×•×ª: (valid_tokens, token_level_values)\n",
    "for tokens, shap_vals in all_explanations:  # ×›×œ ×“×•×’××”\n",
    "    for token, val in zip(tokens, shap_vals):\n",
    "        token_scores[token].append(val)\n",
    "\n",
    "# ×—×©×‘ ×××•×¦×¢ ×•×”×©×¤×¢×” ×›×•×œ×œ×ª\n",
    "token_stats = {\n",
    "    token: {\n",
    "        'mean': np.mean(vals),\n",
    "        'count': len(vals),\n",
    "        'abs_mean': np.mean(np.abs(vals)),\n",
    "    }\n",
    "    for token, vals in token_scores.items()\n",
    "}\n",
    "\n",
    "# ××™×•×Ÿ ×œ×¤×™ ×”×©×¤×¢×” ×××•×¦×¢×ª ×—×™×•×‘×™×ª ××• ×©×œ×™×œ×™×ª\n",
    "most_positive = sorted(token_stats.items(), key=lambda x: x[1]['mean'], reverse=True)[:10]\n",
    "most_negative = sorted(token_stats.items(), key=lambda x: x[1]['mean'])[:10]\n",
    "most_influential = sorted(token_stats.items(), key=lambda x: x[1]['abs_mean'], reverse=True)[:10]\n",
    "print(\"ğŸ”´ Top Positive Tokens:\")\n",
    "for tok, stats in most_positive:\n",
    "    print(f\"{tok}: mean={stats['mean']:.4f}, count={stats['count']}\")\n",
    "\n",
    "print(\"\\nğŸ”µ Top Negative Tokens:\")\n",
    "for tok, stats in most_negative:\n",
    "    print(f\"{tok}: mean={stats['mean']:.4f}, count={stats['count']}\")\n",
    "\n",
    "print(\"\\nğŸŸ¡ Top Influential Tokens (abs):\")\n",
    "for tok, stats in most_influential:\n",
    "    print(f\"{tok}: abs_mean={stats['abs_mean']:.4f}, count={stats['count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tokens_to_words(token_stats_list):\n",
    "    \"\"\"\n",
    "    ×××™×¨ ×¨×©×™××ª ×˜×•×§× ×™× ×¢× ×”×©×¤×¢×•×ª ×œ××™×œ×™× ××§×•×¨×™×•×ª ×××•×—×“×•×ª\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    current_tokens = []\n",
    "    current_scores = []\n",
    "\n",
    "    for token, stats in token_stats_list:\n",
    "        score = stats[\"mean\"] if \"mean\" in stats else stats[\"abs_mean\"]\n",
    "        \n",
    "        # ×”×ª×—×œ×” ×©×œ ××™×œ×” ×—×“×©×”\n",
    "        if not token.startswith(\"##\"):\n",
    "            if current_tokens:\n",
    "                # ×©××™×¨×” ×©×œ ×”××™×œ×” ×©×”×¡×ª×™×™××”\n",
    "                word = tokenizer.convert_tokens_to_string(current_tokens)\n",
    "                avg_score = np.mean(current_scores)\n",
    "                merged.append((word, avg_score, len(current_scores)))\n",
    "            # ×”×ª×—×œ×” ×©×œ ××™×œ×” ×—×“×©×”\n",
    "            current_tokens = [token]\n",
    "            current_scores = [score]\n",
    "        else:\n",
    "            # ×××©×™×›×™× ××ª ××•×ª×” ××™×œ×”\n",
    "            current_tokens.append(token)\n",
    "            current_scores.append(score)\n",
    "\n",
    "    # ×”×•×¡×¤×ª ×”××™×œ×” ×”××—×¨×•× ×”\n",
    "    if current_tokens:\n",
    "        word = tokenizer.convert_tokens_to_string(current_tokens)\n",
    "        avg_score = np.mean(current_scores)\n",
    "        merged.append((word, avg_score, len(current_scores)))\n",
    "\n",
    "    # ××™×•×Ÿ ××—×“×©\n",
    "    merged_sorted = sorted(merged, key=lambda x: x[1], reverse=True)\n",
    "    return merged_sorted[:10]  # Top 10\n",
    "\n",
    "# ×”××¨×” ×©×œ ×›×œ ××—×ª ××”×§×˜×’×•×¨×™×•×ª\n",
    "top_words_positive = merge_tokens_to_words(most_positive)\n",
    "top_words_negative = merge_tokens_to_words(most_negative)\n",
    "top_words_influential = merge_tokens_to_words(most_influential)\n",
    "\n",
    "# ×”×“×¤×¡×”\n",
    "print(\"\\nğŸ”´ Top Positive Words:\")\n",
    "for word, score, count in top_words_positive:\n",
    "    print(f\"{word}: mean={score:.4f}, parts={count}\")\n",
    "\n",
    "print(\"\\nğŸ”µ Top Negative Words:\")\n",
    "for word, score, count in top_words_negative:\n",
    "    print(f\"{word}: mean={score:.4f}, parts={count}\")\n",
    "\n",
    "print(\"\\nğŸŸ¡ Top Influential Words (abs):\")\n",
    "for word, score, count in top_words_influential:\n",
    "    print(f\"{word}: abs_mean={score:.4f}, parts={count}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "judgeEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
